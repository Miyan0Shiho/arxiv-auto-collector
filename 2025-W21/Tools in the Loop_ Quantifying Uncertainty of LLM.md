# Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools

**Authors**: Panagiotis Lymperopoulos, Vasanth Sarathy

**Published**: 2025-05-22 01:34:23

**PDF URL**: [http://arxiv.org/pdf/2505.16113v1](http://arxiv.org/pdf/2505.16113v1)

## Abstract
Modern Large Language Models (LLMs) often require external tools, such as
machine learning classifiers or knowledge retrieval systems, to provide
accurate answers in domains where their pre-trained knowledge is insufficient.
This integration of LLMs with external tools expands their utility but also
introduces a critical challenge: determining the trustworthiness of responses
generated by the combined system. In high-stakes applications, such as medical
decision-making, it is essential to assess the uncertainty of both the LLM's
generated text and the tool's output to ensure the reliability of the final
response. However, existing uncertainty quantification methods do not account
for the tool-calling scenario, where both the LLM and external tool contribute
to the overall system's uncertainty. In this work, we present a novel framework
for modeling tool-calling LLMs that quantifies uncertainty by jointly
considering the predictive uncertainty of the LLM and the external tool. We
extend previous methods for uncertainty quantification over token sequences to
this setting and propose efficient approximations that make uncertainty
computation practical for real-world applications. We evaluate our framework on
two new synthetic QA datasets, derived from well-known machine learning
datasets, which require tool-calling for accurate answers. Additionally, we
apply our method to retrieval-augmented generation (RAG) systems and conduct a
proof-of-concept experiment demonstrating the effectiveness of our uncertainty
metrics in scenarios where external information retrieval is needed. Our
results show that the framework is effective in enhancing trust in LLM-based
systems, especially in cases where the LLM's internal knowledge is insufficient
and external tools are required.

## Full Text


<!-- PDF content starts -->

arXiv:2505.16113v1  [cs.LG]  22 May 2025Tools in the Loop: Quantifying Uncertainty of LLM Question Answering
Systems That Use Tools
Panagiotis Lymperopoulos
Tufts University
plympe01@tufts.eduVasanth Sarathy
Tufts University
vasanth.sarathy@tufts.edu
Abstract
Modern Large Language Models (LLMs) often require external tools, such as machine learning classifiers
or knowledge retrieval systems, to provide accurate answers in domains where their pre-trained knowledge
is insufficient. This integration of LLMs with external tools expands their utility but also introduces a
critical challenge: determining the trustworthiness of responses generated by the combined system. In
high-stakes applications, such as medical decision-making, it is essential to assess the uncertainty of both
the LLM’s generated text and the tool’s output to ensure the reliability of the final response. However,
existing uncertainty quantification methods do not account for the tool-calling scenario, where both the
LLM and external tool contribute to the overall system’s uncertainty. In this work, we present a novel
framework for modeling tool-calling LLMs that quantifies uncertainty by jointly considering the predictive
uncertainty of the LLM and the external tool. We extend previous methods for uncertainty quantifi-
cation over token sequences to this setting and propose efficient approximations that make uncertainty
computation practical for real-world applications. We evaluate our framework on two new synthetic QA
datasets, derived from well-known machine learning datasets, which require tool-calling for accurate an-
swers. Additionally, we apply our method to retrieval-augmented generation (RAG) systems and conduct a
proof-of-concept experiment demonstrating the effectiveness of our uncertainty metrics in scenarios where
external information retrieval is needed. Our results show that the framework is effective in enhancing
trust in LLM-based systems, especially in cases where the LLM’s internal knowledge is insufficient and
external tools are required.
1 Introduction
As large language models (LLMs) have been in-
creasingly deployed in practical applications, their
problem-solving capacity has enabled practitioners
to combine them with external tools to augment
their reasoning abilities and available information
beyond their training distribution[18]. This has
greatly expanded LLMs’ applicability beyond text-
processing applications to problems involving com-
plex decision making, which find applications in var-
ious critical domains such as medicine[21] and the
law[9]. While recent work has developed metrics
that help decide whether to trust LLM outputs [8],
such metrics are not applicable to tool-calling sys-
tems directly, as they do not take into account the
trustworthiness of the tool itself. This is especially
important in highly specialized domains, where the
LLM’s own knowledge from training may not sufficeand external tools are required.
Uncertainty quantification has been a critical
component in machine learning for determining
when to trust model outputs. In particular, un-
certainty measures such as predictive entropy, have
proven useful in understanding whether a model’s
prediction is reliable, or whether the model is likely
to produce unreliable predictions due to, for in-
stance, distribution shift. Techniques such as Monte
Carlo dropout [5] and deep ensembles [10] have
been applied to improve estimates of uncertainty in
model predictions, especially in domains like health-
care, where model missteps can be costly. These
methods allow practitioners to decide when further
human intervention may be needed or when the sys-
tem might fail. Extending this notion to LLMs,
uncertainty quantification over token sequences or
meanings has emerged as a tool for determining
1

when to trust generated content [8, 3, 14]. Here,
the uncertainty can be tied to the LLM’s internal
representation of the sequence, which influences the
model’s capacity to produce coherent and accurate
text.
Recently, generative AI agents have emerged as
a powerful way to extend the capabilities of LLMs
beyond text generation [16, 13, 18], allowing these
models to act as agents that interact with the world.
By calling external APIs, models, or databases,
LLMs can retrieve information, perform tasks, or
even make decisions based on external computa-
tions. This paradigm shift has enabled LLMs to
step into more agentic roles, executing actions that
can impact real-world systems. However, this abil-
ity also comes with heightened safety and trust
requirements[7]. However, uncertainty quantifica-
tion methods for agentic LLM systems are currently
lagging behind, and advancing these techniques is
crucial to making such systems safer for deployment
in sensitive and high-stakes applications.
In this work, we take a first step towards increas-
ing the trustworthiness of LLM agents by building a
framework for quantifying the uncertainty in tool-
calling question-answering (QA) systems. We be-
gin by modeling the tool-calling LLM system and
deriving uncertainty metrics for it, taking into ac-
count both the uncertainty of the LLM and the tool
being called. In our work, we make the key as-
sumption that we are operating in the white-box
setting, where the uncertainty of external tools is
known and we have access to model logits. To
quantify uncertainty over the meaning of the com-
bined system’s answer, we adapt semantic entropy
[8], into our framework. Finally, we make the un-
certainty computation more efficient by deriving
an approximation that makes our framework fea-
sible in many practical settings. We also apply
our framework to retrieval-augmented-generation
(RAG) tasks. We evaluate our approach on two
novel synthetic question-answering (QA) datasets
requiring tool calling, derived from the well-known
IRIS flower classification [4] and PIMA diabetes
datasets. Finally, we also apply our framework
to a simple retrieval-augmented generation [6, 11]
system and conduct a proof-of-concept experiment
quantifying the uncertainty of answering questions
from the BoolQ[2] dataset of yes/no questions using
an external database of references.2 Related works
In the field of uncertainty quantification (UQ) for
large language models (LLMs), a growing body of
research has focused on developing techniques to
estimate when these models might produce unre-
liable outputs. One approach involves using su-
pervised models to predict uncertainty by lever-
aging the LLM’s internal states, such as logits or
hidden activations, as well as ground truth labels
from training data. For example, recent work[12]
has proposed training a separate uncertainty model
based on LLM logits to improve the estimation of
confidence in the generated outputs.
Another method involves modeling semantic
uncertainty[8, 3], which quantifies the LLM’s uncer-
tainty over the meanings of generated text rather
than just over individual tokens. This approach
is particularly useful for capturing more complex
ambiguities. Semantic uncertainty has proven ef-
fective in detecting hallucinations and other in-
consistencies in LLM-generated content, offering a
pathway to more reliable responses. This method
relies on entailment models to establish semantic
classes of samples obtained from the LLM using the
same prompt. Then, the uncertainty over semantic
classes is estimated empirically using those samples.
While these uncertainty quantification techniques
are crucial for understanding the reliability of
LLMs, they do not directly account for the uncer-
tainty introduced by external tools in tool-calling
systems as they only involve the distribution over
tokens (or meanings) expressing the system’s final
answer. Our framework for modeling tool-calling
systems allows us to use these techniques for the
LLM portion of the system, as well as combine them
with the predictive uncertainty of the external tools
in a principled way, thereby enabling more compre-
hensive uncertainty assessments in tool-augmented
LLM systems.
Recent work on tool-calling LLMs [18, 17] has
largely focused on training large language models
(LLMs) to invoke external tools in a structured and
useful manner, as well as generating datasets that
encourage models to learn how and when to use
tools [13]. Techniques such as fine-tuning LLMs
on datasets designed for tool use, and building
frameworks that allow LLMs to interact with APIs,
databases, and external systems, have proven highly
effective in enabling models to extend their capabil-
ities beyond text generation. However, while these
2

Figure 1: Illustration of our model of the LLM+tool system. The system receives an input prompt x,
such as a question that requires a tool (e.g. a classifier) to answer. The LLM produces a tool call awhich
acts as input to the tool. The tool produces output z, which in turn is mapped to a token sequence and
provided to the LLM, alongside the original prompt. Finally, the LLM produces the final answer y. Yellow
indicates the features that the LLM needs to extract for the tool call. Green indicates the final question the
system needs to answer and red indicates the final answer provided by the combined system. Within this
framework we can quantify the uncertainty of the final answer while taking into account the uncertainty
of the classifier.
methods drive practical applications by enhancing
the model’s functional scope, they do not address
the issue of uncertainty quantification.
In parallel, several tool-calling benchmarks have
been introduced to evaluate the capabilities of LLM
agents [22, 15]. These benchmarks test various as-
pects of agentic systems, including the ability to se-
lect appropriate tools when needed, call them with
correct arguments, and utilize their results mean-
ingfully. They often evaluate models on tasks that
range from coding, API calls, and database queries
to information retrieval and mathematical compu-
tations. While these benchmarks offer comprehen-
sive assessments of an agent’s ability to use tools
correctly, they do not focus on quantifying the un-
certainty of the systems evaluated. In addition,
many of the tools used in these benchmarks are of-
ten deterministic, or too complex so that assessing
their uncertainty, especially in terms of predictive
uncertainty, is challenging. As a result, we do not
use them in our study and instead conduct con-
trolled experiments aimed at assessing the efficacy
of our framework in simple QA tasks that require
calling tools. These more controlled environments
allow us to study the more limited but still widely
applicable setting when the tools provide known
and reliable uncertainty estimates.3 Method
In this section we present our framework for un-
certainty quantification in tool-using LLMs. Let S
be the set of all token sequences. Let x∈ S be a
user-provided sequence representing the prompt to
the LLM. Let a∈ A ⊂ S be a sequence represent-
ing an invocation of an external tool. Let z∈ Z
be a result produced by the external tool, where Z
is the space of possible responses (e.g. Z={0,1}
for binary classifiers). Finally, let y∈ S be a se-
quence corresponding to the response produced by
the LLM after receiving the prompt and invoking
the tool. Note that this formulation covers the case
of multiple tools, as they can be encapsulated into
a single tool with a more complex tool call a.
Our framework makes the following assumptions:
1) The final response yis independent of the invo-
cation of the tool agiven the tool response z. 2)
The predictive entropy H(z|a) of the tool p(z|a) is
known.
We model the tool-calling process as a sequential
process encompassing two calls to the LLM and one
call to the tool.
pθ(y, z, a|x) =pθ(y|z, x)p(z|a)pθ(a|x),(1)
Equation (1) shows the joint distribution over the
variables in the system, where θcorresponds to the
3

parameters of the LLM. Figure 2 illustrates our
framework for modeling tool-calling LLM systems.
First, the prompt xis provided to the LLM to gen-
erate the tool call a. For instance, xcould con-
tain the medical information of a patient and the
description of two treatments for different patient
subgroups. The LLM would first have to extract
the appropriate features from the prompt and for-
mat them to produce a. Then ais sent to the clas-
sifier p(z|a) to classify the patient into appropriate
treatment subgroups, represented by z. Finally, the
LLM receives the tool output zalong with the orig-
inal prompt and produces a final answer y. In prac-
tice, the classifier output may be deterministically
mapped to a token sequence to make it more in-
formative to the LLM. For instance, if the tool is
a diagnostic model, zmay become ”The patient is
diagnosed with ¡disease¿ ”.
3.1 Uncertainty quantification for tool-
calling systems.
Within our framework, we quantify uncertainty
using entropy. We now present a derivation for
the predictive entropy H(y|x) in our framework in
terms of the known H(z|a) and other terms.
H(y|x) =H(y, z, a|x)−H(z, a|x, y),
H(y|x) =H(y|z, a, x ) +H(z|a, x) +H(a|x)
−H(z|x, y, a )−H(a|x, y).
By the conditional independence in eq. (1) we ob-
tain:
H(y|x) =H(y|z, x) +H(z|a) +H(a|x)
−H(z|y, a)−H(a|x, y).(2)
Equation (2) is the predictive entropy of the final
answer produced by the combined LLM+tool sys-
tem given the prompt. The first and third terms are
predictive entropies over sequences of tokens pro-
duced by the LLM, which may be estimated em-
pirically through samples. The second term is the
predictive entropy of the tool, which is assumed to
be known. The final two terms are based on the pos-
teriors p(z|y, a) and p(a|y, x). These terms present
a major challenge as they are very difficult to esti-
mate: They involve marginalization over the space
of token sequences, which is very large. Later in this
section we discuss ways to tackle this intractability.Semantic entropy for tool-calling systems.
Semantic entropy [8] is a technique for quantifying
LLM’s uncertainty over meanings rather than token
sequences. In summary, this is achieved by cluster-
ing sample responses from the LLM into a set of
semantic classes Cand empirically estimating the
distribution over meanings with samples. In our
framework, we can apply the same principle in the
final answer y∼pθ(y|x, z) produced by the system,
after observing the tool result. Then, similarly to
the predictive entropy case, we can derive semantic
entropy within our framework:
H(C|x) =H(C|z, x) +H(z|a)
+H(a|x)−H(z|y, a)−H(a|x, y).(3)
As in the original formulation of semantic entropy,
in our framework we can estimate H(C|z, x) with
samples. Note that we do not compute semantic
entropy over the tool call a, as in our experiments,
tools are simple to call for the LLMs. However, if
there are many ways to express the same tool call
(say, a question to a domain-specialized language
model), then the entropy over equivalent classes of
tool calls should be considered.
Intractability of entropy calculation. Equa-
tions (2) and (2) express the predictive and seman-
tic entropy within our framework for tool-calling
LLMs. However, the negative terms H(z|y, a) and
H(a|x, y) are problematic. Here we explain the
challenge and discuss two approaches to deal with
the intractability. H(z|y, a) is the entropy of the
posterior distribution p(z|y, a), which expresses the
probability of the classifier output, after the final
answer of the LLM is observed. Similarly, H(a|y, x)
is the entropy of p(a|y, x). Using Bayes’ rule and
marginalizing over z, we can obtain:
p(a|x, y) =P
z∈Zpθ(y|z, x)p(z|a)pθ(a|x)
pθ(y|x).
Even though in some cases the space Zis not nec-
essarily large (e.g. when the tool is a binary clas-
sifier), the normalization term in the denominator
is intractable as it requires marginalization of token
sequences for the tool call a. As a result, we can-
not compute it directly. Similar reasoning applies
top(z|y, a).
One approach to addressing the intractability is
to estimate the posteriors empirically, by fitting
a model to samples from the system. Consider
4

Figure 2: Illustration of our framework applied to RAG. The system receives an input question x, shown
in green, that requires additional documents to answer. In our framework, we describe the document
retriever as a categorical distribution over documents. The system samples relevant documents zfrom that
distribution which are added to the LLM context. Finally, the LLM produces the final answer y. Yellow
indicates the most relevant passage. The bar plot shows that in this example the retrieval distribution is
low entropy, so uncertainty in the retrieval is low. Red indicates the final answer provided by the combined
system. Within this framework we can quantify the uncertainty of the overall system answer, taking into
account the uncertainty of the retreival system.
the case where p(z|a) is a binary classifier, which
receives an input vector of dfeatures and out-
puts a binary label. We can fit a neural network
pϕ(z|y, a) with parameters ϕto tuples of ( y, z, a )∼
pθ(y, z, a|x), where inputs are vector embeddings of
zandaand the output is a probability over Z.
Similarly, we can use vector embeddings of yand
xto predict continuous values for the features in
a. However, empirically fitting the posterior distri-
bution from a few samples can be difficult and the
approximation not sufficiently accurate for quanti-
fying uncertainty.
Estimating entropies To estimate the entropy
of the LLM’s final answer H(y|z, x), as well as
the semantic entropy counterpart, we follow pre-
vious work and estimate them directly with sam-
ples. For a collection of samples of final answers
D={(yi, z, x)|i= 1. . . N}, the predictive entropy
of the final answer is estimated by:
H(y|z, x)≃1
NNX
i=1logp(yi|z, x). (4)
Similarly, given a collection of samples D′=
{(Ci, yi, z, x)|i= 1. . . N}, where Ci∈Cis the
semantic class index of yi, the semantic entropy ofthe final answer can be estimated by:
H(C|z, x)≃1
|C||C|X
j=1logX
y∈Cjp(y|z, x).(5)
3.2 Strong Tool Approximation
By making some additional assumptions about
the setting, we can obtain a much simpler approx-
imation that is also more efficient to compute for
equations (2) and (3). We consider the strong tool
setting with the following conditions:
•There is strong dependence between yandz.
This means that the question is very difficult
for the LLM to answer without knowing the
tool output, but almost obvious given the tool
output. At the same time, it is relatively easy
to infer zifyis known.
•All of the information about ais contained in x.
In other words, since it is possible to determine
afrom x, knowledge of yis unlikely to provide
much benefit in predicting a.
Note that these conditions do not express a re-
quirement on the quality of the tool, but is rather a
feature of the application domain. We consider this
a realistic setting as it is in-line with expected uses
5

of tool-calling systems: if a tool is not necessary for
the application, and if the information for calling
it is not to be found in the input, it is unlikely to
be implemented in the first place. For example, in
a medical application that suggests treatments to
medical professionals, the external tool could be a
complex diagnostics model over a patient’s blood-
work. The LLM may use its general knowledge to
suggest treatments after knowing the diagnosis, but
is unlikely to provide an accurate diagnosis with-
out the use of an expert tool. At the same time,
knowing a patient’s treatment plan is not useful in
formulating the call to the diagnostic model, given
the patient information.
Examining equations (2),(3) in this setting allows
us to make some simplifications. First, we notice
that since ystrongly depends on z, the distribu-
tionp(z|y, a) is going to be low-entropy, contribut-
ing only slightly to the computation. Additionally,
since yis not very informative about a, the values
H(a|x) and H(a|x, y) are likely to be similar and
cancel out. We therefore arrive at the Strong Tool
Approximation (STA) of Predictive and Semantic
Entropy ( STA P,STA S):
STA P(x) =H(y|z, x) +H(z|a), (6)
STA S(x) =H(C|z, x) +H(z|a). (7)
These metrics are simple to compute, amounting to
only additively combining the entropy of the LLM’s
final answer, which can be estimated using exist-
ing methods, and the entropy of the tool response,
which is assumed to be known. While the STA ig-
nores some terms, notably the entropy of the tool
call, in our experiments we show that in the strong
tool condition, these metrics can be more reliable
than fitting the posterior terms with a few samples,
which is error-prone.
Application to RAG With some modification,
we can apply our framework to QA with RAG,
where the system first retrieves documents relevant
to a query from a corpus and then answers the query
with the relevant documents inserted to the con-
text. In this setting there is no tool call, since the
document retrieval happens automatically.
pθ(y, z|x) =pθ(y|z, x)p(z|x), (8)
We consider a soft form of RAG, where the
retrieval system defines a categorical distributionp(z|x) over documents in the corpus. Here, the
space of z,Zis the corpus of documents. There
are many ways to define p(z|x), for instance, vec-
tor embeddings can be computed for the query and
documents in the corpus, and cosine similarity can
be used to compute logits over the documents, as
is common in RAG applications. Then samples can
be drawn from this distribution to retrieve relevant
documents.
The predictive entropy for the system is:
H(y|x) =H(y|z, x) +H(z|x)−H(z|y, x).(9)
As previously, if the dependency between zandy
is strong, we can compute the STA metrics, using a
similar formulation for the semantic entropy. How-
ever, estimating H(z|y, x) empirically may be very
difficult, especially since the document corpus may
be large.
In general for most queries, only a small num-
ber of documents are likely to be relevant, making
the distribution low-entropy. If no relevant docu-
ments are available, then most documents will have
similar probabilities, making the retrieval distribu-
tion high-entropy. Since the LLM is more likely to
answer correctly if a relevant passage is found, the
tool entropy is likely to be helpful in predicting the
correctness of the overall system response.
4 Experiments
In this section we present experiments validating
our framework and the derived metrics. First, we
discuss the creation of the synthetic datasets we use
for evaluating the metric in the QA with tool calling
setting. Then we describe and discuss our experi-
ments on those datasets and the RAG QA task.
4.1 Datasets
As discussed earlier, current datasets for evalu-
ating tool calling LLM agents are not well suited
for the study of uncertainty quantification, due to
the complexity of the queries and tools (e.g. writ-
ing code) which are aimed at evaluating the abil-
ity of systems to take advantage of tools to answer
queries, rather than to quantify uncertainty. As
a result we create 2 novel synthetic QA datasets
based on the IRIS[4] and Pima Indians diabetes[19]
classification datasets.
We create a QA dataset from each machine learn-
ing dataset as follows: First, we fit a gaussian mix-
ture model to each class and sample synthetic data-
6

Figure 3: Samples from the IRIS QA and the Diabetes QA datasets. Yellow indicates the portion of the
question that the LLM needs to extract features for the tool call from. Green indicates the question that
the system needs to answer based on the prompt and the tool response. Red indicates the answer.
Model STASSTAPSem. Entropy Pred. Entropy Sem. Entropy FA Pred. Entropy FA Tool Entropy
Meta-Llama-3-8B-Instruct 0.845 0.824 0.766 0.710 0.615 0.553 0.806
Meta-Llama-3.1-8B-Instruct 0.752 0.668 0.640 0.603 0.642 0.529 0.692
Mistral-7B-Instruct 0.786 0.718 0.712 0.683 0.667 0.605 0.753
Table 1: Experimental results from the IRIS QA dataset. Columns show AUROC of uncertainty metrics
for different models. High values indicate the metric is predictive of response accuracy. In this dataset, the
metrics based on the strong tool approximation outperform the other metrics. Interestingly, semantic and
predictive entropy computed via empirical posteriors are not as predictive of accuracy, likely due to poor
approximation. The entropy of the tool itself is informative, but the STA metrics offer additional benefits.
points to ensure that the LLMs have not seen these
data points during their training. Then, we sam-
ple 150 points, evenly split among each class. We
preprocess the data to better match the formatting
of the original dataset (e.g. rounding to 1 decimal
place). We construct a few example questions by
hand, where we pose a natural language question
related to the classification result. We also ensure
to include all features in the question and that the
answer strongly depends on the classification result.
Finally, we make sure that the question can be an-
swered briefly, typically in one word. Using the con-
structed examples we query GPT-4o with few shot
prompting to construct new questions based on our
sampled points and examine the results manually
to ensure quality. We include the exact prompt for
each dataset in the supplemental material along-
side our code submission. As a post-processing step,
we ”anonymize” the dataset, replacing class names
with identifiers (e.g. Iris-setosa with flower type 1)
to encourage the LLM’s to base their response on
the tools rather than their general knowledge. We
are releasing our datasets alongside the code to gen-
erate them as part of our contribution with thiswork.
For the RAG QA task, we sample 150 questions
randomly from the BoolQ dataset [2] along with
their corresponding wikipedia excerpts containing
their answers. We then mix the passages for half
the questions with a corpus of other excerpts from
wikipedia collected from an existing wikipedia QA
dataset[20] to create the document corpus for re-
trieval.
Evaluation Methodology To evaluate our
framework we follow prior work and use AUROC
as our evaluation metric [8, 1]. The intuition be-
hind this criterion is to treat the uncertainty metric
as a score expressing whether we should trust the
LLM output, with high entropy values indicating we
should not, and low entropy values indicating that
we should. Then, we can compare this score against
the answer accuracy of the system. In short, if we
trust the system when it answers correctly, that cor-
responds to a true positive prediction. If we trust
it when it is wrong, this corresponds to a false posi-
tive prediction. AUROC enables us to evaluate the
quality of metrics across decision thresholds.
7

Model STASSTAPSem. Ent. Pred. Ent. Sem. Ent. FA Pred. Ent. FA Tool Ent.
Meta-Llama-3-8B-Instruct 0.791 0.730 0.635 0.652 0.663 0.634 0.642
Meta-Llama-3.1-8B-Instruct 0.675 0.662 0.656 0.674 0.515 0.580 0.664
Mistral-7B-Instruct 0.782 0.702 0.671 0.666 0.516 0.570 0.781
Table 2: Experimental results from the Diabetes QA dataset. Columns show AUROC of uncertainty
metrics for different models. High values indicate the metric is predictive of response accuracy. Similarly
to the IRIS QA results, the metrics based on the strong tool approximation tend to outperform the other
metrics in most cases. Also here, likely due to poor approximations, the metrics using empirical estimates
do not perform as well as those based on the STA as they are more noisy.
In our experiments we compute the following un-
certainty metrics derived from our framework:
•STA P: The strong tool approximation applied
to predictive entropy.
•STA S: The strong tool approximation applied
to semantic entropy.
•Sem. Entropy: The semantic entropy in our
framework (eq (3)), computed with empirically
estimated posteriors.
•Pred. Entropy: : The predictive entropy in our
framework (eq (2)), computed with empirically
estimated posteriors.
We also compare these metrics against naive base-
lines that do not combine sources of uncertainty:
•Sem. Entropy FA: The semantic entropy of the
final answer produced by the LLM, given the
tool output.
•Pred. Entropy FA: The predictive entropy of
the system’s final answer, given the tool out-
put.
•Tool Entropy: The entropy of the tool itself.
We run our experiments on a server with one
NVIDIA A600 GPU, an AMD Ryzen Threadrip-
per PRO 5955WX CPU with 16-Cores and 130GB
of RAM. Due to computational limitations, we
only experiment with smaller LLMs. However, the
framework is applicable as is to larger LLMs and
prior work suggests results are likely to generalize
to larger models as well [8]. We use the instruction-
tuned variants of the models, as they are better
suited for the tool calling setting.4.2 Experiment 1: Quantifying uncer-
tainty in tool-calling QA
In this experiment we evaluate our framework
in quantifying the uncertainty of QA tool-calling
LLM systems using the IRIS QA and Diabetes QA
datasets.
Experiment Settings To compute uncertainties
we take 10 samples of the final answer over 3 runs of
the combined system to estimate uncertainties. We
use the samples from the first 30 examples from each
dataset to train the posterior models to estimate se-
mantic and predictive entropy. Additional training
details for the posterior estimates are available in
the supplemental material. We evaluate all metrics
on the remaining 120 examples. We prompt using
few-shot prompting with 3 examples both for the
tool-calling step and for the final answer. Example
prompts are available in the supplemental material
alongside our code submission. The classifier called
by the LLM is a simple lookup table with added
uncertainty. For half the samples, the classifier uni-
formly samples a class to output. For the other half,
it gives high probability to the correct class and low
probability to the rest.
Results Table 1 shows experimental results in the
IRIS QA dataset for three models. Overall in this
dataset the metrics based on the STA seem to per-
form better than the alternatives. This indicates
that the conditions identified for STA to apply are
satisfied, and that the noise introduced by the ap-
proximate posteriors is reducing the quality of the
computed metrics. The baseline approaches com-
puting entropies only over the final answer are in
most cases much less informative, as they only con-
sider uncertainty from one part of the system. Sim-
8

Model STAS STAP Sem. Ent. FA Pred. Ent. FA Tool Entropy
Llama-3.1-8B-Instruct 0.675 0.646 0.662 0.622 0.635
Meta-Llama-3-8B-Instruct 0.648 0.645 0.570 0.575 0.629
Mistral-7B-Instruct 0.668 0.705 0.502 0.711 0.667
Table 3: Experimental results in the RAG QA task. In this setting, the STA metrics still offer a benefit
by combining sources of uncertainty, though the improvement is smaller than in other tasks. Nevertheless,
they outperform baseline approaches in almost every case. One possible explanation for the smaller im-
provement, is that in the RAG setting, the STA is not as applicable as in other settings. This is because the
retrieved document may include useful information for answering the question, but significant additional
reasoning may be necessary to obtain a correct answer.
ilarly, the tool entropy is also not as informative
as the STA metrics, since they combine uncertainty
from the whole system.
Table 2 shows experiment results from the Dia-
betes QA dataset. Overall, the results are consis-
tent with those from the IRIS QA dataset: STA S
andSTA Poutperform the other metrics in almost
every case and the empirical approximation of se-
mantic and predictive entropies seems to be too
noisy.
There is some surprising variation, particularly
with the final answer entropies occasionally being
more informative than expected. This variation
may be due to how different models make use of the
tool result, which may also depends on prompt engi-
neering. If a model is prone to ignore the tool result
for some samples, the entropy of the final answer
may be more informative of the response accuracy
than if the model is very faithful to the tool. Still,
given the same prompts, STA Ssignificantly outper-
forms the other metrics in both datasets, showing
that when the STA assumptions hold, it is a viable
and easy to compute metric for predicting response
accuracy,
4.3 Experiment 2: Proof of concept ap-
plication to RAG QA
In this experiment, we apply our framework to
the RAG QA setting and quantify the uncertainty
of the combined retrieval and LLM system.
Experiment settings To compute uncertainties,
we take 10 samples of the final answer from the
LLMs after retrieving documents. We construct the
document retrieval system in two stages: First, we
compute the cosine similarity of embeddings for the
input question with the entire corpus of documentsand select the top K items. Then, we use the scores
over those K items as logits of a categorical distri-
bution, and sample M documents from that distri-
bution. In our experiments we use K=5 and M=1.
We use the entropy of the categorical distribution as
the entropy of p(z|x). Additional implementation
details are available in the supplemental material.
In this experiment we only compute the STA and
the baseline metrics, as computing the posterior
term for the retrieval system is much more complex
than for a classifier model, and obtaining a good
approximation is unlikely.
Results Table 3 shows the experiment results for
the RAG QA task using questions from the BoolQ
dataset. For both Llama models, the STA Smetric
outperforms STA Pand the baseline metrics, pro-
viding a more informative signal for when to trust
the retrieval+LLM system. However, the perfor-
mance difference is not as impressive as in the IRIS
QA and Diabetes QA datasets, and in the case
of Mistral-7B-Instruct model, there is no improve-
ment, perhaps even a small degradation. This may
be because in the RAG QA problem, the condi-
tions of the strong tool setting are less applicable
than in the previous task. In our experiment, this
is specifically the case with the first condition, since
there is no tool call. While the retrieval component
provides useful information to the LLM, informa-
tion without which answering the question is more
difficult, making use of the retrieved information
requires additional reasoning. As a result, the de-
pendence between the final answer yand document
retrieved zis not as strong, making the approxima-
tion worse.
Another reason for the smaller improvement of
the STA metrics over the baseline metric may be
9

that, since the questions in this dataset are more
general in nature than the specialized questions of
previous tasks, the LLM has some general knowl-
edge that it can use to answer the questions, or
may have seen some of these questions during train-
ing. As a result, whether or not the retrieval system
found the right document has a smaller influence
on the final answer produced. Still, since the LLM
does base its responses on the documents in some
occasions, tool entropy is still somewhat predictive
of the response accuracy and combining the met-
rics through the STA tends to improve our ability
to predict response accuracy.
5 Discussion
Our proposed framework for uncertainty quan-
tification in tool-calling LLM systems presents a
flexible and extensible approach to modeling uncer-
tainty. One of its key strengths is that it is agnostic
to the specific techniques used to compute entropies
over token sequences or meanings. This flexibility
allows for the integration of future advancements in
uncertainty quantification without requiring signif-
icant changes to the underlying framework. For in-
stance, emerging methods for better capturing LLM
uncertainties, such as refined logit-based entropy
estimation or latent semantic representation mod-
els, can be seamlessly incorporated into the frame-
work as drop-in replacements. This modularity en-
sures that uncertainty quantification in tool-calling
LLM systems continues to track developments in
the broader field of LLM uncertainty estimation,
and offers the potential for improving predictive ac-
curacy by leveraging these innovations.
Our framework is also compatible with various
architectures of tool-calling systems, making it suit-
able for a wide range of applications. For instance,
in our experiments we consider the case where a
tool is called for every query to the LLM. However,
in practice, we may want to let the LLM decide
whether to call a tool. This can be easily accom-
modated by adding a special entry in the tool call a,
representing a null tool with zero entropy. In cases
where the LLM has enough knowledge to answer a
query without external assistance, this entry allows
the system to omit the tool call while keeping the
entropy calculation consistent.
Moreover, the framework can handle more com-
plex architectures, such as those involving multiple
tool calls. When more than one tool may be called,the decision to call the correct tool can be incor-
porated into the action space a, allowing the LLM
to choose between tools dynamically. This ensures
that our framework scales to scenarios where mul-
tiple tools are available, each with different com-
petencies, and where tool selection plays a critical
role in generating accurate answers. One challenge
to consider is that depending on the application and
tools, the STA may be weaker in this setting. This
is because it may be harder to decide how to call
the tool given the input, making the gap between
H(a|x) and H(a|x, y) larger. In that case, more
sophisticated methods for estimating the posteriors
may be required, and the full metrics used.
Additionally, our framework supports parallel
tool calls, in which the output space Zis formed
by combining the outputs of all possible tools. This
generalization allows the LLM to process informa-
tion from multiple tools simultaneously, further en-
hancing its capabilities while still fitting within
the entropy-based uncertainty quantification sys-
tem. However, as with the previous case, this may
make the STA weaker.
Another possible extension to the framework is
considering multiple rounds of tool-calling, inter-
leaved with text generation for reasoning. In such
cases, our framework can be extended by adding
additional intermediate variables, with dependen-
cies analogous to those of x, zandy. However, it
is worth noting that the strong tool approximation
(STA) is also likely to degrade in that setting, as
errors introduced by early rounds of tool calls ac-
cumulate, potentially leading to a loss in predictive
power. Addressing this degradation may require
further refinement of the approximation techniques,
especially in domains requiring long chains of tool
calls.
Turning to our experimental results, the applica-
tion of our framework to retrieval-augmented gener-
ation (RAG) systems yielded smaller performance
gains compared to the tool-calling QA experiments.
We hypothesize that this is because the LLMs used
in the RAG task were able to answer some of the
questions directly from their internal knowledge,
without heavily relying on the retrieved documents.
In contrast, tool-calling settings typically involve
specialized domains where the LLM’s knowledge is
insufficient, and uncertainty metrics derived from
our framework offer a clearer benefit in predict-
ing response accuracy. We expect that in more
10

domain-specific RAG tasks, where the LLM’s in-
ternal knowledge is less complete, the STA-based
uncertainty metrics will demonstrate a greater ad-
vantage.
One limitation of our methodology was in com-
puting the full semantic and predictive entropy met-
rics as derived in our framework. While we at-
tempted to empirically estimate the posterior dis-
tribution, the small datasets and the inherent dif-
ficulty of the task meant we had limited success.
We still consider their inclusion necessary however
as advancements in inference with LLMs may make
computing such terms more feasible in the future.
Lastly, we acknowledge that our experiments
were conducted using smaller LLMs due to compu-
tational resource limitations. Despite this, there is
nothing inherent in our framework that prevents its
application to larger models. Future work should
explore applying our framework to larger LLMs to
confirm its scalability and evaluate its performance
in more demanding scenarios.
Broader Impact Statement This work paves
the way towards more responsible and trustwor-
thy use of tool-calling LLM systems by provid-
ing a framework for quantifying uncertainty, which
can help mitigate user overtrust in these systems
and encourage more responsible use. By identify-
ing when a system’s predictions are likely unreli-
able, we can reduce the risk of over-reliance on AI
in critical applications, such as healthcare or legal
decision-making, ultimately promoting safer AI de-
ployment. However, the computational cost of com-
puting uncertainty metrics, even approximate ones,
is a significant drawback. Current methods require
taking multiple samples from the system, which in-
creases the already high energy demands of LLMs
and raises concerns about the environmental impact
of such models. As LLMs become more widely used,
balancing the benefits of uncertainty quantification
with the environmental costs will be an important
challenge for the field.
6 Conclusion
In this work, we present a novel framework for
uncertainty quantification in tool-calling large lan-
guage models (LLMs). A key aspect of our ap-
proach is its flexibility, allowing for a variety of en-
tropy computation methods to be integrated into
the framework. This allows us to apply the semanticuncertainty idea to the tool-calling setting and de-
rive efficient approximations, enabling practical de-
ployment of uncertainty quantification in real-world
scenarios.
We validate our framework on two new synthetic
QA datasets, derived from two well-known machine
learning datasets, which demonstrate the frame-
work’s capability to handle tool-calling settings.
Additionally, we conduct a proof-of-concept exper-
iment in a retrieval-augmented generation (RAG)
scenario, showing that while the improvement from
our uncertainty metrics was smaller in the RAG set-
ting, the framework holds promise for specialized
domains where LLM knowledge is limited. Finally,
we provide extensive discussion of how our frame-
work relates to different LLM agent architectures
for tool-calling, how it can be extended to accom-
modate them and possible challenges in that direc-
tion.
References
[1] Neil Band, Tim GJ Rudner, Qixuan Feng, An-
gelos Filos, Zachary Nado, Michael W Dusen-
berry, Ghassen Jerfel, Dustin Tran, and Yarin
Gal. Benchmarking bayesian deep learning on
diabetic retinopathy detection tasks. arXiv
preprint arXiv:2211.12717 , 2022.
[2] Christopher Clark, Kenton Lee, Ming-Wei
Chang, Tom Kwiatkowski, Michael Collins,
and Kristina Toutanova. Boolq: Exploring
the surprising difficulty of natural yes/no ques-
tions. In NAACL , 2019.
[3] Sebastian Farquhar, Jannik Kossen, Lorenz
Kuhn, and Yarin Gal. Detecting hallucina-
tions in large language models using semantic
entropy. Nature , 630(8017):625–630, 2024.
[4] R. A. Fisher. Iris. UCI Machine
Learning Repository, 1936. DOI:
https://doi.org/10.24432/C56C76.
[5] Yarin Gal and Zoubin Ghahramani. Dropout
as a bayesian approximation: Representing
model uncertainty in deep learning. In inter-
national conference on machine learning , pages
1050–1059. PMLR, 2016.
[6] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangx-
iang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei
Sun, and Haofen Wang. Retrieval-augmented
11

generation for large language models: A sur-
vey. arXiv preprint arXiv:2312.10997 , 2023.
[7] Dan Hendrycks, Nicholas Carlini, John Schul-
man, and Jacob Steinhardt. Unsolved
problems in ml safety. arXiv preprint
arXiv:2109.13916 , 2021.
[8] Lorenz Kuhn, Yarin Gal, and Sebastian Far-
quhar. Semantic uncertainty: Linguistic in-
variances for uncertainty estimation in nat-
ural language generation. arXiv preprint
arXiv:2302.09664 , 2023.
[9] Jinqi Lai, Wensheng Gan, Jiayang Wu, Zhen-
lian Qi, and Philip S Yu. Large language
models in law: A survey. arXiv preprint
arXiv:2312.03718 , 2023.
[10] Balaji Lakshminarayanan, Alexander Pritzel,
and Charles Blundell. Simple and scalable pre-
dictive uncertainty estimation using deep en-
sembles. Advances in neural information pro-
cessing systems , 30, 2017.
[11] Patrick Lewis, Ethan Perez, Aleksandra
Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich K¨ uttler, Mike
Lewis, Wen-tau Yih, Tim Rockt¨ aschel,
et al. Retrieval-augmented generation for
knowledge-intensive nlp tasks. Advances in
Neural Information Processing Systems , 33:
9459–9474, 2020.
[12] Linyu Liu, Yu Pan, Xiaocheng Li, and Guant-
ing Chen. Uncertainty estimation and quantifi-
cation for llms: A simple supervised approach.
arXiv preprint arXiv:2404.15993 , 2024.
[13] Weiwen Liu, Xu Huang, Xingshan Zeng, Xin-
long Hao, Shuai Yu, Dexun Li, Shuai Wang,
Weinan Gan, Zhengying Liu, Yuanqing Yu,
et al. Toolace: Winning the points of llm func-
tion calling. arXiv preprint arXiv:2409.00920 ,
2024.
[14] Andrey Malinin and Mark Gales. Uncertainty
estimation in autoregressive structured predic-
tion. arXiv preprint arXiv:2002.07650 , 2020.
[15] Yun Peng, Shuqing Li, Wenwei Gu, Yichen
Li, Wenxuan Wang, Cuiyun Gao, and Michael
Lyu. Revisiting, benchmarking and exploring
api recommendation: How far are we?, 2021.[16] Yujia Qin, Shengding Hu, Yankai Lin, Weize
Chen, Ning Ding, Ganqu Cui, Zheni Zeng,
Yufei Huang, Chaojun Xiao, Chi Han, et al.
Tool learning with foundation models. corr,
abs/2304.08354, 2023. doi: 10.48550. arXiv
preprint arXiv.2304.08354 , 10.
[17] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi
Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu,
and Ji-Rong Wen. Tool learning with large
language models: A survey. arXiv preprint
arXiv:2405.17935 , 2024.
[18] Timo Schick, Jane Dwivedi-Yu, Roberto
Dessi, Roberta Raileanu, Maria Lomeli, Eric
Hambro, Luke Zettlemoyer, Nicola Can-
cedda, and Thomas Scialom. Toolformer:
Language models can teach themselves
to use tools. In A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, Advances in Neural Infor-
mation Processing Systems , volume 36, pages
68539–68551. Curran Associates, Inc., 2023.
URL https://proceedings.neurips.
cc/paper_files/paper/2023/file/
d842425e4bf79ba039352da0f658a906-Paper-Conference.
pdf.
[19] Jack W Smith, James E Everhart, WC Dick-
son, William C Knowler, and Robert Scott Jo-
hannes. Using the adap learning algorithm to
forecast the onset of diabetes mellitus. In Pro-
ceedings of the annual symposium on computer
application in medical care , page 261. Ameri-
can Medical Informatics Association, 1988.
[20] Noah A Smith, Michael Heilman, and Rebecca
Hwa. Question generation as a competitive
undergraduate course project. In Proceedings
of the NSF Workshop on the Question Gener-
ation Shared Task and Evaluation Challenge ,
volume 9, 2008.
[21] Arun James Thirunavukarasu, Darren
Shu Jeng Ting, Kabilan Elangovan, Laura
Gutierrez, Ting Fang Tan, and Daniel Shu Wei
Ting. Large language models in medicine.
Nature medicine , 29(8):1930–1940, 2023.
[22] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian
Sun, and Chao Zhang. Toolqa: A dataset for
12

llm question answering with external tools. Ad-
vances in Neural Information Processing Sys-
tems, 36:50117–50143, 2023.
13