# A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks

**Authors**: Sara Rosenthal, Maeda Hanafi, Yannis Katsis, Lucian Popa, Marina Danilevsky

**Published**: 2025-10-13 19:58:10

**PDF URL**: [http://arxiv.org/pdf/2510.11897v1](http://arxiv.org/pdf/2510.11897v1)

## Abstract
Grounding conversations in existing passages, known as Retrieval-Augmented
Generation (RAG), is an important aspect of Chat-Based Assistants powered by
Large Language Models (LLMs) to ensure they are faithful and don't provide
misinformation. Several benchmarks have been created to measure the performance
of LLMs on this task. We present a longitudinal study comparing the feedback
loop of an internal and external human annotator group for the complex
annotation task of creating multi-turn RAG conversations for evaluating LLMs.
We analyze the conversations produced by both groups and provide results of a
survey comparing their experiences. Our study highlights the advantages of each
annotator population and the impact of the different feedback loops; a closer
loop creates higher quality conversations with a decrease in quantity and
diversity. Further, we present guidance for how to best utilize two different
population groups when performing annotation tasks, particularly when the task
is complex.

## Full Text


<!-- PDF content starts -->

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG
Tasks
SARA ROSENTHAL∗and MAEDA HANAFI∗,IBM, USA
YANNIS KATSIS,IBM, USA
LUCIAN POPA,IBM, USA
MARINA DANILEVSKY,IBM, USA
Grounding conversations in existing passages, known as Retrieval-Augmented Generation (RAG), is an important aspect of Chat-Based
Assistants powered by Large Language Models (LLMs) to ensure they are faithful and don’t provide misinformation. Several benchmarks
have been created to measure the performance of LLMs on this task. We present a longitudinal study comparing the feedback loop
of an internal and external human annotator group for the complex annotation task of creating multi-turn RAG conversations for
evaluating LLMs. We analyze the conversations produced by both groups and provide results of a survey comparing their experiences.
Our study highlights the advantages of each annotator population and the impact of the different feedback loops; a closer loop creates
higher quality conversations with a decrease in quantity and diversity. Further, we present guidance for how to best utilize two
different population groups when performing annotation tasks, particularly when the task is complex.
ACM Reference Format:
Sara Rosenthal, Maeda Hanafi, Yannis Katsis, Lucian Popa, and Marina Danilevsky. 2025. A Longitudinal Study on Different Annotator
Feedback Loops in Complex RAG Tasks. 1, 1 (October 2025), 26 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 Introduction
Chat-based platforms such as ChatGPT [ 61] and Claude [ 3] have become increasingly popular tools for asking questions
and having conversations [ 50]. Given a question by a user, the task of grounding the answers in the conversation
generated by the large language model (LLM) has become increasingly important to avoid hallucination or misinfor-
mation [ 36]. To address this problem, a considerable amount of research has been conducted in retrieval-augmented
generation (RAG) methods [ 26,47], which combine (1) a retriever model for fetching relevant passages from a corpus,
with (2) a generator model to create a response to the user question based on the retrieved passages.
A major aspect of evaluating a RAG method is understanding how well it handleschallengingandcomplexquestions,
which are important properties of ahigh-qualityconversation. For example, can the RAG system find the correct
documents to answer the human user’s question, or else determine that giving a grounded answer is impossible? Can
the RAG system handle vague user questions (“What time does the bank open?”), and successfully recover by, for
example, responding by asking the user for clarification (“Which branch are you planning to visit?”). Thus, there is
a breadth of work on multi-turn RAG benchmarks [ 19,21,23,39,45]. A high-quality benchmark for conversational
∗Both authors contributed equally to this research.
Authors’ Contact Information: Sara Rosenthal, sjrosenthal@us.ibm.com; Maeda Hanafi, maeda.hanafi@ibm.com, IBM, USA; Yannis Katsis, IBM, USA;
Lucian Popa, IBM, USA; Marina Danilevsky, IBM, USA.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on
servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
Manuscript submitted to ACM
Manuscript submitted to ACM 1arXiv:2510.11897v1  [cs.HC]  13 Oct 2025

2 Hanafi et al.
RAG serves to illuminate the capabilities and boundaries of the underlying RAG method - in contrast with simpler
conversations and easily answered turns.
Human annotation is an important aspect of building high-quality benchmarks. However, manually creating con-
versational RAG data is hard; past research has shown it to be a cognitively demanding task even for experienced
annotators [ 28]. The challenge lies in working with underlying retrieval and generation models to create a faithful and
naturally flowing conversation (rather than, e.g., a set of disjoint individual questions and answers). The annotator
must drive the conversation by creating a thoughtful sequence of questions; on they other hand they must also edit the
underlying RAG model’s default-generated responses to be accurate and complete. Finally, the annotator must also
ensure that each turn is accompanied by one or more passages that are used to ground the information in the response.
These aspects are all necessary to ensure the quality and complexity of the final conversation [10, 77, 78].
Common approaches for collecting human-annotated data vary from hiring professional annotation services to hiring
crowd workers via platforms such as Mechanical Turk1and Scale.AI2, with various tradeoffs in cost, worker reliability,
and data quality [ 4,44,68,75,76]. Most previous literature in crowdsourcing assumes adirect communication
feedback loopbetween task requesters and annotators. However, in some real-world cases, task requesters cannot
directly communicate with the annotators and must go through an intermediary - a common arrangement when
hiring external annotation services. Prior work has shown that different annotator communication strategies can
impact the final dataset [ 18,33]; for instance, different annotator hiring mechanisms support various feedback loops of
communication (e.g., direct/indirect) and frequency (e.g., weekly/daily).
In this work, we tasked human annotators to create challenging and complex conversations for evaluating RAG
models. In particular, we worked with two groups of annotators, which we refer to throughout the paper asinternaland
external. The internal annotators were hired as part of the same organization as the researchers (task requesters); they
have adirect communication feedback loopwith the task requesters and an hourly pay rate. The external annotators
were hired as part of a dedicated external crowdsourcing annotation service, and have anindirect communication
feedback loop, where all communication—including task instructions, task guideline materials, and feedback and per
task financial incentive—between the task requesters and annotators passes through an intermediary representative
from the annotation service.
In this paper, we present the results from a longitudinal study over an approximately one-year period (May 2024 to
May 2025), based on observations ofinternalandexternalannotators. We instruct both groups to create complex
and high-quality multi-turn conversations for RAG. This task is complex and hard [ 28], and it allows us to showcase
both the challenges and the differences when employing direct and indirectfeedback loopsbetween the task requester
and annotators. In particular, we show how the different feedback loop structures impact the quality of the created
conversation data, and address the question of which strategies should be employed to improve final data quality.
The annotators interact with an AI conversational agent via our annotation tool calledRAGAPHENE[ 22] that
has been specifically designed to support building complex conversational datasets. An example of the resulting
conversation structure is shown in Figure 1. We report on the quality of the created conversations (quality is defined in
Section 3.3) for both annotator populations for three phases: pilot, creation, and review. We evaluate our findings using
automated metrics, including the number of accepted/rejected conversations after manual review, and metrics that
compare conversation properties. We also ask participating annotators to take a survey regarding their experiences,
and compare the results between the two annotator groups in terms of their perceptions of both the tool and the task.
1https://www.mturk.com/
2https://scale.com/
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 3
Fig. 1. A sample of what the structure of a multi-turn RAG conversation looks like. This conversation has two turns (a question from
the human followed by an answer from the AI conversational agent). The first question is unanswerable and has no relevant passages.
The second question is answerable and has three relevant passages that were found from two different queries (Q1 and Q2). Both
answers were edited from the original answer by adding (green) and/or removing (red) text.
Analyzing both data quality and annotator experience helps us identify the most important aspects of the task and how
to best utilize the two annotator groups.
Our contributions are as follows:
•Insight on how different communication feedback loops impact annotator data quality from annotators, specif-
ically on complex tasks (conversational data for RAG). We find a gap between the two groups of annotators,
wherein annotators with a closer communication feedback loop created higher-quality data than annotators
who did not have a close communication feedback loop.
•Insight on how different annotator groups perceive the task and annotation tool via a comprehensive analysis
of surveys regarding their experience. We find that there are differences in how the two groups of annotators
perform the task and perceive the annotation tool; in particular, that annotators with a closer communication
feedback loop spend more time on the task and have a more positive impression of the tool.
•Concrete strategies for developing a rigorous process of data creation to handle real-world constraints. Our
suggestions can help mitigate downstream data quality issues while increasing quantity and variety, especially
in cases where one group of annotators lacks the benefits of a closer communication feedback loop.
In the rest of this paper, we first describe related work regarding RAG for LLMs, data quality in human annotations,
tooling methods, and human communication structure. In Section 3, we describe and define the task of creating and
reviewing conversations. We also define the metrics used to quantify the challenging and complexity level of the
conversations created to ensure high quality. In Section 4, we describe our methods of communication and the phases
of data collection. Finally, we end with a discussion of our insights on utilizing internal and external annotators in
Section 5, and limitations and conclusions in Sections 6 and 7.
2 Related Works
There are four primary areas related to our work: retrieval-augmented generation (RAG), data annotation quality,
tooling methods for creating conversations and communication structures. Our work is unique in that we explore how
real-world constraints on communication structures impact the quality of data annotation, specifically in the realm of
complex tasks (in our case, creating multi-turn conversational RAG data).
Manuscript submitted to ACM

4 Hanafi et al.
2.1 Retrieval-Augmented Generation (RAG) for LLMs
Retrieval-augmented generation orRAG[ 48,69] is a popular two-stage method for augmenting large language models
with passage retrieval to generate grounded responses [ 37]. In a RAG setup, AI agents generate a response to a human
query in a two-stage manner: (1) calling aretrieval modelthat returns documents relevant to the human query and (2)
calling agenerative modelthat generates a response given both the human query and the relevant documents. RAG
is an especially popular method for enterprises to augment chatbots with up-to-date or domain-specific knowledge
regarding a pre-existing corpus of documents [28].
Existing work in RAG primarily focuses on the system components in a RAG method [ 21,26,47], and benchmarks
such as multi-turn RAG datasets including RAD-Bench [ 45], RAGBench [ 25], RGB [ 12], and MTRAG [ 39]. Benchmark
datasets for RAG need to be high-quality and complex for the language model it is testing. Synthetically generating
and augmenting conversational data is a popular method, however, they are not of the same quality as human-driven
conversations [ 55,62,67,70,72]. Converser [ 35] generates conversations at a per-turn level, where the synthesized
questions do not necessarily depend on each other in the same way naturally human-generated questions unfold in
conversations. Our findings in Section 4.2.2 on the quality of synthetic data are similar. On the other hand, creating
multi-turn conversational datasets is also challenging for human annotators [ 28]. Previous existing papers that use
humans to create multi-turn RAG conversations explore hired experts [ 1,2], crowdsourcing [ 14,17,64], and hybrid [ 39]
approaches but none have analyzed the impact of the different feedback loops on the quality of the data. We describe
this process in detail in Section 3.1. Interviews with professional annotators show that the multi-layer aspect of creating
such data results in a high cognitive load for users [28, 71].
2.2 Data Quality in Human Annotation
Improving the data quality from annotators is a well-studied area, including annotation quality management prac-
tices [44], techniques involving Human-LLM collaboration [30, 77], tradeoffs between cost and quality [68], etc. Most
of these works are limited tomicro-tasks[ 38,56,68], which an annotator can complete quickly, requiring non-complex
annotator input such as labeling an image or a sentence.
Past research has pointed out the varying quality of data from different kinds of annotators [ 11,68]. Crowd annotators,
who are typically hired via crowdsourcing platforms like Mechanical Turk, can be tasked at cheaper rates and hired at a
larger scale to create large labeled datasets very quickly. However, the data may contain noisy labels and not be up to
par in terms of the quality needed for any downstream task [68].
Moreover, existing strategies for gathering high-quality annotation data from diverse crowd workers, such as
comparing the gold standard to annotators’ outputs [ 57] or breaking up the task into MapReduce-like tasks [ 43], are not
applicable for the complex task of creating conversational data for RAG. Creating a conversation is a mix of a creative task
and a generative task, with the final output judged by metrics far different than simple accuracy or precision/recall [ 39].
The task of creating conversational data for RAG is essentially anon-decomposable macrotask[33, 66], where the task
simply cannot be broken up and given to crowd workers in parallel to complete without degradation of the final
conversation quality [28].
There have been past works [ 4,18,40,75,76] that tasked different groups of annotators of varying qualities. In
Wang et al.’s work [ 75,76], an existing dataset was evaluated by two groups of annotators: an expert group and a
more diverse “general” group, referred to as thecrowd group. While they found that expert annotators and the general
annotators disagree on what parts of the data are considered safe in the context of AI safety, it is not clear what the
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 5
impact of differing communication feedback structures between the two such annotator groups had on the final data.
They observed differences in labeling between the two different groups of annotators and argued that it is not linked to
a definition of accuracy (or an error) but rather a difference in domain expertise, where the expert annotators bring
in-depth knowledge regarding institution specific policies and crowd annotators bring in diverse perspectives from their
sociocultural background. We build on top of this notion to help us utilize the different strengths we identified in our
two groups of annotators with different communication feedback structures to aid us in the complex and cognitively
demanding task of creating conversational data for RAG.
The usage of a multi-stage data creation process with human annotators has been used across many different
applications. In Asher et al.’s work [ 4], conversational data was collected during multi-player game sessions, and then,
as a separate process, novice and expert annotators annotated the conversational data. However, this work did not
study differences between the novice and expert annotators and their impact on the final data. In Lu et al. ’s work [ 79], a
two-stage process involving crowd annotators was used to complete design tasks. They showed that even if the crowd
annotators do not possess a high level of expertise, they can search and utilize expert-generated resources to accomplish
design tasks. However, this work is also limited to a certain kind of task and cannot easily be generalized to more
complex tasks like creating conversational data for RAG. We take an extra step further with our exploration of a similar,
multi-stage process involving our two groups of internal and external annotators: we explore in depth how our two
groups of annotators, each with differing communication feedback structures impact the quality of the final data and
how we overcame barriers introduced in different communication feedback structures.
Ensemble [ 40] proposes a framework for dividing certain tasks, such as providing goals, merging tasks, generating
ideas, doing detailed edits, etc., that are assigned to either leaders or crowd collaborators. They studied a hierarchical
type of communication structure between leaders and crowd collaborators for a creative type of task, specifically
storytelling. However, their insights and their proposed framework for dividing up work cannot easily be extended to
the task of creating conversational RAG data, as this task cannot be divided up into smaller pieces without compromising
the quality of the whole conversation (such as disrupting the natural flow of the conversation). We also acknowledge
similar lines of work to Ensemble, that belong in the realm ofonline collaboration, including similar tools for online
collaborative document editing [ 7], computational notebooks [ 31], etc. Our toolRAGAPHENE[ 22], is, by nature, not
meant to support online collaboration, but rather the online aspect is in the human communication with the agent as
part of the process of creating a conversation. The different annotator groups collaborate on the same conversation
object in an offline fashion, in a sequential manner, given the complexity involved in the task.
One technique we borrow from the literature is utilizing self-assessment and expert feedback [ 16,18,32], which has
been proven to help annotators improve their work. We show how critical this strategy was in improving the quality of
the created conversational RAG data.
2.3 Tooling Methods for Creating Conversational Data
There is a multitude of previous work involving tooling for creating and annotating conversational data, [ 42,49,54,60,81].
Some works have explored gamified ways of creating conversational data and annotating it, such as Manuvinakurike et
al.’s work [ 54], where they use a gamified crowdsourcing approach to gather conversational data, involving utterances
between the players. However, their gaming platform cannot be easily extended to more complex forms of data structures
(such as in our conversational RAG dataset) since the gameplay involves players identifying a target image amongst
other images displayed on the screen.
Manuscript submitted to ACM

6 Hanafi et al.
Ogawa et al. uses Minecraft3, an online sandbox building game, as a way to gamify the process of creating such
complex data structures [ 60]. However, the conversational data created from this tool is a simpler structure than one
involving RAG, as it consists of utterances between the players, and each utterance may be assigned classification labels.
A social aspect is integrated into the system via a ranking system of the players to incentivize players to annotate the
conversational data. We also borrow these ideas and explore the usage of annotators commenting on each other’s work
to learn and improve the final quality of the created and annotated data (see Section 3.2).
Moreover, past work has not yet explored the creation and annotation of conversational data for RAG, which is
more complex given that a set of relevant passages is attached at each of the AI agent’s turns and the answers must be
faithful to those passages. The added complexity at the agent’s turn has been shown to bring annotators to a state of
cognitive overload [71] while creating and annotating conversation data for RAG [28].
2.4 Human Communication Structure
Prior work explores the impact of different communication structures on a process, such as Cataldo and Ehrlich’s study
on communication between coworkers in product development [ 9] or Erete and Burrell’s study on communication
between citizens and leaders in local government participation [ 20]. In both of these works, a longitudinal study was
employed to capture how different communication structures between people impact certain outputs in the process.
In Erete and Burrell’s study of communication and the role of technology across three different local communities,
with different communication of email, discussion boards or instant messaging, they observed that technology use
helps citizens amplify their concerns and bring accountability to local leaders, but it does not necessarily increase
their political power. Cataldo and Ehrlich’s work studied hierarchical structures (manager →report) versus smaller
structures (peer↔peer), where they found that hierarchical communication structures have a bigger impact on product
delivery than smaller communication structures. While these insights from the different longitudinal studies are useful
for their particular niches, it is not clear how these insights extend to the impact of data quality when the annotator
groups have different communication feedback structures (direct or through an intermediary).
There is extensive research on the concept of feedback in the HCI literature, including teacher-to-student feedback
in classroom settings [ 5,15,65], methods for improving and reusing feedback from novices [ 51,58,80], peer-review
feedback [ 8,73], and feedback between peers and experts [ 13,59]. While many of these works use the term feedback to
mean improving one’s understanding of a particular task, none of these works explore what it means when that feedback
is disjoint, as in our case with our external annotators, where all communication is done through an organization’s
intermediary. In our work, we show how differences in feedback loop structures have a significant impact on the final
output data.
3 Tasks
Multi-Turn Retrieval-Augmented Generation (RAG)is the task of grounding an AI agent’s response to each human
input in the conversation using relevant passages. This is achieved by accessing relevant passages via retrieval of the
provided corpus. The corpus can be the entire web, or a smaller domain-specific corpus that focuses on one topic (e.g.
finance or tech). Each question-passages-answer pair asked by the human, retrieved from the corpus, and answered by
the agent is considered aRAG turnin a conversation. The general structure of a RAG conversation can be found in
Figure 1.
3https://www.minecraft.net/en-us
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 7
1
23
4
(a) A view of the agent’s response. The overlap button (1) highlights texts that
overlap between the passage and the agent’s response in yellow. (2) The annotator
must mark passages as relevant/irrelevant to the turn. (3) The annotator can
click on the regenerate button after changing the passages (adding or deleting
passages from requerying). (4) Hints appear at the top of the conversation if it
detects quality issues such as passage diversity, passage relevancy labels, missing
enrichments, low number of edits, etc.
(b) Requery tool for finding passages to attach
to the turn.
(c) The annotator edits the agent’s response (top half is the edited
response). When the annotator edits the agent’s response, a visu-
alization of the differences between the original agent’s response
and the edits is shown (bottom half contains differences).
(d) Enrichments on the user’s text.
Fig. 2. View of creating a conversation inRAGAPHENE, when an annotator creates an initial message, “I am looking for a good
program to study financial planning that does not require test scores to enroll”.
Manuscript submitted to ACM

8 Hanafi et al.
Fig. 3. Checklist that an annotator has to go through before exporting the created conversation.
The primary steps involved in creating a RAG conversation given to our annotators are: (1) creating a conversation
and (2) reviewing a conversation.RAGAPHENE[ 22] is a chat-based annotation tool that enables users to simulate
real-world multi-turn conversations. In addition to the ability to create and review conversations it has several key
features that made it ideal for our use case: real-time retrieval and generation and the ability to edit responses. We
describe how these tasks are accomplished usingRAGAPHENEin the following sections. Screenshots describing how
we utilized the annotation tool are shown in Figure 2.
3.1 Task: Creating conversational datasets for RAG
The task of creating multi-turn RAG conversations is a complex and time-consuming task [ 28]. The process for creating
a single high-quality complex conversation occurs per turn. We describe how an annotator, Alice, creates a single turn
for a conversation:
(1)Create a question: Alice writes a question as shown in Turn 1 of Figure 1. An example of a question is shown
in Figure 2d.
(2)Receive a response: Alice’s message triggers a response from the AI agent as shown in Turn 1 of Figure 1. The
AI agent invokes the two-step process as part of RAG to generate a response: (1) invokes the retrieval model,
which then retrieves relevant passages based on Alice’s text, (2) invokes the generator model to output a response
based on both Alice’s text and the retrieved relevant passages. An example of a response is shown in Figure 2a.
(3)Review the passages from the retriever model. This step involves two primary annotator actions (as shown
in Figure 2a):
(a)Review the attached passages:RAGAPHENEreturns at least three relevant passages for each AI agent’s
generated response. Alice reviews the retrieved passages to ensure they are relevant and useful for answering
Alice’s question. A document isrelevantto a turn in the conversation if it is useful to the generator model
for generating a response to the human’s message. An annotator can mark the retrieved documents from the
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 9
retrieval model as either relevant or irrelevant. Figure 1 shows relevant (green) and irrelevant (red) passages
for the two turns. To help her in comparing the passages with the generated AI agent’s response, she clicks on
theoverlap iconinRAGAPHENE. The overlap icon highlights the texts that overlap each other in both the
passages and the AI agent’s response (also shown in Figure 2a).
(b)Search and attach new passages: Arequeryis an annotator action that queries the AI agent’s retrieval
model for relevant documents based on annotator-specified queries (see Figure 2b for the requery tool in
RAGAPHENE). Requerying allows Alice to independently search for relevant documents beyond those that the
AI agent initially retrieved. Alice must be cognizant of the primary goal of creating a high-quality conversation
as she uses the requery tool: It is important to query with the goal of increasing the coverage of relevant
passages for the question, including querying using answer words. Turn 2 in Figure 1 has 2 queries.
(4)Update the generator’s outputto reflect the latest changes in the attached passages. Alice can click on the
regenerate button inRAGAPHENE(shown in Figure 2a) to regenerate the AI agent’s response so that it is based
on the latest changes in the attached passages.
(5)Edit the latest response from the generator model.Aneditis an annotator action that modifies the AI agent’s
response. Alice edits the AI agent’s turn inRAGAPHENE(shown in Figure 2c). Answer text can be improved
by removing (red) or adding (green) text as shown in Figure 1. The AI agent’s response must be grounded in
the retrieved relevant passages, including the passages that Alice added while using the requery tool. Alice
checks the response for (a) itsfaithfulness, or groundedness, to the relevant passages, (b) itscompleteness(the
response is not missing any information that can be found in the attached passages), (c) itsstyle(the response
is not awkwardly styled), and (d) itsappropriateness(the response is useful and concise for answering the
question).
(6)Enrich the conversation. Enrichmentsare custom labels at each turn of the conversation, such as “Answerable”
or “Unanswerable”, indicating whether a turn is answerable or not (see enrichments in Figure 1 and 2d). Many
custom enrichment types can be supported. Alice enriches the turns with task-specific labels, as specified by the
instructions given to her. Enrichments are useful for any downstream tasks that ingest the final conversational
data.
(7)Repeat steps 1 to 5 for the remaining turns.As Alice moves on to the next turn,hintsor tooltips appear at
the top of the screen inRAGAPHENEwhenever there are missing labels, no enrichments, or not enough diverse
passages (an example hint is shown in Figure 2a). Alice is done creating all the turns in her conversation when
she has met the minimum required length specified in the task instructions. She then exports the conversation in
a JSON format. Before she can export the conversation,RAGAPHENEwill show a checklist, which serves as a
final reminder of the properties the conversation needs to meet before exporting (see Figure 3).
The task of creating a conversation cannot be broken up into smaller sub-tasks without compromising the overall
quality and natural flow of the conversation [ 28]. The annotators can "undo" a question if it reduces the flow, passage
diversity and/or complexity of the conversation: 1) A question that is immediately answered correctly by the generator
and doesn’t need any edits can be discarded for lack of complexity; 2) A question whose answer is already part of a
prior answer can be discarded for lack of passage diversity; 3) An unanswerable question that follows an unanswerable
question can be discarded for poor conversation flow. As a conversation proceeds, the annotator cannot go back and
change the questions or passages that occurred in previous turns because it will disrupt the flow of following questions.
Manuscript submitted to ACM

10 Hanafi et al.
(a) Annotator comments inRAGAPHENE’s review mode.
 (b) Automatically generated comments inRAGAPHENE’s review
mode.
Fig. 4. Examples of annotator and automated comments in review mode.
For quality assurance, we designed a review task, where conversations are reviewed and commented on by other
annotators.
3.2 Task: Reviewing a conversation for RAG
Reviewinga conversation is an annotator task involving addingcommentsregarding the quality of the conversation
andeditingthe conversation to improve it when possible. Due to the complex nature of the task, all created conversations
must go through the review process. In review mode, questions cannot be changed and passages cannot be added as it
would disrupt the flow of the conversation. The annotator can only change the relevance of passages, edit the answers,
change enrichments and provide feedback via comments. Comments are objects attached to the full conversation or at
the turn level to be consumed by annotators as feedback to improve the quality of future conversations. We now walk
through how an annotator, Bob, would review the conversation Alice has created, which he views through the review
mode inRAGAPHENE.
Review flow: Bob reviews the general flow of the conversation, ensuring that it flows naturally and coherently. We
explore two scenarios:
•Scenario 1: Bob notices that the conversation talks about enrolling in courses for financial planning, which then
naturally moves to the topic of the cost of attending such courses.
•Scenario 2: Bob notices that out of the 4 questions in the conversation, 3 of them are unanswerable.
Review complexity: A conversation iscomplexif the AI agent finds the annotator’s turns challenging to respond
to, which results in the AI agent generating a response that may be incomplete or wrong. This then results in the
annotator needing to edit the AI agent’s response. We quantifyconversation complexitythrough the number of
turns that were edited by the annotator. An automated comment is provided to help measure the number of edits
(Figure 4b). Bob reviews the total number of turns that Alice has edited. The total number of edited turns informs Bob
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 11
how complex Alice’s turns were for the AI agent to answer. A high number of significant edits is desirable for a complex
and high-quality conversation.
•Scenario 1: 3 out of 4 turns have significant edits.
•Scenario 2: 1 out of 4 turns has a minor edit fixing a grammar mistake.
Review diversity: Bob then reviews the passages to ensurepassage diversity, where each turn should ideally have
different (i.e., unique) passages attached. An automated comment regarding the number of unique relevant passages is
provided to help measure diversity. Generally, it is allowed to have some passages reappear in other AI agent turns
in the same conversation. However, it is desirable for ahigh-quality conversationto have the properties ofpassage
diversity, in which new passages appear at different turns, all the while maintaining thenatural flow of conversation,
i.e. the conversation topic does not seemingly jump between random goals nor do the turns appear disjoint from each
other (just to make the passages diverse).
•Scenario 1: Out of 15 total passages attached to a conversation with 4 turns, 9 passages were unique.
•Scenario 2: Out of 4 total passages attached to a conversation with 4 turns, 2 passages were unique.
Review annotations: Bob also reviews whether Alice marked the passage relevance correctly and enriched the
turns properly. Bob directly fixes any enrichments or labels that were given erroneously. Bob also adds a comment
justifying his changes for the task requester and Alice to view at a future time (as shown in Figure 4b).
Accept or reject the conversation: Bob can accept the conversation if the conversation ishigh qualityandcomplex
as shown in Scenario 1. Bob can reject the conversation if this is not the case as shown in Scenario 2. As soon as Bob
notices that the conversation needs to be rejected he doesn’t have to review the full conversation further.
3.3 Metrics
In this section, we describe the metrics from our prior work [ 39] that we used to measure the quality of the multi-turn
conversations from the two annotator populations. We quantify conversation quality based on thecomplexity and
challenginglevel of the conversation:
•Average number of turns: The length of the conversation, computed as the average number of turns in the
conversation. Later turns in the conversation tend to be more challenging for the retriever and generator [39].
•Average number of edits: The average number of turns that have been edited/repaired by the annotator
by adding or removing text from the original answer produced by the generator. A conversation that is more
complex will have more edits, indicating it was a challenge for the generator.
•Average number of queries: The average number of times the annotator queried in the conversation. This
includes the initial question and additional re-querying performed per question. We consider this as a measure of
complexity for the retriever. A conversation that is more complex will need more queries to find more relevant
passages.
•Average number of unique passages: The average number of unique relevant passages for all turns in the
conversation. We consider this as a measure of passage diversity. A conversation is not diverse if it has few
unique passages and a conversation is diverse if it has many unique passages. A conversation that is more
complex will be more diverse.
The higher the value of all of these metrics, the more likely it is considered acomplex and challenging conversation
which makes it a higher quality conversation for evaluating retrievers and generators. These metrics are a proxy for
Manuscript submitted to ACM

12 Hanafi et al.
Group PIDAnnotation
ExperienceTechnology
FamiliarityAI
BackgroundRAG
BackgroundAge (Years) Gender# Created
Conv
ExtExt1 Less than 1 year 50 to 60 Female 75 to 100
Ext2 1 to 3 years 40 to 50 Female 100+
Ext3 1 to 3 years 18 to 30 Female 100+
Ext4 1 to 3 years 40 to 50 Female 100+
Ext5 1 to 3 years 18 to 30 Female Less than 25
Ext6 1 to 3 years 30 to 40 Female 25 to 50
Ext7 1 to 3 years 40 to 50 Male 100+
Ext8 1 to 3 years 40 to 50 Male 50 to 75
Ext9 1 to 3 years 30 to 40 Female 75 to 100
Ext10 1 to 3 years 30 to 40 Female 75 to 100
Ext11 1 to 3 years 30 to 40 Male 50 to 75
Ext12 1 to 3 years 30 to 40 Female 100+
Ext13 1 to 3 years 50 to 60 Female 100+
Ext14 1 to 3 years 40 to 50 Male 75 to 100
Ext15 1 to 3 years 30 to 40 Female 100+
Ext16 1 to 3 years 50 to 60 Female 25 to 50
Ext17 3 to 5 years 30 to 40 Female 100+
Ext18 3 to 5 years 50 to 60 Male 75 to 100
Ext19 3 to 5 years 18 to 30 Female 100+
Ext20 3 to 5 years 18 to 30 Male 25 to 50
Ext21 5 to 10 years 18 to 30 Male 25 to 50
Ext22 10+ years 30 to 40 Male 50 to 75
Ext23 10+ years 40 to 50 Female 100+
Ext24 10+ years 30 to 40 Male 50 to 75
IntInt1 1 to 3 years 60+ Female 25 to 50
Int2 1 to 3 years 18 to 30 Male 100+
Int3 5 to 10 years 60+ Female 25 to 50
Int4 5 to 10 years 60+ Female 75 to 100
Int5 10+ years 60+ Female 100+
Int6 10+ years 60+ Female 100+
Int7 10+ years 60+ Female 75 to 100
Table 1. Demographics of all 7 internal annotators (IntAnno) and 24 external annotators (ExtAnno). Each bar represents a rating
on the Likert scale, i.e. 1 bar represents “Beginner” and 5 bars represent “Expert”. # Created Conv denotes the reported number of
conversations the annotator has created at the time of collecting the demographics after 1 3/4 years of the task.
quality though it may sometimes not be the case, particularly along one dimension; e.g. a conversation can have many
turns and still be of poor quality. In Section 4.2 we show that these metrics align with the human understanding of
conversation quality based on the accept/reject rate of conversations. Further details about these metrics and their
motivation can be found in [39] .
4 Method
The task of creating multi-turn conversations was carried out during an approximately one-year period using the
two annotator populations of different feedback loops: internal and external. The populations consisted of 7 internal
annotators and 40 external annotators. In this section, we describe the annotator’s communication structures and
demographics (Section 4.1) and the method for data collection during the longitudinal study (Section 4.2).
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 13
4.1 Annotators
Our study explored two groups of annotators: 7 internal and 40 external annotators. Internal annotators were from the
same company as the researchers of this paper who created and defined the task. External annotators were recruited
through an external, professional annotation company.
4.1.1 Communication feedback structures and material.We did not have the ability to directly communicate with the
external annotators. All communication was done via an intermediary represented by the external annotation service.
Task instructions and materials, such as tutorials, including videos and slide decks, were sent to external annotators
through the intermediary. We met with the intermediary weekly to discuss progress and share feedback. The annotators
were able to send us questions through the intermediary but they could not ask us directly. Responses and feedback
were returned in a batch to the intermediary who could then choose how to disseminate the feedback to the annotators.
In contrast, we communicated directly with the internal annotators through e-mail, instant messaging in Slack groups or
direct messages and video calls on Microsoft Teams and shared folders for all material. The annotators were able to ask
questions and receive immediate and personal replies and feedback. We met with the internal annotators individually
and in a group frequently, as needed.
4.1.2 Demographics & Skill Level.We collected detailed information about the annotators demographics and level of
practice after all three primary tasks were given to both groups of annotators (see Table 1). We use the termlevel of
practiceto refer to the annotator’s skill level in the RAG conversational creation task. It is important to note that while
an annotator may have created a lot of conversations, it does not necessarily translate to them being good/skilled at the
task (hence the term level of practice).
Given the nature of our communication structure with the external annotators, only a subset of the 40 external
annotators who participated in the data creation tasks also chose to complete the survey. We asked the annotators their
gender, age, years of professional annotation experience, and additional questions regarding their technical background
(rated over a Likert scale from 1 to 5):
•Technology Familiarity: “On a scale of 1-5, would you say you are familiar with most common computer programs
and online tools?”, with 1 being “Not familiar at all” and 5 being “Extremely familiar”.
•AI Background: “On a scale of 1-5, with 1 being beginner and 5 being expert, how would you rate your under-
standing of AI?”
•RAG Background: “On a scale of 1-5, with 1 being beginner and 5 being expert, how would you rate your
understanding of RAG?”
All our professional annotators exhibited the same diversity that previous research [ 4,18,40,75,76] describes.
Compared to the internal annotators, more external annotators report having a competent (3 on a Likert scale)
understanding of AI and RAG. The vast majority of the external annotators (16/24) report having less than 3 years of
annotation experience, while a majority of internal annotators (5/7) have at least 5 years of annotation experience. Our
internal annotators generally have more experience in annotation and are much older (60+ years old) than the external
annotators.
4.1.3 Compensation.All annotators from both groups are considered professional annotators and were compensated
well above minimum wage for their annotation expertise. Internal annotators were paid hourly and external annotators
were paid per accepted conversation (low-quality conversations may lead to a rejection or a redo of the task without
Manuscript submitted to ACM

14 Hanafi et al.
MetricsPilot Creation Synthetic
External Internal External Internal Pre-Review
Num conversations 33 53 491127 200
Avg num turns 4.0 3.7 4.27.6 5.9
Avg num edits 1.1 1.5 3.07.0 -
Avg num queries 1.0 1.0 6.212.7 NA
Avg num unique passages 4.5 4.0 7.317.1 4.6
Table 2. Comparison of conversations per task and annotator group for the Pilot and Create phases
MetricsExternal Internal Synthetic
Pre Post Pre Post Pre Post
Num conversations 251 173 127 110 75 53
Avg num edits 2.9 3.4 7.0 7.3 - 2.0
Relevant yes 10.1 8.4 20.0 19.4 - 11.2
Relevant no 5.4 5.2 8.7 9.4 - 13.9
Table 3. Comparison of the statistics pre- and post- Review Phase for the main task
additional pay). The pay structure, which is an important aspect of the feedback loop, may have impacted the quality
of the work, as previous studies have shown that differences in financial incentives impact participant recruitment
[24, 63].
4.2 Phases of Data Collection
The total number of conversations created was approximately 1500 by the internal annotators and 5000 by the external
annotators. In this section, we describe and report results of our longitudinal study for both annotator populations on a
subset of the data for the three phases of the task: (1) pilot phase, (2) creation phase, and (3) review phase. Our findings
are shown in Tables 2 and 3. We also explore synthetic data as an alternative to human annotations (Section 4.2.2).
4.2.1 Phase: Pilot.The pilot task was a small-scale experiment to measure whether the task and instructions were
clear and to calibrate the task as needed. The statistics from the pilot are shown in Table 2. In this task we reviewed
the conversations from both annotator populations to gain insights into how the annotators performed the task. The
initial feedback was the same, the conversations were not complex enough and too easy for the agent. There was on
average one edited answer and only one unique passage per question for the entire conversation. Our feedback was to
ask questions that diversify the relevant passages and that the questions should be challenging enough that the initial
answer provided by the agent should need to be edited. A screenshot comparing what a question looked like in the
pilot changed to a more complex question is shown in Figure 5.
4.2.2 Phase: Creation.Following the review of the pilot we shared feedback with both groups of annotators by updating
the slides to improve clarity and describe the desired complexity. We also provided several examples showing how
to improve existing conversations. We had virtual meetings with the internal annotators and updated the additional
instructions provided to the external annotators. Further, we added additional features to the tool to improve the
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 15
OriginalTurn 2New Turn 2
Turn 1
Fig. 5. Comparison of original turn and more complex alternative. The original turn 2 was not complex. It provided the same three
passages and repeated a lot of the same information in the answer for Turn 1. The New Turn 2: 1) is more ambiguous because it is a
keyword, 2) it requires a significant amount of editing, and 3) it required re-querying which was used to find a new relevant passage
as shown.
tooling and encourage more complex conversations. We added hints containing tips and an export checklist as shown
in Figures 2a and 3. The annotators then began creating new conversations that were more complex.
A small pilot was provided with some real-time and follow-up feedback to the internal annotators and then the full
task began. A comparison of conversations created during the complexity task is shown in Table 2. The number of
conversations for the external annotators is following a review by the QA team of the external provider where 309/800
conversations were rejected. Since the external annotators are a larger population group they can create considerably
more conversations. However, the internal annotator conversations are longer, have more edits, and considerably more
querying and unique passages than the external annotator conversations.
An LLM can also be used to generate conversations as an alternative to human-generated conversations. The
advantage is clear: an LLM can generate significantly more conversations in a very short span of time. We leverage
state-of-the-art techniques [ 46] to generate synthetic conversations over our corpora and compare them to the man-
ually created conversations in Table 2. The synthetic conversations are of reasonable length, but not as long as the
conversations of the internal annotators as the conversation tends to degrade as it gets longer [ 46]. These conversations
also have less passage diversity than the conversations from both human populations. There are no edits or requerying
by the synthetic data generator so the quality may be lower which we will analyze in the next section.
4.2.3 Phase: Review.Following the completion of the task of creating conversations, a review phase was performed. All
conversations went through an automated review where comments were generated automatically to address passage
diversity, amount of edits, and missing enrichments. A conversation is considered to have passage diversity if for the
amount of Questions, 𝑄𝑛, the number of unique relevant passages, 𝑃, is𝑃>=(𝑄 𝑛−1)×2. So a conversation with
3 questions should have at least 4 passages and a conversation with 5 questions should have at least 8 passages. We
Manuscript submitted to ACM

16 Hanafi et al.
(a) Self-reported average time it takes for annotators to
create a single conversation.
(b) Self-reported average number of passages annotators
read per turn when creating conversations.
Fig. 6. Survey results illustrate the difficulty and time it takes to create a single RAG conversation.
(a) Self-reported understanding of RAG (on a Likert scale of 1
“Beginner” to 5 “Expert”).
(b) Self-reported number of conversations created.
Fig. 7. Order of annotator actions per turn when creating a conversation. The correct order of actions are as follows: (A) Create a
turn, (B) Edit the outputs of the retriever model, and (C) Edit the latest response from the generator model. Counts are grouped
based on the feedback structure (internal/external).
identified both major and minor edits by computing the Rouge-L score using HuggingFace4between the original and
edited response. Major edits were more important than minor changes (e.g. typo fixes). An example of the automated
comments is shown in Figure 4b. These comments were used to help guide the annotators during manual review.
Next, the internal annotators manually reviewed the conversations from both annotator pools and synthetic conver-
sations. In some cases, this was a subset of the annotations created. The review provides the ability to accept, reject
or edit and accept the conversations and provide comments as feedback. 87% internal, 69% external and 72% of the
synthetic conversations were accepted. The repair during the review increased the number of edits and decreased
the number of relevant passages for all populations. Even though a nice amount of the synthetic conversations were
accepted they were considerably less diverse than the human conversations as passages can not be added during the
review phase. The accepted conversations created and then reviewed by the internal annotators during this phase were
used in our released MTRAG [39] benchmark. All other accepted conversations can be used as future training data.
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 17
4.3 Survey
We surveyed both groups of annotators in February 2025, after all three major tasks had been given to them. We
wanted to understand how the different feedback structures impacted the quality of the created data. We also wanted to
understand how the different feedback structures impacted how they performed and perceived the task.
To quantify their skill level in the complex task of creating RAG conversations after the three tasks: we asked them,
“Approximately, how many conversations have you created using Workbench?” (see Table 1). 15/24 external annotators
and 5/7 internal annotators reported that they created at least 75 conversations.
4.3.1 Feedback structure impacts the duration and effort to complete the task.To understand how the different feedback
structures impacted the duration and effort it took for them to create a single conversation, we asked annotators:
•“Approximately, how long (how many minutes) does it take to create a single conversation?” Analysis of the
time it takes to do the task is shown in Figure 6a.
•“Approximately, how many passages do you feel you read at each turn when you create a conversation?” Analysis
of the average number of passages annotators read per turn when creating a single conversation inRAGAPHENE
is shown in Figure 6b.
Most of the external annotators reported that it took them less than 45 minutes to create a single conversation,
while all of the internal annotators reported that the task took atleast 45 minutesto complete. The internal annotators
remarked on the time it took to read the passages at each turn, as Int4 says, “Way too much reading and re-reading is
involved.” and Int3 says, “It takes time to carefully read.” Interestingly, no external annotators have expressed such
concerns in the survey. While the instruction materials emphasized reading and verifying all of the attached passages,
the distant feedback structure between the task requester and the external annotators influenced the reported speed
and care the external annotators took to deal with core sub-tasks in creating a RAG conversation.
We also observe a similar disparity in theireffort, or self-reported number of passages that are read per turn when
creating a conversation (see Figure 6b). External annotators, on average, read fewer passages per turn than the internal
annotators. Most of the external (15/24) and internal annotators (3/7) reported to have read an average of 6 to 12
passages per turn. We emphasize in the instructions that they need to read all of the attached passages, 3 of which are
the default number of passages thatRAGAPHENEattaches to the response after the annotators create a turn. They
are also required to requery and read (or at a minimum skim) the additional passages from requerying. Each requery
returns 9 passages ranked by the retriever for relevance to the requery words that were used. Hence, the total read
passages at a turn should be around 12 if an annotator indeed read all 3 attached passages in the response and passages
from several requeries.
4.3.2 Feedback structure impacts the order of annotator actions in the task.We wanted to understand the impact of the
different feedback structures on their understanding of how one should create a turn in a RAG conversation, specifically
the order of annotator actions in a turn when creating a conversation. We asked annotators to order the actions they
perform when they create a turn in a conversation. Ideally, the following should be the order of actions: (A) Create a
question, (B) Edit the outputs of the retriever model (which includes verifying the retrieved passages, requerying, and
labeling the attached passages), (C) Edit the latest response from the generator model (which includes regenerating the
agent’s response and editing it). Enriching the turn can occur at any point in time. See Figure 7, where we grouped the
4https://huggingface.co/docs/evaluate/en/choosing_a_metric
Manuscript submitted to ACM

18 Hanafi et al.
counts based on the feedback structure (internal/external) and their self-rated understanding of RAG (on a Likert scale
of 1 to 5, with 1 being “Beginner” and 5 being “Expert”).
We observe that none of the internal annotators reported an understanding of RAG greater than 4 (“Proficient” and
“Expert”) while all reported the expected order of annotator actions. On the other hand, some external annotators
reported a more proficient understanding of RAG while also reporting an incorrect order of annotator actions when
creating a turn in a RAG conversation. Some of the external annotators also have reported a technical background
with a proficient background in AI (see Table 1), which we confirm as either formal training in school or courses in AI.
However, even with such a background, it is clear how the impact of a different feedback loop affects the annotator’s
performance in completing complex tasks.
We also charted the number of annotators with correct and incorrect order of annotator actions, and grouped
those counts based on their feedback structure and their skill level in creating RAG conversations (i.e., the number of
conversations they created) (see Figure 7b). 14/24 external annotators created more than 50 conversations and reported
the correct order of annotator actions, indicating that skill level and practice have also helped them understand how
to correctly complete the complex task. However, for some of the external annotators, a higher skill level does not
translate to reporting the correct, expected order of annotator actions; 10/24 external annotators created more than
100+ conversations, but 3 of them did not report the correct order of annotator actions.
4.3.3 Feedback structure impacts annotator perceptions of the influence the tool features have on data quality.We
also asked the annotators regarding their perception of different tool features and their impact on the quality of the
conversation if we were to remove that feature fromRAGAPHENE. Specifically, we asked them, “On a scale of 1 to 5, if
we were to remove the ability for you to use <INSERTRAGAPHENEFEATURE>, how would this decrease the quality
of the conversations you create?”, where 1 is “No decrease in quality at all” and 5 is “Extreme decrease in quality”. We
computed the absolute mean difference (𝜇difference) between the two populations (see Table 4).
There were larger differences between internal and external annotators on the tool’s hints ( 𝜇difference = 1.41),
which are pop-ups that appear as the annotator creates the RAG conversation (see Figure 2a). Hints appear on demand
when the tool detects issues in the data quality, such as missing annotations or low counts of diverse passages with
respect to the number of turns in the current conversation. Intuitively, removing hints from the tool would make the
already-complex task even harder, especially when one group of annotators has an indirect feedback loop with the task
requesters.
There were also larger differences between internal and external annotators regarding the tool features for changing
the retrieved passages, specifically requerying ( 𝜇difference = 0.78), marking passages relevant/irrelevant ( 𝜇difference =
0.78), and the yellow highlights that mark texts overlap in the passages and the generated response ( 𝜇difference =
0.75). The external annotators did not view changing the passages as important as the internal annotators. This is also
reflected in the disparity between the self-reported number of average passages the external annotators read per-turn
(see Figure 6b) as well as the average number of unique passages that were actually marked as relevant (see Table 2),
suggesting that the disjointed feedback structure made them not notice the importance of editing the retriever model
outputs at the same level of consciousness as the internal annotators did.
On the other hand, editing the generator model’s outputs is viewed equally important by both internal and external
annotators ( 𝜇difference = 0.04). An external annotator, Ext24, says, “Without this functionality, it would often be
impossible to generate conversations according to the instructions.” An internal annotator, Int2, also echoes the same
sentiment, “Sometimes the initial agent answer has too much additional/unnecessary information.”
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 19
Tool FeatureIntAnno ExtAnno 𝜇
Difference𝜇 𝜎 𝜇 𝜎
Hints 4.290.76 2.88 1.42 1.41
Requery Tool 4.570.53 3.79 1.50 0.78
Marking passages relevant/irrelevant 3.861.07 3.08 1.50 0.78
The overlap icon highlights overlapping text in the passage and response 4.710.49 3.96 0.91 0.75
Regenerating the agent response 3.86 0.90 4.171.01 0.31
Enriching the questions 2.57 1.33 2.791.28 0.22
Checklist before export 3.871.21 3.71 1.40 0.16
Editing the agent responses 4.290.76 4.25 0.85 0.04
Table 4. Survey results (conducted after all three tasks given to both groups of annotators) about tool features rated by 7 internal
annotators (IntAnno) and 24 external annotators (ExtAnno) based on their impact on the quality of created conversational data. A
higher average value ( 𝜇) indicates a bigger impact on conversational data quality (1 is “No decrease [in data quality] at all” and 5 is
“Extreme decrease in quality”.).𝜇Difference is the absolute difference between the two means from both groups of annotators.
5 Discussion
In this section, we distill insights from utilizing two groups of annotators with different feedback loops. We believe
these practices can be applied to other tasks that utilize two populations for annotation, particularly when the task is
complex and cannot be broken up like traditional micro-tasks [ 56]. Our insights touch on various aspects of the overall
conversational RAG data creation process, specifically task design (Section 5.1 and 5.2), tutorial materials (Section 5.3),
and tooling (Section 5.4).
5.1 Assign Creation and Review Tasks According to Annotator Group’s Feedback Loop to Increase
Throughput
Unlike micro-tasks, which are common in crowdsourced tasks [ 56], complex tasks like creating conversational RAG
data cannot be broken up into smaller tasks without compromising the quality and the overall natural flow of the
conversation [ 28]. Moreover, the real-world limitation of an indirect type of feedback loop where the taskers cannot
directly communicate with the crowd workers may decrease the quality of the created conversation. Given these two
real-world constraints on task complexity and feedback annotator loop, we instead designed an additional, separate
task, a review task, for the group of internal annotators who have a direct feedback loop with us (taskers). This resulted
in the two-phase process as described in the data collection (Section 4.2), which we also observed in the literature,
where creative tasks such as story writing cannot be broken into smaller tasks [ 6,41]. Instead, creative tasks are ordered
in a sequential flow of creation and review of writing tasks [6].
Moreover, our study and survey results indicate that the direct and frequent feedback provided to the internal
annotators helped them produce higher-quality conversations that are more complex, i.e., annotators attached a
diverse set of passages to the conversation turns. However, it takes a considerable amount of time for them to create
conversations, and there are fewer internal annotators. It took the internal annotators an average of 60-75 minutes
to create a conversation and 30-45 minutes to review a conversation compared to the external annotators who took
an average of 30-45 minutes to create a conversation. It is important to take advantage of both annotator pools to
increase throughput. The findings in our review phase highlight that conversations can be continually improved. We
Manuscript submitted to ACM

20 Hanafi et al.
propose a two-phase process where external annotators create the conversations, which will increase throughput
and improve question diversity, and follow up with a review phase by the internal annotators to improve the quality
of the conversation. A separate review task [ 82], or an expert review [ 16], is not uncommon; however, most quality
assurance strategies in crowdsourced tasks often employ filtering strategies [ 16,32] (using qualification tests to filter
out annotators), which is not possible in our setting. In our case, our group of expert annotators are characterized
by their feedback loop, i.e., direct communication with the taskers as opposed to indirect communication through an
intermediary, which has not been previously studied.
5.2 Provide Targeted Tasks to Increase Data Diversity
In the crowdsourcing literature, there is a strong emphasis on breaking up tasks to reduce human error [ 16,43] but
these best practices cannot be easily applied to non-decomposable macrotasks [ 66] such as creating complex RAG
conversations. For instance, in the case of complex RAG conversations, we observed that the task of creating the initial
seed question for the conversation can be performed as a separate task. The benefit of breaking up the conversation
creation task lies in the reduced burden on the annotators, especially when creating the initial seed question itself is an
involved task, as it requires the annotator to come up with questions that are related to the corpora at hand and thus
requires them to have also some level of insight into what kinds of questions may follow about such documents in the
subsequent turns.
In our experience, task requesters can further constrain the design task of creating the initial seed question to be
moretargeted. For instance, a targeted task for an initial seed question asks annotators to come up with questions that
force the AI agent to respond with aclarifying question, such as, “Which park are you referring to?”, when the initial
seed question asks, “What time does the park close?”. Such targeted tasks reduce the chances of the annotator creating
questions that are trivial and non-complex (that would be discarded since they do not lead to long, meaningful and
complex conversations). Moreover, targeted tasks can be given to crowd-like annotators, such as external annotators,
while the task of creating the remaining turns can be given to the expert annotators (internal annotators). This design
helps increase the diversity of the questions asked while maintaining high quality, which is also observed in prior
work [ 68] where the data utilized from crowdsourcing can perform as well as expert annotations as long as the task is
broken down into less complex and more refined, smaller goals.
5.3 Utilize the Direct Feedback Loop to Improve the Annotation Guidelines
In Section 4, we described the different kinds of instructions and feedback provided to both annotator groups. We
observed that the combination of a slide-deck and a direct feedback loop, which includes access to instant messaging via
Slack and weekly meeting sessions via Teams, with the internal annotators sufficed in training the internal annotators
for creating complex and high-quality conversations. The direct feedback loop also gave us access and insight on
areas of improvement for the tutorial materials. However, instructions and guideline materials given to the external
annotators needed to be more comprehensive; in our case, we created video-based tutorial materials based on the
slide-deck. Given the natural constraint of the indirect feedback loop, we also never received feedback regarding the
quality of the tutorial materials from the external annotators. In the earlier phases of the data collection process, we
observed that many of the data quality issues that came out of the final conversations created by the external annotators
were the same issues the internal annotators had communicated with us directly. Examples of data quality issues
included those that stemmed from grasping with how one would use the tool to do certain sub-tasks core to the overall
conversation creation task, such as ensuring that the passages attached to the turns were diverse across the entirety
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 21
of the conversation. Using feedback from the internal annotators, we refined the video tutorials and the annotation
guidelines for the external annotators. This quick iterative process is important for spotting and resolving confusion
to avoid both groups of annotators spending time creating conversations that would need to be discarded due to the
confusion. Similar to how we utilized the internal annotators for a dedicated expert review task for quality assurance of
the created conversations, we extended that direct feedback loop for improving the tutorial materials and instruction
guidelines, which is especially important for the annotators with the indirect feedback loop. Previous work [ 52,53]
on using feedback to improve the task instructions often assume a direct feedback communication loop with all the
annotators. In our work, we observed that feedback from a subset of the annotators, i.e. the internal annotators with
direct feedback loop, can potentially improve task instructions and guidelines for the other subset of annotators with
whom task requesters may not have direct communication.
5.4 Design Tooling to Facilitate Feedback that Cannot be Provided Directly
Conversational interfaces have been explored previously, but in different contexts, such as helping annotators effectively
understand and complete crowdsourcing tasks or used for annotating complex artifacts such as code bases [ 34],
computational notebooks [ 74], conversations[ 27], or even logs containing conversations between an AI voice agent and
a customer [ 29]. Conversational interfaces for creating conversations, specifically complex conversations, have been
explored previously [ 22,28], but the extent of best practices of design is not yet fully understood. In this work, we find
that the collaborative aspect of the tooling (such as commenting on certain turns or attached passages of the created
conversation) helped annotators understand which specific parts of the created conversation needed to be fixed. In
short, tooling should be designed to facilitate some of the communication feedback that task requesters are unable to
provide directly. In our case, our survey results show that the annotators perceived that the hints that appeared in the
tool as they created the conversation were key in ensuring the quality of the final conversation (see Section 4.3.3). Hints
act as a form of feedback that task requesters would otherwise provide in direct feedback loops. In another example, the
tooling feature in review mode where annotators could comment on specific parts of the created conversation, allowed
other annotators to iterate and fix the mistakes or improve the quality of the created conversation. It was important for
comments to be given at a fine-grain level, i.e., at a turn level, so that fixes can be made at that particular turn. Enabling
expert annotators to leave fine-grained comments on created conversations helped improve the quality of the final
conversation and also served as detailed feedback that was otherwise inaccessible for the external group of annotators
that had an indirect feedback loop with the original task requesters.
6 Limitations and Biases
We acknowledge that our study is not a controlled lab study; we report a real-world study, which primarily centers
around our observations from tasking two groups of annotators on a complex, multi-layered task that has not been found
in previous literature. Traditional quantitative methods popular in HCI literature also do not apply in our population
due to the smaller pool of internal annotators compared to the number of external annotators. We also acknowledge
that the task requesters (the authors of the paper) learned about the challenges of the task as we tasked the annotators.
We also acknowledge that there are certain biases that may occur in each population due to their different incentives
and tasks based on their feedback loop. The external annotators are paid per conversation, which encourages quantity
but can impact quality, while the internal annotators are paid hourly, which discourages quantity and can improve
quality.
Manuscript submitted to ACM

22 Hanafi et al.
The review task is performed primarily by the internal annotators, and they may prefer their own conversations. We
performed an analysis of the comments from the review of the external annotator conversations and found that 74% of
the conversations rejected by the internal annotators had at least one automated comment, indicating the external
annotator did not follow the instructions for complexity. On the other hand, 60% of the accepted conversations also
had an automated comment, which indicates that the internal annotators were able to edit many of the conversations
so they could be accepted. Some examples of comments from the annotators were: “Rejected because there are no
edits at all." [Int5], “Three of the four questions were unanswerable, so I rejected the conversation." [Int6], “I rejected
the conversation because of the lack of diversity and flow. 3 of the question’s answers were not found in the selected
paragraphs. " [Int6], “I reject this conversation since the first three questions are all asking about the same difference"
[Int2]. These comments highlight either a lack of complexity (e.g., low passage diversity or no edits) and/or conversation
flow (e.g., too many similar questions), which are important qualities that each conversation must have.
7 Conclusion
In conclusion, our longitudinal study explores the impact of different feedback loops for creating multi-turn RAG
conversations across a three-phase task during the one-year period of: pilot, create and review conversations and an
accompanying survey. We find that a closer feedback loop with a small population of internal annotators, such as
hands on meetings and Slack channels helped quickly refine guidelines for both populations and resulted in annotators
that were experts in creating higher quality conversations according to our metrics. The larger population of external
annotators with a larger feedback loop presents advantages in more diverse output and higher quantity of data. Our key
takeaways for managing two population groups for complex annotation tasks are 1) utilize internal group to quickly
refine guidelines that can be comprehensive to aid both groups, 2) give targeted tasks that are less complex and can be
successfully completed by the external annotators, and 3) give the tasks completed by the external group to the internal
annotators for review, editing and completion of tasks.
Acknowledgments
We would like to thank our internal and external annotators for their high-quality work in creating conversations
using RAGAPHENE and participating in our surveys: Mohamed Nasr, Joekie Gurski, Tamara Henderson, Hee Dong Lee,
Roxana Passaro, Chie Ugumori, Marina Variano, and Eva-Maria Wolfe and Defined.AI. We would like to thank Kshitij
Fadnis for his work developing RAGAPHENE.
References
[1]Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. 2022. TopiOCQA: Open-domain Conversational Question
Answering with Topic Switching. arXiv:2110.00768 [cs.CL] https://arxiv.org/abs/2110.00768
[2]Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021. Open-Domain Question
Answering Goes Conversational via Question Rewriting. arXiv:2010.04898 [cs.IR] https://arxiv.org/abs/2010.04898
[3] Anthropic. 2024. Introducing the next generation of Claude. https://www.anthropic.com/news/claude-3-family
[4]Nicholas Asher, Julie Hunter, Mathieu Morey, Benamara Farah, and Stergos Afantenos. 2016. Discourse Structure and Dialogue Acts in Multiparty
Dialogue: the STAC Corpus. InProceedings of the Tenth International Conference on Language Resources and Evaluation (LREC‘16), Nicoletta Calzolari,
Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, and
Stelios Piperidis (Eds.). European Language Resources Association (ELRA), Portorož, Slovenia, 2721–2727. https://aclanthology.org/L16-1432/
[5]Leopold Bayerlein. 2014. Students’ feedback preferences: how do students react to timely and automatically generated assessment feedback?
Assessment & Evaluation in Higher Education39, 8 (2014), 916–931.
[6]Michael S. Bernstein, Greg Little, Robert C. Miller, Björn Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, and Katrina Panovich. 2015.
Soylent: a word processor with a crowd inside.Commun. ACM58, 8 (July 2015), 85–94. doi:10.1145/2791285
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 23
[7]Jeremy Birnholtz and Steven Ibara. 2012. Tracking changes in collaborative writing: edits, visibility and group maintenance. InProceedings of the
ACM 2012 Conference on Computer Supported Cooperative Work(Seattle, Washington, USA)(CSCW ’12). Association for Computing Machinery, New
York, NY, USA, 809–818. doi:10.1145/2145204.2145325
[8]Julia Cambre, Scott Klemmer, and Chinmay Kulkarni. 2018. Juxtapeer: Comparative Peer Review Yields Higher Quality Feedback and Promotes
Deeper Reflection. InProceedings of the 2018 CHI Conference on Human Factors in Computing Systems(Montreal QC, Canada)(CHI ’18). Association
for Computing Machinery, New York, NY, USA, 1–13. doi:10.1145/3173574.3173868
[9]Marcelo Cataldo and Kate Ehrlich. 2012. The impact of communication structure on new product development outcomes. InProceedings of the
SIGCHI Conference on Human Factors in Computing Systems(Austin, Texas, USA)(CHI ’12). Association for Computing Machinery, New York, NY,
USA, 3081–3090. doi:10.1145/2207676.2208722
[10] Tuhin Chakrabarty, Philippe Laban, and Chien-Sheng Wu. 2025. Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI
Alignment in the Writing Process through Edits. arXiv:2409.14509 [cs.CL] https://arxiv.org/abs/2409.14509
[11] Joel Chan, Steven Dang, and Steven P. Dow. 2016. Improving Crowd Innovation with Expert Facilitation. InProceedings of the 19th ACM Conference
on Computer-Supported Cooperative Work & Social Computing(San Francisco, California, USA)(CSCW ’16). Association for Computing Machinery,
New York, NY, USA, 1223–1235. doi:10.1145/2818048.2820023
[12] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking Large Language Models in Retrieval-Augmented Generation.
arXiv:2309.01431 [cs.CL] https://arxiv.org/abs/2309.01431
[13] Kwangsu Cho, Christian D Schunn, and Davida Charney. 2006. Commenting on writing: Typology and perceived helpfulness of comments from
novice peer reviewers and subject matter experts.Written communication23, 3 (2006), 260–294.
[14] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question Answering in
Context. InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier,
and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 2174–2184. doi:10.18653/v1/D18-1241
[15] Maria Cutumisu and Daniel L Schwartz. 2018. The impact of critical feedback choice on students’ revision, performance, learning, and memory.
Computers in Human Behavior78 (2018), 351–367.
[16] Florian Daniel, Pavel Kucherbaev, Cinzia Cappiello, Boualem Benatallah, and Mohammad Allahbakhsh. 2018. Quality Control in Crowdsourcing:
A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions.ACM Comput. Surv.51, 1, Article 7 (Jan. 2018), 40 pages.
doi:10.1145/3148148
[17] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of Wikipedia: Knowledge-Powered
Conversational Agents. InInternational Conference on Learning Representations. Curran Associates, Inc, USA, 0–8. https://openreview.net/forum?
id=r1l73iRqKm
[18] Steven Dow, Anand Kulkarni, Scott Klemmer, and Björn Hartmann. 2012. Shepherding the crowd yields better work. InProceedings of the ACM 2012
Conference on Computer Supported Cooperative Work(Seattle, Washington, USA)(CSCW ’12). Association for Computing Machinery, New York, NY,
USA, 1013–1022. doi:10.1145/2145204.2145355
[19] Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M. Ponti, and Siva Reddy. 2022. FaithDial: A Faithful Benchmark for
Information-Seeking Dialogue.Transactions of the Association for Computational Linguistics10 (2022), 1473–1490. doi:10.1162/tacl_a_00529
[20] Sheena Erete and Jennifer O. Burrell. 2017. Empowered Participation: How Citizens Use Technology in Local Governance. InProceedings of the 2017
CHI Conference on Human Factors in Computing Systems(Denver, Colorado, USA)(CHI ’17). Association for Computing Machinery, New York, NY,
USA, 2307–2319. doi:10.1145/3025453.3025996
[21] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RAGAs: Automated Evaluation of Retrieval Augmented Generation. In
Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, Nikolaos Aletras
and Orphee De Clercq (Eds.). Association for Computational Linguistics, St. Julians, Malta, 150–158. https://aclanthology.org/2024.eacl-demo.16/
[22] Kshitij Fadnis, Sara Rosenthal, Maeda Hanafi, Yannis Katsis, and Marina Danilevsky. 2025. RAGAPHENE: A RAG Annotation Platform with Human
Enhancements and Edits. arXiv:2508.19272 [cs.CL] https://arxiv.org/abs/2508.19272
[23] Song Feng, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021. MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and
Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 6162–6176. doi:10.18653/v1/2021.
emnlp-main.498
[24] Andrew T. Fiore, Coye Cheshire, Lindsay Shaw Taylor, and G.A. Mendelsohn. 2014. Incentives to participate in online research: an experimental
examination of "surprise" incentives. InProceedings of the SIGCHI Conference on Human Factors in Computing Systems(Toronto, Ontario, Canada)
(CHI ’14). Association for Computing Machinery, New York, NY, USA, 3433–3442. doi:10.1145/2556288.2557418
[25] Robert Friel, Masha Belyi, and Atindriyo Sanyal. 2025. RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems.
arXiv:2407.11005 [cs.CL] https://arxiv.org/abs/2407.11005
[26] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-
augmented generation for large language models: A survey.arXiv preprint arXiv:2312.109972 (2023), 1.
[27] Emer Gilmartin and Nick Campbell. 2016. Capturing Chat: Annotation and Tools for Multiparty Casual Conversation.. InProceedings of the Tenth
International Conference on Language Resources and Evaluation (LREC‘16), Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko
Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources
Manuscript submitted to ACM

24 Hanafi et al.
Association (ELRA), Portorož, Slovenia, 4453–4457. https://aclanthology.org/L16-1705/
[28] Maeda F Hanafi, Kshitij Fadnis, Marina Danilevsky, Sara Rosenthal, and Yannis Katsis. 2025. Creating Conversational Datasets for Retrieval-
Augmented Generation Applications is Hard: Challenges & Research Opportunities. InProceedings of the Extended Abstracts of the CHI Conference on
Human Factors in Computing Systems (CHI EA ’25). Association for Computing Machinery, New York, NY, USA, Article 157, 7 pages. doi:10.1145/
3706599.3719962
[29] Maeda F Hanafi, Frederick Reiss, Yannis Katsis, Robert Moore, Mohammad H Falakmasir, Pauline Wang, David Wood, and Changchang Liu. 2025.
Diagnosing and Prioritizing Issues in Automated Order-Taking Systems: A Machine-Assisted Error Discovery Approach. InProceedings of the 2025
CHI Conference on Human Factors in Computing Systems (CHI ’25). Association for Computing Machinery, New York, NY, USA, Article 654, 18 pages.
doi:10.1145/3706598.3713312
[30] Zeyu He, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Shaurya Rohatgi, and Ting-Hao Kenneth Huang. 2024. If in a Crowdsourced Data
Annotation Pipeline, a GPT-4. InProceedings of the 2024 CHI Conference on Human Factors in Computing Systems(Honolulu, HI, USA)(CHI ’24).
Association for Computing Machinery, New York, NY, USA, Article 1040, 25 pages. doi:10.1145/3613904.3642834
[31] Andrew Head, Fred Hohman, Titus Barik, Steven M. Drucker, and Robert DeLine. 2019. Managing Messes in Computational Notebooks. InProceedings
of the 2019 CHI Conference on Human Factors in Computing Systems(Glasgow, Scotland Uk)(CHI ’19). Association for Computing Machinery, New
York, NY, USA, 1–12. doi:10.1145/3290605.3300500
[32] Jeffrey Heer and Michael Bostock. 2010. Crowdsourcing graphical perception: using mechanical turk to assess visualization design. InProceedings of
the SIGCHI Conference on Human Factors in Computing Systems(Atlanta, Georgia, USA)(CHI ’10). Association for Computing Machinery, New York,
NY, USA, 203–212. doi:10.1145/1753326.1753357
[33] Danula Hettiachchi, Vassilis Kostakos, and Jorge Goncalves. 2022. A Survey on Task Assignment in Crowdsourcing.ACM Comput. Surv.55, 3,
Article 49 (Feb. 2022), 35 pages. doi:10.1145/3494522
[34] Amber Horvath, Michael Xieyang Liu, River Hendriksen, Connor Shannon, Emma Paterson, Kazi Jawad, Andrew Macvean, and Brad A Myers.
2022. Understanding How Programmers Can Use Annotations on Documentation. InProceedings of the 2022 CHI Conference on Human Factors
in Computing Systems(New Orleans, LA, USA)(CHI ’22). Association for Computing Machinery, New York, NY, USA, Article 69, 16 pages.
doi:10.1145/3491102.3502095
[35] Chao-Wei Huang, Chen-Yu Hsu, Tsu-Yuan Hsu, Chen-An Li, and Yun-Nung Chen. 2023. CONVERSER: Few-shot Conversational Dense Retrieval with
Synthetic Data Generation. InProceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Svetlana Stoyanchev,
Shafiq Joty, David Schlangen, Ondrej Dusek, Casey Kennington, and Malihe Alikhani (Eds.). Association for Computational Linguistics, Prague,
Czechia, 381–387. doi:10.18653/v1/2023.sigdial-1.34
[36] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin,
et al.2025. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.ACM Transactions on
Information Systems43, 2 (2025), 1–55.
[37] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin,
and Ting Liu. 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions.ACM Trans. Inf.
Syst.43, 2 (Nov. 2024), 1–55. doi:10.1145/3703155 Just Accepted.
[38] Hideaki Joko, Faegheh Hasibi, Krisztian Balog, and Arjen P. de Vries. 2021. Conversational Entity Linking: Problem Definition and Datasets. In
Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval(Virtual Event, Canada)(SIGIR ’21).
Association for Computing Machinery, New York, NY, USA, 2390–2397. doi:10.1145/3404835.3463258
[39] Yannis Katsis, Sara Rosenthal, Kshitij Fadnis, Chulaka Gunasekara, Young-Suk Lee, Lucian Popa, Vraj Shah, Huaiyu Zhu, Danish Contractor,
and Marina Danilevsky. 2025. MTRAG: A Multi-Turn Conversational Benchmark for Evaluating Retrieval-Augmented Generation Systems.
arXiv:2501.03468 [cs.CL] https://arxiv.org/abs/2501.03468
[40] Joy Kim, Justin Cheng, and Michael S. Bernstein. 2014. Ensemble: exploring complementary strengths of leaders and crowds in creative collaboration.
InProceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(Baltimore, Maryland, USA)(CSCW ’14).
Association for Computing Machinery, New York, NY, USA, 745–755. doi:10.1145/2531602.2531638
[41] Joy Kim, Sarah Sterman, Allegra Argent Beal Cohen, and Michael S. Bernstein. 2017. Mechanical Novel: Crowdsourcing Complex Work through
Reflection and Revision. InProceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing(Portland, Oregon,
USA)(CSCW ’17). Association for Computing Machinery, New York, NY, USA, 233–245. doi:10.1145/2998181.2998196
[42] Tae Soo Kim, Seungsu Kim, Yoonseo Choi, and Juho Kim. 2021. Winder: Linking Speech and Visual Objects to Support Communication in
Asynchronous Collaboration. InProceedings of the 2021 CHI Conference on Human Factors in Computing Systems(Yokohama, Japan)(CHI ’21).
Association for Computing Machinery, New York, NY, USA, Article 453, 17 pages. doi:10.1145/3411764.3445686
[43] Aniket Kittur, Boris Smus, Susheel Khamkar, and Robert E. Kraut. 2011. CrowdForge: crowdsourcing complex work. InProceedings of the 24th Annual
ACM Symposium on User Interface Software and Technology(Santa Barbara, California, USA)(UIST ’11). Association for Computing Machinery, New
York, NY, USA, 43–52. doi:10.1145/2047196.2047202
[44] Jan-Christoph Klie, Richard Eckart de Castilho, and Iryna Gurevych. 2024. Analyzing Dataset Annotation Quality Management in the Wild.
arXiv:2307.08153 [cs.CL] https://arxiv.org/abs/2307.08153
[45] Tzu-Lin Kuo, Feng-Ting Liao, Mu-Wei Hsieh, Fu-Chieh Chang, Po-Chun Hsu, and Da-Shan Shiu. 2024. RAD-Bench: Evaluating Large Language
Models Capabilities in Retrieval Augmented Dialogues. arXiv:2409.12558 [cs.CL] https://arxiv.org/abs/2409.12558
Manuscript submitted to ACM

A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks 25
[46] Young-Suk Lee, Chulaka Gunasekara, Danish Contractor, Ramón Fernandez Astudillo, and Radu Florian. 2024. Multi-Document Grounded Multi-Turn
Synthetic Dialog Generation. arXiv:2409.11500 [cs.CL] https://arxiv.org/abs/2409.11500
[47] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et al .2020. Retrieval-augmented generation for knowledge-intensive NLP tasks.Advances in Neural Information Processing Systems33
(2020), 9459–9474. https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf
[48] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. InProceedings of the
34th International Conference on Neural Information Processing Systems(Vancouver, BC, Canada)(NIPS ’20). Curran Associates Inc., Red Hook, NY,
USA, Article 793, 16 pages.
[49] Na Li and Mary Beth Rosson. 2014. Using annotations in online group chats. InProceedings of the SIGCHI Conference on Human Factors in Computing
Systems(Toronto, Ontario, Canada)(CHI ’14). Association for Computing Machinery, New York, NY, USA, 863–866. doi:10.1145/2556288.2557209
[50] Chien-Chang Lin, Anna Y. Q. Huang, and Stephen J. H. Yang. 2023. A Review of AI-Driven Conversational Chatbots Implementation Methodologies
and Challenges (1999–2022).Sustainability15, 5 (2023), 13 pages. doi:10.3390/su15054012
[51] Kurt Luther, Jari-Lee Tolentino, Wei Wu, Amy Pavel, Brian P. Bailey, Maneesh Agrawala, Björn Hartmann, and Steven P. Dow. 2015. Structuring,
Aggregating, and Evaluating Crowdsourced Design Critique. InProceedings of the 18th ACM Conference on Computer Supported Cooperative Work &
Social Computing(Vancouver, BC, Canada)(CSCW ’15). Association for Computing Machinery, New York, NY, USA, 473–485. doi:10.1145/2675133.
2675283
[52] V. K. Manam and Alexander Quinn. 2018. WingIt: Efficient Refinement of Unclear Task Instructions.Proceedings of the AAAI Conference on Human
Computation and Crowdsourcing6, 1 (Jun. 2018), 108–116. doi:10.1609/hcomp.v6i1.13338
[53] V. K. Chaithanya Manam, Joseph Divyan Thomas, and Alexander J. Quinn. 2022. TaskLint: Automated Detection of Ambiguities in Task Instructions.
Proceedings of the AAAI Conference on Human Computation and Crowdsourcing10, 1 (Oct. 2022), 160–172. doi:10.1609/hcomp.v10i1.21996
[54] Ramesh Manuvinakurike, Maike Paetzel, and David Devault. 2015. Reducing the cost of dialogue system training and evaluation with online,
crowd-sourced dialogue data collection. InProceedings of the 19th Workshop on the Semantics and Pragmatics of Dialogue - Full Papers. SEMDIAL,
Gothenburg, Sweden, 8 pages. http://semdial.org/anthology/Z15-Manuvinakurike_semdial_0016.pdf
[55] Kelong Mao, Zheng Liu, Hongjin Qian, Fengran Mo, Chenlong Deng, and Zhicheng Dou. 2024. RAG-Studio: Towards In-Domain Adaptation of
Retrieval Augmented Generation Through Self-Alignment. InFindings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan,
Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 725–735. doi:10.18653/v1/2024.findings-
emnlp.41
[56] Panagiotis Mavridis, Owen Huang, Sihang Qiu, Ujwal Gadiraju, and Alessandro Bozzon. 2019. Chatterbox: Conversational Interfaces for Microtask
Crowdsourcing. InProceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization(Larnaca, Cyprus)(UMAP ’19).
Association for Computing Machinery, New York, NY, USA, 243–251. doi:10.1145/3320435.3320439
[57] Tanushree Mitra, C.J. Hutto, and Eric Gilbert. 2015. Comparing Person- and Process-centric Strategies for Obtaining Quality Data on Amazon
Mechanical Turk. InProceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems(Seoul, Republic of Korea)(CHI ’15).
Association for Computing Machinery, New York, NY, USA, 1345–1354. doi:10.1145/2702123.2702553
[58] Tricia J. Ngoon, C. Ailie Fraser, Ariel S. Weingarten, Mira Dontcheva, and Scott Klemmer. 2018. Interactive Guidance Techniques for Improving
Creative Feedback. InProceedings of the 2018 CHI Conference on Human Factors in Computing Systems(Montreal QC, Canada)(CHI ’18). Association
for Computing Machinery, New York, NY, USA, 1–11. doi:10.1145/3173574.3173629
[59] Thi Thao Duyen T. Nguyen, Thomas Garncarz, Felicia Ng, Laura A. Dabbish, and Steven P. Dow. 2017. Fruitful Feedback: Positive Affective
Language and Source Anonymity Improve Critique Reception and Work Outcomes. InProceedings of the 2017 ACM Conference on Computer Supported
Cooperative Work and Social Computing(Portland, Oregon, USA)(CSCW ’17). Association for Computing Machinery, New York, NY, USA, 1024–1034.
doi:10.1145/2998181.2998319
[60] Haruna Ogawa, Hitoshi Nishikawa, Takenobu Tokunaga, and Hikaru Yokono. 2020. Gamification Platform for Collecting Task-oriented Dialogue
Data. InProceedings of the Twelfth Language Resources and Evaluation Conference, Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid
Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 7084–7093. https://aclanthology.org/2020.lrec-1.876/
[61] OpenAI. 2024. ChatGPT: Conversational AI Model. https://openai.com/chatgpt. Accessed: 2025-03-07.
[62] Alexandros Papangelis, Karthik Gopalakrishnan, Aishwarya Padmakumar, Seokhwan Kim, Gokhan Tur, and Dilek Hakkani-Tur. 2021. Generative
Conversational Networks. InProceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Haizhou Li, Gina-Anne
Levow, Zhou Yu, Chitralekha Gupta, Berrak Sisman, Siqi Cai, David Vandyke, Nina Dethlefs, Yan Wu, and Junyi Jessy Li (Eds.). Association for
Computational Linguistics, Singapore and Online, 111–120. doi:10.18653/v1/2021.sigdial-1.12
[63] Jessica Pater, Amanda Coupe, Rachel Pfafman, Chanda Phelan, Tammy Toscos, and Maia Jacobs. 2021. Standardizing Reporting of Participant
Compensation in HCI: A Systematic Literature Review and Recommendations for the Field. InProceedings of the 2021 CHI Conference on Human
Factors in Computing Systems(Yokohama, Japan)(CHI ’21). Association for Computing Machinery, New York, NY, USA, Article 141, 16 pages.
doi:10.1145/3411764.3445734
[64] Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. CoQA: A Conversational Question Answering Challenge.Transactions of the Association
for Computational Linguistics7 (2019), 249–266. doi:10.1162/tacl_a_00266
Manuscript submitted to ACM

26 Hanafi et al.
[65] Ronja Schiller, Johanna Fleckenstein, Ute Mertens, Andrea Horbach, and Jennifer Meyer. 2024. Understanding the effectiveness of automated feedback:
Using process data to uncover the role of behavioral engagement.Comput. Educ.223, C (Dec. 2024), 16 pages. doi:10.1016/j.compedu.2024.105163
[66] Heinz Schmitz and Ioanna Lykourentzou. 2018. Online Sequencing of Non-Decomposable Macrotasks in Expert Crowdsourcing.Trans. Soc. Comput.
1, 1, Article 1 (Jan. 2018), 33 pages. doi:10.1145/3140459
[67] Minju Seo, Jinheon Baek, James Thorne, and Sung Ju Hwang. 2024. Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks.
arXiv:2402.13482 [cs.CL] https://arxiv.org/abs/2402.13482
[68] Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for
Natural Language Tasks. InProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Mirella Lapata and Hwee Tou
Ng (Eds.). Association for Computational Linguistics, Honolulu, Hawaii, 254–263. https://aclanthology.org/D08-1027/
[69] Heydar Soudani, Evangelos Kanoulas, and Faegheh Hasibi. 2024. Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge. In
Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region
(SIGIR-AP 2024). ACM, USA, 12–22. doi:10.1145/3673791.3698415
[70] Heydar Soudani, Roxana Petcu, Evangelos Kanoulas, and Faegheh Hasibi. 2024. A Survey on Recent Advances in Conversational Data Generation.
arXiv:2405.13003 [cs.CL] https://arxiv.org/abs/2405.13003
[71] John Sweller. 1994. Cognitive load theory, learning difficulty, and instructional design.Learning and instruction4, 4 (1994), 295–312.
[72] Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu.
2024. Large Language Models for Data Annotation and Synthesis: A Survey. InProceedings of the 2024 Conference on Empirical Methods in Natural
Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA,
930–957. doi:10.18653/v1/2024.emnlp-main.54
[73] April Yi Wang, Yan Chen, John Joon Young Chung, Christopher Brooks, and Steve Oney. 2021. PuzzleMe: Leveraging Peer Assessment for In-Class
Programming Exercises.Proc. ACM Hum.-Comput. Interact.5, CSCW2, Article 415 (Oct. 2021), 24 pages. doi:10.1145/3479559
[74] April Yi Wang, Zihan Wu, Christopher Brooks, and Steve Oney. 2020. Callisto: Capturing the "Why" by Connecting Conversations with Computational
Narratives. InProceedings of the 2020 CHI Conference on Human Factors in Computing Systems(Honolulu, HI, USA)(CHI ’20). Association for
Computing Machinery, New York, NY, USA, 1–13. doi:10.1145/3313831.3376740
[75] Ding Wang, Mark Díaz, Alicia Parrish, Lora Aroyo, Christopher Homan, Greg Serapio-García, Vinodkumar Prabhakaran, and Alex S Taylor. 2024. A
case for moving beyond “gold data” in AI safety evaluation. InHEAL Workshop at CHI. ACM, NY, NY, USA, 0–8.
[76] Ding Wang, Mark Díaz, Alicia Parrish, Lora Aroyo, Chris Homan, Greg Serapio-García, Vinodkumar Prabhakaran, and Alex Taylor. 2023. All that
Agrees Is Not Gold: Evaluating Ground Truth Labels and Dialogue Content for Safety.
[77] Xinru Wang, Hannah Kim, Sajjadur Rahman, Kushan Mitra, and Zhengjie Miao. 2024. Human-LLM Collaborative Annotation Through Effective
Verification of LLM Labels. InProceedings of the 2024 CHI Conference on Human Factors in Computing Systems(Honolulu, HI, USA)(CHI ’24).
Association for Computing Machinery, New York, NY, USA, Article 303, 21 pages. doi:10.1145/3613904.3641960
[78] Zonghai Yao, Benjamin J Schloss, and Sai P. Selvaraj. 2025. Improving Summarization with Human Edits. arXiv:2310.05857 [cs.CL] https:
//arxiv.org/abs/2310.05857
[79] Lixiu Yu, Aniket Kittur, and Robert E. Kraut. 2016. Encouraging “Outside- the- box” Thinking in Crowd Innovation Through Identifying Domains of
Expertise. InProceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing(San Francisco, California, USA)
(CSCW ’16). Association for Computing Machinery, New York, NY, USA, 1214–1222. doi:10.1145/2818048.2820025
[80] Alvin Yuan, Kurt Luther, Markus Krause, Sophie Isabel Vennix, Steven P Dow, and Bjorn Hartmann. 2016. Almost an Expert: The Effects of Rubrics
and Expertise on Perceived Value of Crowdsourced Design Critiques. InProceedings of the 19th ACM Conference on Computer-Supported Cooperative
Work & Social Computing(San Francisco, California, USA)(CSCW ’16). Association for Computing Machinery, New York, NY, USA, 1005–1017.
doi:10.1145/2818048.2819953
[81] Amy X. Zhang and Justin Cranshaw. 2018. Making Sense of Group Chat through Collaborative Tagging and Summarization.Proc. ACM Hum.-Comput.
Interact.2, CSCW, Article 196 (Nov. 2018), 27 pages. doi:10.1145/3274465
[82] Haiyi Zhu, Steven P. Dow, Robert E. Kraut, and Aniket Kittur. 2014. Reviewing versus doing: learning and performance in crowd assessment. In
Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(Baltimore, Maryland, USA)(CSCW ’14).
Association for Computing Machinery, New York, NY, USA, 1445–1455. doi:10.1145/2531602.2531718
Manuscript submitted to ACM