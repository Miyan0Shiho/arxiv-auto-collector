# SpareCodeSearch: Searching for Code Context When You Have No Spare GPU

**Authors**: Minh Nguyen

**Published**: 2025-10-14 19:48:50

**PDF URL**: [http://arxiv.org/pdf/2510.12948v1](http://arxiv.org/pdf/2510.12948v1)

## Abstract
Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

## Full Text


<!-- PDF content starts -->

SpareCodeSearch: Searching for Code ContextWhen You Have No Spare GPUMinh NguyenSchool of Computer ScienceUniversity College DublinDublin, Irelandminh.a.nguyen@ucdconnect.ieAbstract—Retrieval-Augmented Generation (RAG) frame-works aim to enhance Code Language Models (CLMs) by includ-ing another module for retrieving relevant context to constructthe input prompt. However, these retrieval modules commonlyuse semantic search, requiring substantial computational re-sources for training and hosting these embedded models, makingthem infeasible to integrate into lightweight applications such asin-IDE AI-based code completion. In this solution paper, we provethat using keyword-search is sufﬁcient to retrieve relevant anduseful code context inside large codebases, without the need forextensive GPU resources. The usefulness of code contexts foundby our solution is demonstrated through their completion resultson the Code Context Competition’s benchmark, reaching 0.748and 0.725 chRF scores on Kotlin and Python tracks, respectively.Index Terms—Code Context, Code Search, Code LanguageModels, Code Completion, Keyword-based SearchI. MOTIVATIONCode Language Models (CLMs) have shown great promisein generating code, given the right contexts in their inputprompts [1], [2]. Our solution -SpareCodeSearch- is de-veloped with a philosophy that emphasizes simplicity andefﬁciency, focusing on the question of how to develop a codesearch tool that is most suitable for the task of automatedcode completion. This means that the tool must satisfy therequirements of lightweight deployment and fast retrievaltimes, easily integrate into existing CLMs, and, at the sametime, be easily extensible to multiple programming languages.Semantic code search, while it has been proven effective inretrieving code snippets from natural language queries [3],is not suitable for these requirements, due to its relianceon resource-intensive language models to generate vectorembeddings [4], which cannot be easily deployed in resource-constrained environments such as IDEs. In those scenarios,indexing a large codebase could take a signiﬁcant amount oftime and resources, making it impractical to combine withthe computationally demanding LLM-based code completion.The learning-based nature of this method requires retrainingand ﬁne-tuning to mitigate knowledge cut-off, causing it toperform differently across different programming languages[3], [5].We believe that a keyword-based search approach caneffectively address these challenges. It has been shown thatmost developers when searching code, look for speciﬁc codeexamples or patterns that match their pre-existing knowledgeand familiarity [6]. This tendency gives rise to the developmentof IDE features such as “Go to Deﬁnition” or “Search Every-where”1which are heavily reliant on user-provided keywords.Coming back to the Code Context Competition, we extendthe original question of “How to ﬁnd relevant code contextfrom a codebase that helps language models generate bettercode completion” into “How to effectively integrate existingkeyword-based code search engines into automated code com-pletion systems”II. TECHNICALDETAILSA. System Architecture
Fig. 1: System architecture overview of SpareCodeSearch.The code search engine used in our solution is Zoekt2.Zoekt is an open-source fast, scalable code search engine thatis designed to handle large codebases efﬁciently. Zoekt hasbeen adpated by Sourcegraph and Gitlab as their backendfor indexed code search [7]. Sourcegraph even integrated itinto their own conversation-based AI Coding Assistant Cody[8]. Zoekt also shares the similar query language with otherpropietary code search engine such as Github Code Search [9]and Google Code Search [10], making our solution extensibleto these engines in the future. Zoekt also has a separateindex server and web server, allowing it to be deployed inmicroservices architectures, making it easy to integrate andcustomize with other systems.1https://www.jetbrains.com/help/idea/searching-everywhere.html2https://github.com/sourcegraph/zoekt

Looking at Figure 1, our system can be divided into twomain phases: Ofﬂine Indexing and Online RetrievalB. Ofﬂine Indexing Phase with ZoektWith the provided public Kotlin and Python sets providedby the competition, we build an index using Zoekt. Foreach datapoint in the original JSONL ﬁle, Zoekt indexerserver looks at its corresponding revisions, and creates anindex shard. Each shard is a compressed representation of thecodebase at that speciﬁc revision, optimized for fast keyword-based search [10]. The indexing process requiresctags3forextracting and saving symbols from the codebase to the index.For Kotlin public dataset, there are a total of 400 shards,belonging to 19 unique repositories. For Python, the totalnumber of shards is 247, indexed from 20 repositories. SeeFigure 2 for the distribution of the number of revisions perunique repository. We build our index locally, on a MacbookM3 Air with 16GB ram, using Docker containers to ensurea consistent and isolated environment. The indexing processtook approximately 10 minutes for each dataset, with thecreated shards being mounted to a Docker volume. Afterindexing, Zoekt web server will be turned on to serve theindexed shards, and expose its/searchJSON API for retrieval.LanguageNumber of Revisions (Completion point)Number of unique reposKotlin40019Python24720TABLE I: Number of revisions per unique repositories forKotlin and Python in public sets.C. Online Context Retrieving PhaseThe Online Retrieval phase begins after the initial indexingis complete, and the Zoekt web server is running. This phaseinvolves two main modules: the Zoekt Query Generator andthe Post-processor of search results.Query VariationKotlinPythonfunctionsclassesnaive371237functionsclassesor371237functionsclassestop5214201functionsclassestop4250209functionsclassestop3275218functionsclassesregex337230navigationnaive346219navigationunpacked346219navigationunpackedor346219navigationunpackedtop5293154navigationunpackedtop4307176navigationunpackedtop3320191navigationregex346219identiﬁersnaive396244identiﬁersor396244identiﬁerstop5386226identiﬁerstop4387230identiﬁerstop3388236identiﬁersregex396244TABLE II: Number of queries generated for each query con-struction strategy, for Kotlin and Python datasets respectively.3https://github.com/universal-ctags/ctags
Fig. 2: Number of revisions for each unique repository indexedby Zoekt. (Public set only)1) Zoekt Query Construction Strategies:For every originalcompletion point, we manufacture a search space using ourQuery Generator module, creating 19 possible Zoekt querycandidates. The ultimate goal is trying to brute-force all thepossible ways of constructing a Zoekt query, in order tomaximize the chance of ﬁnding relevant code context. Thekey-word terms used to compose these queries are collectedfrom the diff string associated with its completion point.This diff string can easily be constructed by comparing theconcatenated string ofpreﬁxandsufﬁxwith the original codeﬁle before being modiﬁed by the revision. An example aboutthe diff string could be found in Table V of the Appendix4.The diff string acts as the source of reference, so that we canuse differentsymbols gatheringtechniques, utltilizing Tree-sitter for extracting meaningful identiﬁers from the locatedcompletion point. We start from function and class names,expanding it to navigation expression, and ﬁnally using allidentiﬁers existing inside the diff string. The Tree-sitter syntaxused for this process can be veriﬁed in Table VII of theAppendix.Toconstruct Zoekt queriesfrom the gathered identiﬁers,4Link to online Appendix https://doi.org/10.5281/zenodo.17045695

we used multiple types of combinations based on the ofﬁcialZoekt query language guideline5. We start with naive exactmatching, including all found identiﬁers in the queries, andgradually reduce the difﬁculty level of the query by reducingthe number of terms inside the query, using regex for fuzzysearch, and using OR logic to increase the search boundary.We ranked identiﬁers by their occurrence in the diff stringand by their close proximity to the completion points, whichis useful when ﬁltering top-k identiﬁers. Table II shows thename and number of each query variation constructed in thepublic Kotlin and Python datasets. Examples of each queryvariation can be found in Table VI in the Appendix.However, during online phase, not all query variationsare sent to the Zoekt web server for searching. The QueryGenerator iterates on each variation, sending the generatedquery to the Zoekt web server, until it ﬁnds a non-emptysearch result. This ensures that we do not need to exhaustivelysearch all query variations while the ﬁrst few and most difﬁcultvariations have already successfully returned the results. Weapply fallback, time-out and retry mechanisms to ensurethat no requests are dropped when the server is overloaded.Another thing worth mentioning is that Zoekt allows Cross-shard searching, which means that queries can span multiplecode revisions in the same repository. This can be done byomitting the speciﬁc revision ID in the query (Figure 3). Thiscan be another way to increase the search coverage, similar tohow developers searched through branches and commits. Weexperimented with both Single-shard and Cross-shard settingsto see which settings have the highest successful search rate (or hit rate). Results are reported in Table III.Results returned from Zoekt contain metadata of thesearched code snippets, including start, end lines and thecontaining ﬁle paths. The Post-processor module then usesthis information to fetch the actual code snippets from theindexed shards.
Fig. 3: Example of a Cross-shard Zoekt query spanningmultiple code revisions in a repository.2) Post-processing of search results:The Zoekt search APIcan return numerous results, to the extent of thousands of codesnippets, especially in the case of Cross-shard querying. Weﬁrst ranked them using Relevance Scores returned from Zoekt,and then ﬁnd the containing ﬁle using the returned ﬁlepath.5https://github.com/sourcegraph/zoekt/blob/main/doc/querysyntax.mdWe use HuggingFace tokenizer to determine if the containingﬁle is too large to be included in the ﬁnal context window. If itdoes, we only return the code snippets or their union versionsif they overlap with each other. We use Jetbrains Mellum’stokenizer [11] for both Python and Kotlin tokenization. Atthe same time, we implement a dynamic way to adjust thecontext window based on the size of the returned snippets.Speciﬁcally, we dynamically calculate the token constraintTof each completion point by subtracting the maximumnumber of Mellum tokens from the total number of tokensexisting in the diff string’s preﬁx and sufﬁx. A small tokenbuffer is reserved to ensure there is enough token space forthe CLMs to generate during the evaluation. Then, the post-processor selects up to top-k ranked ﬁles/snippets, ensuringeach ﬁts within a per-ﬁle token budgetRand the overalltoken constraintT, and concatenates their contexts for output(Equation 1 in the Appendix).III. FINDINGSA. Hit Rate between Single-shard and Cross-shard SettingWe deﬁne a “hit” as a response from Zoekt server whichcontains at least one successful search result. From TableIII, the hit rate for Cross-shard setting was found to besigniﬁcantly higher than that of Single-shard setting. Thissuggests that there could be more contexts located in otherrevisions of the same repository, which could be useful forcode completion. This is analogous to how developers oftensearch through branches and commits to ﬁnd previous exam-ples of implementations. We also investigated special caseswhere a query failed in the Single-shard Setting but succeededin the Cross-shard Setting, and found that there was a highpercentage of local contexts (found in the same containing ﬁleas the completion point) existing in other revisions. (Figure 4in the Appendix)Single-shard QueryCross-shard QueryPython Hit213239Python Miss348Kotlin Hit344390Kotlin Miss5610TABLE III: Hit and Miss counts for Single-shard and Cross-shard settings in Python and Kotlin.B. Results in Public and Private set using Cross-shard settingSince cross-shard queries achieve a much higher hit rate, weused submitted contexts found by this setting to the organizer’sevaluation server for both the Public and Private phases. Theresults are shown in Table IV. Our solution ranked ﬁrst in thePublic Phase for Kotlin, reaching a chRF score of 0.7125. ThePython public submissions also earned 0.6152, which qualiﬁedfor second position. These optimistic results can be attributedto the high hit rate in Cross-shard queries (97.5% comparedto 96.7% for Python), which leads to more relevant contextsbeing retrieved for code completion. For the Private Phase,we cannot conﬁrm this assumption, as we do not have accessto the evaluation data or hit rate information for that phase.

However, the results in the Private Phase showed a similartrend, with our submission outperforming every other in theKotlin track. At the same time, the Python private submissionranked second, only slightly behind the ﬁrst solution that usessemantic search.It is also worth noting that searching through all 400Kotlin public revisions took 18 minutes, while processingall 247 Python revisions required 12 minutes. Each retrievalrequest used a default timeout of 0.2 seconds when catchingoverloaded server exceptions. All of our results are collectedon a Macbook M3 Air, using only the default container con-ﬁguration of 1 CPU and 8GB RAM. This result demonstratesthat our solution is lightweight but powerful in terms ofperformance, making it suitable for integration into existingin-IDE CLM-based code completion systems.LanguagePublic PhasePrivate PhaseKotlin0.71250.748Python0.61520.725TABLE IV: chRF scores for Kotlin and Python in Public andPrivate phases.IV. DISCUSSIONA. Threats to ValidityThe effect of contexts found by Single-shard towards thequality of code completed is not investigated, leading toselection bias. There was no proven correlation between thehit percentage with the quality of code completion in a sameprogramming language setting. Also, in the online retrievalphase, only the contexts are collected. No information aboutthe effect of which speciﬁc symbols gathering techniquesor Zoekt query construction variations on the search resultsis provided, which could limit the understanding of theircontributions to the overall performance. Cross-shard settingcan be considered as a threat to validity, as no informationabout the revision order is taken into account. In other words,a code snippet from a later revision might be retrieved tocomplete a code snippet from an earlier revision, which couldbe considered data leakage. Further data preparation steps areneeded to mitigate this issue, and to prevent SpareCodeSearchfrom looking for contexts in future repository snapshots.B. Future WorkApart from addressing the aforementioned threats to va-lidity, our work opens new avenues for research. Trying toexplore this solution in the context of secure code generationis a promising avenue. A prior work has used regex asContext Retrieval module [12] to localize Bug context usedfor automated patch generation with CLMs. Second, while ourcurrent implementation uses a ﬁxed set of 19 query candidatesfor tractability, the space of all possible query formulationsis signiﬁcantly larger and potentially indeﬁnite when con-sidering different identiﬁer combinations, regex patterns, andlogical operators. Automated query optimization using search-based software engineering techniques [13] could explorethis broader search space, discovering novel query strategiesbeyond our current hand-crafted variations.IDEs and plugins developers could adapt SpareCodeSearchinto their existing automated code completion features at ease.The Query Generator could make use of currently availablecode and AST parsing tools. Zoekt can be hosted entirely ona consumer laptop, and cross-shard setting could also beneﬁtfrom temporal contexts extracted from version control tools.We publish SpareCodeSearch on Github6, and welcome everycontributions from the community to improve, extend it or toreplicate our solution on other datasets and competitions.REFERENCES[1]M. Sapronov and E. Glukhov, “On pretraining for project-levelcode completion,” inICLR 2025 Third Workshop on Deep Learningfor Code, 2025. [Online]. Available: https://openreview.net/forum?id=t9RN9WX4Ic[2]L. Pasqualeet al., “Challenges to using large language models in codegeneration and repair,”IEEE Security Privacy, vol. 23, no. 2, pp. 81–88,2025.[3]D. Nguyenet al., “The vault: A comprehensive multilingual datasetfor advancing code understanding and generation,” inFindings ofthe Association for Computational Linguistics: EMNLP 2023, Dec.2023, pp. 4763–4788. [Online]. Available: https://aclanthology.org/2023.ﬁndings-emnlp.316/[4]X. Zhou, D. Han, and D. Lo, “Assessing generalizability of codebert,”in2021 IEEE International Conference on Software Maintenance andEvolution (ICSME), 2021, pp. 425–436.[5]F. Chen, F. H. Fard, D. Lo, and T. Bryksin, “On the transferability ofpre-trained language models for low-resource programming languages,”inProceedings of the 30th IEEE/ACM International Conference onProgram Comprehension, ser. ICPC ’22, 2022, p. 401–412. [Online].Available: https://doi.org/10.1145/3524610.3527917[6]C. Sadowski, K. T. Stolee, and S. Elbaum, “How developerssearch for code: a case study,” inProceedings of the 201510th Joint Meeting on Foundations of Software Engineering, ser.ESEC/FSE 2015. New York, NY, USA: Association for ComputingMachinery, Aug. 2015, pp. 191–201. [Online]. Available: https://dl.acm.org/doi/10.1145/2786805.2786855[7]D. Gruzd, “Exact code search: Find code faster acrossrepositories.” [Online]. Available: https://about.gitlab.com/blog/exact-code-search-ﬁnd-code-faster-across-repositories/[8]A. Isken, “How cody understands your codebase|source-graph blog.” [Online]. Available: https://sourcegraph.com/blog/how-cody-understands-your-codebase[9]T. Clem, “The technology behind GitHub’s newcode search,” Feb. 2023. [Online]. Available:https://github.blog/engineering/architecture-optimization/the-technology-behind-githubs-new-code-search/[10]B. Liu and J. Dorfman, “Code Search at Google: The Storyof Han-Wen and Zoekt|Sourcegraph Blog.” [Online]. Available:https://sourcegraph.com/blog/zoekt-creating-internal-tools-at-google[11]N. Pavlichenkoet al., “Mellum-4b-base.”[12]H. Zheng, I. Shumailov, T. Fan, A. Hall, and M. Payer, “Fixing7,400 Bugs for 1$: Cheap Crash-Site Program Repair,” May 2025,arXiv:2505.13103 [cs]. [Online]. Available: http://arxiv.org/abs/2505.13103[13]M. Harman and B. F. Jones, “Search-based software engineering,”Information and Software Technology, vol. 43, no. 14, pp. 833–839,Dec. 2001. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0950584901001896
6https://github.com/SPARE-UCD/spare-code-search