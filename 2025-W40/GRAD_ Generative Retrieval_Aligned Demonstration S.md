# GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning

**Authors**: Oussama Gabouj, Kamel Charaf, Ivan Zakazov, Nicolas Baldwin, Robert West

**Published**: 2025-10-01 17:52:41

**PDF URL**: [http://arxiv.org/pdf/2510.01165v1](http://arxiv.org/pdf/2510.01165v1)

## Abstract
Large Language Models (LLMs) achieve strong performance across diverse tasks,
but their effectiveness often depends on the quality of the provided context.
Retrieval-Augmented Generation (RAG) enriches prompts with external
information, but its reliance on static databases constrains adaptability and
can result in irrelevant demonstrations. In this work, we propose a Generative
Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach
where an LLM model is trained to generate input-specific concise
demonstrations. By tailoring demonstrations to each input, our method offers
better contextual support than traditional RAG approaches. We demonstrate the
superiority of GRAD under budget constraints, where we limit both the number of
tokens used per demonstration and the number of tokens used for the final
output. Trained solely on a math dataset, GRAD consistently outperforms strong
baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM
questions, highlighting GRAD's robust generalization to out-of-distribution
(OOD) domains such as physics, chemistry, and computer science. Furthermore, we
show that demonstrations generated by trained smaller models can effectively
guide larger target models, reducing training costs while maintaining
competitive accuracy. Overall, this work introduces a scalable demonstration
generator model presenting the first step toward a dynamic few-shot learning
paradigm in resource-constrained settings. We release the code used for the
project.

## Full Text


<!-- PDF content starts -->

GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient
Few-Shot Reasoning
Oussama Gabouj∗Kamel Charaf∗Ivan Zakazov∗
Nicolas Baldwin Robert West
EPFL, Lausanne, Switzerland
oussama.gabouj@gmail.com
publication.charaf@gmail.com
{ivan.zakazov, nicolas.baldwin, robert.west@epfl.ch}
Abstract
Large Language Models (LLMs) achieve
strong performance across diverse tasks, but
their effectiveness often depends on the quality
of the provided context. Retrieval-Augmented
Generation (RAG) enriches prompts with
external information, but its reliance on
static databases constrains adaptability and
can result in irrelevant demonstrations.
In this work, we propose a Generative
Retrieval-Aligned Demonstrator (GRAD),
a dynamic demonstration-based approach
where an LLM model is trained to generate
input-specific concise demonstrations. By
tailoring demonstrations to each input, our
method offers better contextual support than
traditionalRAGapproaches. Wedemonstrate
the superiority of GRAD under budget
constraints, where we limit both the number of
tokens used per demonstration and the number
of tokens used for the final output. Trained
solely on a math dataset, GRAD consistently
outperforms strong baselines on Qwen2.5-14B
across mathematical reasoning and advanced
STEM questions, highlighting GRAD’s
robust generalization to out-of-distribution
(OOD) domains such as physics, chemistry,
and computer science. Furthermore, we
show that demonstrations generated by
trained smaller models can effectively guide
larger target models, reducing training costs
while maintaining competitive accuracy.
Overall, this work introduces a scalable
demonstrationgeneratormodelpresentingthe
first step toward a dynamic few-shot learning
paradigm in resource-constrained settings.
We release the code used for the project:
https://github.com/charafkamel/
GRAD-demonstration-sampler
1 Introduction
LargeLanguageModels(LLMs)excelinmulti-step
reasoningtasks,oftenoperatinginazero-shotset-
ting. Techniques such as context augmentation and
∗Equal contribution.few-shotlearning(Brownetal.,2020)arepowerful
and widely used to further enhance efficiency on
downstream tasks. However, the effectiveness of
these methods strongly depends on the quality and
relevanceofthedemonstrations. Awell-established
methodtoaddressthischallengeandtoenablecon-
textualizationisRetrieval-AugmentedGeneration
(RAG) (Lewis et al., 2020), (Guu et al., 2020),
whichincorporatesexternalknowledgebypulling
information from static databases. However, due to
thelimiteddocumentset,retrievedexamplesmay
not always align with the query. Recent studies
havealsoquestionedtheeffectivenessoffew-shot
promptingforout-of-distribution(OOD)tasks,sug-
gesting that when demonstrations are mismatched,
theycandegradeperformance(Singhaletal.,2022).
Toaddressthesechallenges,weintroduceGRAD:
a Generative Retrieval-Aligned Demonstrator de-
signed to combine the adaptability of generation
withtheefficiencyofretrieval. GRADdynamically
generatestask-specificdemonstrationsunderastrict
tokenbudget,enablingbetterperformanceinboth
in-distribution(ID)andOODtasks. Toachievethis,
wetrainGRADwithreinforcementlearning(RL),
which is the core mechanism to produce demon-
strations, steering target models toward correct and
concise generations.
Additionally,weconsideravariantcalledGRADi
(GRADinitialized), where the generator is first ini-
tializedwithsupervisedfine-tuning(SFT).Thisaux-
iliarystepprovidesinitialguidancetothedemon-
strator, after which the training process is identical
to the one used with GRAD. Unless stated other-
wise,weuseGRADtodenoteourmethodingeneral,
andwewillexplicitlydistinguishbetweenGRAD
and GRADi in the results section.
Weevaluateourapproachondiversereasoning
benchmarks(seeSection3.6)usingmodelsranging
from3Bto14BparametersandobservethatGRAD
variants consistently outperform RAG across all
evaluated datasets for our largest model.arXiv:2510.01165v1  [cs.CL]  1 Oct 2025

ízHow much does it cost you for lunch today at Subway if you pay $40 for a foot-long fish sub and thrice as much for a
six-inch cold-cut combo sub?
úLet the cost of the six-inch cold-cut combo sub be ( x ).
According to the problem, the cost of the foot-long fish
sub is $40, and the cost of the six-inch cold-cut combo
sub is three times less than the foot-long fish sub.  [...]
Therefore, the total cost for lunch today at Subway is
approximately $53.33. However , since the exact value is
required:
#### 53.33
**Reasoning**: 
- Cost of foot-long fish sub = $40
- Cost of six-inch cold-cut combo sub = 3 * $40 = $120
- Total cost = $40 + $120 = $160
#### [160]Query
### Question: The Early Bird Dinner of fered dinner
meals at half of f the menu price if you eat between 2-4
pm.  Curtis ordered the Salisbury Steak that costs
$16.00 and Rob ordered the Chicken Fried Steak at
$18.00.  If they ate at 3 pm, what was the cost of their
total bill?
### Answer: Since Curtis and Rob ate during the Early
Bird Dinner time frame, their meals were half of f, making
the Salisbury Steak $8 and the Chicken Fried Steak $9.
Thus, the total cost of their meal was $8 + $9 = $17. The
answer is $17. [END_DEMO]RAG
### Question: Mike and John dined at the Taco Palace
restaurant.  They each ordered the Taco Grande Plate as
their main meal, but Mike also ordered a side salad for
$2, a plate of cheesy fries for $4, and a diet cola for $2. 
As a result, Mike's lunch bill was twice as large as John's
bill.  What was the combined total cost, in dollars, of Mike
and John's lunch?
### Answer: Let x represent the cost of John's Taco
Grande Plate and y represent the total cost of Mike's
meal. [...] The combined total cost of their lunch is 8 + 16
= 24. The answer is 24 dollars. [END_DEMO]Example 1
Example 2
AnswerExample 1:
**Scenario**: At a local café, a customer buys a large
coffee for $3 and three times that amount for a slice of
cake. How much did they spend in total?
**Reasoning**: 
- Cost of cof fee = $3
- Cost of cake = 3 * $3 = $9
- Total cost = $3 + $9 = $12
#### [12]
[END_DEMO]Example 1
Example 2:
**Scenario**: A student buys a notebook for $5 and
spends three times that amount on a set of colored
pencils. What is the total expenditure?
**Reasoning**: 
- Notebook cost = $5
- Colored pencils cost = 3 * $5 = $15
- Total cost = $5 + $15 = $20
#### [20]
[END_DEMO]Example 2
AnswerGRADFigure1: ExampleinputqueryfromGSM8KwithdemonstrationsandoutputsfromRAGandGRAD.RAGretrieves
demonstrations from a static database, whereas GRAD generates task-specific demonstrations within a token budget.
Afinaloutputlengthconstraintisappliedinbothcases;GRADproducesshorterdemonstrationsandamoreconcise
final answer.
Our main contributions are as follows:
•We propose GRAD, an RL-trained genera-
tive model that is optimized to produce task-
specific, token-constrained demonstrations
that generalize to OOD tasks.
•WepresentGRADi,analternativetoGRAD
thatiswarm-startedwithSFTbeforeapplying
the same RL pipeline as used in GRAD.
•Weintroduceacompositerewardfunctionthat
trains GRAD and GRADi to generate infor-
mativeyetbudget-constraineddemonstrations.
Thiseffectivelymitigatesthetendencyoflarge
models to produce excessively long outputs,
resulting in shorter, more compact answers.2 Related Work
In-context Learning and Prompting.Few-shot
promptingenablesLLMstoreasonmoreeffectively
by embedding relevant examples into the input.
Brown et al. (2020) demonstrated that prompting
with examples allows large models to generalise
withoutgradient updates. Weietal.(2023)showed
that larger models show new capabilities in ICL
settings,particularlyinadaptingtounusualinput-
output pairs that differ from those seen during
training. Subsequent work by Min et al. (2022)
and L. Zhao and Li (2023) emphasized the im-
portance of input format and semantic similarity
between prompts and queries. Kojima et al. (2022)

showedthatevenminimalpromptslike“Let’sthink
stepbystep”significantlychangereasoningstyle.
Complementingthesefindings,Chenetal.(2023)
examinedtheeffectofdemonstrationquantity,re-
vealingdiminishingreturnsbeyondasmallnumber
of examples. These studies confirm that prompting
structure influences not only model accuracy but
also the verbosity of the output. Building on this,
our method focuses on applying ICL by generating
high-qualitydemonstrations,enhancingthemodel’s
ability to generate concise reasoning leading to the
correct solution.
RAG.RAG improves LLM performance by en-
hancing the model’s input with information re-
trieved from external sources. The original RAG
framework laid the foundation by combining a
retriever and a generator for open-domain tasks.
More recently, Chen et al. (2024) systematically
assessed how different LLMs benefit from RAG,
and Gao et al. (2023) offered a survey that outlined
its challenges and design space. Notably, while
ICL traditionally relied on fixed demonstrations,
integrating RAG to fetch relevant in-context exam-
ples has proven to be more effective. Liu et al.
(2021) found that selecting semantically similar in-
contextexamplesenhancesGPT-3’sperformance,
while Huang et al. (2023) introduced CoT-Influx,
a method that prunes less informative tokens to
include more concise Chain-of-Thought examples,
significantly boosting mathematical reasoning. De-
spiteitsadvantages,RAGfaceschallengesinOOD
scenarios. Finlaysonetal.(2025)highlighted that
fine-tuning LLMs with RAG can lead to perfor-
mance degradation when the training data is OOD,
causing misalignment between retrieved content
and target responses. Motivated by these limita-
tions, we propose a dynamic alternative: instead
of retrieving, our model learns to actively generate
demonstrations relevant to the input.
Generative Demonstration Learning.Agrow-
inglineofresearchfocusesongeneratingdemon-
strations dynamically, rather than solely relying
on retrieval. Methods like Self-ICL Wang et al.
(2022) and Auto-CoT Zhang et al. (2022) synthe-
sizetask-specificexemplars,guidingmodelswith
tailored chains of thought that often enhance per-
formance,especiallyinOODsettings. Morerecent
approaches, such as Auto-Demo Prompting Feng
et al. (2024), create demonstrations during batch
inferencebyreusingearlieroutputs,reducingtoken
overheadwhilepreservingaccuracy. Similarly,inlong-context QA, context recycling can generate
effective few-shot examples from the input pas-
sage itself Cattan et al. (2024). These generative
strategies offer fine-grained control over demon-
strationlengthandstructure,balancingreasoning
quality with efficiency in both token usage and
compute cost. However, these methods rely on
vanilla models prompted to generate demonstra-
tions. In contrast, we propose training a model
toquery-adaptive,token-budgeteddemonstrations
that condition the target model’s reasoning.
Reinforcement Learning RL.RL allows us to
move beyond static fine-tuning by optimizing the
modelbasedonoutcome-drivenfeedback. Schul-
man et al. (2017) introduced Proximal Policy Opti-
mization(PPO),offeringstablepolicyoptimization.
Ouyang et al. (2022) demonstrated the effective-
ness of RLHF for instruction-following models,
while Rafailov et al. (2023) proposed DPO to align
outputswithpreferenceswithoutexplicitrewards.
Formathematicalreasoning,recentworkssuchas
Zhang and Zuo (2025) and Shao et al. (2024) intro-
ducedGroupRelativePolicyOptimization(GRPO),
which promotes more stable training and improves
performance on complex multi-step problems. Re-
centworkalsoexplorestrainingefficiency: Wang
etal. (2025)showedthat minimalsupervisioncan
drive reasoning gains, and Li et al. (2025) pro-
posed AGPO to stabilize and optimize training.
These findings clearly show that integrating RL
in our model is particularly promising for gener-
atinghighlycontextuallyrelevantdemonstrations
effectively.
Sentence Embedding and Dataset Similarity.
Accurately measuring semantic similarity is essen-
tialforselectingrelevantdemonstrationsinretrieval-
basedsystems. Motivatedbythis,Jiangetal.(2023)
investigatestheuseoflargelanguagemodelsforsen-
tenceembeddings,showingthatin-contextlearning
improves embedding quality without fine-tuning.
Zhangetal.(2024)proposedpromptengineering
techniques like Pretended Chain of Thought and
Knowledge Enhancement. Comprehensive surveys
by Farouk (2019) and AI (2025) provide insights
into various approaches for measuring sentence
similarity. These key insights motivate our use
of similarity scoring in our work: we employ it
to efficiently retrieve relevant documents in our
RAG pipeline and systematically evaluate and rank
datasets based on their similarity to the training
data.

3 Methodology
3.1 Data Preprocessing
Dataset Collection and Splitting.The main
datasetthatweuseistheMathReasoningDataset
with Diverse Difficulty (MRD3) (Sun and Zhang,
2023). MRD3 was created by merging the training
datasets of math reasoning benchmarks - including
GSM8K,MAWPS,MAWPS-singleand1000ran-
dom samples from AQuA. GPT-4 is then used to
generate formatted Chain-Of-Thought (CoT) rea-
soning steps for each question.
The original MRD3 contains 9.7k question-
answerpairs. Toremoveredundancy,wedropdupli-
catedquestionsbasedonpairwisecosine-similarity
overtheTF-IDF(SaltonandBuckley,1988)vectors
of the input queries. For each sample, we compare
thesimilaritywithallsubsequentinputsandremove
anyentrywithasimilarityscoreof1. Afterfiltering,
theresulting8081distinctsamplesaresplitintotwo
stages. First, 10% is set aside as an independent
test set. The remaining 7273 samples are further
splitintoevaluation(10%),RAG(25%)andtrain
(65%) subsets. The RAG corpus is used to retrieve
relevant demonstrations during GRADi training
and evaluation. During SFT training, RAG demon-
strationsserveasexamples,showinghowqueries
similartotheuser’sshouldbestructured. Besides
training, RAG also serves as a solid baseline to
compare our method with.
Preparation for RAG Integration.To integrate
MRD3intotheRAGsystem,eachdatasamplewas
transformedintoastandardizedstructure,expressed
asQuestion + CoT Reasoning + Answer . We
consider one instance of this structure as a demon-
stration. After formatting the dataset with the new
schema, eachdemonstrationisembeddedusingthe
pre-trained sentence-transformer modelall-mpnet-
base-v2(Sentence-Transformers, 2020) and stored
inChroma DB(Chroma, 2023). During inference,
for each input query, the system retrieves the top
twomostrelevantdemonstrationsfromtheretrieval
database. Thesedemonstrationsareconcatenated
withtheinitialqueryandpassedtoasecond,frozen
LLM(targetmodel),whichgeneratesthefinalan-
swer (see Figure 2).
3.2 Answer Extraction and Evaluation
To ensure consistent answer extraction, we prompt
the model to generate structured output by con-
cludingitsreasoningwiththeformat #### {finalanswer}, as specified in Section A.6. In some
cases, particularly with smallermodels, the output
may deviate from this format. To address this,
we apply a regular expression designed to extract
all digits, fractions, or numerical values from the
model’s output, retaining the last extracted number
asthemodel’sfinalprediction. Wealsotrackand
report instances where the extraction process fails.
Thefinal answerisvalidated bycomparingit with
the expected output, with a tolerance of 10−4for
numerical values to qualify as correct.
3.3 Token Budget and Demonstration
Constraints
To ensure a fair comparison between GRAD and
our other baselines, we constrain the number of
tokensusedforgeneratingboththeinstructionsand
the final output.
Instruction Length Constraint.As GRAD is a
generativemodel thatproducesavariablenumber
of tokens per demonstration, it is crucial to limit
its output tokens to ensure fair comparison with
the baseline methods. To establish a suitable token
budget,weanalyzedthetokenlengthdistribution
of the RAG demonstrations in the RAG split and
foundthat,onaverage,eachRAGsamplehas150
tokens. Asweretrieve2demonstrationsfromRAG
for each input query, to match this number with
our GRAD-generated demonstrations, we had to
applyacapof300tokens,i.e. demonstrationsare
truncated if they exceed this threshold. As this
limit is also applied during RL training, the model
will learn to generate demonstrations that are short
and complete under this constraint. Having this
constraint, we also eliminate the possibility that
GRAD outperforms RAG due to more generated
tokens provided as a context.
Final Output Length Constraint.To ensure
comparability with the RAG baseline and to pre-
vent unnecessary token usage, we limit the final
output of the model (i.e. the reasoning trace and
the final answer) to 256 tokens. We find that the
average length of the final output in the MRD3
dataset is around 150 tokens, demonstrating that
effectivereasoningcanbeexpressedconcisely-fur-
ther supporting the 256-token limit as a practical,
performance-aligneddesignchoice. Furthermore,
our training encourages the model to internalize
that generating shorter, high-quality demonstra-
tions within a fixed budget improves downstream
performance.

3.4 Training
3.4.1 Supervised Finetuing (SFT)
For the SFT-only baseline and for the warm-up
stage of GRADi, we train the LLM to replicate the
behavior of the RAG system. To do this, we use
training data formed by pairing each input query
with its corresponding retrieved demonstrations.
For each input query, we ask the model to generate
two relevant demonstrations as explained in Sec-
tion 3.1. This process helps the model learn to
outputwell-formedandcoherentreasoningpaths.
AlthoughtheSFT-aligneddemonstratoriseffective
in generating high-quality outputs in ID settings, it
often struggles with OOD scenarios, because SFT
is optimized to mimic reference RAG demonstra-
tions, rather than to explicitly maximize the final
answer accuracy. As a result, the model may pro-
ducesuboptimaldemonstrationswhenfacedwith
unfamiliar inputs. To overcome this limitation, we
introduce RL to directly align demonstration gener-
ationwiththefinaltaskobjective,therebyenabling
more robust generalization in OOD scenarios.
3.4.2 Reinforcement Learning (RL)
We evaluated and compared two RL algorithms,
namely PPO and GRPO. While PPO is commonly
used in previous works, GRPO consistently offered
better stability and efficiency in our experiments.
Therefore, we adopt GRPO for all final results
reported in this work. Our RL training pipeline (as
shown in Figure 2) follows four core steps:
Step1: DemonstrationGeneration.Theinput
query is passed to the Demonstration Generation
module, which produces a set of relevant few-shot
demonstrations within the instruction length con-
straint defined in Section 3.3.
Step 2: Final Answer Generation.The gener-
ateddemonstrationsarecombinedwiththeoriginal
query and the system prompt, then passed to the
target model. The model produces a complete
reasoning process and a final answer in the same
format as the provided demonstrations. The final
answer is extracted from the output (as described
in Section 3.2), and the entire output is constrained
to 256 tokens (as explained in Section 3.3).
Step3: LogProbabilityExtraction.Toevaluate
howwellthedemonstrationssupportcorrectanswer
generation,wereplacethefinalansweroftheoutput
reasoning with the correct answer and feed it back
into the frozen LLM. The model then computesthe log probabilities of the tokens in the correct
answer, denoted as [ 𝑃1,𝑃2, ...,𝑃𝑛]. These log
probabilities provide a fine-grained view into how
confidently the model predicts each token of the
correct answer given thecontext, i.e. input query +
generateddemonstrations+targetmodelreasoning.
Step 4: Multi-objective Truncation-Aware Re-
ward.Therewardfunctionisdesignedtoimprove
modelaccuracywhileencouragingthegeneration
of useful demonstrations without exceeding a fixed
token budget. Inspired by Huang et al. (2023), the
reward comprises three components: Log Proba-
bilities Reward ( 𝑅p), Accuracy Reward ( 𝑅acc), and
Demonstration Count Reward (𝑅 demo).
The𝑅pevaluates the model’s ability to predict
thecorrecttokensbasedontheirlogprobabilities
as defined by Equation 1. The 𝐿llmis the mean
negativelogprobabilityofthecorrectanswergiven
thecontext. Weusethemeanlogprobability(i.e.,
geometricmeanoftokenprobabilities)toprevent
biasagainstlongeranswers. Byconstruction,lower
𝐿llmvalues (i.e., higher predicted probabilities)
correspond to higher rewards. This equation not
onlyrewards confidentandcorrect predictionsbut
also emphasizes the value of less confident but
correct predictions. Moreover, it provides partial
creditforincorrectanswerswhenthecorrectoption
also has high confidence.
𝑅𝑝=1
1+𝐿llm(1)
The𝑅accisdesignedtoencouragethemodelto
produce a correct and complete final answer. It
is a binary reward: the model receives a score of
1 if the final answer is correct and is fully gen-
erated (i.e., not truncated) and 0 otherwise. This
truncation-awareformulationencouragesthemodel
toprioritizeaccuracywhilestayingwithinthetoken
budget limit.
The𝑅demogiven by Equation 2incentivizes the
generation of valid demonstrations. Let 𝑛denote
the number of generated demonstrations and 𝐷the
expected target count. We set 𝐷=2to match
the RAG baseline. We maximize the 𝑅demofor
GRAD if it generates four demonstrations, as more
conciseandrelevantdemonstrationswouldtypically
exceed the 300-token budget. This maintains fair
comparisonwiththeRAGbaseline,asweusethe
same token limit, and it allows for greater diversity.
𝑅demo=𝑛
𝐷·⊮{𝑛≤4} (2)

Question
Demonstrations
System Prompt
  - Demonstration 1
  - Demonstration 2
Question
System prompt
Final Answer
[P1, P2 ..., Pn]
Reward Function
Step 1: Demonstrations generation from the question Step 2: Final answer generation
Step 3: Extract the probability of the correct answer Step 4: Reward computation
Answer
Extractor
Adapters
Demonstrations
Generator LLM
Correct Answer
LLM
Reasoning
Reasoning 
#### Correct answer
Reasoning 
#### Final answer
Figure2: OverviewoftheGRADpipeline. Step1: Themodelgeneratesdemonstrations,whichareconcatenated
with the system prompt and the user’s query to form the context. Step 2: The context guides the target model to
produceareasoningtraceandthefinalanswer. Step3: Thepredictedanswerisreplacedwiththecorrectanswerand
passedthroughthefrozenLLMtocomputethetoken-levellogprobabilities. Step4: Computingamulti-objective
reward to ensure confidence and correctness of the final answer and compliance with the token budget.
This cap ensures the model doesn’t exploit the
reward function by generating excessive or low-
quality demonstrations. Since at least 100 tokens
areallocatedperdemonstration,thetokenbudget
naturally limits the number of useful demonstra-
tions, andthisrulefurther preventsrewardhacking.
The Final Reward is given by Equation 3:
Reward=𝑅 𝑝+𝑅acc+𝑅demo (3)
In summary, the reward balances 3 objectives:
ensuring answer accuracy ( 𝑅acc), rewarding high-
confidence reasoning ( 𝑅𝑝), and promoting the gen-
eration of relevant, concise, and valid demonstra-
tions (𝑅demo) all while respecting the token budget.
3.5 Demonstration sampling strategies
In our framework, GRAD refers to the RL-only
variant, and GRADi refers to the version that is
initialized with an additional SFT stage before RL.
Duringevaluation,wecomparebothvariantswith
the baseline strategies. All the demonstration sam-
plersarepromptedtogeneratetwofactuallycorrect
anddifferentexamplesusinglessthan300tokens.
All target models are instructed to generate the
correctfinalanswerinthepre-definedformatusing
only 256 tokens (see A.6). The following provides
a brief explanation of each strategy considered.
Zero-shotmodel.Themodelgeneratesanswers
without demonstrations, and an extractor retrieves
the final response. This zero-shot setup serves as
the baseline for comparison.RAG.Themodelretrievestwodocumentsfrom
afixeddatabasebasedonquerysimilarity. These
are combined with the input to guide the LLM’s
answer. WhileRAGimprovesperformanceusing
added context, it’s limited by the fixed database,
whichmaylackrelevantexamplesforOODqueries.
SFT-onlymodel.Weconductedanotherevalua-
tiononthemodelswhichweretrainedwithSFTbut
without RL. This pipeline also serves as a baseline
for comparison with the GRAD variants.
BASEmodel.Inthissetup,theuntrained(vanilla)
modelcreatesitsowndemonstrations. Theseself-
generateddemonstrationsarethenusedtoanswer
the final question. Although this setup benefits
from the model’s ability to adapt dynamically to
the input query, the lack of training might result in
suboptimal performance.
GRAD model.GRAD is our RL-only variant,
inwhichthemodelisoptimizeddirectlytogener-
ate task-specific demonstrations using a composite
reward (see Step 4 in Section 3.4.2). These demon-
strations are concatenated with the input to provide
richer context, allowing the model to adapt to di-
versequeriesandovercomethelimitationsofRAG.
GRADi model.GRADi combines the two train-
ing stages by first initializing the generator with
SFT and then continuing optimization with RL.
Unlike the SFT-only baseline, where no additional
training is applied after the SFT step, GRADi uses
SFT as a warm-start to stabilize the format and the
structure of the demonstrations before using RL.

Model MethodDatasets
GSM8Kdraw
structuredMathQA*deepmind
basic_mathARC
ChallengeMMLU*
Qwen2.5
7B
InstructZero-shot 83.89 36.50 44.79 67.78 87.71 62.22
RAG 83.59 36.50 43.78 63.33 85.92 59.51
SFT-only 74.05 41.00 42.38 60.00 84.39 59.88
BASE 85.8038.50 49.73 64.4488.1462.59
GRAD 84.2743.00 54.72 70.0088.0564.20
GRADi 84.7347.00 53.11 68.8988.0562.71
LLaMA
3.1-8B
InstructZero-shot 78.24 42.00 44.04 43.3383.5349.13
RAG 76.79 29.50 40.67 38.89 73.89 39.75
SFT-only 71.91 33.50 39.9160.0075.51 42.72
BASE 75.73 37.50 39.97 48.89 82.5152.47
GRAD 78.85 46.50 45.1246.67 80.80 50.00
GRADi 77.1043.0042.70 38.89 81.48 51.23
Qwen2.5
14B
InstructZero-shot 72.75 30.50 27.73 58.89 91.13 48.27
RAG 83.89 27.50 37.50 64.44 90.70 48.52
SFT-only 83.66 36.50 42.00 65.56 74.83 40.74
BASE 84.12 34.00 43.78 70.0092.3259.75
GRAD 90.92 40.50 56.98 72.2291.6465.31
GRADi 90.46 45.00 57.80 70.0091.9865.06
Table 1: Performance Comparison across Models and Methods (Accuracy in %). The same backbone model is used
for both the demonstration sampler and the target model. Datasets are ordered by their semantic similarity from left
to right in decreasing order. Blue indicates cases where GRAD outperforms all baselines (independent of GRADi),
whilegreenindicatescaseswhereGRADidoesso(independentofGRAD).Ifthebest-performingmodelonagiven
benchmark is neither GRAD nor GRADi, it is reported in bold.
3.6 Evaluation Setup
Evaluation Datasets.To evaluate the model’s
ability to generalize beyond the training domain,
we conduct experiments on ID and OOD bench-
marks. Specifically, we assess our strategies on
theGSM8Kdataset, which serves as the primary
IDbenchmark,andonfivediverseOODdatasets:
MMLU,MathQA,draw-structured,DeepMind ba-
sic_math, andARC_challenge.
For theMMLUbenchmark (Hendrycks et al.,
2021), we selected five subsets:college_physics,
formal_logic,college_computer_science,col-
lege_chemistry, andmachine_learning, and
grouped them under a single merged evaluation
dataset referred to asMMLU*. Similarly, from the
MathQAdataset(Aminietal.,2019),weincluded
subsets such asphysics,gain,other,general, and
geometry,andmergedthemintoacombineddataset
labelledMathQA*. We use each dataset’s standard
testsplitandreporttheaccuracyusingapredefined
answer format and token-budget.Dataset Similarity Computation.To measure
similaritybetween eachevaluationdataset andthe
MRD3 training set, we compute pairwise cosine
similaritiesusingsentenceembeddings. Foreach
sample in our evaluation dataset, we identify the
2mostsimilartrainingexamplesbasedoncosine-
similarity,aswewoulddowhenretrievingtheRAG
documents. We then average these top-2 similarity
scores for each test sample, and finally compute
the overall similarity by averaging across the entire
evaluationdataset. Thisresultsinasinglesimilarity
score between the train and a benchmark dataset.
Lower values of similarity indicate that the dataset
is more OOD compared to the training data.
4 Results and Discussion
4.1 Models performance
Wecomparetheperformanceofourmethodsonsix
benchmark datasets. The results can be found in
Table1,whichpresentstheaccuracy(%)ofvarious
models; all of them having the same backbone.

Models inthe tableare arrangedbyincreasing size,
fromthesmallermodelsatthetop(e.g. Qwen2.5-
7B) to larger ones at the bottom (e.g. Qwen2.5-
14B). Within each model group, six methods are
comparedasdescribedinSection3.5. Thedatasets
are also ordered by their semantic similarity to the
trainingdataset,fromlefttoright:GSM8K(most
similar)toMMLU*(mostdissimilar). Thissetup
allowsustoevaluatethegeneralizationperformance
of the baseline strategies and both GRAD versions.
4.2 Results Analysis
Table 1 presents a consistent pattern: as model
sizeincreases,theperformanceofGRADmodels
improvessubstantiallyacrossalldatasets.Qwen2.5-
7Bachieves the highest accuracy on 4 out of 6
datasets,significantlyoutperformingbothZero-shot
and RAG. The advantage is even more pronounced
at the scale of 14B, where GRAD consistently
dominates nearly all benchmarks.
TheheatmapinFigure3showshowtheperfor-
mance of GRAD and RAG differs. Lighter shades
indicate similar differences, while red and blue
show gains favoring GRAD and RAG, respectively.
Inthetop-leftregion,whichcorrespondstosmaller
models on more ID datasets, the differences are
marginal. Moving to the right, where smaller mod-
els are evaluated on increasingly OOD datasets,
GRAD shows significant improvements, demon-
strating its ability to generalize beyond training
distributions. In the bottom row, the 14B model
consistently outperforms RAG across nearly all
datasets, highlighting the robustness of large-scale
GRADinfew-shotsettings. Furtheranalysiscom-
paring GRAD with alternative baseline approaches
is presented in Appendix A.5
In summary, our findings confirm that dynam-
ically generated demonstrations produced by RL-
trained demonstrators outperform static retrieval,
especiallyforOODdatasets. Thisadvantagegrows
significantly with larger models, which produce
better-aligned examples.
4.3 Can Smaller Models Provide Good
Demonstrations for Larger Models?
Weinvestigatewhetherdemonstrationsgenerated
by fine-tuned, smaller, cheaper models can be ef-
fectively used by a much stronger model (Qwen2.5-
14B) during inference. This approach could signif-
icantly reduce computational cost during demon-
stration generation. To isolate the effect of the
demonstrationsource,wefixtheanswer-generationmodel to Qwen2.5-14B and vary only the model
responsible for generating the demonstrations.
Table 2 reports the performance when using
demonstrationsgeneratedbythe3B,7B,and14B
versions of Qwen2.5. The smaller models (3B and
7B) are fine-tuned to generate demonstrations for
the bigger (14B) model, while the 14B-BASE is
the vanilla, untrained version. We observe that
demonstrationsgeneratedbythesmaller,fine-tuned
models can yield surprisingly competitive perfor-
mance when used by a much larger model for final
answer generation. Fine-tuned smaller models can
serve as cost-efficient demonstration generators
without significantly compromising performance.
Dataset3B-
Demo7B-
Demo14B-
BASE
GSM8K 66.67 72.2284.12
draw_structured 42.547.0034
MathQA* 51.2354.1343.78
deepmind
basic_math66.6772.2270
MMLU* 50.73 57.28859.57
Table2: Accuracy(%)ofQwen2.5-14Busingdemon-
strationsgeneratedbyitself(untrained,vanillamodel)
orbysmaller,fine-tunedmodels. Datasetsareordered
by their semantic similarity from top to bottom in de-
creasingorder. Themodelwiththehighestperformance
on the corresponding benchmark is highlighted in bold.
4.4 Generalization to Non-Mathematical
Tasks
While our training focused primarily on mathe-
matical reasoning, we evaluated performance on
two diverse benchmarks: ARC Challenge v2 and
selected MMLU subsets (e.g., formal logic, com-
puter science, physics). These represent a new
multiple-choice setting that differs substantially
from mathematical reasoning and also span dis-
tinctdomains. Thisevaluationallowsustoassess
whether the capabilities learned in a math-centered
training pipeline can transfer effectively to broader
reasoningchallenges. Despitethisshift,onMMLU,
our GRAD pipeline shows strong generalization,
withlargermodels(e.g.,Qwen2.5-14B)consistently
outperforming the zero-shot baseline by more than
17.5%and surpassingRAGby over16.5%. In the
case of ARC Challenge v2, the zero-shot already
attainshighaccuracy,andaddingdemonstrations
yieldsnomeasurablegainsandcanintroducenoise.

GSM8k_structured (0.69)draw_structured (0.67) mathqa_physics (0.63)mathqa_gain (0.63)mathqa_other (0.60)
mathqa_general (0.59)mathqa_geometry (0.58)
deepmind_mathematics_basicmath (0.52)high_school_mathematics (0.46)ai2_arc (0.34)
college_physics (0.34)formal_logic (0.32)
college_computer_science (0.30)college_chemistry (0.28) machine_learning (0.22)
DatasetLlama-3.2-3B
Qwen-2.5-3B
Qwen-2.5-7B
Llama-3.1-8B
Qwen-2.5-14BModel1.22 -5.00 1.03 3.06 -6.60 -1.16 -1.71 4.44 26.66 36.43 17.64 24.60 27.00 19.00 20.53
-4.42 -0.50 -2.26 -3.83 -1.10 -0.26 0.00 -8.89 33.33 36.69 19.61 16.67 17.00 12.00 14.28
0.68 6.50 12.09 6.91 18.68 11.58 9.40 6.67 9.25 2.13 -3.92 0.00 3.00 7.00 6.25
2.06 17.00 3.48 6.40 16.49 3.34 0.00 7.78 10.74 6.91 16.66 5.56 14.00 4.00 10.72
7.03 13.00 20.49 17.65 19.78 20.08 17.09 7.78 17.78 0.94 32.35 11.11 16.00 15.00 8.93Heatmap to compare GRADi with RAG
40
20
02040
Accuracy difference: (GRADi with RAG)Figure3: HeatmapofaccuracydifferencesbetweenGRADiandRAG.ReddenotesgainsforGRADi,blueforRAG,
and lighter cells indicate similar performance. Datasets on the x-axis are ordered by their semantic similarity from
lefttorightindecreasingorder. Modelsaredefinedinthey-axis,orderedbasedontheirsizefromtoptobottom.
Eachcellshowsthemeanpercentage-pointdifferenceinexact-matchaccuracyforthat(model,dataset)pair,with
the colorbar indicating magnitude and sign.
5 Conclusion
Thisworkintroducedgenerativeretrieval-aligned
demonstrationsamplers(GRADandGRADi)for
creatinginput-aligneddemonstrationsunderstrict
token budgets, that consistently outperforms tra-
ditionalRAGandzero-shotmethods,particularly
for OOD scenarios. Our findings highlight the
benefits of dynamic generation over static retrieval,
particularlythegeneralizabilityofourapproachbe-
yondthetrainingdomaintoOODqueriesthatdiffer
in both task and domain. We also show that our
approach is modular and cost-efficient: demonstra-
tionsgeneratedbysmaller,lessexpensivemodels
canboosttheaccuracyoflargermodels. Wealso
emphasizethatourapproachdoesnotreplaceRAG.
Retrievalisstillapplicableforknowledge-intensive
tasksthatmightrequiregroundingfromanexternal
corpus. Both GRAD variants can be best viewed
as an alternative that aims to generalize to OOD
inputs, which complements, rather than replaces,
retrieval-based methods.6 Future work
An important and promising direction for future
research is to explore a new, hybrid GRAD (called
H-GRAD) architecture that would combine the
strengthsofgenerativeandretrieval-basedmethods.
Inthissetup,themodelwoulddynamicallyselect
betweenthetopretrievedandgenerateddemonstra-
tions, based on the relevance to the input query.
H-GRAD could maintain the robustness and the
OODgeneralizationofGRADwhileleveragingthe
factualadvantageofRAG.Webelievethislineof
research has the potential to deliver a more reli-
able few-shot reasoning system that will improve
applicability and accuracy. Additionally, we will
explorehowthenumberofdemonstrationsgener-
ated,thedemonstrationlengthandthetokenbudget
affect model training dynamics and the final output
token length, providing deeper insights into the
interplay between the number of demonstrations,
demonstrationsize,outputconciseness,andanswer
correctness.

7 Limitations
7.1 Token Length Constraint
In all experiments, we limited the output tokens of
thefinalanswergenerationtoafixednumberof256
tokens. Complex reasoning tasks (especially those
involvingmultiplesteps)mayrequirelongeroutputs
to fully express the solution. We also applied a
separate cap of 300 tokens for the total length of
generateddemonstrationsi.ethemodelcangenerate
as many demonstrations as it can, as long as the
total remains within this token limit. While we
fixed these hyperparameters for consistency and
comparability,thisconstraintimplicitlylimitsthe
number of demonstrations that can be provided,
especially for tasks that require detailed examples.
7.2 Fixed Number of Demonstrations
AcrosstheRAGandGRADpipelines,afixednum-
ber of demonstrations were used per input query,
regardless of its complexity. GRAD was trained to
generateexactlytwodemonstrations,whileRAGre-
trievedastaticsetoftwoexamples. Thisconstraint
may lead to under-informing complex queries or
overwhelming simple ones. A fixed number of
examples reducesflexibilityand maydegradeper-
formance on tasks with highly variable difficulty.
7.3 Demonstration Factuality and Reliability
While GRAD dynamically generates demonstra-
tions tailored to each input, there is no verification
step to ensure these demonstrations are factually
accurate and reliable. The system assumes that
the generated examples are educationally sound,
but no formal check is applied during training or
inference. This limitation opens the possibility
for the model to use misleading reasoning paths,
which could negatively impact the final answer
quality. Althoughpriorworkhasshownthateven
partiallyincorrectdemonstrationscansometimes
enhance performance, it would be valuable to filter
out incorrect demonstrations.
8 Ethical consideration
In this work, we demonstrate that GRAD outper-
formstraditionalRAGmethodsinOODscenarios
under a budget constraint. However, generative
approachesraiseimportantethicalconsiderations.
While RAG systems retrieve documents from a
controlled database, generative models produce
demonstrations dynamically, with no inherent con-
trol over the content. GRAD-generated demon-strations might reflect some biases present in the
training data or introduce false, misleading demon-
strations. Sincegenerateddemonstrationsarecre-
ated dynamically, it is difficult to fully control and
remove potentially harmful generations.
References
Cohere AI. 2025. What is sentence sim-
ilarity? https://docs.cohere.com/docs/
what-is-sentence-similarity.
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-
Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
2019. Mathqa: Towards interpretable math word
problem solving with operation-based formalisms.
arXiv preprint arXiv:1905.13319.
TomB.Brown,BenjaminMann,NickRyder,Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,Pranav Shyam,GirishSastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, and 12 others. 2020. Lan-
guage models are few-shot learners.Preprint,
arXiv:2005.14165.
Arie Cattan, Alon Jacovi, Alex Fabrikant, Jonathan
Herzig,RoeeAharoni,HannahRashkin,DrorMarcus,
Avinatan Hassidim, Yossi Matias, Idan Szpektor, and
1others.2024. Canfew-shotworkinlong-context? re-
cyclingthecontexttogeneratedemonstrations.arXiv
preprint arXiv:2406.13632.
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
2024. Benchmarking large language models in
retrieval-augmented generation. 38(16):17754–
17762.
JiuhaiChen,LichangChen,ChenZhu,andTianyiZhou.
2023. How many demonstrations do you need for in-
context learning?arXiv preprint arXiv:2303.08119.
Chroma. 2023. Chroma: A fast, scalable vector
database.
Mamdouh Farouk. 2019. Measuring sentences similar-
ity: a survey.arXiv preprint arXiv:1910.03940.
Longyu Feng, Mengze Hong, and Chen Jason Zhang.
2024. Auto-demoprompting: Leveraginggenerated
outputsasdemonstrationsforenhancedbatchprompt-
ing.arXiv preprint arXiv:2410.01724.
MatthewFinlayson,IliaKulikov,DanielMBikel,Barlas
Oguz, Xilun Chen, and Aasish Pappu. 2025. Post-
traininganllmforrag? trainonself-generateddemon-
strations.arXiv preprint arXiv:2502.10596.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen
Wang,andHaofenWang.2023. Retrieval-augmented
generationforlargelanguagemodels: Asurvey.arXiv
preprint arXiv:2312.10997, 2:1.

KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,
and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. InInternational confer-
ence on machine learning, pages 3929–3938. PMLR.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt.2021. Measuringmassivemultitasklanguage
understanding.Proceedings of the International Con-
ference on Learning Representations (ICLR).
XijieHuang,LiLynaZhang,Kwang-TingCheng,Fan
Yang,andMaoYang.2023. Fewerismore: Boosting
llm reasoning with reinforced context pruning.arXiv
preprint arXiv:2312.08901.
Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing
Wang,andFuzhenZhuang.2023. Scalingsentence
embeddings with large language models.arXiv
preprint arXiv:2307.16645.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid,
Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large
languagemodelsarezero-shotreasoners.Advances
in neural information processing systems, 35:22199–
22213.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez,HaoZhang,andIonStoica.2023. Efficient
memory management for large language model serv-
ingwithpagedattention.Preprint,arXiv:2309.06180.
K. Liu L. Zhao and B. Li. 2023. What makes good
in-context examples for gpt-3?
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni,VladimirKarpukhin,NamanGoyal,Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
and1others. 2020. Retrieval-augmentedgeneration
forknowledge-intensivenlptasks.Advances in neural
information processing systems, 33:9459–9474.
Chen Li, Nazhou Liu, and Kai Yang. 2025. Adap-
tive group policy optimization: Towards stable train-
ing and token-efficient reasoning.arXiv preprint
arXiv:2503.15952.
JiachangLiu,DinghanShen,YizheZhang,BillDolan,
Lawrence Carin, and Weizhu Chen. 2021. What
makes good in-context examples for gpt- 3?arXiv
preprint arXiv:2101.06804.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
MikeLewis,HannanehHajishirzi,andLukeZettle-
moyer. 2022. Rethinking the role of demonstrations:
Whatmakesin-contextlearningwork?arXiv preprint
arXiv:2202.12837.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, PamelaMishkin, Chong Zhang,
Sandhini Agarwal, KatarinaSlama, Alex Ray, and 1
others. 2022. Training language models to follow in-
structions with human feedback.Advances in neural
information processing systems, 35:27730–27744.Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pherDManning,StefanoErmon,andChelseaFinn.
2023. Directpreferenceoptimization: Yourlanguage
modelissecretlyarewardmodel.Advances in Neural
Information Processing Systems, 36:53728–53741.
Gerard Salton and Christopher Buckley. 1988. Term-
weightingapproachesinautomatictextretrieval.In-
formation processing & management,24(5):513–523.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
malpolicyoptimizationalgorithms.arXiv preprint
arXiv:1707.06347.
Sentence-Transformers. 2020. all-mpnet-base-v2 tok-
enizer.
ZhihongShaoand1others.2024. Deepseekmath: Push-
ing the limits of mathematical reasoning in open
language models.arXiv preprint arXiv:2402.03300.
PrasannSinghal,JaradForristal,XiYe,andGregDur-
rett. 2022. Assessing out-of-domain language model
performance from few examples.arXiv preprint
arXiv:2210.06725.
Y. Sun and A. Zhang. 2023. Fewer is more: Boosting
llm reasoning with reinforced context pruning.
YipingWang,QingYang,ZhiyuanZeng,LiliangRen,
Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He,
Kuan Wang, Jianfeng Gao, and 1 others. 2025. Re-
inforcement learning for reasoning in large language
models with one training example.arXiv preprint
arXiv:2504.20571.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A Smith, Daniel Khashabi, and Han-
nanehHajishirzi.2022. Self-instruct: Aligninglan-
guage models with self-generated instructions.arXiv
preprint arXiv:2212.10560.
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert
Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
DaHuang,DennyZhou,and1others.2023. Larger
language models do in-context learning differently.
arXiv preprint arXiv:2303.03846.
Bowen Zhang, Kehua Chang, and Chunping Li. 2024.
Simple techniques for enhancing sentence embed-
dingsingenerativelanguagemodels. InInternational
Conference on Intelligent Computing, pages 52–64.
Springer.
Jixiao Zhang and Chunsheng Zuo. 2025. Grpo-lead: A
difficulty-aware reinforcement learning approach for
concise mathematical reasoning in language models.
arXiv preprint arXiv:2504.09696.
ZhuoshengZhang,AstonZhang,MuLi,andAlexSmola.
2022. Automaticchainofthoughtpromptinginlarge
language models.arXiv preprint arXiv:2210.03493.

A Appendix
A.1 Training setup
Givenafixedcomputationalbudget,wecarefully
selected the parameters and techniques we used to
ensurethebestpossibleperformanceunderresource
constraints. All the obtained results in this study
were computed using NVIDIA A100 GPUs with
80 GB of RAM. The input length was fixed at 256
tokens,theinstructionlengthat768tokens,andthe
output length at 256 tokens.
SFT.Eachtrainingsessionlastedlessthan1hour,
using 2 epochs with a learning rate of 4e-4 and a
batch size of 32 samples.
RL.Thetrainingrequiredsignificantlymorecom-
putationalresourcesduetotheinclusionofmultiple
models and the on-the-fly generation of demonstra-
tions. We used distributed training with 4 NVIDIA
A100 GPUs. Initially, training took approximately
15hoursforasingleepoch,usingalearningrateof
1e-5and2demonstrationspersamplefortheGRPO
model. Increasing the number of demonstrations
resulted in out-of-memory (OOM) errors, which is
a serious limitation that had to be dealt with.
Tomitigatethememoryconstraints,weemployed
Low-Rank Adaptation (LoRA), an efficient fine-
tuningapproachthatallowedustoloadonlythebase
model and apply lightweight adapters dynamically
at runtime. The same model was used for both
demonstration and final answer generation. To
furtherreducememoryconsumption,weenabled
gradientcheckpointing,whichallowedustotrade
compute for memory efficiency. Additionally, all
models were loaded using 16-bit floating point
precision for efficiency.
To acquire faster training speed, we employed
a vllm server - an efficient, high-throughput, low-
latencyinferenceengineforLLMthatintroduces
PagedAttention, a memory-optimized attention
mechanism (Kwon et al., 2023).
Usingthesetechniques,wereducedtrainingtime
toapproximately12hours. Leveragingthesestrate-
gies, we managed to train with 7 demonstrations
persampleforallmodels,exceptforthe14Bmodel,
which was limited to 3 due to memory constraints.
A.2 Used Models
Table 3 provides an overview of all models used in
ourexperiments,includingbothLLaMAandQwen
families across varying model sizes.Family Model Name Size
Meta Llama-3.2-3B-Instruct 3B
Qwen Qwen2.5-3B-Instruct 3B
Qwen Qwen2.5-7B-Instruct 7B
Meta Llama-3.1-8B-Instruct 8B
Qwen Qwen2.5-14B-Instruct 14B
Table 3: Overview of the models used in training
A.3 Dataset similarity
Tobetterunderstandtherelationshipbetweenour
training dataand evaluation benchmarks, wecom-
putedatasetsimilaritywiththetrainingdataset. Ta-
ble 4 presents these similarity scores, ordered from
highest to lowest. As shown,GSM8k_structuredis
themostsimilartoMRD3,whilemachine_learning
is the least similar.
Dataset Similarity # samples
GSM8k_structured 0.6903 1310
draw_structured 0.6729 200
mathqa_physics 0.6348 488
mathqa_gain 0.6329 391
mathqa_other 0.5981 91
mathqa_general 0.5934 777
mathqa_geometry 0.5839 117
mathematics_basicmath 0.5167 90
mmlu-
high_school_mathematics0.4645 270
ai2-ARC-Challenge 0.3406 1172
college_physics 0.3374 102
formal_logic 0.3216 126
college_computer_science 0.2993 100
college_chemistry 0.2829 100
machine_learning 0.2195 112
Table4: SemanticsimilarityscoresbetweenMRD3and
various evaluation datasets
A.4 Token distribution length
Inthissection,wepresentthetokenlengthdistribu-
tionsforbothtypesofoutputs. Thefirstcorresponds
tothe outputofthedemonstrationsampler, which
reflects the instruction token length. We truncate
this output at 300 tokens. The second corresponds
totheoutputofthetargetmodel,whichwetruncate
at 256 tokens.

Figure 4 illustrates the token distributions across all models considered. We observe that for the target
modeloutputs,manytruncatedresponsesoccurwiththe RAGdemonstrations(showninblue). However,
GRAD generate more concise answers with fewer truncations. This indicates that providing short and
focused demonstrations helps the target model to answer concisely and use its limited token budget more
effectivelyforreasoningandfinalresponses. Asimilartrendisobservedforthedemonstrationsampler
outputs: GRAD produces shorter demonstrations with reduced output length.
01000200030004000meta-3.2-3BInstruction T oken Lengths
RAG - Instructions
GRPO_SFT - Instructions
BASE_MODEL - Instructions
0200400600800Output T oken Lengths
RAG - Outputs
GRPO_SFT - Outputs
BASE_MODEL - Outputs
05001000150020002500300035004000qwen2.5-3B
02004006008001000
0500100015002000250030003500qwen2.5-7B
0200400600800100012001400
0500100015002000250030003500meta-3.1-8B
02004006008001000
100 200 300 400 500 600
T oken Length010002000300040005000qwen2.5-14B
50 100 150 200 250
T oken Length025050075010001250150017502000
Figure 4: Token distribution length
Figure 4 shows the token length distribution for both instructions and outputs across models. Our
method,GRAD,isusedduringbothinstructiongenerationandfinaloutputgeneration. ComparedtoRAG
and the base model, GRAD consistently produces shorter prompts and outputs.

A.5 Further analysis
GRADi compared with Zero-shot model.Figure 5 shows a similar comparison for GRADi versus
zero-shot performance. Smaller models on less similar datasets perform better in the zero-shot setup,
suggestingtheytreatGRADi’sdemonstrationsasnoise. Incontrast,largermodels(Qwen2.5-14B)gain
significantlyfromGRADionsimilardatasets,indicatingthatbiggermodelsleveragedemonstrationsmore
effectively than smaller ones.
GRADi compared with BASE model.Figure 5 compares the accuracy of different target models
usingthetrainedGRADiandtheuntrained(vanilla)model. GRADiworksbestoninputssimilartothe
queriesbutofferssmallergainsthanZERO,suggestingBASEproduceslessrelevantdemonstrations. It
outperforms BASE on smaller models like Qwen2.5-3B, though both lag behind some baselines.
GRADi compared with SFT-only model.Figure 5 clearly highlights that GRADi outperforms the
SFT-only version with most models on nearly all datasets. The most significant improvement is observed
with the largest model, particularly when the queries are less similar to the training data.
Llama-3.2-3B
Qwen-2.5-3B
Qwen-2.5-7B
Llama-3.1-8B
Qwen-2.5-14BModel-3.06 -3.00 -3.07 -2.05 -3.30 2.32 1.71 1.11 -3.70 -7.33 3.92 1.59 -3.00 9.00 -1.79
0.92 9.00 -1.85 1.79 6.59 2.31 0.86 -7.78 0.00 -12.38 -5.88 -8.73 -13.00 -20.00 2.68
0.84 10.50 8.41 5.12 14.28 9.66 5.12 1.11 5.18 0.34 -3.93 0.00 2.00 0.00 -7.14
-1.14 1.00 -2.26 -2.55 2.19 -0.25 -3.42 -4.44 -1.48 -2.05 2.94 -5.55 10.00 2.00 11.61
17.71 14.50 30.53 26.09 38.47 29.34 28.21 11.11 22.22 0.85 32.35 9.52 16.00 17.00 -1.78GRADi with Zero-shot model
Llama-3.2-3B
Qwen-2.5-3B
Qwen-2.5-7B
Llama-3.1-8B
Qwen-2.5-14BModel-1.23 -5.00 -2.46 1.02 -6.59 3.09 0.86 -2.22 -1.85 -1.10 0.00 0.79 -3.00 1.00 -2.68
1.91 2.50 2.05 2.81 0.00 -2.32 0.86 -1.11 8.14 7.59 10.79 8.73 10.00 -12.00 11.61
-1.07 8.50 1.23 2.56 1.10 6.18 -1.71 4.45 1.48 -0.09 -5.89 3.17 5.00 2.00 -7.14
1.37 5.50 2.04 1.54 9.89 2.58 5.13 -10.00 -2.60 -1.03 5.88 -11.11 0.00 -1.00 5.36
6.34 11.00 13.73 10.74 21.98 13.64 11.11 0.00 11.85 -0.34 4.90 -1.59 0.00 5.00 2.68GRADi with BASE-model
GSM8k_structured (0.69)draw_structured (0.67) mathqa_physics (0.63)mathqa_gain (0.63)mathqa_other (0.60)
mathqa_general (0.59)mathqa_geometry (0.58)
deepmind_mathematics_basicmath (0.52)high_school_mathematics (0.46)ai2_arc (0.34)
college_physics (0.34)formal_logic (0.32)
college_computer_science (0.30)college_chemistry (0.28) machine_learning (0.22)
Dataset (Similarity Score)Llama-3.2-3B
Qwen-2.5-3B
Qwen-2.5-7B
Llama-3.1-8B
Qwen-2.5-14BModel6.71 -3.50 0.82 3.58 -1.10 7.08 7.70 -1.11 1.11 -1.45 -0.98 3.17 4.00 0.00 2.68
1.75 4.00 6.55 8.69 3.29 4.37 11.97 -12.22 11.85 -5.64 6.86 4.77 2.00 -12.00 5.36
10.68 6.00 10.25 6.91 9.89 12.87 11.96 8.89 6.29 3.66 2.94 3.97 2.00 6.00 -8.93
5.19 9.50 0.61 5.89 0.00 2.32 6.84 -21.11 18.52 5.97 7.84 -8.73 9.00 -2.00 13.39
6.80 8.50 12.91 11.51 18.69 18.79 8.55 4.44 30.37 17.15 25.49 12.70 25.00 23.00 22.32GRADi with SFT-only model
40
20
02040
Accuracy Difference (%)
40
20
02040
Accuracy Difference (%)
40
20
02040
Accuracy Difference (%)GRADi Performance Comparison with Baselines
Figure 5: Heatmap of accuracy differences between GRADi and different baselines. Red denotes gains for GRADi,
blue for other baselines, respectively, and lighter cells indicate similar performance.

A.6Prompts
Themodelemploystwodistinctpromptsfordifferentstages: oneforgeneratingdemonstrationsandanother
for producing the final answer. Additionally, we design prompts specific to the benchmark type. For
benchmarks that are aligned with our training dataset, we create a default prompt. For the multiple-choice
question, we introduce a dedicated prompt to accommodate its format better.
Generating demonstrations:
You are an educational chatbot specialized in mathematical reasoning. For each question provided
by the user, do the following:
1. You have a budget of 300 tokens to create **two different examples** that are inspired by the
user’s input.
2. Make sure that the generated examples are short and differ significantly in **context and
reasoning steps**.
3. Do not repeat the same problem using different numbers or different words. **Each example must
be genuinely unique**.
4. For each example, describe the problem scenario and its context.
5. Provide a step-by-step reasoning to solve the problem defined in the example.
6. Finish each example with the numerical answer, labelled clearly as #### [your numerical result
here without any units or symbols].
7. Append the [END_DEMO] token at the end of each example.
Ensure that the generated examples are short, unique, factually correct, clearly described and
solvable. Focus on **maximum educational value**.
For the multiple-choice question we designed:
You are an educational assistant specializing in multiple-choice science reasoning. For each user
question, follow these steps:
1. You have a budget of 300 tokens to create **two different multiple-choice examples** that are
inspired by the user’s input.
2. These should test **different scientific ideas** or applications related to the concept.
3. Each example must include:
- A clearly marked and worded question
- Four answer choices labeled A, B, C, and D
- A brief explanation of the correct answer
- The final answer written as: #### [correct letter]
- The [END_DEMO] token after the final answer.
4. Do not copy or rephrase the original question. Focus on generating fresh, diverse examples that
are educational and well-structured.
5. Remember: Generate **clearly marked** and **unique** questions, answer choices, explanation a
final answer and the [END_DEMO] token.
Final Answer Generation:For final question answering, for the aligned benchmarks the model receives
the following system prompt:
You are an educational chatbot specialized in math reasoning.
Your task is to solve ONLY the final question provided.
DO NOT generate new questions or examples.
You are allowed to use only 256 tokens to answer the question.
"Provide a step-by-step solution and end with the final numerical answer labeled clearly as ####
[your numerical result here without any units or symbols].
For the multiple-choice question type, the model receives:
Remember:
1. Review all choices (A–D)
2. Provide a very short explanation using scientific logic
3. Do NOT start with the answer
4. After the explanation, write only the final answer on a new line like:
#### [A single letter A–D]

B Example of demonstrations
B.1 ID Example from GSM8K
ThisexampleillustratestheIDsetting,whereboththetaskandtheretrieveddemonstrationscomefrom
GSM8K math reasoning problems.
B.1.1 RAG ID
Question: Jerry has three times as many stickers as George. George has 6 fewer stickers than his
brother Fred. If Fred has 18 stickers, how many stickers does Jerry have?
Demonstrations:
### Question: Dan has two times as many stickers as Tom. Tom has 3 times as many stickers as Bob.
If Bob has 12 stickers, how many does Dan have?
### Answer: First, determine the number of stickers Tom has by multiplying Bob’s stickers by 3: 3
x 12 = 36. Next, find Dan’s number of stickers by multiplying Tom’s stickers by 2: 2 x 36 = 72.
Thus, the answer is 72 [END_DEMO].
### Question: Karl, Ryan, and Ben are fond of collecting stickers. Karl has 25 stickers. Ryan has
20 more stickers than Karl. Ben has 10 fewer stickers than Ryan. They placed all their stickers in
one sticker book. How many stickers did they place altogether?
### Answer: To find the total number of stickers, first determine Ryan’s count by adding 20 to
Karl’s 25, which equals 45 stickers. Next, calculate Ben’s total by subtracting 10 from Ryan’s 45,
resulting in 35 stickers. Finally, add Karl’s 25, Ryan’s 45, and Ben’s 35 together: 25 + 45 + 35 =
105 stickers. The answer is 105 [END_DEMO].
Final answer:
First, determine the number of stickers George has by subtracting 6 from Fred’s 18 stickers: 18 -
6 = 12. Next, find Jerry’s number of stickers by multiplying George’s stickers by 3: 3 x 12 = 36.
Thus, the answer is 36. #### 36✗
Correct Answer: 60
B.1.2 GRAD ID
Question: Jerry has three times as many stickers as George. George has 6 fewer stickers than his
brother Fred. If Fred has 18 stickers, how many stickers does Jerry have?
Demonstrations:
Example 1: **Scenario:** A student subscribes to a monthly magazine for 12. Another student buys
the same magazine weekly, paying 3 each week. Calculate how much more the second student pays
annually.
**Reasoning:**
- Monthly subscription cost:12∗12=144
- Weekly purchase cost:3∗52=156
- Difference:156−144=12#### 12
[END_DEMO]
Example 2: **Scenario:** Two friends buy coffee every day. One gets a monthly card for 30, while
the other buys a cup daily for 2. Find out how much extra the daily buyer spends in a year.
*Reasoning:**
- Monthly card cost:30∗12=360
- Daily purchase cost:2∗365=730
- Extra spent:730−360=370
#### 370 [END_DEMO]
Final answer:
**Reasoning:**
- Grant’s annual cost for newspaper delivery: 200.00
- Juanita’s weekly cost for newspapers:(0.50∗6)+2.00=3.00+2.00=5.00
- Juanita’s annual cost for newspapers:5.00∗52=260.00
- Difference in annual costs:260.00−200.00=60.00
#### 60✓
Correct Answer: 60

B.2 OOD Example from MMLU-college-physics
WeaimtotestbothRAGandGRADinanOODscenariowherenoin-domaindemonstrationsareavailable.
The retrieval pool only contains GSM8K math reasoning examples, but the target task is multiple-choice
question answering(MCQ) in acompletely different domain,namely college physics. This setupallows
us to evaluate the generalization ability of the models when faced with mismatched demonstrations.
B.2.1 RAG OOD
For RAG, the retrieved demonstrations are not semantically related to the user query. They are simply the
closestavailablematches,whichalsomakesthereasoningchainunnecessarilylongsincethedemosare
not helpful.
Question:
What physical property distinguishes elements on the left side of the Periodic Table of the
Elements from elements on the right side?
A. number of protons
B. state of matter
C. atomic mass
D. temperature
Demonstrations:
### Question: Shawn collected 40 plain pebbles. He painted 9 pebbles red and 13 pebbles blue. He
then divided the remaining pebbles equally into 3 groups, and painted them purple, yellow, and
green. What is the difference between the number of blue and yellow pebbles?
### Answer: Shawn collected 40 pebbles in total, painting 9 red and 13 blue. This left 18 pebbles
(40 - 9 - 13), which he divided equally into 3 groups of 6. He painted one group yellow, and the
difference between blue and yellow pebbles is 13 - 6 = 7. The answer is 7 [END_DEMO]
### Question: Megan bought 2 dozen eggs. As she was walking to her car, she dropped a tray of eggs.
3 eggs broke, and twice as many cracked. What is the difference between the eggs that are still in
perfect condition and those that are cracked?
### Answer: Megan bought 2 dozen eggs, which is 2 x 12 = 24 eggs. When she dropped the tray, 3 eggs
broke and twice as many cracked, so 3 x 2 = 6 eggs cracked. The total number of damaged eggs is 3
+ 6 = 9 eggs. The difference between the perfect eggs and the cracked eggs is found by subtracting
the damaged eggs from the total and then subtracting the cracked eggs: 24 - 9 = 15, and 15 - 6 = 9.
The answer is 9. [END_DEMO]
Final answer:
Elements on the left side of the Periodic Table tend to be metals, while those on the right
side are nonmetals or metalloids. Metals generally have properties such as being good conductors
of heat and electricity, having a shiny appearance, and being malleable and ductile. Nonmetals
and metalloids do not share these characteristics. However, the key distinguishing factor based
on their position is their electron configuration, particularly the number of valence electrons.
Elements on the left typically have fewer valence electrons, whereas those on the right have more.
This difference in the number of valence electrons affects their chemical behavior and physical
properties.
The number of protons (option A) does not distinguish left-side elements from right-side
elements because it defines the element itself rather than its position relative to others.
State of matter (option B) can vary depending on conditions and does not consistently
differentiate left from right.
Atomic mass (option C) varies widely across the table and does not reliably distinguish
left from right.
Temperature (option D) is an external condition and does not define elemental properties.
Thus, the correct answer is related to the number of valence electrons, which correlates
closely with the number of protons in determining the element’s position.
#### [A]✗
Correct Answer: B

B.2.2 GRAD OOD
ForGRAD,thedemonstrationsareconstructedtobetask-alignedanddomain-relevant,enablingthemodel
to adapt more effectively to the OOD setting.
Question:
What physical property distinguishes elements on the left side of the Periodic Table of the
Elements from elements on the right side?
A. number of protons
B. state of matter
C. atomic mass
D. temperature
Demonstrations:
### Example 1:**Question:** Which characteristic is more likely to be found in metals located on
the left side of the periodic table compared to non-metals on the right side?
A. They are typically gases at room temperature
B. They tend to lose electrons easily
C. They have lower melting points
D. They form acidic oxides
*Explanation:** Metals on the left side of the periodic table tend to lose electrons easily, making
them good conductors of electricity and heat. Non-metals on the right side often gain electrons,
forming negative ions. This contrasts with the metallic behavior seen on the left.
#### [correct letter]: B [END_DEMO]
### Example 2:**Question:** How does the typical reactivity of elements change as you move from
the left to the right across the periodic table?
A. Reactivity increases because the atomic radius decreases
B. Reactivity decreases because the atomic mass increases
C. Reactivity remains constant due to similar electron configurations
D. Reactivity increases due to higher temperatures
*Explanation:** As you move from left to right across the periodic table, the atomic radius
generally decreases, leading to increased reactivity for metals (left) and decreased reactivity
for non-metals (right), due to their ability to attract electrons.
#### [correct letter]: A [END_DEMO]
Final answer:
**Explanation:** The state of matter differs significantly between elements on the left and right
sides of the periodic table. Elements on the left are predominantly metals, which are solid at room
temperature, while elements on the right include non-metals that can exist as gases or liquids at
room temperature.
#### [correct letter]: B✓
Correct Answer: B
C Extended Results
We present all results across the six different strategies using models of varying sizes. In all experiments,
thedemonstrationsamplerandthetargetmodelarethesame. Weinvestigatebothsmall-andmedium-scale
models. For small-scale settings, we evaluate with 3B parameter models such asLLaMA3.2-3B Instruct
andQwen2.5-3B Instruct. For medium- to large-scale settings, we includeQwen2.5-7B Instruct,
LLaMA3.1–8B Instruct, andQwen2.5-14B Instruct.
Table 5 presents all results. We observe that our method does not perform well only with the 3B
architectures. ForLLaMA3.2-3B, the zero-shot setting dominates, indicating that the model can answer
questions effectively without relying on additional context. Since the model is relatively small, even
relevant context can introduce confusion and reduce performance.
In contrast, forQwen 3B, the RAG strategy performs best, suggesting that Qwen benefits from

demonstrations. However, our method still underperforms with these lightweight models, which indicates
that the demonstration sampler may not be adequate for such small model sizes. Overall, 3B models
appear insufficient for generating high-quality demonstrations compared to their larger counterparts.
Model MethodDataset
GSM8Kdraw
structuredMathQA*deepmind
basic_mathARC
ChallengeMMLU*
LLaMA
3.2-3B
InstructZero-shot 71.9133.0039.8638.8971.8438.02
RAG 67.4837.5038.74 38.89 27.82 15.19
SFT-only 62.14 33.50 35.20 41.11 65.96 36.42
BASE 70.08 35.00 38.95 42.22 65.6139.14
GRAD 68.70 32.50 38.7443.3364.25 38.64
GRADi 68.85 30.00 39.54 40.00 64.51 38.02
Qwen
2.5-3B
InstructZero-shot 71.98 30.00 43.29 47.7876.71 50.25
RAG 74.73 39.00 46.51 52.2230.89 21.36
SFT-only 71.15 35.00 38.2652.2269.97 39.38
BASE 70.99 36.50 44.31 41.11 56.74 37.66
GRAD 70.31 38.50 44.96 43.33 67.58 43.09
GRADi 72.9039.0044.52 40.00 64.33 44.44
Qwen2.5
7B
InstructZero-shot 83.89 36.50 44.79 67.78 87.71 62.22
RAG 83.59 36.50 43.78 63.33 85.92 59.51
SFT-only 74.05 41.00 42.38 60.00 84.39 59.88
BASE 85.8038.50 49.73 64.4488.1462.59
GRAD 84.2743.00 54.72 70.0088.0564.20
GRADi 84.7347.00 53.11 68.8988.0562.71
LLaMA
3.1-8B
InstructZero-shot 78.24 42.00 44.04 43.3383.5349.13
RAG 76.79 29.50 40.67 38.89 73.89 39.75
SFT-only 71.91 33.50 39.9160.0075.51 42.72
BASE 75.73 37.50 39.97 48.89 82.5152.47
GRAD 78.85 46.50 45.1246.67 80.80 50.00
GRADi 77.1043.0042.70 38.89 81.48 51.23
Qwen2.5
14B
InstructZero-shot 72.75 30.50 27.73 58.89 91.13 48.27
RAG 83.89 27.50 37.50 64.44 90.70 48.52
SFT-only 83.66 36.50 42.00 65.56 74.83 40.74
BASE 84.12 34.00 43.78 70.0092.3259.75
GRAD 90.92 40.50 56.98 72.2291.6465.31
GRADi 90.46 45.00 57.80 70.0091.9865.06
Table 5: Performance Comparison across Models and Methods (Accuracy in %). The same backbone model is used
for both the demonstration sampler and the target model. Datasets are ordered by their semantic similarity from left
to right in decreasing order. Blue indicates cases where GRAD outperforms all baselines (independent of GRADi),
whilegreenindicatescaseswhereGRADidoesso(independentofGRAD).Ifthebest-performingmodelonagiven
benchmark is neither GRAD nor GRADi, it is reported in bold.