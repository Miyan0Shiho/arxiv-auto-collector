# RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering

**Authors**: Lovely Yeswanth Panchumarthi, Sai Prasad Gudari, Atharva Negi, Praveen Raj Budime, Harsit Upadhya

**Published**: 2025-10-02 02:49:09

**PDF URL**: [http://arxiv.org/pdf/2510.01612v1](http://arxiv.org/pdf/2510.01612v1)

## Abstract
The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.

## Full Text


<!-- PDF content starts -->

RAG-BioQA: Retrieval-Augmented Generation for Long-Form
Biomedical Question Answering
Lovely Yeswanth Panchumarthi
Emory University
Atlanta, GA, USA
lpanch2@emory.eduSai Prasad Gudari
Emory University
Atlanta, GA, USA
sgudari@emory.eduAtharva Negi
Emory University
Atlanta, GA, USA
anegi2@emory.edu
Praveen Raj Budime
Trine university
Detroit, MI, USA
budimepraveenraj@gmail.comHarsit Upadhya
Emory University
Atlanta, GA, USA
hupadhy@emory.edu
Abstract
The exponential growth of biomedical literature creates signifi-
cant challenges for accessing precise medical information. Current
biomedical question-answering systems primarily focus on short-
form answers, failing to provide the comprehensive explanations
necessary for clinical decision-making. We present RAG-BioQA, a
novel framework combining retrieval-augmented generation with
domain-specific fine-tuning to produce evidence-based, long-form
biomedical answers. Our approach integrates BioBERT embeddings
with FAISS indexing and compares various re-ranking strategies
(BM25, ColBERT, MonoT5) to optimize context selection before syn-
thesizing evidence through a fine-tuned T5 model. Experimental
results on the PubMedQA dataset show significant improvements
over baselines, with our best model achieving substantial gains
across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.
1 Introduction
1.1 Problem Statement and Motivation
The biomedical domain generates vast amounts of research litera-
ture, with PubMed alone containing over 34 million citations and
growing exponentially[ 6] [9]. This data deluge creates significant
challenges for healthcare professionals and researchers attempting
to extract precise answers to complex medical questions. Unlike gen-
eral domain question answering, biomedical QA requires domain
expertise, contextual understanding, and the ability to synthesize
information across multiple sources to generate comprehensive,
evidence-based responses.
Current approaches to biomedical question answering typically
focus on short-form answers or extractive techniques that select
specific sentences from source documents[ 8]. While effective for
factoid questions, these methods fail to address the need for ex-
planatory, long-form answers that provide context, evidence, and
reasoningâ€”elements crucial for clinical decision-making and re-
search advancement[ 10].The gap between short-form QA systems
and the demand for detailed explanations represents a significant
barrier to the accessibility of medical knowledge[8].
1.2 Background and Related Work
Early biomedical QA approaches relied primarily on rule-based
systems and information retrieval techniques[ 7]. With the adventof deep learning, encoder-only models like BERT and BioBERT
were fine-tuned for classification or short-answer extraction from
biomedical corpora. Jin et al [ 13]. introduced PubMedQA, a dataset
specifically designed for biomedical research question answering,
which has become a standard benchmark for evaluating biomedical
QA systems.
Recent advances in language modeling have led to the develop-
ment of more sophisticated approaches for biomedical QA. Models
like BioMedLM and domain-adapted variants of T5 have demon-
strated impressive capabilities in understanding biomedical text
[14]. However, these models still face challenges with long-form
answer generation, particularly when synthesizing information
from multiple sources. Retrieval-Augmented Generation (RAG) has
emerged as a promising approach to address these limitations by
combining the strengths of retrieval-based and generation-based
methods[ 15]. RAG frameworks retrieve relevant documents from a
corpus and use them as context for generating answers, enhancing
the factual accuracy and relevance of responses. However, applying
RAG to the biomedical domain requires specialized techniques to
handle domain-specific terminology and the unique structure of
medical literature.
Our work builds upon these foundations by developing a special-
ized RAG framework for biomedical QA that emphasizes long-form
answer generation[ 4]. We integrate domain-specific embeddings
with efficient retrieval mechanisms and fine-tune generative models
to produce comprehensive, evidence-based responses to complex
biomedical questions[2].
2 Methodology
2.1 Framework Overview
Our RAG-BioQA framework consists of three main components:
(1) a preprocessing pipeline for dataset preparation and embedding
generation, (2) a retrieval module with re-ranking strategies, and (3)
an answer generation module based on fine-tuned T5 models. Figure
1 illustrates the overall architecture of our proposed framework.
2.2 Preprocessing Pipeline
Our retrieval process operates exclusively over pre-processed question-
answer (QA) pairs from the PubMedQA dataset, rather than raw
biomedical documents. This design prioritizes direct alignmentarXiv:2510.01612v1  [cs.CL]  2 Oct 2025

Lovely Yeswanth Panchumarthi, Sai Prasad Gudari, Atharva Negi, Praveen Raj Budime, and Harsit Upadhya
Figure 1: Overview of the RAG-BioQA framework. The sys-
tem first retrieves relevant contexts using BioBERT embed-
dings and FAISS indexing, then applies re-ranking strategies
to select the most informative contexts. These contexts are in-
tegrated with the query to generate comprehensive answers
using a fine-tuned T5 model.
between queries and existing QA knowledge, enabling faster evi-
dence synthesis compared to document-level retrieval. The pipeline
includes the following steps:
(1)Data Filtering and Cleaning:We filter the PubMedQA
dataset to remove instances with overly long contexts or
low-quality annotations. Text normalization techniques spe-
cific to biomedical text are applied, including standardizing
medical abbreviations and handling specialized terminol-
ogy.
(2)Embedding Generation:We generate dense vector repre-
sentations for each question-context pair using BioBERT,
a BERT model pre-trained on biomedical text. We utilize
mean pooling over the token embeddings to create fixed-
length representations suitable for similarity search.
(3)Index Construction:The generated embeddings are in-
dexed using FAISS (Facebook AI Similarity Search), an effi-
cient library for similarity search and clustering of dense
vectors[ 11]. We utilize a flat L2 index for exact nearest
neighbor search, which provides optimal retrieval accuracy
for our relatively modest-sized dataset.
This preprocessing approach results in a structured database
of question-context pairs with associated embeddings, enabling
efficient retrieval during the question-answering process[12].
2.3 Retrieval Module with Re-ranking Strategies
Relevance scores are computed using dense vector similarity be-
tween query embeddings and stored QA pair embeddings , gener-
ated via BioBERT mean pooling[ 5]. FAISS performs exact nearest-
neighbor search via L2 distance, ensuring retrieved QA pairs are
semantically aligned with the input question. This approach priori-
tizes alignment with the original answerâ€™s content (encoded in the
QA pair embeddings) rather than lexical overlap alone.
The retrieval module is responsible for identifying the most rele-
vant contexts to inform answer generation. Our approach employs
a two-stage retrieval process:(1)Initial Retrieval:Given a query question, we embed it
using the same BioBERT model and perform approximate
nearest neighbor search using FAISS to retrieve an initial
set ofğ‘˜relevant question-context pairs ( ğ‘˜= 16 in our ex-
periments).
(2)Re-ranking:We investigate several re-ranking strategies
to refine the initial set of retrieved contexts and select the
most informative ones for answer generation:
â€¢FAISS (Baseline):Using the raw similarity scores
from FAISS without additional re-ranking.
â€¢BM25:A classical lexical matching algorithm that we
adapt for re-ranking by computing BM25 scores be-
tween the query and each retrieved context, consider-
ing term frequency and inverse document frequency.
â€¢ColBERT:A contextualized late interaction model
that computes fine-grained interactions between each
query token and document token, enabling more nu-
anced relevance assessment.
â€¢MonoT5:A sequence-to-sequence model fine-tuned
for relevance classification, which scores documents
based on their predicted relevance to the query.
After re-ranking, we select the top ğ‘›contexts for answer
generation (ğ‘›= 4 in our experiments).
2.4 Answer Generation
The answer generation module synthesizes information from the
retrieved contexts to produce comprehensive, long-form answers to
biomedical questions[ 3]. Our approach is based on fine-tuning T5
models, which have demonstrated strong performance on various
natural language generation tasks[1].
(1)Context Integration:We concatenate the retrieved con-
texts with the query question into a unified input format:
Context: [Retrieved QA Pair 1] [Retrieved QA Pair
2] ... [Retrieved QA Pair n] Question: [Query]
Answer:
Each retrieved QA pair is formatted as Question: [Retrieved
Question] Answer: [Retrieved Answer] , providing
both the question and its corresponding answer as con-
text.
(2)Model Fine-tuning:We fine-tune a pre-trained T5 model
(FLAN-T5-base) on our formatted training data. To improve
efficiency and reduce memory requirements, we implement
Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank
Adaptation (LoRA), which updates only a small subset of
model parameters while maintaining performance.
(3)Answer Generation:At inference time, we provide the for-
matted input to the fine-tuned T5 model, which generates
a coherent long-form answer that synthesizes information
from the retrieved contexts. We use beam search with a
beam size of 4 and apply length normalization to encourage
longer, more detailed responses.

RAG-BioQA: Retrieval-Augmented Generation for Long-Form Biomedical Question Answering
3 Experimental Setup
3.1 Dataset
We evaluate our framework on a compiled large RAG database from
PubMedQA, MedDialog, and MedQuAD, which contains expert-
annotated question-context-answer triplets extracted from PubMed
articles. The dataset includes 181,488 question and long-form an-
swers provided by medical professionals. The compiled dataset
was strictly split into non-overlapping subsets : 70% training, 15%
validation, and 15% test. No QA pairs in the training set appear
in the validation or test sets , ensuring no data leakage. Stratified
sampling maintained balanced distributions of question types (e.g.,
mechanism-of-action, clinical outcomes) across splits.
3.2 Implementation Details
Our implementation uses the following technologies and parame-
ters:
â€¢Embedding Model:BioBERT-base-cased-v1.1 (768-dimensional
embeddings)
â€¢Retrieval:FAISS IndexFlatL2 with ğ‘˜=16initial retrievals
â€¢Re-ranking:Various methods (FAISS, BM25, ColBERT,
MonoT5) selectingğ‘›=4contexts
â€¢Generator:FLAN-T5-base (250M parameters) fine-tuned
with LoRA
â€¢LoRA Configuration:rank ğ‘Ÿ=16, alphağ›¼=32, dropout
ğ‘=0.1
â€¢Training:Batch size of 8, learning rate of 5e-5, 3 epochs
â€¢Hardware:NVIDIA A100 GPUs with 128GB memory
3.3 Baselines and Evaluation Metrics
We compare our approach against the following baselines:
â€¢Base T5 + FAISS:Using the pre-trained T5 model without
fine-tuning, with contexts retrieved using FAISS without
re-ranking.
â€¢Finetuned T5 + FAISS:Our framework with FAISS re-
trieval but without additional re-ranking strategies.
We evaluate our models using multiple complementary metrics:
(1)BLEU-1 (Bilingual Evaluation Understudy - Unigram
Precision)
BLEU-1 measures the precision of unigrams (single words)
in the generated text compared to the reference text.
BLEU-1=BPÂ·exp (logğ‘ 1)
Where:
ğ‘1=Ã
wordâˆˆgenerated min(count gen(word),count ref(word))
Ã
wordâˆˆgenerated count gen(word)
BP=(
1ifğ‘>ğ‘Ÿ
ğ‘’(1âˆ’ğ‘Ÿ
ğ‘)ifğ‘â‰¤ğ‘Ÿ
â€¢ğ‘: length of the generated sentence
â€¢ğ‘Ÿ: length of the reference sentence
(2)ROUGE-1 (Recall-Oriented Understudy for Gisting
Evaluation - Unigram Recall)ROUGE-1 evaluates the recall of unigramsâ€”how many
words from the reference text are captured in the generated
text.
ROUGE-1=Ã
wordâˆˆref min(count ref(word),count gen(word))Ã
wordâˆˆref count ref(word)
(3)BERTScore (Precision)
BERTScore uses contextual embeddings to compute token
similarity. The precision version measures how well gener-
ated tokens semantically match the reference tokens.
BERTScore Precision =1
|ğ‘‹|âˆ‘ï¸
ğ‘¥ğ‘–âˆˆğ‘‹max
ğ‘¦ğ‘—âˆˆğ‘Œcosine(ğ‘¥ ğ‘–,ğ‘¦ğ‘—)
Where:
cosine(ğ‘¥ ğ‘–,ğ‘¦ğ‘—)=ğ‘¥ğ‘–Â·ğ‘¦ğ‘—
âˆ¥ğ‘¥ğ‘–âˆ¥âˆ¥ğ‘¦ ğ‘—âˆ¥
â€¢ğ‘‹: token embeddings of the generated sentence
â€¢ğ‘Œ: token embeddings of the reference sentence
(4)METEOR (Metric for Evaluation of Translation with
Explicit ORdering)
METEOR evaluates alignment between generated and ref-
erence sentences, incorporating exact, stem, synonym, and
paraphrase matches.
METEOR=ğ¹ meanÂ·(1âˆ’Penalty)
Where:
ğ¹mean=10Â·ğ‘ƒÂ·ğ‘…
ğ‘…+9ğ‘ƒ
ğ‘ƒ=ğ‘š
ğ‘¤gen, ğ‘…=ğ‘š
ğ‘¤ref
Penalty=ğ›¾Â·ğ‘â„
ğ‘šğœƒ
â€¢ğ‘š: number of mapped unigrams (matches)
â€¢ğ‘¤ gen: total unigrams in the generated sentence
â€¢ğ‘¤ ref: total unigrams in the reference sentence
â€¢ğ‘â„ : number of chunks (contiguous matched subsequences)
â€¢ğ›¾,ğœƒ: tunable parameters (commonlyğ›¾=0.5,ğœƒ=3)
4 Results and Discussion
Model BLEU-1 ROUGE-1 BERTScore METEOR
Base T5 + FAISS 0.2065 0.2618 0.1132 0.1948
Finetuned T5 + FAISS0.2415 0.2918 0.2054 0.2264
Finetuned T5 + BM25 0.2221 0.2714 0.1318 0.2054
Finetuned T5 + ColBERT 0.2218 0.2713 0.1364 0.2053
Finetuned T5 + MonoT5 0.2172 0.2632 0.1277 0.2023
Table 1: Performance comparison of different model config-
urations on the PubMedQA test set.

Lovely Yeswanth Panchumarthi, Sai Prasad Gudari, Atharva Negi, Praveen Raj Budime, and Harsit Upadhya
4.1 Experimental Results
All re-ranking strategies (BM25, ColBERT, MonoT5) were applied in
a zero-shot manner , relying on general-purpose pre-training with-
out domain-specific fine-tuning. Despite their theoretical strengths,
none outperformed FAISS, which leverages BioBERT embeddings
fine-tuned on biomedical text. This suggests that domain adapta-
tion is critical for re-ranking in biomedical QA. Table 1 presents
the performance of our different model configurations on the test
set. The results demonstrate that fine-tuning significantly improves
performance across all metrics compared to the base T5 model.
Our main findings include:
(1)Fine-tuning Impact:Fine-tuning the T5 model with domain-
specific data more than doubles the BLEU-1 score (from
0.2065 to 0.2415), highlighting the importance of domain
adaptation.
(2)Re-ranking Strategies:Interestingly, the original FAISS
retrieval without re-ranking outperforms all re-ranking
strategies across metrics. This suggests that for biomedical
QA, dense embedding-based retrieval using BioBERT effec-
tively captures semantic relationships without requiring
additional re-ranking steps.
(3)Re-ranking Comparison:Among the re-ranking strate-
gies, BM25 slightly outperforms the neural re-rankers (Col-
BERT and MonoT5). This indicates that lexical matching
still plays an important role in biomedical retrieval, possibly
due to the precise, terminology-heavy nature of medical
text.
4.2 Qualitative Analysis and Ablation Studies
Through manual examination of the generated answers, we iden-
tified key patterns in the modelâ€™s behavior. Notably, the model
demonstrates strong evidence integration capabilities, effectively
synthesizing information from multiple retrieved contexts to pro-
duce comprehensive responses that address complex biomedical
questions from various angles. It also exhibits proficient use of
domain-specific language, consistently employing accurate biomed-
ical terminology and maintaining a formal, evidence-based tone
characteristic of medical literature. However, some limitations were
observed, including the generation of overly general responses
when the retrieved contexts lack sufficient detail, and occasional
contradictions arising from the synthesis of conflicting information
across different sources.
Impact of Initial Retrieval Candidates (k): While ablation studies
focused on the number of final contexts (n), we observed that in-
creasing the initial retrieval candidates (k) from 8 to 16 improved
ROUGE-1 scores by 5.2%. However, memory constraints limited
scaling beyond k=16 , as larger batches caused GPU out-of-memory
errors during FAISS indexing. Performance peaked at n=4 retrieved
contexts, with diminishing returns beyond this point. This suggests
that a moderate number of high-quality QA pairs provides suffi-
cient evidence without introducing noise. PEFT Impact Comparing
full fine-tuning versus LoRA showed that LoRA achieves 97% of
the performance of full fine-tuning while updating only 0.5% of
the parameters, demonstrating its efficiency for biomedical domain
adaptation.5 Conclusion
In this paper, we presented RAG-BioQA, a novel framework for
long-form biomedical question answering that combines retrieval-
augmented generation with domain-specific fine-tuning. Our ap-
proach addresses the unique challenges of biomedical QA by lever-
aging specialized embeddings, efficient retrieval mechanisms, and
parameter-efficient fine-tuning techniques. Experimental results
demonstrate that our framework significantly outperforms base-
line approaches, with the best configuration achieving a BLEU-1
score of 0.2415 and a ROUGE-1 score of 0.2918. Notably, we found
that simple dense retrieval with BioBERT embeddings and FAISS
indexing outperforms more complex re-ranking strategies in the
biomedical domain. Our work contributes to the growing field of
biomedical natural language processing by providing an effective
framework for generating comprehensive, evidence-based answers
to complex medical questions.
References
[1]Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,
Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al .2023. Parameter-
efficient fine-tuning of large-scale pre-trained language models.Nature Machine
Intelligence5, 3 (2023), 220â€“235.
[2] Wenjie Dong, Shuhao Shen, Yuqiang Han, Tao Tan, Jian Wu, and Hongxia Xu.
2025. Generative models in medical visual question answering: A survey.Applied
Sciences15, 6 (2025), 2983.
[3] Yichun Feng, Jiawei Wang, Ruikun He, Lu Zhou, and Yixue Li. 2025. A Retrieval-
Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical
Research and Clinical Support.arXiv preprint arXiv:2503.23029(2025).
[4] Aidan Gilson, Xuguang Ai, Thilaka Arunachalam, Ziyou Chen, Ki Xiong Cheong,
Amisha Dave, Cameron Duic, Mercy Kibe, Annette Kaminaka, Minali Prasad,
et al.2024. Enhancing Large Language Models with Domain-specific Retrieval
Augment Generation: A Case Study on Long-form Consumer Health Question
Answering in Ophthalmology.arXiv preprint arXiv:2409.13902(2024).
[5] Shashank Gupta. 2023. Top K Relevant Passage Retrieval for Biomedical Question
Answering.arXiv preprint arXiv:2308.04028(2023).
[6] Lawrence Hunter and K Bretonnel Cohen. 2006. Biomedical language processing:
whatâ€™s beyond PubMed?Molecular cell21, 5 (2006), 589â€“594.
[7] Qiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu, Huaiyuan Ying, Chuanqi Tan,
Mosha Chen, Songfang Huang, Xiaozhong Liu, and Sheng Yu. 2022. Biomedical
question answering: a survey of approaches and challenges.ACM Computing
Surveys (CSUR)55, 2 (2022), 1â€“36.
[8]Amila Kugic, Ingrid Martin, Luise Modersohn, Peter Pallaoro, Markus
Kreuzthaler, Stefan Schulz, and Martin Boeker. 2024. Processing of Short-Form
Content in Clinical Narratives: Systematic Scoping Review.Journal of medical
Internet research26 (2024), e57852.
[9] Izet Masic and Asima Ferhatovica. 2012. Review of most important biomedical
databases for searching of biomedical scientific literature.Donald School Journal
of Ultrasound in Obstetrics and Gynecology6, 4 (2012), 343â€“361.
[10] Gina Musolino and Gail Jensen. 2024.Clinical reasoning and decision making in
physical therapy: facilitation, assessment, and implementation. Taylor & Francis.
[11] Paras Nath Singh, Sreya Talasila, and Shivaraj Veerappa Banakar. 2023. Analyzing
embedding models for embedding vectors in vector databases. In2023 IEEE
International Conference on ICT in Business Industry & Government (ICTBIG).
IEEE, 1â€“7.
[12] Sathianpong Trangcasanchai. 2024.Improving Question Answering Systems with
Retrieval Augmented Generation. Ph. D. Dissertation. University of Helsinki.
[13] Georgiana Tucudean, Marian Bucos, Bogdan Dragulescu, and Catalin Daniel
Caleanu. 2024. Natural language processing with transformers: a review.PeerJ
Computer Science10 (2024), e2222.
[14] Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao
Li, and Jie Fu. 2023. Pre-trained language models in biomedical domain: A
systematic survey.Comput. Surveys56, 3 (2023), 1â€“52.
[15] Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Lutao Jiang, Haiwei Xue, Bin Ren, Danda
Paudel, Nicu Sebe, Luc Van Gool, and Xuming Hu. 2025. Retrieval Augmented
Generation and Understanding in Vision: A Survey and New Outlook.arXiv
preprint arXiv:2503.18016(2025).