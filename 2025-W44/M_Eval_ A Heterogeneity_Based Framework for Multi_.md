# M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems

**Authors**: Mengzhou Sun, Sendong Zhao, Jianyu Chen, Haochun Wang, Bin Qin

**Published**: 2025-10-28 01:57:40

**PDF URL**: [http://arxiv.org/pdf/2510.23995v1](http://arxiv.org/pdf/2510.23995v1)

## Abstract
Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing
medical question-answering systems through the integration of large language
models (LLMs) with external medical literature. LLMs can retrieve relevant
medical articles to generate more professional responses efficiently. However,
current RAG applications still face problems. They generate incorrect
information, such as hallucinations, and they fail to use external knowledge
correctly. To solve these issues, we propose a new method named M-Eval. This
method is inspired by the heterogeneity analysis approach used in
Evidence-Based Medicine (EBM). Our approach can check for factual errors in RAG
responses using evidence from multiple sources. First, we extract additional
medical literature from external knowledge bases. Then, we retrieve the
evidence documents generated by the RAG system. We use heterogeneity analysis
to check whether the evidence supports different viewpoints in the response. In
addition to verifying the accuracy of the response, we also assess the
reliability of the evidence provided by the RAG system. Our method shows an
improvement of up to 23.31% accuracy across various LLMs. This work can help
detect errors in current RAG-based medical systems. It also makes the
applications of LLMs more reliable and reduces diagnostic errors.

## Full Text


<!-- PDF content starts -->

M-Eval: A Heterogeneity-Based Framework for
Multi-evidence Validation in Medical RAG Systems
1stMengzhou Sun
Faculty of Computing
Harbin Institute of Technology
Harbin, China
mzsun@ir.hit.edu.cn2ndSendong Zhao
Faculty of Computing
Harbin Institute of Technology
Harbin, China
sdzhao@ir.hit.edu.cn3rdJianyu Chen
Faculty of Computing
Harbin Institute of Technology
Harbin, China
hcwang@ir.hit.edu.cn4rdHaochun Wang
Faculty of Computing
Harbin Institute of Technology
Harbin, China
hcwang@ir.hit.edu.cn
5thBing Qin
Faculty of Computing
Harbin Institute of Technology
Harbin, China
qinb@ir.hit.edu.cn
Abstract—Retrieval-augmented Generation (RAG) has demon-
strated potential in enhancing medical question-answering sys-
tems through the integration of large language models (LLMs)
with external medical literature. LLMs can retrieve relevant med-
ical articles to generate more professional responses efficiently.
However, current RAG applications still face problems. They
generate incorrect information, such as hallucinations, and they
fail to use external knowledge correctly. To solve these issues, we
propose a new method named M-Eval. This method is inspired
by the heterogeneity analysis approach used in Evidence-Based
Medicine (EBM). Our approach can check for factual errors in
RAG responses using evidence from multiple sources. First, we
extract additional medical literature from external knowledge
bases. Then, we retrieve the evidence documents generated by
the RAG system. We use heterogeneity analysis to check whether
the evidence supports different viewpoints in the response. In
addition to verifying the accuracy of the response, we also assess
the reliability of the evidence provided by the RAG system. Our
method shows an improvement of up to 23.31% accuracy across
various LLMs. This work can help detect errors in current RAG-
based medical systems. It also makes the applications of LLMs
more reliable and reduces diagnostic errors.
I. INTRODUCTION
In medical research, artificial intelligence (AI) technologies
have long been recognized for their potential to assist in clini-
cal diagnosis and medical studies [8]. With the rise of powerful
medical large language models (LLMs), researchers are dis-
covering that applying these models to various medical tasks
can greatly improve efficiency [21, 1]. Recently, these models
have achieved impressive results across multiple-choice-based
medical exam datasets. In some medical question-answering
tasks, they have even been regarded as surpassing human
medical experts [17]. As a result, researchers are working with
hospitals to improve these models and better integrate them
into healthcare services.
However, the deployment of LLMs in healthcare faces
challenges, mainly due to hallucinations, which are a known
issue with these models [15, 9]. Hospital researchers argue
that if the model cannot guarantee the accuracy of its outputs,
Claim
Input LLM Response
Evidence articlesQuery
Retrieving extra 
evidence articles.
Knowledge 
base
Heterogeneity 
analysis
Response: Correct / Not correct
Given evidence : Poor / Sound 
Output Fig. 1. The task of the M-Eval checking system. M-Eval is designed to
detect factual errors in the responses of medical RAG systems and analyze
the quality of their evidence. The output of the task should include the label
of the given responses and the evaluation of the evidence.
no one can be held responsible for the consequences of any
mistakes. To address this problem, researchers have proposed
various methods to detect and reduce hallucinations in LLMs.
One such approach is Retrieval-augmented Generation (RAG),
which has proven effective [5]. RAG works by retrieving
evidence from medical literature databases, extracting relevant
papers, and assisting the LLMs in generating their responses.
This approach aligns closely with the principles of Evidence-
Based Medicine (EBM). However, despite its effectiveness,
RAG has limitations. Because medical literature is constantly
evolving, the documents retrieved by the model often presentarXiv:2510.23995v1  [cs.CL]  28 Oct 2025

Heterogeneity  
Analysis 
ClaimResponse ：In medical contexts, gauge is a 
number describing the diameter of the outer tube 
(cannula); larger numbers have smaller inner 
diameter (ID), or equivalently thinner (small) 
wall thicknesses, whereas smaller numbers 
denote larger (wider/thicker) sizes of gauge. ……  
Input
Given evidence articles ：
Article I, Article II, Article III
Extra Evidence Retrieval:
 The evidence articles from 
PUBMED:
 Article 1 ~ 5
Result:
Correct!  
Main 
Evidence(3,1,7)
Evidence main 
from：provided 
evidence! Output RetrievalHeterogeneity analysis :
Q:Is there any evidence against 
the claim? Is there any evidence  
in alignment  claim?
A:Evidence Group1.;Evidence 
Group2
Heterogeneity  judge:
score sum 1 vs score sum 2
The evidence group(1,3,7) wins. 
group(2,6,4) is dropped.
Analysis 
score：2.7
score：3.3
score：1.5
Reliability score 
calculation Stance checking
 Result analysis
ResponseClaim 1
Claim 2
Claim 3
Claim extractionFig. 2. The pipeline of the M-Eval checking system. The main part of the system is the heterogeneity analysis detailed at the bottom side. We calculate the
reliability score and the stance on the claim of extra evidence and given evidence. The reliability score is based on their revised date, publication type, and
mesh heading. Then we test their stance on the claim and analyze the final label of the claim.
differing viewpoints. The inconsistent articles retrieved may
confuse the model and lead to errors in its responses. Addi-
tionally, since the model has its internal knowledge, it may
occasionally generate responses that don’t fully match the
retrieved evidence. [6]
To address these issues, we have developed a backend verifi-
cation system for the RAG-based medical question-answering
outputs. Our approach is inspired by the heterogeneity analysis
used in EBM [4]. We use this analysis to evaluate all the claims
by relevant evidence and verify the accuracy of the model’s
responses. As shown in Figure 1, we first separate the output
from the RAG system into two parts: the response and the
evidence. By calculating the relevance of the questions, we
gather the claims that need to be verified. Then we search
for relevant articles in an additional PubMed database and
combine them with the evidence given in the input. All the
evidence articles will be checked if the viewpoints of the
evidence align with the claim. Articles with differing opinions
will be compared based on the reliability scores of their
groups, and the final claim label will be decided accordingly.
We replicate the heterogeneity analysis from EBM and
apply its principles to validate the outputs of RAG in the
medical field. Our method effectively identifies errors in the
responses and evaluates the quality of the evidence provided
by the RAG system. It also alerts users about the relevance and
freshness of the evidence. Our contributions are as follows:
•We propose a medical model output verification system
based on EBM, which reduces the chances of users being
misled by incorrect responses and evidence.
•We evaluate the reliability and stance of the evidence,
helping users assess the evidence extraction process in
the given RAG answering system.•By simplifying the heterogeneity analysis in EBM, we
introduce a multi-evidence medical response detection
method, improving the model’s experimental capability
in such scenarios.
II. RELATED WORKS
A. Medical LLMs Application
The advent of powerful LLMs like GPT-4 by OpenAI has
drawn significant attention due to their impressive question-
answering capabilities. As a result, LLMs have been intro-
duced in various specialized fields, including the military,
healthcare, and law [21, 27, 16, 3]. For instance, Google
launchedMed-PaLM2[18], a highly fine-tuned version of
PaLM2, specifically aligned with extensive medical knowledge
curated by experts and scholars. Google has claimed thatMed-
PaLM2outperforms current medical professionals, which has
generated considerable excitement in the research community.
This development has spurred the creation of several other
specialized models, such asChatDoctor[11],MedAlpaca[7],
PMC-LLaMA[26], andBenTsao[23]. Experts in the respective
languages typically evaluate these models, revealing a need for
more efficient and effective evaluation methods.
With the advent of RAG technology, the medical field has
a strong need for a generation approach that allows LLMs to
reference external knowledge in a logical and evidence-based
manner. Some multimodal researchers have adopted RAG to
interpret and explain images or augment information [19, 33].
Meanwhile, another group of researchers has used methods
similar to EBM to supplement knowledge for clinical ques-
tions, thus assisting models in generating responses. This
approach enhances safety by making medical diagnoses more
reasonable and helps address problems like language barriers
or other factors [25, 28].

B. Hallucination of LLMs on Medical Task
Considering several studies [32, 10, 22, 12], we conclude the
classification and definition of the hallucination phenomenon
in LLMs. Most researchers define hallucinations as the wrong
outputs that conflict with real-world facts, fail to meet user
requirements, or are unverifiable. In the medical domain,
model responses often diverge from traditional categories,
resulting in phenomena such as excessive repetition or vague
responses, and overly cautious answers. The meticulous nature
of medicine and the inherent complexities of annotation have
made current evaluation methods in medical tasks largely
ineffective. For example,Med-PaLM2[18] relies on expert
evaluations, while models likeHuatuoGPT[31] perform self-
checks using LLMs. Inspired by theFactoolsmethodology
[2], we try the traditional models to verify medical tasks.
However, their performance decreases clearly when we change
the input to the responses from another LLM. We also test the
self-detection method using the model [29, 30, 14]. Compared
to traditional tasks, the medical RAG task faces the challenge
of ensuring evidence consistency, as medical literature is
time-sensitive. As a result, the model is often misled when
generating responses, and no matter how much introspection
is applied, the model fails to understand the underlying issue.
C. Hallucination of LLMs on Medical Task
In recent years, researchers have recognized and accepted
EBM as an important discipline. EBM aims to make the best
clinical decisions by integrating the best research evidence,
clinical expertise, and patient preferences [20, 13]. With the
advancement of information technology and data science,
the application and development of EBM have been greatly
enhanced. Traditional fact-checking work typically involves
verifying claims against existing evidence, focusing on iden-
tifying conflicts between the claim and the evidence to detect
erroneous facts [29, 30, 14]. This method is highly effective
for conventional tasks. However, the outputs of medical LLMs
are often highly misleading, which is a major reason they can
easily misguide users. Traditional fact-checking methods using
classical models struggle to perform such fine-grained error
detection. Therefore, we extended fact-checking through het-
erogeneity analysis, resulting in the current MEV AL method.
III. METHOD
A. Task Definition
Our task is to validate the response of a typical RAG-
based medical system and evaluate the credibility of the
evidence it provides. The input format for this task is shown
in the Figure 2. We define a typical RAG medical system as
consisting of at least two parts: the model’s response to the
question and the evidence extracted during the generation of
that response. We perform extra retrieval and fact-checking
on this information and evaluate the claims in the responses
based on heterogeneity analysis. Additionally, we assess the
quality of the evidence provided by the RAG system. The
factual evaluation of the results helps reduce the system’s
diagnostic error rate, while the evidence quality evaluation aids
InputQuesiton:
A 46 -year-old woman comes to the physician because of a 2 -week history of 
diplopia and ocular pain when reading the newspaper. She also has a 3 -month 
history of amenorrhea, hot flashes, and increased sweating …… 
Response ：Sympathetic hyperactivity of levator palpebrae superioris
Explanation:  The patient has Graves disease, which is an autoimmune disease 
involving hyperthyroidism due to autoantibodies directed against thyroid -
stimulating hormone (TSH) receptors. The patient has Graves 
ophthalmopathy, which can be due to sympathetic hyperactivity of levator 
palpebrae superioris and is ……
Claim 
extraction
Claim 1: question + response choise
Claim 2~5: sentences related to the question A 46 -year-old woman comes to the physician because of a 2 -week 
history of diplopia and ocular pain when reading the newspaper. …… 
Which of the following is the most likely cause of this patient's ocular 
complaints?  Sympathetic hyperactivity of levator palpebrae superioris
Claim 2: The patient has Graves disease , which is an autoimmune 
disease involving hyperthyroidism due to autoantibodies directed 
against thyroid -stimulating hormone (TSH) receptors
…… Fig. 3. The example of the Claim extraction. our extraction is separated into
two parts. The main claim is combined with the question and the choice in
the response. The other claims are selected as the ones most related to the
question.
in analyzing whether the front system suffers from outdated
evidence or issues such as the model disregarding external
evidence.
B. Claim Extraction
Medical LLMs are often fine-tuned to improve safety or
reduce user reliance, which leads to responses that often
include a lot of irrelevant information. As shown in Figure 3,
the model may sometimes use avoidant phrases to discourage
over-reliance on it. Additionally, to ensure the validity of the
response, the model might generate lengthy explanations or
repeat content from the evidence. These factors can complicate
the evaluation of the responses. Therefore, the first step in
assessing the RAG model’s output is to extract the claims.
Our approach has two main parts. First, we use a medical-
specific Spacy model to break down the RAG model’s re-
sponse into segments. Then, these segments are compared for
similarity with the question using a model that has medical
knowledge. We calculate the top four sentences that most
contribute to answering the question and treat these sentences
as the claims that could influence the model’s performance.
To prevent the model’s response from relying too heavily
on relevant literature, which could cause it to stray from
the main question, we introduced an additional claim. This
claim includes the question along with the model’s chosen
answer to reduce omissions. We denote these claims by
C={c 1, c2, . . . , c k}.
C. Evidence Retrieval
After obtaining the claims to be verified, we need to
search for relevant evidence. We use the PubMed dataset for
this task. To ensure the reliability of the extracted evidence,
we perform similarity calculations on the article’s abstract,
title, and MeSH terms simultaneously. The BM25 method is
employed to extract the 15 most relevant articles. Then, we

Base info ：
Query:  A P1G0 diabetic woman is at risk of delivering at 
30 weeks gestation. ……  
Options:  [A. Preventing infection of immature lungs ,…… ]
Gold Option: B. Increasing the secretory product of type II 
alveolar cells
Evidence info ：
[Evidence 1:
,"article_title" ,"abstract_text" ,"mesh_headings":
    the core information of the evidence. We use the contents 
of these three keys to perform a similarity -based basic 
search.
"article_date" ,"date_revised": The scoring basis of the 
date score
"publication_type": The scoring basis of the Level score.
Reliability score ：
Base score : Given by the evidence level(publication type)
7:"Systematic Review" , "Meta -Analysis"
6:"Randomized Controlled Trial",  "Clinica l Trial, 
Phase I",……  
Adjust score:
1.Date score: latest +1.0. second +0.8 …… 
2.Level penalty score: Adjust by LLM
   Prompt  = "You are a professional medical expert. Please 
analyze the following content to determine  whether the 
data was randomly selected , whether the data is 
complete , whether the conclusions are unbiased,  and 
whether used the blind selection method ."
  Level penalty score= -0.3* Aspects
Final reliability score = Base score+ adjust score
     Fig. 4. The method we calculate the reliability of each medicine article. For all
the evidence, we need the information including publication date, publication
type, and the mesh heading to analyze whether the article is reliable.
assess the reliability of each article based on its publication
date and the journal in which it was published. As shown
in the figure, we assign scores to each article following this
rule-based approach and re-evaluate the evidence level of each
article. Finally, we select the topmhighest-scoring articles as
additional evidence for the entire experiment. We denote the
given articles asG={g 1, g2, . . . , g n}and the extra retrieved
articles asE={e 1, e2, . . . , e m}.
D. Heterogeneity Analysis
After identifying the claims to verify, we search for relevant
evidence using the PubMed database [24]. To ensure the
reliability of the extracted evidence, we calculate the similarity
of the article’s abstract, title, and keywords. We use the BM25
method to extract the 15 most relevant articles. Then, we
evaluate each article’s reliability based on its publication date
and the journal in which it was published. As shown in
Figure 4, we assign scores to each article using this rule-based
approach and reassess the quality of the evidence. Finally,
we select the top nine highest-scoring articles as additional
evidence for the experiment. We denote the reliability score
of each article asR(e i)and also the same for theR(g n)
ym(ck, ei) =

1,ife isupportsc k
−1,ife icontradictsc k
0,else(1)
Ask LLMs: Front+ Evidence +Prompt2+ 
claim+ Output restriction  to LLMsClaim:  Visual acuity is 20/20 bilaterally. 
Neurologic examination shows a fine resting 
tremor of the hands .……  
Retrieved evidence:  Evidence,  reliability score
Given evidence: Evidence,  reliability score
LLM Response:  
[Irrelevant  ,Contradicts ,…… ,Supported ]
Heterogeneity filter:
*Score of group[ Supported]  > Score of 
group[ Contradicts]
filter out  group[ Contradicts]Prompt：
Please analyze the claim based on the background 
article and determine whether they are "
'contradicts', 'supports', or is 'irrelevant'. "
Front：
You are a professional medical expert. Please 
analyze the following content to determine 
whether the two articles present conflicting 
viewpoints."
Heterogeneity  analysis :
Claim1 label: Support
*The same score will be counted into the group[Supported] 
Gather labels :
Query label: [Support, Contradicts, Support, 
Support, Support]
If there is Contradicts label then:
Query final label: MistakeFig. 5. The detailed pipeline of the Heterogeneity analysis. We utilize different
LLMs to compare whether the knowledge in the evidence article supports the
claim. And we gather the claims label to get the final label of the response.
Next, we calculate the scores for all evidence related to the
claim. We refer to the defination in the DerSimonian–Laird
random-effects model in Formula 2 and Formula 3 [4].
Q=kX
i=1qi=kX
i=1wi 
yi−Pk
i=1wiyiPk
i=1wi)2(2)
τ2
DL= max(
Q−(k−1)
Pk
i=1wi−Pk
i=1w2
iPk
i=1wi,0)
(3)
This model requires the random-effects varianceτ2
DLand
Cochran’sQto show the heterogeneity of the evidence. Higher
values of them indicate greater heterogeneity in the evidence.
To ensure consistency, we filter studies to minimize both
τ2
DLandQ. The formula requires the evidence labelsy iand
each study’s sampling variancev m=w−1
i. However, Most
studies do not report sampling variancev iin their abstracts or
metadata. We have to set thev ias a constant. Also, We take the
reliability score to give higher weight to more authoritative and
reliable evidence. Therefore, we define the heterogeneity score
in Formula 4 to define the label of the remaining evidence.
Finally, if all five claims are correct, the response is deemed
accurate. If any claim is incorrect, the response is classified

as having factual errors.
Me(c) =nX
j=1q 
c, gj
r 
gj)
+mX
i=1q 
c, ei
r 
ei)(4)
IV. EXPERIMENTS ANDRESULTS
A. Experiments Construction
DatasetsThe input format is shown in Figure 6. This task
consists of a response from an LLM, along with two pieces of
valid evidence. The information for these pieces of evidence
should be listed separately, and the articles should provide
detailed and relevant information to ensure the authenticity
and validity of the evidence. To test the effectiveness of our
method, we create two evidence groups to generate responses
with different levels of response and evidence quality. As
shown in the figure, we extract up to 15 pieces of evidence
for each question. First, we rank these pieces based on
their relevance to the model. Using relevance scores and
the evidence level of each article, we filter and reorder the
evidence. The top three most relevant pieces are selected as
the Finer group evidence, and the remaining 12 are randomly
selected from the 15 to form the Random group evidence. We
then provide both evidence groups to the LLMs for response
generation. This results in 2000 responses with an accuracy
rate of 54% and 2000 responses with an accuracy rate of
24%. However, we observe that when the model’s response
accuracy is too low, the outputs contain a substantial amount
of noise. Some LLMs can not realize the claim, which leads to
meaningless responses. Therefore, we prepare another pair of
datasets. These response have 150 responses with an accuracy
rate of 43.3% from the Finer group and 147 responses with an
accuracy rate of 31.4% from the Random group. The second
dataset is labeled * in Table I and Table II.
EvaluationAfter determining the correctness of the given
response, we also evaluate the evidence provided by the
front-end RAG model. During the heterogeneity analysis, we
label each piece of evidence based on whether it supports or
opposes the final outcome. We track each piece of evidence’s
contribution to the voting process. If the evidence consistently
supports the side opposite to the final outcome, it misguides
the model’s response. If the evidence casts many irrelevant
votes, it suggests a problem with the front-end retrieval
module. On the other hand, if the evidence consistently aligns
with the final result, it is considered strong and appropriate.
After evaluating the contribution of the three types of internal
evidence, we can assess the overall effectiveness of the front-
end RAG model’s evidence.
B. Main Result
In Table I and Table II, we show the performance im-
provement of M-Eval compared to traditional baselines. As
additional evidence is progressively added, the model’s final
accuracy gradually stabilizes, and both its Recall (Rec) and
Response ：D, In medical contexts, gauge is a 
number describing the diameter . ……  
Input
Given evidence articles ：
Article I
    Article title: “Hypothermic overdose, not all bad? ”
    Article abstract: ……  
    Date revised:  "2021 -10-21"
    Publication type:  "Case Reports"
Article II ：
    Article title: “Type 1 diabetes woman with repeated 
miscarriages successfully gave birth ”
    Article abstract: ……  
    Date  revised:  "2021 -06-22"
    Publication type:  "Case Reports"
Question ：A 23 -year-old pregnant woman at 22 
weeks gestation presents with burning upon 
urination.……  
A. Doxycycline  B. Ceftriaxone  C. Ciprofloxacin  D. 
Nitrofurantoin  E. Ampicillin
Gold answer:  NitrofurantoinFig. 6. The Method to construct the datasets used in this work. The Finer
group is given better knowledge evidence and the accuracy is higher than the
normal group.
TABLE I
ACCURACY(%), RECALL(%),ANDSPECIFICITY(%)OFM-EVAL AND
OTHER BASELINES.
MethodRandom Group Finer Group
Acc Rec Spe Acc Rec Spe
Qwen2.5-14BM-Eval 72.10 9.91 90.89 72.1 10.09 90.94
w/o Evi 68.70 25.54 81.26 62.7 68.13 55.74
Self 75.55 3.00 97.52 46.55 2.84 99.11
Mistral-7BM-Eval 71.45 10.78 89.78 71.30 10.52 89.77
w/o Evi 32.22 89.27 12.28 54.60 98.90 0.26
Self 70.40 15.67 86.63 49.65 25.46 78.55
Gemma2-9BM-Eval 62.95 23.92 74.74 63.20 24.25 75.03
w/o Evi 45.15 61.69 39.97 62.85 68.13 56.57
Self 23.84 – – 54.83 – –
Gemma1.1-7B*M-Eval 59.86 26.53 76.53 60.67 25.00 75.47
w/o Evi 54.90 55.10 39.80 55.00 43.18 61.32
Self 58.90 24.49 80.61 57.33 31.82 67.92
Llama3-8B*M-Eval 60.78 50.00 65.71 53.49 46.67 57.14
w/o Evi 35.37 91.84 7.14 49.33 87.69 20.00
Self 37.41 83.67 14.29 44.00 72.31 22.35
Qwen2.5-7B*M-Eval 60.54 16.33 82.65 58.67 15.91 76.42
w/o Evi 56.67 33.33 66.67 50.00 31.47 68.52
Self 54.90 55.10 39.80 46.67 9.09 90.57
Specificity (Spe) steadily adjust, indicating that the method’s
judgments are becoming more balanced. The numbers 1, 2,
3, 4, and 5 under the dataset name represent the number of
extra evidence articles provided. Our scenario involves judging
misdiagnoses in a zero-shot environment based on the outputs
of LLMs.
We select two baselines for comparison in our experiment.
Only LLMs capable of handling different types of noisy
information while still producing correct outputs were con-
sidered. Inw/o Evi, the LLM uses no evidence and only

TABLE II
THE DETAILS OFM-EVAL ACCURACY(%)WITH DIFFERENT NUMBERS OF EXTRA ARTICLES.
MethodRandom Group Finer Group
1 2 3 4 5 1 2 3 4 5
Gemma2-9BAcc 62.95 61.15 59.75 58.40 57.10 63.20 61.50 60.35 59.65 58.00
Rec 23.92 26.72 30.82 34.48 36.21 24.25 27.47 31.97 36.70 37.34
Spe 74.74 71.55 68.49 65.62 63.41 75.03 71.84 68.97 66.62 64.28
Mistral-7BAcc 71.45 70.80 69.85 69.45 68.70 71.30 70.75 69.85 69.45 68.80
Rec 10.78 12.07 14.01 16.38 16.59 10.52 12.45 14.38 17.17 17.38
Spe 89.78 88.54 86.72 85.48 84.38 89.77 88.46 86.70 85.33 84.42
Qwen2.5-14BAcc 72.30 71.75 71.00 70.65 70.05 72.25 71.90 71.20 70.85 70.25
Rec 9.91 11.64 13.79 14.87 15.95 39.48 46.36 54.08 59.24 62.68
Spe 90.89 89.45 87.7 86.85 85.74 90.94 89.77 88.14 87.16 86.18
Llama3-8B*Acc 60.78 58.82 62.75 62.75 60.78 58.14 60.47 48.84 51.16 53.49
Rec 37.50 25.00 37.50 56.25 50.00 66.67 66.67 53.33 46.67 46.67
Spe 71.43 74.29 74.29 65.71 65.71 53.57 57.14 46.43 53.57 57.14
Qwen2.5-7B*Acc 55.78 59.86 60.54 60.54 60.54 54.00 54.67 55.33 57.33 58.67
Rec 22.45 22.45 20.41 18.37 16.33 27.27 18.18 18.18 18.18 15.91
Spe 72.45 78.57 80.61 81.63 82.65 65.09 69.81 70.75 73.58 76.42
Gemma1.1-7B*Acc 59.18 61.22 59.86 59.86 59.86 57.33 58.67 58.67 59.33 60.67
Rec 24.49 28.57 26.53 26.53 26.53 29.55 27.27 25.00 22.73 25.00
Spe 76.53 77.55 76.53 76.53 76.53 68.87 71.70 72.64 74.53 75.47
relies on its internal knowledge to evaluate the given query.
InSelf, we instruct the model to follow the evidence provided
with the RAG response, trusting all the articles given by
the previous RAG system. However, some LLMs exhibited a
pronounced bias when executing the baseline method. Notably,
the Gemma2-9B model classifies all outcomes as correct.
As shown in Table I, our M-Eval method demonstrates a
significant improvement over traditional direct evaluation with
LLMs. This improvement is consistent, regardless of the
quality of evidence or the actual accuracy ratio in the dataset.
C. Extra Evidence Analysis
We also evaluate the quality of the provided evidence. As
shown in Table II, we show the performance changes by differ-
ent numbers of extra evidence. We find that the M-Eval method
consistently performs well across datasets exhibiting diverse
distributions. Also, we investigate how additional evidence
influences the final judgment. The lines in Figure 7 represent
the proportion of queries whose given evidence matches the
final determined label. We observe that when a small amount
of evidence is initially provided, almost all final labels are
consistent with the initial labels. However, as the amount
of additional evidence increased, this proportion noticeably
decreases. The final stable value depends on the quality of
the provided evidence. This also indicates that the given
evidence contains significant gaps, and the original evidence
often misguides the evaluation of the model’s output.D. Ablation Study
To validate the effectiveness of each component of our
method, we conduct ablation experiments. Specifically, we test
the impact of the number of evidence pieces in various parts
of the model. First, in the ablation experiment on reliability
score validation, we check the accuracy by randomly assigning
scores to each piece of evidence and observing how this affects
the final result’s accuracy. In the experiment on heterogeneity
analysis, we examine how the results would change if any
claim contained an error. Finally, we validate our additional
evidence extraction component by removing all the extra
evidence and observing the changes in the results, as shown
in Table III.
TABLE III
THE ABLATION STUDIES OFM-EVAL.
MethodLlama3
Acc Rec Sep
RandomM-Eval 62.75 65.71 56.25
A-Reli. 60.78 50.00 65.71
A-Hete. 59.79 16.67 79.10
A-Retr. 52.17 54.55 51.06
FinerM-Eval 53.49 46.67 53.57
A-Reli. 48.39 41.67 52.63
A-Hete. 45.16 20.83 60.53
A-Retr. 43.55 70.83 26.32
A-Reli: In the first ablation experiment, we randomly assign
a reliability score between 0 and 7 to each article using a

Fig. 7. The result of the proportion of contribution evidence queries. We assess whether the given evidence aligns with the overall results of all the evidence
to determine its contribution to the outcome. The x-axis represents the proportion of contributions made by the given evidence, and the y-axis represents the
number of additional extracted articles.
random seed. As shown in Table III, we observe a significant
drop in model performance. When the score is randomly set
to a specific value, accuracy approaches the scenario where all
responses are judged incorrect, which increases accuracy. In
this case, Recall (Rec) drops significantly, while Specificity
(Sep) rise to nearly 90%. However, such results are not
practical for real-world datasets, so we do not consider them
valid.
A-Hete: In the second ablation experiment, we test how
removing the heterogeneity analysis step would affect our
conclusions. Instead of summing scores for each claim label,
we simply check if there is any evidence that contradicts the
claim. If any piece of evidence negates the claim, we consider
it incorrect. This approach improves accuracy for datasets with
many negative examples. However, as shown in Table III,
rejecting many positive labels causes the overall accuracy to
decrease. This result highlights the importance of properly
analyzing and screening the evidence.
A-Retr: In the third ablation experiment, we test the sys-
tem’s performance when no additional evidence is used, and
only the provided evidence is considered. We find that while
our method still detects issues effectively, the accuracy is much
lower compared to the main experiment, where high-quality
evidence is used.
V. CONCLUSION
LLMs have gained significant attention for their strong
comprehension and generation capabilities. In the medical
field, there is a growing need for such powerful automated
tools to assist with medical tasks. However, even with RAG,
LLMs still generate factual errors. To address this, we propose
a method to evaluate the responses and evidence of RAG.
Our method shows significant improvements in experiments
compared to self-checking approaches. We hope that our workwill help doctors identify factual errors and make them more
practical for use in the medical field.
LIMITATIONS OF THE WORK
We are unable to fully replicate every detail of the metic-
ulous process involved in meta-analysis. To gain insights into
the principles of meta-analysis, we consult several medical
experts. We find that the heterogeneity analysis step in meta-
analysis typically requires obtaining detailed experimental data
from each medical publication for calculation and comparison.
However, our LLM cannot contact the authors of each study to
request this data. Therefore, our heterogeneity analysis is more
of an approximation, imitating the original heterogeneity anal-
ysis process. We have experimented with data-driven methods
based on native approaches, but this process is constrained by
the dataset we had.
REFERENCES
[1] Ian L Alberts et al. “Large language models (LLM) and
ChatGPT: what will the impact on nuclear medicine
be?” In:European journal of nuclear medicine and
molecular imaging50.6 (2023), pp. 1549–1552.
[2] I Chern et al. “FacTool: Factuality Detection in Gen-
erative AI–A Tool Augmented Framework for Multi-
Task and Multi-Domain Scenarios”. In:arXiv preprint
arXiv:2307.13528(2023).
[3] Jan Clusmann et al. “The future landscape of large
language models in medicine”. In:Communications
Medicine3.1 (2023), p. 141.
[4] Rebecca DerSimonian and Nan Laird. “Meta-analysis
in clinical trials revisited”. In:Contemporary clinical
trials45 (2015), pp. 139–145.

[5] Wenqi Fan et al. “A survey on rag meeting llms:
Towards retrieval-augmented large language models”.
In:Proceedings of the 30th ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining. 2024,
pp. 6491–6501.
[6] Shailja Gupta, Rajesh Ranjan, and Surya Narayan
Singh. “A Comprehensive Survey of Retrieval-
Augmented Generation (RAG): Evolution, Current
Landscape and Future Directions”. In:arXiv preprint
arXiv:2410.12837(2024).
[7] Tianyu Han et al. “MedAlpaca–an open-source collec-
tion of medical conversational AI models and training
data”. In:arXiv preprint arXiv:2304.08247(2023).
[8] J Holmes, L Sacchi, R Bellazzi, et al. “Artificial in-
telligence in medicine”. In:Ann R Coll Surg Engl86
(2004), pp. 334–8.
[9] Lei Huang et al. “A survey on hallucination in large
language models: Principles, taxonomy, challenges, and
open questions”. In:ACM Transactions on Information
Systems(2024).
[10] Ziwei Ji et al. “Survey of hallucination in natural lan-
guage generation”. In:ACM Computing Surveys55.12
(2023), pp. 1–38.
[11] Yunxiang Li et al. “Chatdoctor: A medical chat model
fine-tuned on a large language model meta-ai (llama)
using medical domain knowledge”. In:Cureus15.6
(2023).
[12] Potsawee Manakul, Adian Liusie, and Mark JF Gales.
“Selfcheckgpt: Zero-resource black-box hallucination
detection for generative large language models”. In:
arXiv preprint arXiv:2303.08896(2023).
[13] John JV McMurray and Milton Packer. “How should
we sequence the treatments for heart failure and a
reduced ejection fraction? A redefinition of evidence-
based medicine”. In:Circulation143.9 (2021), pp. 875–
877.
[14] Ning Miao, Yee Whye Teh, and Tom Rainforth. “Self-
check: Using llms to zero-shot check their own step-by-
step reasoning”. In:arXiv preprint arXiv:2308.00436
(2023).
[15] Gabrijela Perkovi ´c, Antun Drobnjak, and Ivica Boti ˇcki.
“Hallucinations in llms: Understanding and addressing
challenges”. In:2024 47th MIPRO ICT and Electronics
Convention (MIPRO). IEEE. 2024, pp. 2084–2088.
[16] Nigam H Shah, David Entwistle, and Michael A Pfeffer.
“Creation and adoption of large language models in
medicine”. In:Jama330.9 (2023), pp. 866–869.
[17] Karan Singhal et al. “Toward expert-level medical ques-
tion answering with large language models”. In:Nature
Medicine(2025), pp. 1–8.
[18] Karan Singhal et al. “Towards expert-level medical
question answering with large language models”. In:
arXiv preprint arXiv:2305.09617(2023).
[19] Xingcan Su and Yang Gu. “Implementing retrieval-
augmented generation (rag) for large language modelsto build confidence in traditional chinese medicine”. In:
(2024).
[20] Vivek Subbiah. “The next generation of evidence-based
medicine”. In:Nature medicine29.1 (2023), pp. 49–58.
[21] Arun James Thirunavukarasu et al. “Large language
models in medicine”. In:Nature medicine29.8 (2023),
pp. 1930–1940.
[22] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan
Sankarasubbu. “Med-halt: Medical domain hallucina-
tion test for large language models”. In:arXiv preprint
arXiv:2307.15343(2023).
[23] Haochun Wang et al. “Huatuo: Tuning llama model
with chinese medical knowledge”. In:arXiv preprint
arXiv:2304.06975(2023).
[24] Jacob White. “PubMed 2.0”. In:Medical reference
services quarterly39.4 (2020), pp. 382–387.
[25] Joshua J Woo et al. “Custom Large Language Models
Improve Accuracy: Comparing Retrieval Augmented
Generation and Artificial Intelligence Agents to Non-
custom Models for Evidence-Based Medicine”. In:
Arthroscopy: The Journal of Arthroscopic & Related
Surgery(2024).
[26] Chaoyi Wu et al. “PMC-LLaMA: toward building open-
source language models for medicine”. In:Journal of
the American Medical Informatics Association(2024),
ocae045.
[27] Qianqian Xie et al. “Factreranker: Fact-guided reranker
for faithful radiology report summarization”. In:arXiv
preprint arXiv:2303.08335(2023).
[28] Guangzhi Xiong et al. “Benchmarking retrieval-
augmented generation for medicine”. In:arXiv preprint
arXiv:2402.13178(2024).
[29] Linhao Ye et al. “Boosting conversational question
answering with fine-grained retrieval-augmentation and
self-check”. In:Proceedings of the 47th International
ACM SIGIR Conference on Research and Development
in Information Retrieval. 2024, pp. 2301–2305.
[30] Che Zhang et al. “Learning to Check: Unleashing Po-
tentials for Self-Correction in Large Language Models”.
In:arXiv preprint arXiv:2402.13035(2024).
[31] Hongbo Zhang et al. “HuatuoGPT, towards Taming
Language Model to Be a Doctor”. In:arXiv preprint
arXiv:2305.15075(2023).
[32] Yue Zhang et al. “Siren’s song in the AI ocean: a survey
on hallucination in large language models”. In:arXiv
preprint arXiv:2309.01219(2023).
[33] Yinghao Zhu et al. “REALM: RAG-Driven Enhance-
ment of Multimodal Electronic Health Records Anal-
ysis via Large Language Models”. In:arXiv preprint
arXiv:2402.07016(2024).