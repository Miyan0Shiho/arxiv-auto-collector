# Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation

**Authors**: Guanting Dong, Xiaoxi Li, Yuyao Zhang, Mengjie Deng

**Published**: 2025-06-26 15:35:12

**PDF URL**: [http://arxiv.org/pdf/2506.21384v1](http://arxiv.org/pdf/2506.21384v1)

## Abstract
Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

## Full Text


<!-- PDF content starts -->

arXiv:2506.21384v1  [cs.CL]  26 Jun 2025Leveraging LLM-Assisted Query Understanding
for Live Retrieval-Augmented Generation
Guanting Dong
Renmin University of China
Beijing, China
dongguanting@ruc.edu.cnXiaoxi Li
Renmin University of China
Beijing, China
xiaoxi_li@ruc.edu.cn
Yuyao Zhang
Renmin University of China
Beijing, China
2020201710@ruc.edu.cnMengjie Deng
Renmin University of China
Beijing, China
dengmengjie_777@163.com
ABSTRACT
Real-world live retrieval-augmented generation (RAG) systems face
significant challenges when processing user queries that are of-
ten noisy, ambiguous, and contain multiple intents. While RAG
enhances large language models (LLMs) with external knowledge,
current systems typically struggle with such complex inputs, as
they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the
robustness and effectiveness of RAG systems in live, open-domain
settings. Omni-RAG employs LLM-assisted query understanding
to preprocess user inputs through three key modules: (1) Deep
Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries ( e.g., correcting spelling
errors) and decompose multi-intent queries into structured sub-
queries; (2) Intent-Aware Knowledge Retrieval, which performs
retrieval for each sub-query from a corpus ( i.e., FineWeb using
OpenSearch) and aggregates the results; and (3) Reranking and
Generation, where a reranker ( i.e., BGE) refines document selection
before a final response is generated by an LLM ( i.e., Falcon-10B)
using a chain-of-thought prompt. Omni-RAG aims to bridge the gap
between current RAG capabilities and the demands of real-world
applications, such as those highlighted by the SIGIR 2025 LiveRAG
Challenge, by robustly handling complex and noisy queries.
CCS CONCEPTS
•Information systems →Retrieval models and ranking .
KEYWORDS
Retrieval-Augmented Generation, Query Understanding, Denoising,
Document Ranking
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnnACM Reference Format:
Guanting Dong, Xiaoxi Li, Yuyao Zhang, and Mengjie Deng. 2025. Lever-
aging LLM-Assisted Query Understanding for Live Retrieval-Augmented
Generation. In Proceedings of ACM Conference (Conference’17). ACM, New
York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
The rapid advancement of large language models (LLMs) [ 10,38,50]
has led to transformative progress across a wide range of natural
language processing tasks [ 25,52]. Nevertheless, when tackling
knowledge-intensive tasks, LLMs still rely solely on their internal
knowledge, which often fail short in factual inconsistency and hal-
lucinations [ 15]. To address these issues, researchers have proposed
Retrieval-Augmented Generation (RAG) [ 13], which incorporates
external knowledge sources to assist LLMs in content generation,
significantly improving the accuracy and reliability of the outputs.
However, in real-world live RAG applications, user queries are
rarely atomic or single-intent. Instead, they often involve multi-
ple intents, complex structures, and various types of noise [ 6,21].
While existing RAG methods perform well on standard benchmarks,
they typically select simple and noise-free dataset for fine-tuning or
alignment. Consequently, these systems struggle to accurately inter-
pret intent and generate reliable responses when faced with noisy,
ambiguous, and multi-intent queries in open-domain settings.
To advance research in this direction, the SIGIR 2025 LiveRAG
Challenge introduces the first competition specifically designed
to evaluate the real-time problem-solving capabilities of online
RAG systems. The challenge provides all teams with a fixed knowl-
edge corpus (FineWeb) [ 31] and a pre-trained language model
(Falcon3-10B-Instruct) [32], while dynamically generating diverse
user queries using a configurable synthetic DataMorgana [ 11] simu-
lator to mimic live human query interactions. Participating RAG sys-
tems must complete the task within a two-hour time limit, requiring
efficient handling of complex, noisy, and multi-intent queries. Thus,
the core challenge lies in robustly and efficiently understanding
the underlying intents and semantic noise in user queries, posing a
significant hurdle for building practical live RAG systems.
To bridge this gap, we design Omni-RAG, a framework that
leverages the understanding capabilities of LLMs to preprocess user
queries—through denoising and intent decomposition—enhancing
the robustness of RAG systems in real-world online environments.
The framework comprises three key modules:

Conference’17, July 2017, Washington, DC, USA Guanting Dong, Xiaoxi Li, Yuyao Zhang, and Mengjie Deng
•Deep Query Understanding and Decomposition: Based on
LLMs’ strengths in language understanding, we apply tailored
prompts to guide the model in preprocessing user queries. This in-
cludes rewriting noisy inputs and decomposing complex queries
with multiple intents into clearer, structured sub-queries.
•Intent-Aware Knowledge Retrieval: To retrieve comprehen-
sive and relevant supporting information, we use an OpenSearch
system to perform retrieval over the FineWeb corpus for each
rewritten and decomposed sub-query. The retrieved documents
are then aggregated into a unified corpus that captures a broader
semantic context for generation.
•Reranking and Generation: Before generation, we apply a
BGE reranker to reorder candidate documents for all sub-queries,
selecting the top-10 most relevant ones. These are then integrated
with the rewritten main query into a chain-of-thought prompt,
which is fed into the Falcon-10B model to generate the final
response.
Experimental results show that Omni-RAG framework achieved a
Rank-2 overall performance in Session 1 of the SIGIR Liv-
eRAG Challenge . Additionally, we conduct pseudo-labeling exper-
iments on the dry-test set following the official evaluation metrics,
further demonstrating the framework’s effectiveness in terms of
both generation efficiency and factual consistency.
2 RELATED WORK
Retrieval-Augmented Generation. Retrieval-Augmented Gener-
ation (RAG) [ 13] has emerged as a powerful paradigm that incor-
porates external information or knowledge to enhance the quality,
factuality, and relevance of generated text. Recent efforts [ 5,7,
23,24] have leveraged RAG to address the challenge of halluci-
nation and improve the performance of LLMs across a range of
tasks. To furthur improve retrieval quality, several post-retrieval
strategies [ 18,36,46] have been introduced to fill the gap between
retriever and generator, involving re-ranking, refinement and com-
pression. Reranker [ 17,19,44] reorder the retrieved results from
retriever, enabling better alignment with the information needs of
the LLM. Additionally, some studies [ 2,43] introduce techniques
to mitigate noise in retrieved knowledge documents. To tackle the
problem of long-context limitation, various methods [ 14,45] focus
on compressing retrieved references to fit input length limits and
removing irrelevant content to enhance robustness.
Query Understanding. Query Understanding [ 3,37] encom-
passes a range of techniques aimed at improving the efficiency
and accuracy of retrieval-augmented generation systems in the
pre-retrieval stage, including query rewriting, disambiguation, de-
composition and expansion. Recent advancements [ 1,6] have high-
lighted the pivotal role of LLMs in query understanding in to en-
hance retrieval quality. Query rewriting [ 26,29] involves reformu-
lating the original query into a version more closely aligned with
the information required for effective retrieval, thereby addressing
the common mismatch between human intent and model interpre-
tation. Query disambiguation [ 20,27,28,33] focuses on clarifying
user intent in ambiguous or multi-turn queries by transforming
them into more specific and context-aware search inputs. Query de-
composition [ 34,49,51] breaks down complex queries into simplersub-queries to improve retrieval effectiveness and enable compre-
hensive answer generation. Reasoning-based query decomposition
methods [ 8,39,40,47] focus on generating reasoning traces or
plans for solving complex tasks. Query expansion improves re-
trieval performance by enriching the original query with additional
information derived from internal or external knowledge sources.
Internal expansion methods [ 12,16,41,48] focus on enhancing the
original query using parametric knowledge within LLMs, while
external expansion methods [ 30,35] incorporate supplementary
information from external sources such as knowledge bases.
3 METHOD
Overview. To ensure robust and high-quality RAG responses, we
introduce the Omini-RAG pipeline, powered by LLMs, as shown in
Figure 1. Given a query, an LLM first performs deep understanding,
including rewriting and decomposition. Retrieval and reranking
are then applied to obtain candidate documents for each sub-intent,
followed by response generation via Falcon-10B. The subsequent
subsections detail each stage of the pipeline.
3.1 Problem Definition
Compared to standard text generation, RAG often follows a retrieve-
then-read paradigm [ 9,22], where an additional retriever is intro-
duced to collect external knowledge and enhance the generation
process. Given the input user query be denoted by 𝑞. The primary
objective of the RAG system is to generate a comprehensive and
relevant response 𝑅. Formally, the system aims to find an optimal
response𝑅∗such that:
𝑅∗=arg max
𝑅𝑃(𝑅|𝑞,K) (1)
whereKrepresents the available knowledge base or corpus from
which information can be retrieved. The pipeline described below
outlines the steps to approximate this optimal response.
3.2 Query Understanding and Decomposition
Real-world user queries often contain noise, such as spelling errors
or ambiguous phrasing. Directly using such queries for searching
can lead to inaccurate or irrelevant retrieval results. Therefore, we
first employ an LLM for query understanding, rewriting. Let the
original query be 𝑞. The rewriting process can be represented as:
𝑞′=Rewrite(𝑞,𝜃rewrite) (2)
where𝑞′is the rewritten query, and 𝜃rewrite represents the parame-
ters of the LLM fine-tuned for the rewriting task.
After rewriting, although the semantics of the query 𝑞′become
correct and intuitive, its intent may still be complex or multifaceted.
To further achieve more precise retrieval targets and improve the
recall of relevant documents, we perform query decomposition. In
detail, the rewritten query 𝑞′is input to an LLM, which outputs
a set of𝑀sub-queries{𝑞′
1,𝑞′
2,...,𝑞′
𝑀}. The above process can be
formulated as:
{𝑞′
𝑠}𝑀
𝑠=1=Decompose(𝑞′,𝜃decompose) (3)
where each sub-query 𝑞′𝑠is designed to target a specific aspect of
the original query. These sub-queries are typically generated in a

Omni-RAG: Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation Conference’17, July 2017, Washington, DC, USA
Query
New QueryRephraseDisambiguateDecompose
subqueryRewriterRetriever and RerankerGeneratorsubquery...
Documents
ExpandedQueries
Embedding
Dense / Sparse Retrieval 
Top-K Candidates1  >  2  >  3  >  4  >  5..
SimilarityMatching
1  >  2  >  3  >  4  >  5
4  >  5  >  2  >  1  >  3 ..
Rerank
Query
QueryFinal Documents
+ prompt  
ResponseCorrectnessFaithfulness
Figure 1: The overall pipeline of our Omni-RAG.
structured format, such as JSON, for easy extraction and process-
ing. This lays a solid foundation for retrieving broader and more
accurate knowledge during the search process.
3.3 Intent-Aware Knowledge Retrieval
To obtain more comprehensive and extensive information, an intu-
itive approach is to retrieve information for each sub-intent derived
from the decomposition of a complex query. For each decomposed
sub-query𝑞′𝑠(where𝑠∈{1,...,𝑀}), we utilize a search function
to retrieve the top-K relevant documents from the knowledge base
K. Let𝐷𝑠be the set of documents retrieved for sub-query 𝑞′𝑠:
𝐷𝑠=Search(𝑞′
𝑠,K,𝐾)={𝑑𝑠,1,𝑑𝑠,2,...,𝑑𝑠,𝐾} (4)
where𝑑𝑠,𝑘is the𝑘-th document retrieved for sub-query 𝑞′𝑠. The
initial set of retrieved documents, 𝐷retrieved , is then formed by
taking the union of the documents retrieved for all sub-queries,
𝐷retrieved =Ð𝑀
𝑠=1𝐷𝑠. This ensures a broad coverage of information
related to the different facets of the original query.
3.4 Reranking and Generation:
Reranking: To balance the redundant retrieval results introduced
by sub-queries, reranking and filtering of the information are essen-
tial. After obtaining the initial set of retrieved documents 𝐷retrieved ,
we employ a reranking model to refine the selection and order of
these documents. We use a sophisticated reranking model, such as
BGE-reranker-large, for this purpose. The reranker computes a rele-
vance score between the original query 𝑞(or the rewritten query 𝑞′)
and each document 𝑑∈𝐷retrieved . Let score(𝑞,𝑑)be the relevance
score assigned by the reranker. The documents in 𝐷retrieved are
then sorted based on these scores in descending order. We select
the top-N documents from this sorted list to form the final set of
context documents, 𝐷reranked :
𝐷reranked ={𝑑∗
1,𝑑∗
2,...,𝑑∗
𝑁}⊆𝐷retrieved (5)such that score(𝑞,𝑑∗
𝑖)≥score(𝑞,𝑑∗
𝑖+1)for all𝑖∈{1,...,𝑁−1}, and
𝑁is a predefined number of documents to be used for generation.
After reranking, we obtained high-quality documents relevant to
the query, providing essential support for accurate generation.
Generation: Finally, with the original query 𝑞and the top-N
reranked documents 𝐷reranked , we use an LLM for generating the
final response 𝑅. The LLM is conditioned on both the query and
the contextual information provided by the selected documents:
𝑅=Generate(𝑞,𝐷 reranked,𝜃generate) (6)
where𝜃generate represents the parameters of the LLM used for
generation. This step aims to synthesize the information from the
retrieved documents into a coherent, accurate, and contextually
appropriate answer to the user’s query.
3.5 Pseudo Labeling and Evaluation
Since the reference data does not contain ground-truth answers,
we propose a pseudo-label generation and consistency evaluation
strategy based on LLMs to support performance assessment and
iterative optimization of RAG systems during development. It is
important to note that the use of pseudo-labels strictly follows
the competition guidelines: they are employed solely for system
evaluation and analysis of answerless samples in dry tests, and are
never used during the actual answer generation process.
Specifically, we first adopt Qwen2.5-7B-Instruct (fewer than 10B
parameters) [ 4] as the reference model to generate pseudo answers
by feeding it the input query along with its retrieved documents.
To enable multi-dimensional evaluation, we define two core met-
rics—“relevance” and “faithfulness”—in accordance with the official
evaluation criteria, and design dedicated prompts for each (below).
Taking the relevance evaluation prompt as an example, it begins
by defining the model’s role, followed by key evaluation points
and scoring guidelines. Additionally, four handcrafted examples
are included to represent different rating levels, which enhance the
model’s contextual understanding and improve scoring consistency.

Conference’17, July 2017, Washington, DC, USA Guanting Dong, Xiaoxi Li, Yuyao Zhang, and Mengjie Deng
Table 1: Team Rankings of Session 1 of Live RAG Challenge
Rank Team Name Correctness Faithful
1 RMIT-ADMS 1.1993 0.4774
2 RUC_DeepSearch (Ours) 0.9693 0.3878
3 Ped100X 0.9289 0.0434
4 PRMAS-DRCA 0.9228 0.4106
5 Hybrid Search w. Graph 0.8751 0.3158
6 BagBag 0.6941 -0.9114
7 UniClustRAG 0.6851 0.4601
8 METURAG 0.6735 0.3253
9 DeepRAG 0.5661 0.0978
10 UiS-IAI 0.5523 0.4337
11 SNU-LDILab 0.5174 0.1030
12 Gravitational Lens 0.3766 -0.9881
The prompt for faithfulness evaluation follows the same structure as
that of relevance. Finally, we employ Falcon-10B to independently
execute both relevance and faithfulness evaluations, generating
pseudo scores for each candidate answer accordingly.
Relevance Evaluation Prompt Template
You are an expert evaluator assessing the quality of a predicted
answer to a given question, using the provided reference
answer (golden answer) as the standard. Your task is to assign a
score based on the semantic equivalence and relevance of the
prediction, based on the following scoring criteria:
Your evaluation considers:
- Equivalence: Does the Prediction convey the same meaning as
the Golden Answer?
- Relevance: Does the Prediction directly address the Question
without adding unrelated information?
Scoring Scale:
- 2: Correct and relevant (no irrelevant information).
- 1: Correct but contains irrelevant information.
- 0: No answer provided (abstention).
- -1: Incorrect answer.
Instructions:
Evaluate the prediction based on how well it aligns with the
golden answer and how directly it addresses the question.
Return only the numerical score (2, 1, 0, -1).
In-Context Examples:{examples}
Question: {question}
Golden Answer: {answer}
Prediction: {prediction}
Output:
3.6 Experiment
Experiment Setup. We strictly follow the requirements of the
LiveRAG competition, using OpenSearch to retrieve from the Falcon
corpus, BGE as the reranker, and Falcon-10B as the generator.
It is worth noting that in Table 2, we experiment with the self-
consistency(sc) strategy [ 42], and the metrics are based on in-houseTable 2: Performance comparison across different top- 𝑘set-
tings using OpenSearch. 5 (sc4) denotes generation using top-
5 docs and 4 sampled reasoning paths for self-consistency.
Method top- 𝑘 Relevance Faithfulness
Avg -1 0 1 2 Avg -1 0 1
Omni-RAG1 140 4 2 44 50 62 4 30 66
2 136 6 2 42 50 66 2 30 68
3 146 6 0 36 58 70 0 30 70
4 154 2 0 40 58 76 2 20 78
5 156 4 0 32 64 80 0 20 80
Omni-RAG5 (sc4) 170 2 0 24 74 72 0 28 72
5 (sc8) 148 4 0 40 56 80 0 20 80
pseudo-relevance and faithfulness scores generated by Qwen2.5-
72B-Instruct.
Main Result. The main results are presented in the primary
table, where our Team ( 𝑅𝑈𝐶 _𝐷𝑒𝑒𝑝𝑆𝑒𝑎𝑟𝑐ℎ ) achieved an overall sec-
ond place among the 12 participating teams in Session-1.
Notably, compared to the third-place team, Team Ped100X, our
system achieved over a 4% improvement in Correctness and approx-
imately a 34% improvement in Faithfulness. Similarly, compared to
Team BagBag, which ranked fifth overall, our system outperformed
them by around 9% in Correctness and about 7% in Faithfulness.
These results clearly demonstrate the reliability and effectiveness
of our RobustRAG framework.
Dry Test Analysis. Dry-Test serves as a representative evalua-
tion setting in our study. To further analyze our RAG performance,
we select 50 samples from the Dry-Test set and use Qwen2.5-7B-
instruct ’s answer based on the top-5 documents as references to
evaluate our model. The key findings are as follows.
1.Performance scales with document count: As the number
of retrieved documents increases, the Omni-RAG model exhibits
strong scalability in generation quality. Improvements are observed
in both faithfulness and relevance, indicating that the performance
benefits from richer document contexts.
2.Trade-off in self-consistency path settings: To enhance
inference stability, we introduced a self-consistency mechanism.
However, the 5 (sc8) configuration did not yield the expected perfor-
mance gains, suggesting that more reasoning paths do not necessar-
ily lead to better results. Interestingly, the 5 (sc4) setting improved
relevance but led to a moderate decline in faithfulness, highlighting
the need to balance path quantity with generation quality.
4 CONCLUSION
In this paper, we introduces Omni-RAG, a robust and scalable frame-
work that enhances RAG systems through LLM-assisted query
understanding. By integrating deep query decomposition, intent-
aware retrieval, and reranking-guided generation, Omni-RAG effec-
tively addresses the challenges of complex and noisy queries in live,
open-domain environments. Our approach demonstrates strong
potential for real-world applications, achieving Rank-2 overall per-
formance in Session 1 of the SIGIR LiveRAG Challenge and offer-
ing a practical step toward more reliable and intelligent retrieval-
augmented systems.

Omni-RAG: Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation Conference’17, July 2017, Washington, DC, USA
REFERENCES
[1]Abhijit Anand, Venktesh V, Vinay Setty, and Avishek Anand. 2023. Context
Aware Query Rewriting for Text Rankers using LLM. CoRR abs/2308.16753 (2023).
https://doi.org/10.48550/ARXIV.2308.16753 arXiv:2308.16753
[2]Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.
2023. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-
Reflection. CoRR abs/2310.11511 (2023). https://doi.org/10.48550/ARXIV.2310.
11511 arXiv:2310.11511
[3]Hiteshwar Kumar Azad and Akshay Deepak. 2019. Query expansion techniques
for information retrieval: A survey. Inf. Process. Manag. 56, 5 (2019), 1698–1735.
https://doi.org/10.1016/J.IPM.2019.05.009
[4]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng
Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,
Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng
Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical
Report. CoRR abs/2309.16609 (2023). https://doi.org/10.48550/ARXIV.2309.16609
arXiv:2309.16609
[5]Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-
ford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-
dan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Ro-
man Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon
Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Im-
proving Language Models by Retrieving from Trillions of Tokens. In Proceed-
ings of the 39th International Conference on Machine Learning (Proceedings of
Machine Learning Research, Vol. 162) , Kamalika Chaudhuri, Stefanie Jegelka,
Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 2206–2240.
https://proceedings.mlr.press/v162/borgeaud22a.html
[6]Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo,
and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented
Generation. CoRR abs/2404.00610 (2024). https://doi.org/10.48550/ARXIV.2404.
00610 arXiv:2404.00610
[7]Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu,
Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025. Tool-Star:
Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning.
arXiv:2505.16410 [cs.CL] https://arxiv.org/abs/2505.16410
[8]Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang
Zhou, and Jingren Zhou. 2025. Self-play with Execution Feedback: Improving
Instruction-following Capabilities of Large Language Models. In The Thirteenth
International Conference on Learning Representations, ICLR 2025, Singapore, April
24-28, 2025 . OpenReview.net. https://openreview.net/forum?id=cRR0oDFEBC
[9]Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and
Ji-Rong Wen. 2024. Understand What LLM Needs: Dual Preference Alignment
for Retrieval-Augmented Generation. CoRR abs/2406.18676 (2024). https://doi.
org/10.48550/ARXIV.2406.18676 arXiv:2406.18676
[10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ah-
mad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sra-
vankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien
Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh
Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,
Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne
Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song,
Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Ma-
hajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin,
Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Raden-
ovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson,
Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen,
Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,
Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan
Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der
Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu
Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,
Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Al-
wala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone,
and et al. 2024. The Llama 3 Herd of Models. CoRR abs/2407.21783 (2024).
https://doi.org/10.48550/ARXIV.2407.21783 arXiv:2407.21783
[11] Simone Filice, Guy Horowitz, David Carmel, Zohar Karnin, Liane Lewin-Eytan,
and Yoelle Maarek. 2025. Generating Diverse Q&A Benchmarks for RAG Evalua-
tion with DataMorgana. arXiv preprint arXiv:2501.12789 (2025).
[12] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot
Dense Retrieval without Relevance Labels. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber,and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1762–1777.
https://doi.org/10.18653/V1/2023.ACL-LONG.99
[13] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,
Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.
Retrieval-Augmented Generation for Large Language Models: A Survey.
CoRR abs/2312.10997 (2023). https://doi.org/10.48550/ARXIV.2312.10997
arXiv:2312.10997
[14] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-
light: Efficient and effective retrieval-augmented text generation. In Proceedings
of the 46th International ACM SIGIR Conference on Research and Development in
Information Retrieval . 1437–1447.
[15] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian
Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2023. A Survey on Hallucination in Large Language Models: Principles,
Taxonomy, Challenges, and Open Questions. CoRR abs/2311.05232 (2023). https:
//doi.org/10.48550/ARXIV.2311.05232 arXiv:2311.05232
[16] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu,
Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Aug-
mented Generation. In Proceedings of the 2023 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 .
Association for Computational Linguistics, 7969–7992. https://aclanthology.org/
2023.emnlp-main.495
[17] Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Ö. Arik. 2025. Long-Context
LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG. In The Thir-
teenth International Conference on Learning Representations, ICLR 2025, Singa-
pore, April 24-28, 2025 . OpenReview.net. https://openreview.net/forum?id=
oU3tpaR8fm
[18] Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu,
Zhonghua Li, Qi Ye, and Zhicheng Dou. 2025. Hierarchical Document Refinement
for Long-context Retrieval-augmented Generation. arXiv:2505.10413 [cs.CL]
https://arxiv.org/abs/2505.10413
[19] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael
Bendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs.
arXiv:2401.06954 [cs.CL]
[20] Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang.
2023. Tree of Clarifications: Answering Ambiguous Questions with Retrieval-
Augmented Large Language Models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, Decem-
ber 6-10, 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for
Computational Linguistics, 996–1009. https://doi.org/10.18653/V1/2023.EMNLP-
MAIN.63
[21] Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A Plan-then-
Retrieval Augmented Generation for Generative Large Language Models as Deci-
sion Makers. In Proceedings of the 2024 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies (Vol-
ume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024 , Kevin Duh,
Helena Gómez-Adorno, and Steven Bethard (Eds.). Association for Computational
Linguistics, 6537–6555. https://doi.org/10.18653/V1/2024.NAACL-LONG.364
[22] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural In-
formation Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/
6b493230205f780e1bc26945df7481e5-Abstract.html
[23] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian
Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic Search-Enhanced Large
Reasoning Models. CoRR abs/2501.05366 (2025). https://doi.org/10.48550/ARXIV.
2501.05366 arXiv:2501.05366
[24] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-
Rong Wen, and Zhicheng Dou. 2025. WebThinker: Empowering Large Reasoning
Models with Deep Research Capability. CoRR abs/2504.21776 (2025). https:
//doi.org/10.48550/ARXIV.2504.21776 arXiv:2504.21776
[25] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and
Zhicheng Dou. 2025. From Matching to Generation: A Survey on Generative
Information Retrieval. ACM Trans. Inf. Syst. 43, 3, Article 83 (May 2025), 62 pages.
https://doi.org/10.1145/3722552
[26] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query
Rewriting for Retrieval-Augmented Large Language Models. CoRR abs/2305.14283
(2023). https://doi.org/10.48550/ARXIV.2305.14283 arXiv:2305.14283
[27] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun
Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024. RaFe: Ranking Feedback
Improves Query Rewriting for RAG. CoRR abs/2405.14431 (2024). https://doi.
org/10.48550/ARXIV.2405.14431 arXiv:2405.14431
[28] Raja Sekhar Reddy Mekala, Yasaman Razeghi, and Sameer Singh. 2024.
EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context

Conference’17, July 2017, Washington, DC, USA Guanting Dong, Xiaoxi Li, Yuyao Zhang, and Mengjie Deng
Learning. In Proceedings of the 2024 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies:
Short Papers, NAACL 2024, Mexico City, Mexico, June 16-21, 2024 , Kevin Duh, He-
lena Gómez-Adorno, and Steven Bethard (Eds.). Association for Computational
Linguistics, 399–432. https://doi.org/10.18653/V1/2024.NAACL-SHORT.35
[29] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019.
Multi-hop Reading Comprehension through Question Decomposition and Rescor-
ing. In Proceedings of the 57th Conference of the Association for Computational
Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long
Papers , Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). Association
for Computational Linguistics, 6097–6109. https://doi.org/10.18653/V1/P19-1613
[30] Jeonghyun Park and Hwanhee Lee. 2024. Conversational Query Reformulation
with the Guidance of Retrieved Documents. CoRR abs/2407.12363 (2024). https:
//doi.org/10.48550/ARXIV.2407.12363 arXiv:2407.12363
[31] Guilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin A
Raffel, Leandro Von Werra, Thomas Wolf, et al .2024. The fineweb datasets:
Decanting the web for the finest text data at scale. Advances in Neural Information
Processing Systems 37 (2024), 30811–30849.
[32] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Out-
performing Curated Corpora with Web Data Only. In Advances in Neu-
ral Information Processing Systems 36: Annual Conference on Neural Infor-
mation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, De-
cember 10 - 16, 2023 . http://papers.nips.cc/paper_files/paper/2023/hash/
fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html
[33] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong
Xu, Tong Xu, and Enhong Chen. 2024. Large Language Model based Long-tail
Query Rewriting in Taobao Search. In Companion Proceedings of the ACM on Web
Conference 2024, WWW 2024, Singapore, Singapore, May 13-17, 2024 , Tat-Seng
Chua, Chong-Wah Ngo, Roy Ka-Wei Lee, Ravi Kumar, and Hady W. Lauw (Eds.).
ACM, 20–28. https://doi.org/10.1145/3589335.3648298
[34] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and
Mike Lewis. 2023. Measuring and Narrowing the Compositionality Gap in
Language Models. In Findings of the Association for Computational Linguistics:
EMNLP 2023, Singapore, December 6-10, 2023 , Houda Bouamor, Juan Pino, and
Kalika Bali (Eds.). Association for Computational Linguistics, 5687–5711. https:
//doi.org/10.18653/V1/2023.FINDINGS-EMNLP.378
[35] Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Yibin Lei, Tianyi Zhou,
Michael Blumenstein, and Daxin Jiang. 2024. Retrieval-Augmented Retrieval:
Large Language Models are Strong Zero-Shot Retriever. In Findings of the As-
sociation for Computational Linguistics, ACL 2024, Bangkok, Thailand and vir-
tual meeting, August 11-16, 2024 , Lun-Wei Ku, Andre Martins, and Vivek Sriku-
mar (Eds.). Association for Computational Linguistics, 15933–15946. https:
//doi.org/10.18653/V1/2024.FINDINGS-ACL.943
[36] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike
Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG: Retrieval-Augmented
Black-Box Language Models. CoRR abs/2301.12652 (2023). https://doi.org/10.
48550/ARXIV.2301.12652 arXiv:2301.12652
[37] Mingyang Song and Mao Zheng. 2024. A Survey of Query Optimization in Large
Language Models. CoRR abs/2412.17558 (2024). https://doi.org/10.48550/ARXIV.
2412.17558 arXiv:2412.17558
[38] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucu-
rull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,
Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,
Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,
Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross
Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:
Open Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288 (2023).https://doi.org/10.48550/ARXIV.2307.09288 arXiv:2307.09288
[39] Venktesh V, Sourangshu Bhattacharya, and Avishek Anand. 2023. In-Context Abil-
ity Transfer for Question Decomposition in Complex QA. CoRR abs/2310.18371
(2023). https://doi.org/10.48550/ARXIV.2310.18371 arXiv:2310.18371
[40] Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan
Natarajan, and Amit Sharma. 2024. Plan ×RAG: Planning-guided Retrieval Aug-
mented Generation. arXiv:2410.20753 [cs.CL] https://arxiv.org/abs/2410.20753
[41] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion
with Large Language Models. In Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-
10, 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for
Computational Linguistics, 9414–9423. https://doi.org/10.18653/V1/2023.EMNLP-
MAIN.585
[42] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel
Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language
Models with Self-Generated Instructions. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), ACL
2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and
Naoaki Okazaki (Eds.). Association for Computational Linguistics, 13484–13508.
https://doi.org/10.18653/V1/2023.ACL-LONG.754
[43] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided
Retrieval Augmentation for Large Language Models. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , Houda Bouamor, Juan Pino, and
Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 10303–
10315. https://doi.org/10.18653/v1/2023.findings-emnlp.691
[44] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham
Neubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation.
arXiv:2311.08377 [cs.CL]
[45] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. RECOMP: Improving Retrieval-
Augmented LMs with Context Compression and Selective Augmentation. In The
Twelfth International Conference on Learning Representations, ICLR 2024, Vienna,
Austria, May 7-11, 2024 . OpenReview.net. https://openreview.net/forum?id=
mlJLVigNHp
[46] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li,
and Jing Xiao. 2023. PRCA: Fitting Black-Box Large Language Models for Re-
trieval Question Answering via Pluggable Reward-Driven Contextual Adapter.
InProceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2023, Singapore, December 6-10, 2023 . Association for Compu-
tational Linguistics, 5364–5375. https://aclanthology.org/2023.emnlp-main.326
[47] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan,
and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language
Models. In The Eleventh International Conference on Learning Representations,
ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net. https://openreview.
net/forum?id=WE_vluYUL-X
[48] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal,
Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than
Retrieve: Large Language Models are Strong Context Generators. In The Eleventh
International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
May 1-5, 2023 . OpenReview.net. https://openreview.net/forum?id=fB0hRu9GZUS
[49] Yuyao Zhang, Zhicheng Dou, Xiaoxi Li, Jiajie Jin, Yongkang Wu, Zhonghua
Li, Qi Ye, and Ji-Rong Wen. 2025. Neuro-Symbolic Query Compiler.
arXiv:2505.11932 [cs.CL] https://arxiv.org/abs/2505.11932
[50] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large
Language Models. CoRR abs/2303.18223 (2023).
[51] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang,
Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023.
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.
InThe Eleventh International Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023 . OpenReview.net. https://openreview.net/forum?
id=WZH7099tgfM
[52] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-
long Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Mod-
els for Information Retrieval: A Survey. CoRR abs/2308.07107 (2023). https:
//doi.org/10.48550/ARXIV.2308.07107 arXiv:2308.07107