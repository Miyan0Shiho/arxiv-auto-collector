# Knowledge-Aware Diverse Reranking for Cross-Source Question Answering

**Authors**: Tong Zhou

**Published**: 2025-06-25 14:23:21

**PDF URL**: [http://arxiv.org/pdf/2506.20476v1](http://arxiv.org/pdf/2506.20476v1)

## Abstract
This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

## Full Text


<!-- PDF content starts -->

arXiv:2506.20476v1  [cs.CL]  25 Jun 2025Knowledge-Aware Diverse Reranking for Cross-Source Question
Answering
Tong Zhou
Institute of Automation, Chinese Academy of Sciences
Beijing, China
tong.zhou@ia.ac.cn
Abstract
This paper presents Team Marikarp’s solution for the SIGIR 2025
LiveRAG competition. The competition’s evaluation set, automat-
ically generated by DataMorgana from internet corpora, encom-
passed a wide range of target topics, question types, question for-
mulations, audience types, and knowledge organization methods. It
offered a fair evaluation of retrieving question-relevant supporting
documents from a 15M documents subset of the FineWeb corpus.
Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.
1 Introduction
The SIGIR 2025 LiveRAG Challenge represents a systematic initia-
tive for evaluating modern Retrieval-Augmented Generation [ 1,3]
systems in open-domain question answering scenarios. Organized
by the Technology Innovation Institute, this competition provides
a standardized benchmark comprising two core components: (1) a
fixed document corpus combining FineWeb datasets [ 8] containing
15M documents of cleaned web data, (2) the Falcon3-10B-Instruct
foundation model [9] as the mandatory LLM component.
Participants are tasked with developing RAG systems that inte-
grate retrieval from either custom-built indices or pre-configured
options: a Pinecone dense index utilizing E5-base [ 11] embeddings
on 512-token chunks, and an OpenSearch sparse index implement-
ing BM25. A notable feature of the competition framework is Data-
Morgana [ 2], a synthetic question generator that produces adver-
sarial test queries through controlled perturbations of original doc-
ument content.
1.1 Evaluation
The evaluation dataset for LiveRAG is a 500-item test set generated
by DataMorgana, a lightweight two-stage tool designed for creating
diverse synthetic Q&A benchmarks. Leveraging natural language
descriptions, DataMorgana allows detailed configurations of user
categories (e.g., expert, novice, domain-specific roles like patients
or researchers) and question categories (e.g., factoid, open-ended,
with/without premise, varying phrasing styles such as concise-
natural or long-search-query), enabling combinatorial diversity
across lexical, syntactic, and semantic dimensions. The generation
process involves sampling documents from the FineWeb corpora,
then invoking an Claude-3.5-Sonnet with prompts tailored to the
configured categories to produce questions that reflect realistic
user interactions. The test questions include single-document type
and multi-document type. The multi-document type questions are
generated by DataMorgana based on two relevant documents, and
answering such questions must simultaneously reference both doc-
uments. This dataset ensures high coverage of diverse query types,from fact-based inquiries to complex, premise-including questions,
and incorporates filtering to validate faithfulness to source docu-
ments, providing a robust and customizable benchmark for evalu-
ating RAG systems in the LiveRAG challenge.
Evaluation employs a multi-stage verification protocol: Rele-
vance Scoring : Answers are rated on a 4-level scale assessing both
correctness and concision (scores: -1, 0, 1, 2). Faithfulness Veri-
fication : Responses are checked against retrieved evidence using
a 3-tier grading system (scores: -1, 0, 1). The assessment pipeline
combines automated evaluation using Claude-3.5 Sonnet for initial
scoring with human verification of top submissions.
To address bias and misinformation in RAG reranking, we pro-
pose a knowledge-aware diverse reranking RAG pipeline. This
paper details its architecture, training, and experimental results.
2 Pipeline
This section outlines our Retrieval-Augmented Generation (RAG)
pipeline, focusing on its three main stages: retrieval, reranking, and
answer generation.
2.1 Retrieval
Our retrieval strategy combines sparse and dense methods to max-
imize recall. Questions generated by DM can vary significantly,
ranging from direct inquiries about the target document to ques-
tions requiring substantial rephrasing of keywords or entities. To
address this, we employ a hybrid approach. We treat the question
directly as a query and retrieve the top 𝑁𝑟𝑒𝑡results from both
sparse𝑀𝑠𝑝𝑎𝑟𝑠𝑒
𝑟𝑒𝑡and dense retrievers 𝑀𝑑𝑒𝑛𝑠𝑒
𝑟𝑒𝑡. The top𝑁𝑟𝑒𝑡results
from each are then merged by alternately selecting documents from
their respective ranked lists, preserving intrinsic orderings, and
discarding duplicates until a final list of 𝑁𝑟𝑒𝑡unique documents is
obtained.
2.2 Reranking
Given the inherent limitations of LLM regarding context window
length and their susceptibility to noise content, the number of doc-
uments𝑁𝑎𝑛𝑠provided for answer generation must be significantly
smaller than the initially retrieved set 𝑁𝑟𝑒𝑡. Consequently, an ef-
fective reranking mechanism is critical for prioritizing the most
relevant documents and ensuring optimal LLM performance.
2.2.1 Initial Reranking. We initially employ a large-scale pre-trained
reranking model, denoted as 𝑀𝑟𝑎𝑛𝑘, to perform a preliminary rerank-
ing. This model processes a query 𝑄and document 𝐷pair and out-
puts a relevance score 𝑆𝑟𝑎𝑛𝑘 between 0 and 1. 𝑆𝑟𝑎𝑛𝑘=𝑀𝑟𝑎𝑛𝑘(𝑄,𝐷).
We rank all retrieved documents according to these scores.

SIGIR ’25, July 13–17, 2025, Padova, Italy Tong Zhou
2.2.2 Knowledge Element Declaration. However, relying solely on
the complete query for reranking can introduce a bias towards
specific knowledge facets, particularly within multi-document con-
texts. For instance, consider the query: “Who had more world titles,
Muhammad Ali or Ramon Dekkers?” If the semantic encoding of the
query overemphasizes one athlete’s knowledge or if more relevant
information is retrieved for one athlete concerning world titles , the
reranking might disproportionately favor that individual. This can
result in a ranked list that skews towards one aspect of the knowl-
edge, failing to provide all necessary information to answer the
complete question. Therefore, to mitigate this potential bias and
ensure a more balanced information synthesis, we propose leverag-
ing the semantic understanding capabilities of a Large Language
Model (LLM) to decompose the initial query into independent sub-
queries. This decomposition facilitates a more granular and targeted
reranking process. Nevertheless, the efficacy of such LLM-based
decomposition hinges on a nuanced understanding of the query’s
underlying knowledge requirements. Without this, an LLM might
misinterpret the query’s intent, leading to suboptimal sub-query
generation. For example, when presented with the question, “What
is the difference between llama and falcon in training methods?” , an
LLM lacking relevant contextual information might erroneously in-
terpret llama andfalcon as animal species, decomposing the query
into “How to train a llama?” and “How to tame a falcon?” . Con-
versely, by integrating effective joint semantic retrieval, the LLM
can ascertain from the retrieved corpus that llama andfalcon refer
to large language models. This informed understanding enables a
more accurate decomposition of the original query, aligning the
sub-queries with the user’s true informational needs.
To address the bias issue caused by reranking based on the
original question and the misleading problem due to large lan-
guage models’ lack of understanding of the question context, we
propose a knowledge-aware diverse reranking strategy. We utilize
the LLM𝑀𝑘𝑛𝑜𝑤 referencing the top documents (top 𝑁𝑘𝑛𝑜𝑤 docu-
ments) from the preliminary reranking one by one to deconstruct
the question in order to identify essential knowledge elements 𝐸𝑞𝑖
required for a comprehensive answer. Simultaneously, it extracts
knowledge elements 𝐸𝑑𝑖already present within each document 𝐷𝑖
to guarantee the understanding of the given document, thereby
assessing the document’s specific content coverage. These can also
used for our heuristic automatic judgment of extraction quality.
Specifically, the complete knowledge set 𝐸𝑞𝑖must necessarily in-
clude all information extracted from the current document 𝐷𝑖that
is critical for addressing the target question. This implies 𝐸𝑑𝑖⊆𝐸𝑞𝑖.
Figures 2 and 5 present an example prompt and a corresponding
demonstration, respectively.
2.2.3 Knowledge Element Summarization. Recognizing that indi-
vidual documents provide partial or aspect-specific information, our
strategy further leverages the LLM 𝑀𝑠𝑢𝑚𝑚 to synthesize the declar-
ative knowledge 𝐸𝑞aggregated from these top documents. This
synthesis culminates in two distinct, yet complementary, knowl-
edge representations, 𝐸𝑞𝑠𝑢𝑚 0and𝐸𝑞𝑠𝑢𝑚 1, designed to be core to the
query’s intent while offering diverse and comprehensive coverage
of the relevant information landscape. Figure 3 displays the prompt
used, while Figure 6 provides a representative example.System:
You are a helpful assistant.
You are given a question and a list of documents.
You need to answer the question based on the documents.
Please answer the question concisely, with no more than 200 words.
User:
Question: {question}
Context: {documents}
Figure 1: The prompt for answer generation.
2.2.4 Knowledge Aware Diverse Reranking. We then rerank the top
𝑁𝑟𝑎𝑛𝑘 documents based on these two summarized knowledge ele-
ments𝐸𝑞𝑠𝑢𝑚 0and𝐸𝑞𝑠𝑢𝑚 1as queries. The reranking model 𝑀𝑟𝑎𝑛𝑘
is again employed, taking question concat with knowledge element
as query, to produce two reranked lists focused on different aspects.
𝑆𝐸𝑖
𝑟𝑎𝑛𝑘=𝑀𝑟𝑎𝑛𝑘(𝑄;𝐸𝑞𝑠𝑢𝑚 𝑖,𝐷). Same as merging two retrieved lists
from sparse and dense retrievers, the top-ranked results from these
two lists𝑆𝐸𝑖
𝑟𝑎𝑛𝑘are then interleaved by alternately selecting entries
while preserving their inherent order and eliminating duplicates, ul-
timately producing a consolidated list of 𝑁𝑟𝑎𝑛𝑘 unique documents.
2.3 Answer Generation
To mitigate the impact of noise in the retrieved context, we select
the top𝑁𝑎𝑛𝑠reranked documents and provide them to the LLM
𝑀𝑎𝑛𝑠. We explicitly prompt the LLM to generate a concise answer,
as detailed in Figure 1.
3 Training and Implementation
3.1 DataMorgana Data Generation
While existing DataMorgana provides examples for multiple ques-
tion types, we enhance RAG system robustness through systematic
problem-type expansion. Specifically, we leverage DeepSeek-R1 [ 4],
a state-of-the-art deep reasoning LLM, to brainstorm and augment
the existing single-document and multi-document problem types.
The prompt is shown in Figure 4. We employ a multi-generation pro-
cess followed by manual selection to curate high-quality expansions.
This process ultimately expanded the number of single-document
question categories to 21 and multi-document problem types to 7.
It is worth noting that single-document question types can still be
applied to multi-document question generation. For example, when
generating multi-document questions, the generated questions can
be required to be open-ended and specified as comparative ques-
tions of the multi-document type. Further details on the expanded
problem types are provided in Table 9 and 10. For evaluation, we
generated 300 test questions for both single- and multi-document
scenarios, and 3500 training questions for each.
3.2 Knowledge Declaration Training
To refine the model’s capability in analyzing knowledge elements,
we employ a supervised fine-tuning (SFT) approach leveraging re-
jection sampling [ 6,10]. To mitigate the alignment gap typically

Knowledge-Aware Diverse Reranking for Cross-Source Question Answering SIGIR ’25, July 13–17, 2025, Padova, Italy
observed with external stronger teacher models and to reduce oper-
ational costs, we adopt a self-generation strategy. Specifically, the
Falcon-10B-Instruct model, which is the subject of our fine-tuning,
is utilized to generate the training data. This process involves task-
ing the model with processing inputs from our training set and
subsequently applying heuristic rules to sample high-quality out-
puts.
First, we establish a typology for document relevance based on
documents retrieved for questions within the training set, defining
three categories: Fully Supporting, Partially Relevant, and Irrele-
vant.
Fully Supporting : A document segment (chunk) is labeled Fully
Supporting if, for single-document questions, it is the first chunk
retrieved via sparse retrieval (based on the concatenation of ques-
tion and answer pairs as a query) that corresponds to the gold
document. For multi-document questions, the first correctly re-
trieved chunk from each of the two gold documents (identified
via sparse retrieval using the concatenation of question and an-
swer pairs) is concatenated to form the Fully Supporting evidence.
Preliminary experiments demonstrate that when utilizing a concate-
nated question-answer pair as the query, 97% of multi-document
questions successfully retrieve the gold chunks within the top 400
retrieved passages. Furthermore, for single-document questions,
100% retrieval of the gold chunk is achieved within the top 100
passages.
Partially Relevant: A chunk is designated Partially Relevant ex-
clusively for multi-document questions, where any single correctly
retrieved chunk from a gold document falls into this category.
Irrelevant: Chunks are those not originating from the gold
document(s). It is important to note that while these documents
differ from the gold standard, they may still contain knowledge
pertinent to the question.
Building upon these document labels, we implement a series
of rejection rules to filter the disqualified outputs generated by
Falcon-10B-Instruct. If an output fails this review, the model is
re-prompted to generate, up to a maximum of retry attempts 𝑁𝑟𝑠.
The acceptance criteria include: Formatting Compliance: The out-
put must adhere to predefined structural requirements, including a
discernible thought process and the final answer in JSON format.
Furthermore, the output must be parsable by regular expressions
to extract a list of strings. Knowledge Quantity and Uniqueness:
The number of distinct knowledge pieces identified in the output
must meet specified thresholds and each piece must be unique.
Knowledge Attribution: The knowledge extracted and attributed
to a given document must originate entirely from the information
required to answer the current question, verified by exact string
matching. Relevance-contingent Knowledge Coverage: For Fully
Supporting documents, the extracted knowledge must be identi-
cal to the required knowledge. For Partially Relevant documents,
the extracted knowledge must be a non-empty, strict subset of the
required knowledge. For Irrelevant documents, the extracted knowl-
edge must be a strict subset of the required knowledge and may
be empty. This iterative generation and filtering process yielded
a substantial corpus of data, from which we selected 1,000 Fully
Supporting, 2,500 Partially Relevant, and 6,500 Irrelevant instances
that satisfied all criteria, forming our SFT dataset with 10k samples.
The Falcon model was subsequently fine-tuned for 3 epochs usingLow-Rank Adaptation (LoRA) [ 5]. LoRA was applied to all model
parameters, configured with rank 𝑟=16, a learning rate of 1𝑒−4,
and a batch size of 8. After the SFT process, we get 𝑀𝑘𝑛𝑜𝑤 .
3.3 Parameter Selection and Experiments
Retriever R@10 R@100 R@200 R@400 R@1k R@2k R@4k
sparse&dense 0.41 0.69 0.73 0.79 0.86 0.90 0.93
dense 0.37 0.64 0.69 0.76 0.83 0.87 0.91
sparse 0.36 0.59 0.65 0.72 0.78 0.82 0.86
Table 1: Recall of gold documents under multi-document
dataset using different retrieval methods. If only one gold
document is recalled, the recall rate is 50%.
Retriever R@10 R@100 R@200 R@400 R@1k R@2k R@4k
sparse&dense 0.61 0.81 0.86 0.89 0.93 0.94 0.95
dense 0.53 0.72 0.77 0.84 0.88 0.90 0.93
sparse 0.59 0.76 0.82 0.86 0.89 0.93 0.94
Table 2: Recall of gold documents under single-document
dataset using different retrieval methods.
Retrieval Setting: We utilize the provided Opensearch Sparse In-
dexas𝑀𝑠𝑝𝑎𝑟𝑠𝑒
𝑟𝑒𝑡andPinecone Dense Index as𝑀𝑑𝑒𝑛𝑠𝑒
𝑟𝑒𝑡, with intfloat/e5-
base-v2 as the dense encoder, for retrieval. API requests are used
to obtain results. Based on preliminary recall experiments and effi-
ciency considerations shown in Table 2 and 1, we determined an
optimal retrieval quantity of 𝑁𝑟𝑒𝑡=2000.
Model Params R@3 R@5 R@10 R@20 R@50 R@400
None - 0.45 0.52 0.56 0.64 0.71 0.85
bge-reranker-v2-m3 568M 0.58 0.64 0.68 0.73 0.78 0.85
jina-reranker-v2-base-multi 278M 0.53 0.61 0.69 0.74 0.82 0.91
bge-reranker-v2-gemma 2.51B 0.69 0.76 0.81 0.85 0.90 0.93
jina-reranker-m0 2.44B 0.74 0.81 0.85 0.88 0.91 0.93
jina-reranker-m0 (ft) 2.44B 0.76 0.80 0.83 0.86 0.91 0.94
jina-reranker-m0 (ft-us) 2.44B 0.73 0.77 0.82 0.87 0.89 0.94
Table 3: Comparison of ranking performance of different
open-source ranking models on a subset of single-document
dataset.
Model Selection: The LLM𝑀𝑎𝑛𝑠,𝑀𝑠𝑢𝑚𝑚 we use is the origi-
nal Falcon3-10B-Instruct without further training, and the 𝑀𝑘𝑛𝑜𝑤
used is the Falcon3-10B-Instruct model fine-tuned via LoRA. As
it performed best in our preliminary reranking experiments, the
reranking model 𝑀𝑟𝑎𝑛𝑘 adopts the jina-reranker-m0 model, with
detailed results shown in Table 3 and 4. We also fine-tuned the jina-
reranker-m0 model using a training set generated by DataMorgana.
We explored two positive-to-negative sample ratios: 1:16 (ft) and
4:16 (ft-us). However, the fine-tuned model’s performance did not
consistently surpass that of the original model. Further details are
provided in the Appendix.

SIGIR ’25, July 13–17, 2025, Padova, Italy Tong Zhou
Model Params R@3 R@5 R@10 R@20 R@50 R@400
None - 0.24 0.32 0.39 0.47 0.57 0.73
bge-reranker-v2-m3 568M 0.26 0.33 0.41 0.49 0.60 0.77
jina-reranker-v2-base-multi 278M 0.25 0.33 0.43 0.48 0.62 0.79
bge-reranker-v2-gemma 2.51B 0.31 0.35 0.45 0.51 0.62 0.81
jina-reranker-m0 2.44B 0.36 0.43 0.53 0.62 0.72 0.87
jina-reranker-m0 (ft) 2.44B 0.36 0.42 0.53 0.60 0.71 0.87
jina-reranker-m0 (ft-us) 2.44B 0.39 0.45 0.54 0.62 0.72 0.87
Table 4: Comparison of ranking performance of different
open-source ranking models on a subset of multi-document
dataset. If only one gold document is recalled, the recall rate
is 50%.
𝑁𝑎𝑛𝑠 single-document multi-document
relevance faithfulness relevance faithfulness
30 1.23 0.36 1.17 0.24
20 1.33 0.40 1.22 0.28
10 1.42 0.48 1.27 0.30
5 1.42 0.48 1.12 0.21
3 1.40 0.48 1.06 0.16
Table 5: The relationship between the number of reference
documents 𝑁𝑎𝑛𝑠and the automatically evaluated score of
answers when answering questions. We use Deepseek-v3 for
scoring.
𝑁𝑎𝑛𝑠Selection: Increasing the number of candidate answer doc-
uments𝑁𝑎𝑛𝑠generally raises the probability of including the most
relevant target document. However, a larger 𝑁𝑎𝑛𝑠also introduces
more irrelevant documents, which can misguide the model and lead
to sub-optimal answer generation. To evaluate this trade-off, we
constructed evaluation prompts based on official metrics and used
the DeepSeek-V3 [ 7] model to assess both relevance and faithful-
ness. The prompt is shown in Figure 7 and the evaluation results
are shown in Table 5. Our comparative analysis across different
𝑁𝑎𝑛𝑠ans values demonstrates that 𝑁𝑎𝑛𝑠=10consistently achieves
optimal performance for both question types examined.
Method 𝑁𝑘𝑛𝑜𝑤 R@3 R@5 R@10 R@20
jina-reranker-m0 - 0.36 0.42 0.53 0.61
Ours 0 0.34 0.40 0.52 0.60
2 0.35 0.42 0.53 0.63
5 0.36 0.44 0.55 0.64
8 0.35 0.43 0.54 0.63
Table 6: Performance comparison between reranking of the
proposed knowledge aware diverse reranking pipeline and
using only the jina reranker in multi-document dataset.
Parameters in Knowledge Element Declaration: We test the
performance of the knowledge aware diverse reranking method
using varying numbers of 𝑁𝑘𝑛𝑜𝑤 . As Table 6 and 7 show, our ap-
proach with 𝑁𝑘𝑛𝑜𝑤 =5consistently outperforms the original Jina
reranking model in multi-document scenarios, while showing no
significant performance degradation in single-document settings.Method 𝑁𝑘𝑛𝑜𝑤 R@3 R@5 R@10 R@20
jina-reranker-m0 – 0.75 0.82 0.85 0.88
Ours 0 0.71 0.76 0.82 0.85
2 0.74 0.82 0.84 0.88
5 0.75 0.83 0.85 0.87
8 0.73 0.79 0.84 0.87
Table 7: Performance comparison between reranking of the
proposed knowledge aware diverse reranking pipeline and
using only the jina reranker in single-document dataset.
This demonstrates that diversity reranking effectively mitigates the
bias issue . Notably, when the model generates new knowledge ele-
ments without referencing retrieved documents, performance gen-
erally falls below that of the Jina re-ranking model. This confirms
that our proposed document-referencing mechanism alleviates the
misleading problem .
3.4 Inference Deployment
To enhance the efficiency of the complete RAG workflow, we pro-
pose a cascaded producer-consumer framework. This design ad-
dresses the inherent sequential dependencies within our RAG pro-
cess. The workflow is orchestrated into distinct, concurrently exe-
cuting stages: retrieval, initial reranking, knowledge decomposition,
knowledge summarization, further reranking, and answer gener-
ation. Each stage processes its input and queues the results for
subsequent steps. We leverage Flask for deploying reranking model
and vLLM for LLMs, ensuring a decoupled and scalable pipeline.
Furthermore, for reranking and knowledge decomposition, we em-
ploy multi-process asynchronous task splitting during request pro-
cessing, allowing multiple processes to concurrently handle partial
tasks and integrate results in an ordered manner. This architec-
ture enables dynamic resource allocation for API services based on
performance bottlenecks, thereby optimizing overall efficiency.
4 Results and Conclusion
Our method achieved the highest scores in both Correctness and
Faithfulness automatic evaluation metrics on the Session 2 dataset
of the official online evaluation, as assessed by Claude-3.5 Sonnet
(Table 8). This superior performance also extends to the Session 1
dataset, where our approach outperformed other methods under
the same question distribution.
This paper introduces Knowledge-Aware Diverse Reranking,
a novel RAG framework designed to mitigate bias issue caused
by reranking based on the original question and the misleading
problem due to LLM’s lack of understanding of the question context.
The efficacy of our method is substantiated through comprehensive
local experiments and online evaluations.
Due to temporal constraints, several design choices were not
subjected to exhaustive empirical validation, and numerous initial,
less successful, attempts were not further optimized. These aspects
present opportunities for future investigations within the broader
RAG domain and for subsequent iterations of the LiveRAG compe-
tition. A more detailed discussion of these unexplored avenues and
unrefined explorations is provided in the Appendix.

Knowledge-Aware Diverse Reranking for Cross-Source Question Answering SIGIR ’25, July 13–17, 2025, Padova, Italy
References
[1]Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin,
Tat-Seng Chua, and Qing Li. 2024. A survey on rag meeting llms: Towards
retrieval-augmented large language models. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining . 6491–6501.
[2]Simone Filice, Guy Horowitz, David Carmel, Zohar Karnin, Liane Lewin-Eytan,
and Yoelle Maarek. 2025. Generating Diverse Q&A Benchmarks for RAG Evalua-
tion with DataMorgana. arXiv preprint arXiv:2501.12789 (2025).
[3]Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin
Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented
generation for large language models: A survey. arXiv preprint arXiv:2312.10997
2, 1 (2023).
[4]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin
Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al .2025. Deepseek-r1:
Incentivizing reasoning capability in llms via reinforcement learning. arXiv
preprint arXiv:2501.12948 (2025).
[5]Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, Weizhu Chen, et al .2022. Lora: Low-rank adaptation of large
language models. ICLR 1, 2 (2022), 3.
[6]Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, and Prathap Ramachandra. 2024. Rs-
dpo: A hybrid rejection sampling and direct preference optimization method for
alignment of large language models. arXiv preprint arXiv:2402.10038 (2024).
[7]Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Cheng-
gang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al .2024. Deepseek-v3
technical report. arXiv preprint arXiv:2412.19437 (2024).
[8]Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret
Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The FineWeb
Datasets: Decanting the Web for the Finest Text Data at Scale. In The Thirty-eight
Conference on Neural Information Processing Systems Datasets and Benchmarks
Track . https://openreview.net/forum?id=n6SCkn2QaG
[9] TII Team. 2024. The Falcon 3 family of Open Models.
[10] Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. 2024.
Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving.
Advances in Neural Information Processing Systems 37 (2024), 7821–7846.
[11] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,
Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised
Contrastive Pre-training. arXiv preprint arXiv:2212.03533 (2022).A Leaderboard
See Table 8
B Exploring Unsuccessful Attempts
This section details various unsuccessful strategies we explored to
improve retrieval performance, focusing on query rewriting and
reranking techniques. These attempts, while not yielding significant
improvements in our specific settings, offer valuable insights and
directions for future research.
B.1 Query Rewriting with Falcon3-10B-Instruct
To enhance retrieval recall, we experimented with endowing the
Falcon3-10B-Instruct model with query rewriting capabilities through
supervised fine-tuning. We explored several approaches for con-
structing training data: (1) We leveraged DeepSeek-v3 to extract
the most valuable keywords from questions, standard answers, and
gold documents. The model was then trained to generate these
keywords given a question. (2) To alleviate the capability gap in
the distillation process, we used Falcon3-10B-Instruct to generate
a thought process and valuable retrieval keywords based on the
input question. We sampled 16 iterations and selected the thought
process and keywords that yielded the best retrieval performance
as training data.
Beyond keywords, we also attempted to generate pseudo-documents
or pseudo-answer. However, none of these query rewriting meth-
ods outperformed using the original query with a sparse-dense
hybrid retrieval approach. We hypothesize that for single question
retrieval scenarios, query rewriting heavily relies on the model’s
parametric knowledge. Moreover, rewriting might inadvertently
steer the query towards a specific, potentially suboptimal, search
direction, leading to performance degradation. It’s also possible
that the SFT approach limited the model’s full potential, and rein-
forcement learning might offer a more effective training paradigm.
B.2 Fine-tuning Jina Reranker
We attempted to fine-tune jina-reranker-m0 model using data gen-
erated by DM. We meticulously designed 16 types of hard negative
samples, including those with high ROUGE-L scores relative to the
answer, question, and gold document, as well as the most relevant
documents from dense and sparse retrieval. Training was conducted
using contrastive loss or BCE loss, but no significant performance
improvement was observed. A potential reason for this outcome is
our labeled dataset misclassification of potentially relevant docu-
ments as negative samples, which could confuse the model during
training. Simpler negative sampling strategies or larger-scale train-
ing datasets might be promising avenues for future exploration.
B.3 LLM-based Reranking
We explored a Chain-of-Thought approach using the Falcon3-10B-
Instruct model to assess the value of each document on top of
an initial reranking. Documents deemed "False" by the LLM had
their scores reduced by 1. We distilled DeepSeek-v3’s CoT data for
relevance judgment to train this mechanism.
Experiments indicated that this method improved R@3 and R@5
metrics in single-document scenarios but showed no benefit for
multi-document questions. Furthermore, this approach introduced

SIGIR ’25, July 13–17, 2025, Padova, Italy Tong Zhou
Team Name Correctness Faithfulness
RMIT-ADMS 1.199317 0.477382
RUC_DeepSearch 0.969273 0.387808
Ped100X 0.928893 0.043381
PRMAS-DRCA 0.922780 0.410600
Hybrid Search with Graph 0.875091 0.315802
BagBag 0.694073 -0.911353
UniClustRAG 0.685146 0.460062
METURAG 0.673451 0.325339
DeepRAG 0.566053 0.097828
UIS-IAI 0.552328 0.433697
SNU-LDILab 0.517367 0.103027
Gravitational Lens 0.376637 -0.988097
- - -
(a) Session 1 ResultsTeam Name Correctness Faithfulness
Magikarp (Ours) 1.231578 0.656464
UDInfo 1.200586 0.623175
RAGtifier 1.134454 0.552365
HLTCOE 1.070111 0.340711
Ragmatazz 1.011956 0.519394
ScaledRAG 0.996348 0.418273
Emorag 0.890718 0.556581
Graph-Enhanced RAG 0.875714 0.529335
Multi-Agent Adaptive RAG 0.836110 0.200420
Starlight 0.818337 0.433003
NoobRAG 0.655292 0.154648
UIUC-RAGents 0.565043 -0.302616
AugmentRAG-TUD 0.532533 0.655634
(b) Session 2 Results
Table 8: Comparison of Different RAG Systems with automatic evaluation. Teams ranked by the correctness of the answers.
System:
Please first analyze the given problem and determine which knowledge
elements are required to answer the given problem.
Please first think step by step and then output in numbered list (no
more than 4 points).
Then analyze which knowledge points are provided in the given
document from these required knowledge elements.
Please first think step by step and then output the number of knowlege
elements (if no knowledge elements are provided, output None).
**Example output format:**
Thoughts knowlege requirements:
<your thoughts here>
Knowledge Elements:
<numbered list>
Analysis for given document:
<compare and analyse document and above knowledge point by point>
Given Knowledge:
<selected numbers>
Example Input:
{demo_input}
Example Output:
{demo_output}
User:
Question: {question}
Document: {document}
Figure 2: The prompt for Knowledge Element Declaration.System:
You have identified the knowledge elements required to answer the
given question based on the provided question and the retrieved
relevant documents.
Please select two of the most important, complete, and non-redundant
knowledge elements from the identified knowledge elements for
further retrieving the knowledge base to answer the given question.
Please think step by step first, and finally output the result in the
format of a Python list.
**Example Output Format:**
Thoughts:
<your thoughts here>
Selected Knowledge Elements:
```json
[
"Knowledge Element 1",
"Knowledge Element 2"
]
```
## Examples:
**Example Input:**
{demo_input}
**Example Output:**
{demo_output}
User:
Question: {question}
Knowledge Elements:
```
{knowledge_elements}
```
Figure 3: The prompt for knowledge element summarization.
considerable inference latency, precluding its adoption in the final
solution. In contrast to query rewriting, which demands additional
knowledge, reranking solely relies on matching and judging based
on provided information. This makes it a more promising optimiza-
tion direction for RAG systems built upon LLMs with parametric

Knowledge-Aware Diverse Reranking for Cross-Source Question Answering SIGIR ’25, July 13–17, 2025, Padova, Italy
Categorization Category Description Probability
Factualityfactoid Question seeking a specific, concise piece of information or
a short fact about a particular subject, such as a name, date,
or number.0.5
open-ended Question inviting detailed or exploratory responses, encour-
aging discussion or elaboration.0.5
Premisedirect Question that does not contain any premise or any infor-
mation about the user.0.95
with-premise Question starting with a very short premise, where the user
reveals their needs or some information about himself.0.05
Phrasingconcise-and-natural Phrased in the way people typically speak, reflecting every-
day language use, without formal or artificial structure. It is
a concise direct question consisting of less than 10 words.0.25
verbose-and-natural Phrased in the way people typically speak, reflecting every-
day language use, without formal or artificial structure. It is
a relatively long question consisting of more than 10 words
and less than 20 words.0.75
short-search-query Phrased as a typed web query for search engines (only key-
words, without punctuation and without a natural-sounding
structure). It consists of less than 7 words.0.05
long-search-query Phrased as a typed web query for search engines (only key-
words, without punctuation and without a natural-sounding
structure). It consists of more than 6 words.0.05
Linguistic Variationsimilar-to-document Phrased using the same terminology and phrases appearing
in the document.0.5
distant-from-document Phrased using terms completely different from the ones
appearing in the document.0.5
Answer Formatnumeric Question expecting a numerical answer (e.g., dates, quanti-
ties, percentages).0.2
textual Question expecting a natural-language sentence or para-
graph as an answer.0.5
list Question expecting a structured list of items (e.g., steps,
features, categories).0.3
Question Complexitysimple_lookup Requires finding a single, explicitly stated piece of informa-
tion within the document.0.5
synthesis Requires combining information from multiple sentences
or sections of the document.0.5
Scopenarrow Focuses on a very specific detail, entity, attribute, or event. 0.5
broad Asks about a general theme, summary, main idea, or cate-
gory.0.5
Temporal Focusspecific-time-point Refers to a single date, year, or specific event time. 0.2
time-duration Refers to a span or period of time. 0.15
relative-time Refers to time relative to another event or the present. 0.15
atemporal No specific time focus, asking about general facts or pro-
cesses.0.5
Table 9: Question classification taxonomy for single-document.
limitations. Future work should explore more generalized rein-
forcement learning training methods for reranking. Additionally,
the presence of false positives and false negatives could lead to
ranking instability; therefore, future research should also focus on
soft-combining the judgment results of LLMs with reranker scores.B.4 Knowledge Gap-based Reranking
Our initial motivation for designing a knowledge decomposition
strategy was to identify and reorder documents based on knowledge

SIGIR ’25, July 13–17, 2025, Padova, Italy Tong Zhou
Categorization Category Description Probability
Multi-doccomparison A comparison question that requires comparing two related
concepts or entities. The comparison must be natural and
reasonable, i.e., comparing two entities by a common at-
tribute which is meaningful and relevant to both entities.
Example: “Who is older, Glenn Hughes or Ross Lynch?” Re-
quires information from two documents about respective
entities.0.3
multi-aspect A question about two different aspects of the same en-
tity/concept. Example: “What are the advantages of AI-
powered diagnostics, and what are the associated risks?”
Requires two documents about different aspects.0.3
cause-effect Analysis of causal relationships where cause and effect are
in separate documents. Example: “How did the 2023 El Niño
influence global agricultural yields?” First document details
causes, second describes effects.0.05
conflicting-views Addresses divergent opinions from different sources. Exam-
ple: “How do IPCC and Heartland Institute differ in sea level
rise projections?” Requires two documents with opposing
perspectives.0.05
definition-combination Requires synthesizing two complementary definitions. Ex-
ample: “Combined definition of ’greenwashing’ from policy
documents and corporate reports.” Merges partial defini-
tions from two sources.0.1
temporal-change Asks about changes between time periods. Example: “Gray
wolf population in Yellowstone before (1995) and after (2015)
reintroduction.” Requires documents from different time
periods.0.1
multi-hop Requires multi-step reasoning across documents. Example:
“What was the first company founded by Donald Trump’s
father?” Needs sequential information retrieval.0.1
Table 10: Question classification taxonomy for multi-document.
This is the question type used for generating RAG (Retrieval-
Augmented Generation) questions. The entire list captures various
dimensions of questions, including factuality, premise, phrasing, and
linguistic_variation. Each dimensional aspect further contains parallel
options and possibilities; for example, regarding factuality, there are
two types: factoid and open-ended. Please continue brainstorming new
categorization names and their corresponding categories.
{question_categorizations}
Figure 4: The prompt for new question type brainstorming.
gaps within the top-ranked results. Specifically, we used a knowl-
edge declaration model to identify missing knowledge elementsfrom each of the top 5 documents (i.e., total output knowledge
minus existing knowledge in the current document). These missing
knowledge elements were then used to rerank documents outside
the initial top 5.
However, even in multi-document scenarios, this method did not
outperform using only Jina’s reranking model in our experiments.
We believe the concept of a "knowledge gap" aligns with advanced
methods like DeepSearch. Nevertheless, LLMs struggle to accu-
rately quantify "existing knowledge within the current document,"
especially when dealing with open-ended answers, as this standard
is inherently vague and challenging to define. Future improvements
should focus on employing more robust reasoning models capable
of performing simultaneous reasoning and searching.
C Prompt and examples

Knowledge-Aware Diverse Reranking for Cross-Source Question Answering SIGIR ’25, July 13–17, 2025, Padova, Italy
Input:
Question: What are the key differences between getting lost in the backcountry and being stranded without man-made resources in terms of the
survival skills required to handle each situation?
Document: Bushcraft’ is a word that gets thrown around very often in the survival community, but it’s also a word that far fewer people understand it.
A truly skilled survivalist is someone who can use resources provided by nature exclusively to survive. Ask yourself this: if you were stranded out
in the wilderness tomorrow with nothing but the clothes on your back and could only use completely natural resources to survive, would you be
able to? If your honest answer is no, then you will probably find the information presented in this article useful. We are going to provide you with
a definitive list of bushcraft skills that will allow you to survive in the wilderness using no man-made materials whatsoever. THE BOW DRILL
METHOD Everyone knows how important fire is in any survival situation. But not everyone is capable of starting a fire without a flint striker, lighter,
or matches. It’s imperative that you learn a way to start a fire without any of those kinds of fire starting devices. The best method for starting a fire
without any man-made materials is the bow drill method. This method requires you to collect a flat piece of wood (to serve as the fireboard) with a
notch cut into it. You also need a bow complete with a vine for the cord, and a sharpened stick as the spindle. Wrap the vine around the spindle
and place the point of the spindle right above where the notch is in the fireboard. Proceed to run the bow back and forth very quickly and over an
extended period, friction and heat build between the spindle and board. The small shavings of wood will then fall into the notch. Have a tinder nest
already made and different sized kindling on standby. Once you get an ember or smoke, you can transfer the ember into the tinder next to get your fire
started. Proceed to add kindling, and you’ll have a fire going. The bow drill method may sound simple enough on paper, but it’s going to be more
physically taxing in real life. You never want a true survival situation to be the first time you practice the bow drill method of fire starting. For this
reason, practice extensively now on weekends or whenever you have the time until you become a master at it. That way, it will seem virtually second
nature to you in a life-or-death situation. The skill of tying together two strips of vine or other man-made materials is one that you will not only use
in a survival situation but throughout your life as well
Output:
Thoughts knowledge requirements:
To answer the question about the key differences between getting lost in the backcountry and being stranded without man-made resources, we need
to understand the specific survival skills required for each scenario. Getting lost in the backcountry might involve using some man-made resources
(e.g., a map, compass, or gear), while being stranded without man-made resources requires pure bushcraft skills (e.g., fire-starting, shelter-building,
foraging). The document focuses on bushcraft skills, which are more relevant to the latter scenario.
Knowledge Elements:
1. Understanding the definition and scope of bushcraft (survival using only natural resources).
2. Fire-starting techniques without man-made tools (e.g., bow drill method).
3. The importance of practicing bushcraft skills before a survival situation.
4. General survival skills like tying materials (though not explicitly detailed for shelter or foraging).
Analysis for given document:
The document primarily discusses bushcraft skills, emphasizing fire-starting without man-made tools (bow drill method) and the need for practice. It
does not explicitly contrast backcountry survival (with potential man-made resources) vs. stranded survival (without man-made resources), but it
provides insights into the latter. The document covers points 1, 2, and 3 but does not delve into broader survival skill comparisons (e.g., navigation,
shelter-building for backcountry vs. bushcraft).
Given Knowledge:
1, 2, 3
Figure 5: A Demonstration of Knowledge Element Declaration.

SIGIR ’25, July 13–17, 2025, Padova, Italy Tong Zhou
Input:
Question: How has the understanding of decision-making and choice abundance evolved from Barry Schwartz’s early research on the jam experiment
to more recent psychological approaches like the U-Theory?
Knowledge Elements:
```
Barry Schwartz 's jam experiment and its findings on choice and decision-making.
The adverse effects of choice abundance, including analysis paralysis, buyer 's remorse, and decision fatigue.
The U-Theory and its relevance to understanding decision-making in the context of choice abundance.
The impact of choice abundance on consumer behavior, conversions, retention, and revenue.
Barry Schwartz 's jam experiment and its implications for decision-making.
The concept of choice overload and its effects on decision-making.
The role of cognitive limits in decision-making (e.g., George Miller 's "magical number seven").
The impact of information quantity on purchasing decisions (Iyengar and Lepper 's research).
Barry Schwartz 's jam experiment and its impact on understanding choice overload.
The concept of choice abundance and its psychological effects.
The evolution of psychological theories on decision-making.
Introduction to U-Theory and its application in understanding decision-making.
Barry Schwartz 's jam experiment and its findings on decision-making paralysis.
The concept of choice abundance and its effects on consumer behavior.
U-Theory 's approach to decision-making, emphasizing the balance between too few and too many options.
Evolution of psychological theories on decision-making from Schwartz 's early work to U-Theory.
Barry Schwartz 's jam experiment and its findings on decision-making paralysis.
The concept of choice abundance and its effects on consumer behavior.
U-Theory 's perspective on decision-making and its differences from Schwartz 's early work.
Evolution of psychological approaches to decision-making and choice abundance.
```
Output: Thoughts:
The question asks about the evolution from Barry Schwartz’s jam experiment to U-Theory. The most critical points are the foundational findings of
the jam experiment (highlighting choice overload) and U-Theory’s modern approach to balancing choice abundance. These directly address the
"evolution" in understanding, avoiding redundancy with other supporting concepts like cognitive limits or consumer behavior impacts.
Selected Knowledge Elements:
```json
[
"Barry Schwartz 's jam experiment and its findings on decision-making paralysis.",
"U-Theory 's approach to decision-making, emphasizing the balance between too few and too many options."
]
```
Figure 6: A Demonstration of Knowledge Element Summarization.

Knowledge-Aware Diverse Reranking for Cross-Source Question Answering SIGIR ’25, July 13–17, 2025, Padova, Italy
System:
You are an QA evaluation assistant.
Your task is assess the quality of the answer provided by the model base on metrics **Relevance** and **Faithfulness**.
You are given a question, one or two reference documents, the ground truth answer and the model’s output.
Please first evaluate the relevance of the output with respect to the question and the ground truth answer. And then evaluate the faithfulness of the
output with respect to the documents.
The definition of **Relevance** score is as follows:
Combines elements of equivalence (semantic match with ground truth) and relevance (degree to which the answer directly addresses the question).
Graded on a four-point scale:
2: Correct and relevant (no irrelevant information).
1: Correct but contains irrelevant information.
0: No answer provided (abstention).
-1: Incorrect answer.
The definition of **Faithfulness** score is as follows:
Assesses whether the response is grounded in the retrieved passages.
Graded on a three-point scale:
1: Full support. All answer parts are grounded.
0: Partial support. Not all answer parts are grounded.
-1: No support. All answer parts are not grounded.
Please first think step by step and then output your evaluation scores in a json format.
Example output format:
<your thoughts here>
```json
{
"relevance": <your assessment of relevance>,
"faithfulness": <your assessment of faithfulness>
}
```
User:
**Question:** {question}
**Ground Truth Answer:** {answer}
**Reference Documents:** {gold_context}
**Model’s Output:** {output}
Figure 7: Our prompt for answer scoring using DeepSeek-v3.