# FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation

**Authors**: Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, Xihui Liu

**Published**: 2025-06-23 17:59:16

**PDF URL**: [http://arxiv.org/pdf/2506.18899v1](http://arxiv.org/pdf/2506.18899v1)

## Abstract
AI-driven content creation has shown potential in film production. However,
existing film generation systems struggle to implement cinematic principles and
thus fail to generate professional-quality films, particularly lacking diverse
camera language and cinematic rhythm. This results in templated visuals and
unengaging narratives. To address this, we introduce FilMaster, an end-to-end
AI system that integrates real-world cinematic principles for
professional-grade film generation, yielding editable, industry-standard
outputs. FilMaster is built on two key principles: (1) learning cinematography
from extensive real-world film data and (2) emulating professional,
audience-centric post-production workflows. Inspired by these principles,
FilMaster incorporates two stages: a Reference-Guided Generation Stage which
transforms user input to video clips, and a Generative Post-Production Stage
which transforms raw footage into audiovisual outputs by orchestrating visual
and auditory elements for cinematic rhythm. Our generation stage highlights a
Multi-shot Synergized RAG Camera Language Design module to guide the AI in
generating professional camera language by retrieving reference clips from a
vast corpus of 440,000 film clips. Our post-production stage emulates
professional workflows by designing an Audience-Centric Cinematic Rhythm
Control module, including Rough Cut and Fine Cut processes informed by
simulated audience feedback, for effective integration of audiovisual elements
to achieve engaging content. The system is empowered by generative AI models
like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a
comprehensive benchmark for evaluating AI-generated films. Extensive
experiments show FilMaster's superior performance in camera language design and
cinematic rhythm control, advancing generative AI in professional filmmaking.

## Full Text


<!-- PDF content starts -->

arXiv:2506.18899v1  [cs.CV]  23 Jun 2025FilMaster: Bridging Cinematic Principles and Generative AI for Automated
Film Generation
Kaiyi Huang1∗Yukun Huang1Xintao Wang2†Zinan Lin3Xuefei Ning4
Pengfei Wan2Di Zhang2Yu Wang4Xihui Liu1†
1The University of Hong Kong2Kuaishou Technology3Microsoft Research4Tsinghua University
https://filmaster-ai.github.io
In the vast desert, Godzillaand a lone Cybertruckengaged in a fierce battle, with the beast's footsteps shaking the ground, while the electric truck used its powerful engine and smart tech to desperately escape the endless sandstorm.
(1)(2)(3)(4)(5)(6)
(7)(8)(9)(10)(11)(12)Input:text, reference images 
Output: film with professional camera work and a rich, dynamic soundscape 
Foley/SFX/VO/BG/Music composition:
From real cinematography↓Camera language: Multi-shot synergy with cinematic coherence & expressivenessFrom audience-centric post-production↓Cinematic rhythm: Audiovisual elements with audience engagementFilMaster: learn and emulate cinematic principles
Figure 1. Video samples generated by FilMaster. Using a textual theme and reference images for characters and locations, FilMaster crafts
high-quality films complete with professional camera language and cinematic rhythm, including rich, multi-layered audiovisual outputs
(foley, sound effects (SFX), voice-over(VO), background ambiance, musical scoring, and video).
Abstract
AI-driven content creation has shown potential in revolu-
tionizing film production. However, existing film gener-
ation systems struggle to understand and implement fun-
damental cinematic principles and thus fail to generate
professional-quality films, particularly lacking diverse and
expressive camera language and cinematic rhythm. This
often results in templated visuals and unengaging narra-
tives. To address these limitations, we introduce FilMas-
ter, a comprehensive end-to-end AI-powered automated film
generation system to integrate real-world cinematic prin-
ciples for professional-grade film generation, yielding ed-
itable and industry-standard outputs. FilMaster is built
upon two key cinematic principles: (1) learning cinematog-
∗Part of the work done during intenrship at KwaiVGI, Kuaishou Tech-
nology.†Corresponding author.raphy from extensive real-world film data and (2) emulat-
ing professional, audience-centric post-production work-
flows. Inspired by these principles, FilMaster incorporates
two stages: Reference-Guided Generation Stage which
transforms user input to video clips, and Generative Post-
Production Stage which transforms raw footage into au-
diovisual outputs by orchestrating both visual and audi-
tory elements for cinematic rhythm. Our Reference-Guided
Generation Stage highlights a Multi-shot Synergized RAG
Camera Language Design module to guide the AI system
to generate professional and expressive camera language
in videos by retrieving reference clips from a vast cor-
pus of 440,000 film clips. Our Generative Post-Production
Stage emulates professional post-production by designing
an Audience-Centric Cinematic Rhythm Control module,
including a Rough Cut assembly, a Fine Cut process in-
formed by simulated audience feedback, for the effective
integration of audiovisual elements through video editing
1

and sound design, to achieve engaging content and emo-
tional impact. The whole system is empowered by gener-
ative AI models such as (M)LLMs, and video generation
models. Furthermore, we introduce FilmEval, a compre-
hensive benchmark for evaluating AI-generated films across
key cinematic dimensions. Extensive experiments demon-
strate FilMaster’s superior performance, particularly in so-
phisticated camera language design and nuanced cinematic
rhythm control, paving the way for generative AI in profes-
sional filmmaking.
1. Introduction
Traditional film production involves a complex process
from pre-production to post-production [12]. The rapid
development of generative AI [18, 42] and (Multimodal)
Large Language Models ((M)LLMs) [26, 34] has revolu-
tionized AI-driven content creation, opening up new pos-
sibilities for film production [46]. However, current AI-
driven film generation systems oversimplify the intricate
process of filmmaking, often neglecting core cinematic
principles [3]. They typically use LLMs for scriptwrit-
ing, and then generate sequential video clips independently
through visual generation models [19, 40]. The result-
ing outputs tend to be templated and unengaging, falling
short of professional cinematic standards, which limits their
integration into real-world production workflows. These
limitations largely arise because current AI systems often
struggle to understand and implement fundamental cin-
ematic principles , when applied to the complex art of film-
making [3]. These principles are crucial for crafting effec-
tive cinematic narratives [30], particularly evident in two
key aspects: (1) Camera Language: the artful use of cin-
ematographic techniques to convey narrative and emotion
by crafting the visual language [1, 24]. A lack of effective
camera language leads to visuals that are either templated
or overly generic, lack of coherence, failing to convey the
intended narrative. For instance, while MovieAgent [40]
attempts to integrate camera language, it often produces in-
coherent or uninspired shots, as these elements are gener-
ated based on LLM’s imagination rather than professional
cinematic techniques, resulting in incoherence and a lack of
expressiveness. (2) Cinematic Rhythm: the masterful or-
chestration of pacing and flow through editing and sound
to shape the audience’s emotional journey [25, 31]. The
absence of such control results in a flat, unengaging experi-
ence, where video clips are often simply concatenated with
limited and desynchronized audio [21, 40] (Table 1). This
leads to repetitive narration and a diminished emotional im-
pact for the audience. Consequently, without a deep com-
prehension of how cinematic techniques contribute to films
and how rhythm shapes audience experience, AI-generated
content frequently yields uninspired visuals and fails to cap-
Previous FilMaster(Ours)Script StructuringMulti-Shot Camera Design
Sound DesignVideo EditingTemplated CameraLimited & Disjointed AudioNon-editable OutputAutomating Filmmaking with Cinematic PrinciplesCamera Language Design: For Expressive & Coherent VisualsCinematic Rhythm Control:For Immersive Audiovisual ExperiencesIndustry-Standard Output: For a Seamless Professional Workflow1
2
3PreviousLimitationsFigure 2. Comparison of our FilMaster with AI-driven current
workflow and film generation system.
ture the deliberate artistic choices of skilled filmmakers.
To address these challenges, we propose FilMaster, an
automated film generation system designed to bridge the
gap from script to screen with industry-standard output
(Figure 2). Our approach is built upon two key cine-
matic principles: (1) Learning Cinematography from Ex-
tensive Real-World Film Data . Filmmakers tradition-
ally hone camera language skills, vital for narrative coher-
ence and expressiveness, by studying extensive film ref-
erences. Drawing inspiration from this practice, FilMas-
ter learns from a vast corpus of real film clips to master
and apply professional camera language. (2) Emulating
Professional, Audience-Centric Post-Production Work-
flows . This principle directly addresses the challenge of cin-
ematic rhythm. Post-production is crucial in filmmaking for
an engaging experience, where proper pacing, and the effec-
tive integration of audiovisual elements are paramount [5].
Our FilMaster emulates the post-production workflow to
control narrative pacing, and construct immersive audio-
visual experiences. Inspired by the recent progress of
MLLMs, which show great promise in understanding com-
plex text scripts, videos, and audio, we introduce MLLMs
to implement such cinematic principles in FilMaster.
FilMaster integrates cinematic principles for film gen-
eration through a two-stage process from generation to
post-production, leveraging advanced (M)LLMs for tasks
beyond scriptwriting: (1) Reference-Guided Generation
Stage. This stage takes the input text and the reference
character/location images as input, leverages (M)LLMs for
video content planning and script structuring, and then gen-
erates video clips with video generation models. Guided
by our first principle, we propose Multi-shot Synergized
RAG Camera Language Design module to address the
shortcomings of existing methods that often produce tem-
plated or overly generic camera language by learning pro-
fessional cinematography from real films. FilMaster uti-
lizes RAG with a vast dataset of 440,000 real film clips to
retrieve relevant cinematic descriptions, which are indexed
by film clips’ annotated textual descriptions, based on the
query of scene’s textual context. To synergize the cam-
2

era language between multiple shots within the same scene,
we take multiple shots into consideration with the spatio-
temporal contexts and narrative objectives to retrieve ref-
erence film clips. These retrieved textual descriptions then
serve as references for an LLM to learn professional cin-
ematography to re-plan shots, including shot types, cam-
era movements, angles, and atmospheric characteristics,
aligned with the scene’s narrative objectives. By ground-
ing in professional films, the system is empowered to gen-
erate expressive, contextually relevant shots with coherent
camera language, ensuring both cinematic-standard coher-
ence and narrative expressiveness. (2) Generative Post-
Production Stage. This stage transforms the raw footage
from the previous stage into rich, multi-layered audiovisual
outputs, orchestrating both visual and auditory elements to
achieve cinematic rhythms. Guided by our second prin-
ciple, we propose Audience-Centric Cinematic Rhythm
Control module to address the common issue of flat, un-
engaging AI-generated content by emulating professional,
audience-centric post-production workflows. The process
begins with assembling a Rough Cut to establish the basic
narrative structure. Next, an MLLM, acting as a simulated
audience, reviews this Rough Cut and generates feedback.
This feedback then guides a sophisticated Fine Cut editing
process. To tailor this review, the MLLM can be provided
with a demographic profile, potentially sourced from inter-
net searches, to simulate a specific target audience. This
feedback then guides a sophisticated Fine Cut editing pro-
cess, which focuses on the effective integration of audio-
visual elements: Video editing involves detailed structural
and durational adjustments to video clips. Sound design en-
sures that a rich, multi-layered soundscape is crafted, inte-
grating diverse audio elements (background ambiance, mu-
sical scoring, voice-overs (VO), foley, and sound effects
(SFX)) with multi-scale audiovisual synchronization. No-
tably, MLLMs drive this entire process, from audience-
centric review to Rough and Fine Cut editing, by being
prompted to adopt distinct professional roles ( e.g., audi-
ence, film editor, sound designer). Through this audience-
centric process, FilMaster adjusts narrative pacing and en-
ables effective integration of audiovisual elements, resulting
in outputs with immersive and engaging rhythms.
Furthermore, to address the practical limitations of prior
systems, which often produce non-editable video files,
limiting practical industry integration, FilMaster gener-
ates editable and structured output videos including audio
clips, with multi-track timelines using the industry-standard
OpenTimelineIO (OTIO) format [27]. This allows seam-
less export to professional editing software like DaVinci Re-
solve , directly bridging AI generation with professional film
production workflows.
To address the lack of suitable benchmarks for AI
film generation, we introduce FilmEval. Existing bench-Table 1. Comparison of film generation capabilities across differ-
ent AI methods: Anim-Director [19], MovieAgent [40], and LTX-
Studio (commercial product) [21].
MethodScript
DesignCamera
Language
DesignAudio
TypesA/V
SyncVideo Adj.
(Structure)Video Adj.
(Duration)Audience
ReviewEditable
Output
Anim-Director ✓ × 0 × × × × ×
MovieAgent ✓ Templated 1 × × × × ×
LTX-Studio ✓ Templated 1 Limited × × × ✓
FilMaster (Ours) ✓ ✓ (Film-based) 5 ✓ ✓ ✓ ✓ ✓
marks [47, 49] often focus solely on visual results, lack-
ing capabilities for a holistic film evaluation. In con-
trast, FilmEval is a comprehensive benchmark that evalu-
ates quality across key cinematic dimensions, covering nar-
rative, audiovisual techniques, aesthetics, rhythm, engage-
ment, and overall quality. Our experiments demonstrate Fil-
Master’s superior performance, particularly in sophisticated
camera language design and cinematic rhythm control.
To summarize, our contributions are as follows:
•A Novel System Integrating Cinematic Principles.
We propose FilMaster, the first comprehensive AI-based
film generation system explicitly designed around cine-
matic principles to guide camera language and cinematic
rhythm. It bridges the gap from script to screen and pro-
duces editable, structured output compatible with profes-
sional production workflows.
•Learning Cinematography from Real Films. We intro-
duce a novel Multi-shot Synergized RAG Camera Lan-
guage Design module that generates coherent and expres-
sive visuals by learning cinematographic patterns from a
vast corpus of real films, resulting in synergized multi-
shot outputs.
•AI-Driven Post-Production for Cinematic Rhythm.
We present an innovative Audience-Centric Cinematic
Rhythm Control module that emulates professional post-
production. It leverages MLLMs to control the narra-
tive pacing, and integrates audiovisual elements based on
audience-centric analysis for immersive experiences.
•A Comprehensive Film Evaluation Benchmark. We
establish a new benchmark, FilmEval, for the holistic
evaluation of AI-generated films and provide experiments
that validate FilMaster’s superior performance in creating
high-quality, engaging cinematic content.
2. Related Work
2.1. Video Generation
Existing video generation models can be roughly catego-
rized into diffusion model-based [2, 9, 10, 15, 23, 32, 37, 42,
48], and language model-based [6, 7, 17, 35, 43, 44]. Video
diffusion models excel by progressively refining noisy in-
puts into clean video samples, with recent advancements
like Sora [4], HunyuanVideo [18], and Wan-Video [36]
demonstrating remarkably high-quality visual synthesis via
sophisticated latent diffusion techniques. Video language
models, such as VideoPoet [17], are typically derived from
3

the family of transformer-based language models that can
flexibly incorporate multiple tasks in pretraining, and show
powerful zero-shot capabilities.
2.2. LLMs for Film Production
Recent works for preliminary exploration in film production
have begun to leverage the emerging reasoning and plan-
ning capabilities of LLMs [20, 38]. Anim-Director [19]
automates animation generation by employing large mul-
timodal models (LMMs) to expand user inputs into coher-
ent storylines, generate scene images and videos by itera-
tively optimization. The development of multi-agent sys-
tems [11, 14, 29, 33, 39, 45] has further spurred innovation.
For example, FilmAgent [41] proposes multi-agent collab-
oration through iterative feedback and revisions in con-
structed 3D virtual spaces. MovieAgent [40] multi-agent
Chain of Thought planning for movie generation, which
automatically structures scenes, camera settings, and cin-
ematography. Distinct from these prior efforts, FilMaster
significantly expands the role of LLMs to orchestrate a more
comprehensive end-to-end film production pipeline, from
pre-production through post-production. Crucially, our sys-
tem uniquely integrates the learning of camera language
from real film footage and the emulation of professional
post-production workflows for cinematic rhythm control,
aspects not holistically addressed by previous LLM-based
film generation systems (Table 1).
3. Method
We introduce the overview of our system in Section 3.1, and
detail the two core innovation modules, Multi-shot Syner-
gized RAG Camera Language Design module in Section 3.2
and Audience-Centric Cinematic Rhythm Control module
in Section 3.3, respectively.
3.1. Overview of FilMaster
FilMaster is an automated film generation system designed
to produce complete films from an input text, guided by ref-
erence images for characters and locations, and generating
outputs fully editable and structured, with multi-track time-
lines using the industry-standard OTIO format. As illus-
trated in Figure 3, the overall process can be conceptualized
into a two-stage process:
• Reference-Guided Generation Stage: This stage takes the
input text and the reference character/location images as
input, leverages (M)LLMs for video content planning and
script structuring, and then generates video clips with
video generation models. This involves first, from coarse-
to-fine, progressively refining the initial input text into de-
tailed scene descriptions with spatio-temporal contexts.
Subsequently, the camera language for each shot within
the same scene is synergistically planned. Finally, video
clips are generated based on this designed visual languageand the reference images. Our Multi-shot Synergized
RAG Camera Language Design module (detailed in Sec-
tion 3.2) plays a crucial role in crafting coherent and ex-
pressive visual language.
• Generative Post-Production Stage: Based on the gener-
ated videos from the Reference-Guided Generation Stage,
this stage transforms the raw footage into polished fi-
nal outputs, orchestrating both visual and auditory el-
ements to achieve cinematic rhythms. This stage in-
cludes assembling a Rough Cut , refining this cut based
on simulated audience-centric feedback to achieve a Fine
Cut, with structural and durational adjustments in video
editing and a multi-layered soundscape in sound design.
Our Audience-Centric Cinematic Rhythm Control mod-
ule (detailed in Section 3.3) is central to controlling narra-
tive structure and pacing, ensuring effective integration of
audiovisual elements, and thereby improving emotional
resonance and audience engagement.
The completed videos and audio are packaged into the
industry-standard OTIO format with multi-track timeline.
This ensures compatibility with professional editing soft-
ware, facilitating seamless integration into real-world film
production workflows.
3.2. Multi-shot Synergized RAG Camera Language
Design
Inspired by how professional filmmakers traditionally hone
their camera language skills through extensive study of
film references, FilMaster incorporates a Multi-shot Syner-
gized RAG Camera Language Design module, referenc-
ing over a vast dataset of real film clips for camera language.
This module overcomes the limitations of templated and
overly generic camera work through multi-shot synergized
RAG, which involves: (1) spatio-temporal-aware indexing
for embedding scene contexts, (2) film reference retrieval
for retrieving similar cinematic examples, and (3) shot re-
planning for generating coherent and expressive cinematog-
raphy.
Spatio-Temporal-Aware Indexing. FilMaster first pro-
cesses the input text into scene blocks. A scene block de-
fines a continuous segment of the narrative occurring within
a single, coherent scene, thus maintaining continuity in both
space and time for that specific scene. Each scene block is
described by its spatio-temporal contexts, comprising ele-
ments of multi-shot prompts, the scene’s location, time of
day, present characters, and key visual elements, and its
overarching narrative objective for that scene (example is
shown in Figure 3 top-right part). Each shot within the same
scene block shares the same reference images for charac-
ters and locations to maintain continuity. This coarse-to-
fine script structuring, employing a chain of LLMs, pro-
gressively refines the input text from a synopsis to a simple
storyboard and a detailed storyboard and finally into scene
4

TextReference-Guided Generation Stage
Scene Blocks440kFilm Data
Expressive and Coherent | Real Cinematography LearningFilm Reference RetrievalShot Re-Planning
IndexingEngaging | Audience-centric, Post-Production EmulationOutputGenerative Post-Production Stage
Multi-track OTIO formatEditable & StructuredMulti-Shot Synergized RAG Camera Language Design
Video EditingSound DesignStructureDuration A/V SyncSound MixingAudience-Centric Cinematic Rhythm ControlVideo GenThought: Enrich the prompt by adding camera movements and atmosphere based on the descriptions provided and retrieve similar prompts from the dataset for inspiration. …Retrieved captions: -Caption 1: A lone astronaut in a futuristic,…Thought: Using the retrieved captions, I will design suitable camera movements and atmosphere to enhance ..Shot replan: (1) The camera begins with a wide, sweeping shot of the vastness of space, slowly zooming in on a small, …(2) Transitioning smoothly, the camera shifts to a steadyPrompt: (1) The Little Prince stands on a small rocky asteroid, looking around ….(2) Next to him, a White Fox sits quietly. Its fur moves slightly in  ...Time: Beginning of the journey; Location: Asteroid; Characters: Little Prince, White Fox; Visual Elements: Stars, winds;Objective: Emphasize the insignificance of the characters against the backdrop of the universe.
Rough CutFine CutFeedback. Pacing/Rhythm: The script, as is, reads a bit slow and descriptive. Short drama viewers prefer concise and engaging content. Analysis. Structure adjustment: Revise the narrative to begin with…The demographic profile and characteristics of short drama viewers can be summarized as follows: Preferences: They are drawn to concise storytelling and quick, engaging content...
-0-5s: The video opens with ...-10-15s: Transition to …{"structure shot": [1,3,…]} {"shot 1": "0-5s","duration": 3s,"mode": "accelerate","reason": …}BackgroundMusicFoleySFX
VO
46.8k audio
shotsceneIntra-shot
Reference ImagesExample: Cinematic Rhythm Control
Films Compatible with Editing Software (e.g.DaVinci Resolve)Example: Camera Language DesignScene block
Figure 3. Overview of the FilMaster framework for automated film generation. It processes user input to produce editable, struc-
tured outputs, guided by cinematic principles. Key innovations include the (1) Multi-shot Synergized RAG Camera Language Design
module within the Reference-Guided Generation Stage stage, leveraging real film data for coherent and expressive visuals, and the (2)
Audience-Centric Cinematic Rhythm Control module within the Generative Post-Production Stage stage, emulating professional post-
production for enhanced audience engagement. Examples are shown in gray area.
blocks (detailed in Appendix A). The meticulous design of
the scene block with its spatio-temporal contexts and nar-
rative objectives promotes a highly coherent and expressive
camera language within that scene, leading to synergized
multi-shot outputs. Secondly, the scene block is encoded
into vector representations using an embedding model and
stored in a vector database as text embeddings [8, 22]. Each
scene block, rich with spatio-temporal contexts and nar-
rative objectives, serves as the precise query for the sub-
sequent retrieval and generation process, ensuring that the
learned camera language is deeply aligned with the specific
narrative objectives of each shot within the same scene. Our
real film dataset contains over 440,000 professionally anno-
tated film clips. These annotated textual descriptions detail
key aspects of cinematic descriptions of camera language,
including shot types, camera movements, angles, and atmo-
spheric characteristics. The textual descriptions of the film
dataset are also encoded into a vector representation using
the same embedding model.
Film Reference Retrieval. The scene block, previously
defined through Spatio-Temporal-Aware Indexing and in-
corporating its spatio-temporal contexts and narrative ob-
jective, serves as the query for retrieval. Its vector represen-
tation (the query vector) is compared against the film dataset
vectors, and similarity scores are computed. Then the pro-
cess prioritizes and retrieves the top- Kfilm references that
demonstrate the greatest similarity to this query vector. The
text descriptions of the retrieved flim clips are subsequently
used as references to guide the LLM-based shot re-planning
in the next paragraph.
Shot Re-planning. Building upon the retrieved refer-
ences, FilMaster analyzes recurring cinematic patterns and
extracts professional camera techniques applicable to thecurrent narrative context. This analysis focuses on iden-
tifying effective visual storytelling methods that can en-
hance the visual impact and narrative objectives of each
scene block. Concretely, the original scene block query and
the retrieved film references are synthesized into a coher-
ent prompt for the LLM. The LLM then re-plans the multi-
shot prompts to ensure consistent camera language within a
scene block. The re-plan process can be applied iteratively
with the multi-turn dialogue of LLMs. This multi-shot syn-
ergized design within the scene block, guided by the scene’s
narrative objectives and informed by real film references,
ensures multi-shot continuity and coherence, which is a key
distinction from prior work that often treats shots in isola-
tion. The shot re-planning generation specifies appropriate
shot types, camera movements, angles, and atmospheric de-
scriptions for each shot, while keeping the original narrative
content and objectives intact. This process ensures the final
camera language is not only expressive but also coherent at
the multi-shot scene level.
3.3. Audience-Centric Cinematic Rhythm Control
While our Multi-shot Synergized RAG Camera Language
Design module generates visually coherent scenes, relying
solely on this visual output, without suitable narrative drive
and effective integration of audiovisual elements, would re-
sult in a film with flat, unengaging generated content, fail-
ing to connect with audiences, and falling far short of pro-
fessional standards. Therefore, to address this, we propose
aAudience-Centric Cinematic Rhythm Control module.
Inspired by professional film post-production workflows [5]
that progressively refine rhythm, this module emulates this
process by first assembling and reviewing a Rough Cut from
a simulated audience-centric perspective. It then proceeds
5

to a Fine Cut , where it orchestrates the visual narrative
structure and pacing through video editing, and integrates a
rich, multi-layered soundscape through sound design, mak-
ing the film compelling in both its emotional resonance
and audience engagement. MLLMs drive this entire pro-
cess, from audience-centric review to Rough andFine Cut
editing, by being prompted to adopt professional roles in
film post-production ( e.g., audience, film editor, sound de-
signer).
Audience-Centric Review. Traditional AI approaches
often operate solely from a director’s perspective, poten-
tially limiting the film’s emotional resonance and engage-
ment with the actual audience. To bridge this gap, FilMas-
ter introduces an audience-centric review mechanism. This
approach integrates both the director’s intended narrative
expression with simulated audience expectations. Our Fil-
Master first allows for the specification of a target audience
archetype ( e.g., “short-drama audience”). An MLLM then
utilizes Internet search tools to construct a demographic
profile detailing this archetype’s characteristics, prefer-
ences, and typical viewing expectations (such as a prefer-
ence for concise storytelling or fast-paced engagement). To
facilitate the review, a Rough Cut version is assembled for
basic narrative structure. This currently involves combin-
ing the video sequences generated by the Multi-shot Syner-
gized RAG Camera Language Design module with LLM-
generated audio textual descriptions (VO) for each scene
block, serving as a temporary placeholder for the sound-
scape for easier understanding by the audience. An MLLM,
informed by the constructed audience demographic profile,
then critiques this version. Rather than modifying content
directly, the MLLM identifies potential misalignments in ar-
eas such as structural flow, narrative pacing, scene transi-
tions, and the consistency of the placeholder audio descrip-
tions. Following the audience-centric critique, a separate
LLM served as the analysis module that bridges critique and
implementation by systematically categorizing identified is-
sues into three dimensions: structural organization, timing
and duration, and audio coherence. This analysis produces
actionable recommendations that directly inform the subse-
quent processes, translating audience-centric feedback into
concrete adjustments in a Fine Cut , with video editing and
sound design.
Video Editing. In video editing process, deeper refine-
ment is performed based on audience analysis. Based on
the audience’s feedback and text description of video, an-
notated with precise timecodes, our system employs LLMs
prompted to act as professional film editors to systemati-
cally address narrative structure and pacing, addressing is-
sues related to logical flow and information density, through
two primary mechanisms: (1) Structural Reorganization:
Rearranging or removing redundant shots to create a more
logical and compelling progression of scenes, improvingnarrative structure and audience understanding. (2) Dura-
tion Adjustment: Modifying the temporal length of individ-
ual clips to control information exposure per scene for nar-
rative pacing. Three operations are considered: trimming
(removing redundant visual content), acceleration (speed-
ing up segments to fit pacing needs), and retention (preserv-
ing original timing when appropriate). This editing process
progressively aligns visual storytelling with narrative objec-
tives and audience expectations, refining the narrative struc-
ture and pacing, and culminating in a picture lock, which is
a finalized visual sequence ready for detailed sound design.
Sound Design. Prior AI systems often neglect or im-
plement limited audio, as shown in Table 1, failing to lever-
age the power of effective audiovisual elements to create
atmosphere and emotionally engage audiences beyond vi-
suals alone. Thus, we propose multi-scale A/V synchro-
nization for effective integration of audiovisual elements,
including background ambience, music scoring, VO, fo-
ley, and SFX, forming a multi-layer soundscape, followed
by sound mixing. Directly relying on MLLM to design
multi-track audio (including background ambience, musi-
cal scoring, VO, foley, and SFX) often results in poor syn-
chronization with video content. This is primarily due to
the MLLM’s limited capability in managing the coordina-
tion of multiple audio layers requiring different temporal
scales relative to the video. To address this, we propose
a multi-scale audiovisual synchronization strategy, which
systematically designs different audio types at appropriate
temporal scales. This process is supported either by syn-
thesized voice content for VO, or retrieval-augmented gen-
eration (RAG) from curated audio libraries for other au-
dio types like background ambience, music scoring, fo-
ley and SFX (similar as in Section 3.2). Synchronization
is managed across three key temporal scales: scene-level
for broader elements like background ambiance and music
scoring to establish overall atmosphere; shot-level for com-
ponents like VO that align with specific shots; and intra-
shot for fine-grained foley and SFX tied to precise actions
or visual events within a shot. At the scene-level, an LLM
directly utilizes the scene block to select appropriate mu-
sic and background ambiance. For shot-level sound de-
sign, an LLM is informed by text descriptions of the video
and audience feedback. In contrast, intra-shot level sound
design employs an MLLM to achieve greater precision in
aligning fine-grained audio cues with visual events. Finally,
to address inconsistencies in loudness, frequency balance,
and dynamic range that arise when composing multiple au-
dio tracks, FilMaster applies automated sound mixing tech-
niques (Detailed in Appendix B). These processes, includ-
ing LUFS normalization and frequency adjustments, ensure
cross-track harmonization, voice intelligibility, and overall
sonic cohesion, resulting in a polished soundscape.
6

4. Experiments
4.1. Experiment Setting
Implementation Details. We use GPT-4o [26] for script
generation, RAG, video editing, and sound design (VO,
background, music). For audience-centric review and sound
design (foley and sound effect), we employ Gemini-2.0-
Flash [34]. We use Kling Elements [16] as the video
generation model, capable of incorporating multiple refer-
ence images as conditions. The generated video clips are
at 1920×1080 resolution, comprising 153 frames per se-
quence.
Evaluation Metrics. As this work pioneers an end-
to-end film generation task with comprehensive attention
to camera language and cinematic rhythm, we establish
FilmEval , a holistic evaluation benchmark. FilmEval is
based on six high-level dimensions essential for assess-
ing film quality: Narrative and Script (NS) ,Audiovisu-
als and Techniques (AT) ,Aesthetics and Expression (AE) ,
Rhythm and Flow (RF) ,Emotional and Engagement (EE) ,
andOverall Experience (OE) . These dimensions are further
decomposed into twelve specific criteria for detailed evalu-
ation (criteria are detailed in Appendix Appendix C):
•NS: Script Faithfulness (SF), Narrative Coherence (NC)
•AT: Visual Quality (VQ), Character Consistency (CC),
Physical Law Compliance (PLC), V oice/Audio Quality
(V/AQ)
•AE: Cinematic Techniques (CT), Audio-Visual Richness
(A VR)
•RF: Narrative Pacing (NP), Video-Audio Coordination
(V AC)
•EE: Compelling Degree (CD)
•OE: Overall Quality (OQ)
While our work highlights two key modules for camera
language and cinematic rhythm, it’s important to recognize
that cinematic quality arises from the holistic integration of
various elements. Our evaluation dimensions are therefore
designed to capture not only the direct outputs of each mod-
ule but also their synergistic impact on the final film:
• The Multi-shot Synergized RAG Camera Language De-
sign module’s impact is primarily evaluated through NS
(SF, NC), ensuring visual storytelling aligns with the
script, and through key visual aspects of AT (VQ, CC,
PLC), reflecting the quality and coherence of the planned
visual foundation. This module also lays the groundwork
for effective AE (CT) by designing shots with inherent
cinematic qualities and contributes to the visual compo-
nent of AE (A VR).
• The Audience-Centric Cinematic Rhythm Control mod-
ule’s effectiveness is measured by audio-related aspects
of AT (V/AQ), the realized AE (CT, A VR) through so-
phisticated editing and sound design, the mastery of RF
(NP, V AC), and the resulting EE (CD). This module co-ordinates the visual and auditory elements into a cohesive
and impactful rhythmic experience, evaluated by the ulti-
mate arbiter OE (OQ).
To evaluate our method, we employ both automatic eval-
uation metrics and user studies in FilmEval. Given the ab-
sence of existing automatic metrics tailored for this task, we
propose Gemini-1.5-Flash as the evaluation model designed
to assess generated films across the defined dimensions. To
ensure reliability, we validate the effectiveness of automatic
evaluation by measuring its correlation with human judg-
ments.
Test Dataset. Our evaluation employs a diverse set of 20
test cases comprising two distinct prompt types: 10 cases
from MoviePrompts [40], which feature extensive and de-
tailed descriptions, with an average of 100.4 words; and
10 shorter, more concise prompts with an average of 15.2
words, specifically designed by annotators to evaluate our
method’s flexibility in handling varied input complexities.
Comparing Models. We compare our method against
previous works on automatic film generation: animation
generation method (Anim-Director [19]), movie genera-
tion method (MovieAgent [40]), and a commercial product
(LTX-Studio [21]). Since LTX-Studio supports automatic
SFX, we apply the same to ensure fair comparison.
4.2. Quantitative Results
Automatic Evaluation. The results are shown in Table 2,
with an average improvement of 58.06% in our FilMas-
ter: 43.00% in camera language and 77.53% in cinematic
rhythm, respectively. Our analysis reveals that existing
approaches like Anim-Director [19] and MovieAgent [40]
significantly underperform across multiple dimensions in-
cluding NS, AE, RF, EE, and OE. These methods demon-
strate particularly severe deficiencies in audio quality and
video-audio coordination. In contrast, our proposed method
achieves substantial improvements across all evaluation di-
mensions in FilmEval, with an average performance in-
creases of 75% and 69% compared to Anim-Director and
MovieAgent, respectively. When compared with the com-
mercial product LTX-Studio, we observe that LTX-Studio
struggles with script faithfulness, narrative coherence, nar-
rative pacing, and audio quality, likely due to insufficient
integration of camera language and audiovisual elements.
Our approach outperforms LTX-Studio with an average im-
provement of 19.84%, demonstrating the effectiveness of
our film generation system.
User Study. In addition to quantitative analysis, we con-
duct a user study to evaluate the quality of generated films.
Five participants were asked to rate each video indepen-
dently based on the criteria defined in FilmEval. We ran-
domly selected five cases from our dataset, compared our
FilMaster with the other three methods. In total, we col-
lected 1,200 ratings, with 100 votes per evaluation crite-
7

Table 2. Automatic evaluation of film production generation across 12 evaluation criteria on FilmEval, and 2 derived evaluation criteria
for camera language (CL) and cinematic rhythm (CRh).*denotes commercial product. Blue represents the derived metric for CL, and
green stands for CRh, applicable to the following Table 3.
MethodNS↑ AT↑ AE↑ RF↑ EE↑OE↑ Derived ↑Avg↑
SF NC VQ CC PLC V/AQ CT A VR NP V AC CD OQ CL CRh
Anim-Director 1.60 2.20 4.20 3.45 3.55 1.00 3.05 2.50 2.10 1.00 2.45 2.30 2.96 1.94 2.45
MovieAgent 1.50 1.60 4.10 3.40 3.40 1.00 2.70 2.20 1.60 1.00 2.20 2.20 2.74 1.74 2.24
LTX-Studio*2.50 3.00 4.95 4.10 3.90 3.10 4.10 3.85 3.15 4.10 3.65 3.75 3.74 3.62 3.68
Ours 3.90 4.60 5.00 5.00 4.40 3.80 4.10 4.10 4.40 5.00 4.20 4.40 4.50 4.32 4.41
Derived metrics (CL, CRh) are computed from preceding base metrics: CL is calculated as the average of (SF, NC, VQ, CC, PLC) + 0.5 * average of (CT,
A VR), CRh is calculated as the average of (V/AQ, NP, V AC, CD, OQ) + 0.5 * average of (CT, A VR).
ria. We show the six dimensions in Table 3, and detailed
results in Table 6. The results show that our FilMaster
demonstrates superior performance in film generation com-
pared to the prior methods, with an average increase of
68.44% (70.65% in camera language, and 65.61% in cin-
ematic rhythm).
Table 3. User study on film generation methods.
Method NS ↑AT↑AE↑RF↑EE↑OE↑CL↑CRh↑Avg↑
Anim-Director 1.94 2.16 1.94 2.12 2.12 2.36 2.15 2.04 2.09
MovieAgent 1.57 1.63 1.70 1.70 2.20 2.27 1.66 1.83 1.74
LTX-Studio*2.28 3.04 3.22 2.90 3.07 3.00 2.80 3.05 2.92
Ours 3.70 3.80 3.80 3.73 3.93 3.87 3.76 3.82 3.79
Human Correlation. To validate our proposed auto-
matic evaluation metrics, we measured their correlation
with human judgments using Pearson’s r, Kendall’s τand
Spearmanr’s ρin Table 4, similar to the methods in [13, 28].
The automatic metrics exhibited an average correlation of
0.6230 with the user study results, indicating a strong align-
ment with human assessments.
Table 4. Human correlation coefficients of the average across 12
evaluation criteria in terms of Pearson Correlation r, Spearman’s
ρ, and Kendall’s τCoefficient ( p-value < 0.01).
Corrrelation Pearson rSpearman ρKendall τ
Avg 0.6464 0.6493 0.5732
4.3. Qualitative Results
Example. As shown in Figure 4, our method generates de-
scriptions with camera language and designs a multi-track
audio based on the text prompt derived from the input text,
forming a cohesive audiovisual narrative after camera lan-
guage design and cinematic rhythm control. Additional ex-
amples are provided in Figure 6.
Comparison. The comparison is illustrated in Fig-
ure 5. Among all compared methods, our approach gen-
erates results with character consistency, fluid motion, and
coherent narrative structure. In contrast, existing meth-Table 5. Ablation study on the effects of Camera Language Design
Module and Cinematic Rhythm Control Module on overall perfor-
mance (average across 12 criteria).
Method w/o Camera + Rhythm w/o Rhythm Ours
Avg↑ 3.75 4.17 4.67
ods exhibit various limitations in visual quality, audio de-
sign, and narrative coherence: Visually, Anim-Director pro-
duces static animations that lack natural motion transitions.
MovieAgent faces challenges with character consistency
throughout the video. LTX-Studio, while achieving good
visual quality, struggles with maintaining character identity
preservation across frames. Regarding audio and narrative
elements, significant limitations exist across all compared
methods. Anim-Director lacks audio altogether, severely
restricting its storytelling capabilities. MovieAgent only
implements basic voice-over without diverse audio design.
LTX-Studio relies on automatic audio design without fine-
grained control, resulting in desynchronization between vi-
sual and audio components. Furthermore, LTX-Studio’s
narrative pacing is often slow and repetitive.
4.4. Ablation Study
We ablate our method without Multi-shot Synergized RAG
Camera Language Design module and Audience-Centric
Cinematic Rhythm Control module in Table 5 on one
case. Quantitative results show that removing cinematic
rhythm module significantly decreases the average score in
FilmEval, highlighting its role in cinematic expression with
similar generated content. We also examined the impact
of Multi-shot Synergized RAG Camera Language Design
module, which helps to form a coherent generated content.
5. Conclusion
We introduced FilMaster, the first comprehensive AI-based
film generation system designed for professional-grade film
generation. FilMaster uniquely integrates cinematic prin-
8

Input text: In their space journey, the blond Little Prince and the white fox met a sad rose, so the Prince gave it a star while the fox imitated the rose shaking its head to make it laugh.
Input images: Detailed process example of video clip (1): 1.Scene structuring with spatio-temporal contexts. Prompt: In the vastness of space, the Little Prince stands on a tiny asteroid, his eyes wide, marveling at the universe's endless expanse around him. Stars twinkle in the distance, casting a gentle glow over the asteroid's rocky surface…;  Time: Beginning of the journey; Scene: Asteroid; Characters: Little Prince, White Fox; Visual Elements: Stars, cosmicwinds; Objective: Emphasize the insignificance of the Little Prince and the White Fox against the backdrop of the universe.2.RAG for Camera design. Prompt: Morning, asteroid, Little Prince and White Fox standing quietly with wide eyes, stars twinkling. The camera begins with a wide, sweeping shot of the vastness of space, slowly zooming in on a small, rocky asteroid where the Little Prince stands. The dark expanse is dotted with innumerable stars, their gentle twinkle creating a luminous backdrop. As the camera draws closer, it circles around, capturing the Little Prince's wide-eyed wonder, emphasizing his insignificance in the cosmic vastness.3.Input imagefor video gen: abd; 4. Audience review (for all video clips). 5. Editing Process. Structure: put at the start. Duration: accelerate to 3s (The initial scene can be sped up to quickly establish setting and characters, creating an immediate visual impact). 6. Audio design as below:
abcdefgh
(1)(2)(3)(4)(5)
(6)(7)(8)(9)(10)
(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)"Foley": "None","Soundeffect": "Gentle whooshing sound of space, distant twinkling sounds of stars, a light and airy magical sound effect."“VO”: On a lonely asteroid, the vastness of space embraces the Little Prince and his loyal fox.""Background": "A soft, celestial hum fills the air, with distant, twinkling chimes representing the stars as they shimmer against the dark void. The space feels vast and echoing, capturing the boundless universe.","Music": "A gentle, contemplative melody underscores the scene, composed of soft strings and airy woodwinds, emphasizing the wonder and awe of staring into the expansive depths of space."5 Audio Tracks: Figure 4. The input and corresponding video frame extractions from our FilMaster, illustrated alongside associated multi-track audio. The
detailed process of the video clip (1) is illustrated in gray area.
Anim-DirectorMovieAgentLTX-StudioOurs
Figure 5. Qualitative comparison of film production generation in the case of “little prince”. Anim-Director, MovieAgent, and LTX-Studio
adopt a text-to-image (T2I) followed by image-to-video (I2V) pipeline, where the input is script text only, without any images.
ciples, focusing on camera language design and cine-
matic rhythm control, while ensuring industry-compatible,
editable outputs. We propose a Multi-shot Synergized
RAG Camera Language Design module that learns cine-
matography directly from a vast corpus of 440,000 real
film clips. By leveraging Retrieval-Augmented Gener-
ation (RAG), this module produces expressive, context-
aware camera plans with high cinematic coherence. our
Audience-Centric Cinematic Rhythm Control module em-
ulates professional post-production. This includes Rough
Cutassembly, a Fine Cut process refined by simulated au-dience feedback, including video editing and sound de-
sign, all orchestrated to achieve compelling narrative flow
and profound emotional impact. Furthermore, we proposed
FilmEval, a comprehensive benchmark for assessing AI-
generated films across six key cinematic dimensions. Ex-
tensive experiments demonstrate FilMaster’s state-of-the-
art performance, with an average improvement of 68.44% in
user studies and 58.06% in automatic evaluation compared
to prior methods, showcasing significant advancements in
generating films with expressive visual language and engag-
ing rhythm.
9

References
[1] Daniel Arijon. Grammar of the film language. (No Title) ,
1976. 2
[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22563–22575, 2023. 3
[3] David Bordwell, Kristin Thompson, and Jeff Smith. Film
art: An introduction . McGraw-Hill New York, 2004. 2
[4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,
Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-
man, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya
Ramesh. Video generation models as world simulators.
2024. 3
[5] Dominic Case. Film technology in post production . Rout-
ledge, 2013. 2, 5
[6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T
Freeman. Maskgit: Masked generative image transformer.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11315–11325, 2022.
3
[7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-
phy, William T Freeman, Michael Rubinstein, et al. Muse:
Text-to-image generation via masked generative transform-
ers.arXiv preprint arXiv:2301.00704 , 2023. 3
[8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jin-
liu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and
Haofen Wang. Retrieval-augmented generation for large lan-
guage models: A survey. arXiv preprint arXiv:2312.10997 ,
2(1), 2023. 5
[9] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Qifeng Chen. Latent video diffusion models for high-fidelity
long video generation. arXiv preprint arXiv:2211.13221 ,
2022. 3
[10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 3
[11] Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng,
Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang,
Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu
Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
Metagpt: Meta programming for a multi-agent collaborative
framework, 2024. 4
[12] Eve Light Honthaner. The complete film production hand-
book . Routledge, 2013. 2
[13] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and
Xihui Liu. T2i-compbench: A comprehensive bench-
mark for open-world compositional text-to-image genera-
tion. Advances in Neural Information Processing Systems ,
36:78723–78747, 2023. 8
[14] Kaiyi Huang, Yukun Huang, Xuefei Ning, Zinan Lin, Yu
Wang, and Xihui Liu. Genmac: Compositional text-to-videogeneration with multi-agent collaboration. arXiv preprint
arXiv:2412.04440 , 2024. 4
[15] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan, Roberto Henschel, Zhangyang Wang, Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-
to-image diffusion models are zero-shot video generators. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 15954–15964, 2023. 3
[16] Kling. https://kling.kuaishou.com/ , 2025. Or-
ganization: Kuaishou. 7
[17] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama,
Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan
Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A
large language model for zero-shot video generation. arXiv
preprint arXiv:2312.14125 , 2023. 3
[18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo
Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jian-
wei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong
Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan,
Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jin-
bao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang
Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu,
Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui,
Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang
Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu,
Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu,
Jie Jiang, and Caesar Zhong. Hunyuanvideo: A systematic
framework for large video generative models, 2024. 2, 3
[19] Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Ji-
ashun Zhu, Jinyi Xu, Zhen Zhao, and Min Zhang. Anim-
director: A large multimodal model powered agent for con-
trollable animation video generation. In SIGGRAPH Asia
2024 Conference Papers , pages 1–11, 2024. 2, 3, 4, 7
[20] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal.
Videodirectorgpt: Consistent multi-scene video generation
via llm-guided planning. In COLM , 2024. 4
[21] LTX Studio. LTX Studio. https://app.ltx.
studio/ , 2024. Accessed: 2025-04. Organization:
Lightricks. 2, 3, 7
[22] Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li,
Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo,
and Rongrong Ji. Video-rag: Visually-aligned retrieval-
augmented long video comprehension. arXiv preprint
arXiv:2411.13093 , 2024. 5
[23] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,
Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and
Tieniu Tan. Videofusion: Decomposed diffusion mod-
els for high-quality video generation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10209–10218, 2023. 3
[24] Joseph V Mascelli. The five C’s of cinematography . Grafic
Publications Hollywood, 1965. 2
[25] Walter Murch. In the Blink of an Eye . Silman-James Press
Los Angeles, 2001. 2
[26] OpenAI. Hello GPT-4o. https://openai.com/
index/hello-gpt-4o/ , 2024. Accessed: 2024-11-14.
2, 7
10

[27] OpenTimelineIO. Opentimelineio: An open-source frame-
work for interchange of editorial timeline data. https:
//opentimelineio.readthedocs.io/ , 2023. Ac-
cessed: 2025-05-09. 3
[28] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell,
and Anna Rohrbach. Benchmark for compositional text-to-
image synthesis. In NeurIPS , 2021. 8
[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S. Bernstein.
Generative agents: Interactive simulacra of human behavior,
2023. 4
[30] Michael Rabiger. Directing: Film techniques and aesthetics .
Routledge, 2013. 2
[31] Szilvia Ruszev. Rhythmic trajectories–visualizing cinematic
rhythm in film sequences. Women Cutting Movies: Editors
from East and Central Europe. Special Issue of Apparatus.
Film Media and Digital Cultures of Central and Eastern Eu-
rope, 7, 2018. 2
[32] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792 ,
2022. 3
[33] Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng
Qiu, and Lingpeng Kong. Corex: Pushing the boundaries of
complex reasoning through multi-model collaboration, 2024.
4
[34] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-
Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,
Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a
family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805 , 2023. 2, 7
[35] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-
dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi
Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.
Phenaki: Variable length video generation from open domain
textual descriptions. In International Conference on Learn-
ing Representations , 2022. 3
[36] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei
Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang,
Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou,
Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan,
Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng
Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang,
Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu
Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wen-
meng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xi-
anzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu
Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yi-
tong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun
Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi
Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open
and advanced large-scale video generative models. arXiv
preprint arXiv:2503.20314 , 2025. 3
[37] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,
Xiang Wang, and Shiwei Zhang. Modelscope text-to-video
technical report. arXiv preprint arXiv:2308.06571 , 2023. 3[38] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,
Denny Zhou, Donald Metzler, et al. Emergent abilities of
large language models. arXiv preprint arXiv:2206.07682 ,
2022. 4
[39] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin
Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang,
Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug
Burger, and Chi Wang. Autogen: Enabling next-gen llm ap-
plications via multi-agent conversation, 2023. 4
[40] Weijia Wu, Zeyu Zhu, and Mike Zheng Shou. Auto-
mated movie generation via multi-agent cot planning. arXiv
preprint arXiv:2503.07314 , 2025. 2, 3, 4, 7
[41] Zhenran Xu, Longyue Wang, Jifang Wang, Zhouyi Li, Sen-
bao Shi, Xue Yang, Yiyu Wang, Baotian Hu, Jun Yu, and
Min Zhang. Filmagent: A multi-agent framework for end-
to-end film automation in virtual 3d spaces. arXiv preprint
arXiv:2501.12909 , 2025. 4
[42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu
Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-
han Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video
diffusion models with an expert transformer. arXiv preprint
arXiv:2408.06072 , 2024. 2, 3
[43] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han
Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-
Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit:
Masked generative video transformer. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10459–10469, 2023. 3
[44] Lijun Yu, José Lezama, Nitesh B Gundavarapu, Luca Ver-
sari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim
Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language
model beats diffusion–tokenizer is key to visual generation.
arXiv preprint arXiv:2310.05737 , 2023. 3
[45] Zhengqing Yuan, Yixin Liu, Yihan Cao, Weixiang Sun, Hao-
long Jia, Ruoxi Chen, Zhaoxu Li, Bin Lin, Li Yuan, Lifang
He, Chi Wang, Yanfang Ye, and Lichao Sun. Mora: En-
abling generalist video generation via a multi-agent frame-
work, 2024. 4
[46] Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng
Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao
Kong, Nix Liu Xin, Shanshan Jiang, et al. Generative ai for
film creation: A survey of recent advances. arXiv preprint
arXiv:2504.08296 , 2025. 2
[47] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He,
Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng,
Yu Qiao, et al. Vbench-2.0: Advancing video generation
benchmark suite for intrinsic faithfulness. arXiv preprint
arXiv:2503.21755 , 2025. 3
[48] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models. arXiv preprint
arXiv:2211.11018 , 2022. 3
[49] Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi
Hu, Jiaqi Liao, Zhewei Huang, Hongyuan Wang, Xinyao
Liao, Weiwei Cai, Hengyuan Xu, Xuanyang Zhang, Xian-
fang Zeng, Gang Yu, and Chi Zhang. ViStoryBench: A Com-
11

prehensive Benchmark Suite for Story Visualization. arXiv
preprint arXiv:2505.24862 , 2025. 3
12

A. Scene Structuring
Given an input theme sentence, the script is hierarchically
expanded. This hierarchical approach ensures coherent pro-
gression from abstract concepts to concrete visual descrip-
tions that can be translated into audiovisual content. The
scene structuring unfolds in four progressive phases, each
building upon the output of the previous phase, imple-
mented through a chain of four specialized LLMs working
sequentially:
•Synopsis: The input theme is expanded into a synopsis
that introduces settings, characters, and plots, forming the
foundation for subsequent development. This phase es-
tablishes the core narrative framework without detailed
visual considerations.
•Simplified Storyboard: The synopsis is enriched with
background information, causal chains, narrative devel-
opments, and outcomes, ensuring the story has appropri-
ate twists while maintaining narrative coherence and logi-
cal consistency. This phase transforms narrative concepts
into structured story events.
•Detailed Storyboard: The simplified version is further
elaborated into a detailed storyboard containing concrete
visual narratives. This includes both primary shots (fea-
turing main character actions with specific descriptions)
and contextual shots (showing surrounding environments
and establishing shots), alongside metaphorical imagery
to enhance emotional resonance and thematic depth. This
phase translates story events into visual sequences.
•Scene Block: The detailed storyboard is segmented into
distinct scene blocks that function as spatio-temporal con-
texts within one scene ( i.e., events occurring in the same
location during the same timeframe), based on chrono-
logical order and spatial continuity. These blocks serve
as fundamental units for subsequent audiovisual gener-
ation, ensuring consistent visual and narrative treatment
across related shots. For each block, we extract key ele-
ments including prompt, time, location, characters, visual
elements, and narrative objectives (justifying each scene’s
narrative necessity for plot advancement or character de-
velopment). These extracted elements collectively pro-
vide structured input for the Scene-level Multi-shot Cam-
era Design module. Besides, we use LLM to add rough
sound design by text description for each scene block,
used for audience-centric review and scene-level sound
design (background ambiance and music scoring) in Gen-
erative Post-Production Stage.
B. Sound Design
Directly relying on MLLMs to design multi-track audio (in-
cluding background ambience, musical scoring, voice-over
(VO), foley, and sound effects) often results in poor syn-
chronization with video content. This is primarily due to theLLM’s limited capability in managing coordination across
multiple audio layers and temporal alignment. To address
this, we propose a multi-scale audio-visual synchronization
strategy, which systematically designs different audio types
at appropriate temporal resolutions. This process is further
supported by retrieval-augmented generation (RAG) from
curated audio libraries or synthesized voice content, fol-
lowed by systematic audio preprocessing.
Specifically, synchronization is designed across three
temporal scales:
• Scene-level synchronization: Background ambience and
musical scoring are assigned at the scene level to estab-
lish atmosphere and emotional tone. This builds upon the
rough sound design from Scene Structuring.
• Shot-level synchronization: VO, including narration and
dialogue, is designed at the shot level to maintain close
alignment with visual content and enhance narrative clar-
ity.
• Intra-shot synchronization: Foley and sound effects are
assigned at a fine-grained temporal resolution. MLLM
analyzes video content with second-level timecodes to de-
tect specific actions, movements, and environmental ele-
ments that require sonic representation, ensuring detailed
audiovisual coordination.
To ensure audio quality and contextual relevance, we
employ the RAG approach for non-voice audio tracks. We
construct a curated audio library comprising 46,826 sound
assets, including 5,877 music tracks and 40,949 other audio
assets (atmospheric sounds, foley, and effects) sourced from
the Internet. Each asset is tagged with semantic descrip-
tors, emotional qualities, and acoustic properties to facili-
tate context-aware retrieval. For voice-over content, we uti-
lize ElevenLabs’ text-to-speech services, allowing for flex-
ible and high-fidelity voice generation tailored to specific
narrative demands.
Composing multiple audio tracks directly often leads
to inconsistencies in loudness, frequency balance, and dy-
namic range. To mitigate these issues, we apply audio
preprocessing techniques that ensure cross-track and track-
video harmonization. This includes:
• Loudness units relative to full scale (LUFS) normal-
ization to maintain consistent perceived loudness across
tracks ( e.g. -16 LUFS for voice content, -28 LUFS for
background elements).
• Frequency adjustment to avoid spectral clashes between
different types of audio, particularly ensuring voice intel-
ligibility by attenuating competing frequencies in back-
ground tracks.
• Dynamic equalization to enhance clarity and cohesion.
C. FilmEval Evaluation Instructions
We show the detailed criteria of FilmEval for both auto-
matic evaluation and user study as follows:
13

NARRATIVE AND SCRIPT
Script Faithfulness
1 point: Severely deviates from the original script; scenes and character settings completely inconsistent with the
original.
2 points: Partially follows the original script, but with obvious deviations; multiple key settings changed.
3 points: Generally follows the original script, preserving main scenes and settings, but with details omitted.
4 points: Highly faithful to the original script, accurately reproduces most scenes and settings with rich details.
5 points: Completely faithful to the original script, precisely presents all scenes, settings, and details.
Narrative Coherence
1 point: Chaotic and disorderly story with serious logical contradictions and plot discontinuities.
2 points: Story is basically understandable but contains multiple obvious logical gaps and coherence issues.
3 points: Story is generally coherent with minor logical deficiencies that don’t affect understanding of the main plot.
4 points: Story flows smoothly and coherently with reasonable plot development and almost no obvious logical
issues.
5 points: Story is completely coherent with natural and reasonable plot development, clear cause-effect relationships,
and no logical holes.
AUDIO-VISUALS AND TECHNIQUES
Visual Quality
1 point: Severely broken visuals with numerous missing or distorted visual elements.
2 points: Obvious visual flaws with some scenes showing missing or distorted elements.
3 points: Basically complete visuals with occasional minor errors that don’t affect overall viewing.
4 points: Clear and complete visuals with very few minor imperfections.
5 points: Flawless visuals with all elements perfectly rendered and no visual breakdowns.
Character Consistency (visual)
1 point: Severely inconsistent character designs with dramatic appearance changes of the same character across
different scenes.
2 points: Noticeable fluctuations in character designs with character features changing in some scenes.
3 points: Generally consistent character designs with occasional minor inconsistencies that aren’t obvious.
4 points: Highly consistent character designs that maintain stable features across different scenes and angles.
5 points: Perfectly consistent character designs that maintain precise character features in all scenes and actions.
Physical Law Compliance
1 point: Severely violates physical laws with extremely unnatural movements, collisions, and effects.
2 points: Multiple violations of basic physical laws with obviously unrealistic movements and effects.
3 points: Generally complies with physical laws; some movements or effects appear slightly artificial but acceptable.
4 points: Good compliance with physical laws, natural movements, and believable effects.
5 points: Perfect compliance with physical laws where all movements, collisions, and effects are extremely realistic.
Voice/Audio Quality
1 point: Extremely poor audio quality with unclear voiceovers and chaotic or missing sound effects.
2 points: Poor audio quality with partially unclear voiceovers and simple or inadequate sound effects.
3 points: Average audio quality with basically clear voiceovers and appropriate but not outstanding sound effects.
4 points: Good audio quality with clear voiceovers and rich sound effects that match the scenes.
5 points: Excellent audio quality with extremely clear and vivid voiceovers, and rich, nuanced sound effects with
great expressiveness.
14

AESTHETICS AND EXPRESSION
Cinematic Techniques
1 point: Single, stiff shots with no variation and lack of basic film language.
2 points: Limited shot variation, stiff camera movements, and poor film language.
3 points: Uses common shot techniques with basically smooth camera movements and basic film language expres-
sion.
4 points: Rich film language, smooth and natural camera movements, reasonable and effective shot variations.
5 points: Highly creative shot usage, precise camera movements, and rich shot variations with exceptional expres-
siveness.
Audio-Visual Richness
1 point: Extremely limited visual and auditory expression. The film uses monotonous or repetitive visual/audio
elements with minimal variation or artistic layering.
2 points: Some attempts at audiovisual expression, but overall lacks creativity. Techniques are basic and formulaic,
offering little dynamic or stylistic variation.
3 points: Moderate diversity in audiovisual methods. Certain scenes explore varied styles or tempos, but the richness
is uneven and lacks coherence or artistic depth.
4 points: Visually and sonically expressive. Multiple techniques are used effectively to create layered meaning,
mood shifts, or stylistic nuances across the film.
5 points: Exceptionally rich audiovisual language. Diverse, inventive, and highly expressive use of both sound and
visuals creates a compelling and distinctive artistic voice with strong narrative or emotional impact.
RHYTHM AND FLOW
Narrative Pacing
1 point: Completely uncontrolled pacing, either too fast or too slow, severely affecting story comprehension.
2 points: Obviously inconsistent pacing with some plot developments that are too quick or too slow.
3 points: Generally appropriate pacing with reasonable progression of main plot elements.
4 points: Well-controlled pacing with natural plot progression and good balance between tension and relief.
5 points: Precisely controlled pacing that both serves the story needs and captures audience emotions, with perfect
balance.
Video-Audio Coordination
1 point: Severely unsynchronized audio and video with completely mismatched lip-syncing.
2 points: Obvious lack of synchronization between audio and video with poor coordination between voice and
visuals.
3 points: Basically synchronized audio and video with occasional inconsistencies that don’t obviously interfere with
viewing.
4 points: Good audio-video coordination with highly matched sound and visual actions.
5 points: Perfect synchronization where all sound elements precisely match visual actions, creating a harmonious
viewing experience.
EMOTIONAL AND ENGAGEMENT
Compelling Degree
1 point: No appeal whatsoever; difficult for viewers to feel immersed or emotionally connected.
2 points: Insufficient appeal with weak emotional rendering; difficult to maintain viewer attention.
3 points: Basic appeal that can generate viewer interest but insufficient to create profound emotional resonance.
4 points: Strong appeal with effective emotional rendering that can evoke obvious emotional resonance from viewers.
5 points: Extremely compelling with powerful emotional tension that fully engages viewers throughout and creates
strong emotional resonance.
15

OVERALL EXPERIENCE
Overall Quality
1 point: Extremely poor quality with multiple dimensions severely below standard; completely lacks viewing value.
2 points: Poor quality with main dimensions performing badly; limited viewing value.
3 points: Average quality with average performance across dimensions; has basic viewing value.
4 points: Good quality with good performance across dimensions that work well together; has high viewing value.
5 points: Excellent quality with outstanding performance across all dimensions and perfect coordination; has ex-
tremely high artistic and viewing value.
D. Qualitative Results
We show the qualitative results in Figure 6. Our method can
generate high-quality films with both simple and complex
script input.
E. Quantitative Results
We show the detailed results of user study in Table 6, and
detailed human correlation in twelve criteria in FilmEval in
Table 7. Our method shows superior performances com-
pared with other existing film generation systems. Our pro-
posed automatic evaluation shows high correlation with hu-
man judgement over 12 evaluation criteria.
F. Limitations
While FilMaster represents a significant step towards auto-
mated professional film generation, it currently has certain
limitations. For instance, advanced post-production tech-
niques such as color grading and a diverse range of cine-
matic transitions are not yet incorporated into our system.
These aspects, crucial for achieving a fully polished cine-
matic look and feel, were beyond the primary scope of this
work, which focused on foundational camera language de-
sign and cinematic rhythm control. We acknowledge their
importance and plan to address their integration in future
research.
16

Input text: As Riley enters her teenage years, her mind’s Headquarters undergoes a sudden transformation, welcoming new Emotions. Anxiety takes charge, believing that constant worry will help Riley navigate adolescence, while Envy fuels self-doubt. Joy, Sadness, Anger, Fear, and Disgust struggle to adapt as Anxiety’s influence grows, sidelining their roles. As Riley faces mounting pressure, the imbalance leads to emotional turmoil. Realizing that Anxiety alonecannot define Riley’s experience, the original Emotions work to restore balance. In the end, Riley embraces the complexities of growing up, with both old and new Emotions learning to coexist.
Input images: RileySadnessJoyAnxietyBedroomDining roomStudy roomHeadquarterSchool
Input text: Nemo tried to play hide-and-seek but swam straight into an octopus’s hat, getting inked into a little black fish.
Input images: Inked NemoOctopusOthersea creatures: dolphin, crabSeaNemoFigure 6. Qualitative results of our FilMaster. Our FilMaster can generate high-quality films with either simple or complex script input.
17

Table 6. Quantitative comparison of different methods across 12 evaluation criteria. The “avg” column shows the average score for each
method. Values in bold indicate the best performance in that column.
MethodNS↑ AT↑ AE↑ RF↑ EE↑OE↑Avg↑
SF NC VQ CC PLC V/AQ CT A VR NP V AC CD OQ
Anim-Director 2.08 1.80 2.52 2.24 2.32 1.56 2.20 1.68 2.64 1.60 2.12 2.36 2.09
MovieAgent 1.73 1.40 1.67 1.73 1.73 1.40 1.93 1.47 2.00 1.40 2.20 2.27 1.74
LTX-Studio*2.44 2.12 3.16 3.00 2.84 3.16 3.52 2.92 2.60 3.20 3.16 2.96 2.92
Ours 3.73 3.67 3.87 3.93 3.53 3.87 3.73 3.87 3.93 3.53 3.93 3.87 3.79
Table 7. Quantitative results of correlation coefficients across 12 evaluation points and their average in terms of Pearson Correlation
r, Spearman’s ρ, and Kendall’s τCoefficient ( p-value < 0.01). Green represents the average of each correlation coefficient across 12
evaluation dimension.
CorrelationNS AT AE RF EE OE
SF NC VQ CC PLC V/AQ CT A VR NP V AC CD OQ Avg
r(↑) 0.5818 0.6734 0.6731 0.7114 0.4664 0.7892 0.6301 0.5749 0.6129 0.7964 0.5717 0.6758 0.6464
ρ(↑) 0.5842 0.6676 0.6756 0.7184 0.4328 0.8036 0.6565 0.5867 0.6349 0.7815 0.5673 0.6827 0.6493
τ(↑) 0.5056 0.5864 0.6249 0.6474 0.4003 0.7093 0.5662 0.4928 0.5569 0.6973 0.4893 0.6023 0.5732
18