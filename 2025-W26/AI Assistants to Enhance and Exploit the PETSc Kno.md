# AI Assistants to Enhance and Exploit the PETSc Knowledge Base

**Authors**: Barry Smith, Junchao Zhang, Hong Zhang, Lois Curfman McInnes, Murat Keceli, Archit Vasan, Satish Balay, Toby Isaac, Le Chen, Venkatram Vishwanath

**Published**: 2025-06-25 17:00:05

**PDF URL**: [http://arxiv.org/pdf/2506.20608v1](http://arxiv.org/pdf/2506.20608v1)

## Abstract
Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

## Full Text


<!-- PDF content starts -->

arXiv:2506.20608v1  [cs.AI]  25 Jun 2025AI Assistants to Enhance and Exploit
the PETSc Knowledge Base
Barry Smith
Flatiron Institute
Simons Foundation
New York, NY , USA
bsmith@petsc.devJunchao Zhang*, Hong Zhang*, Lois Curfman McInnes*, Murat Kec ¸eli†
Archit Vasan‡, Satish Balay*, Toby Isaac*, Le Chen‡, and Venkatram Vishwanath§
*MCS Division ,†CPS Division ,‡DSL Division ,§ALCF Division
Argonne National Laboratory , Lemont, IL, USA
{jczhang, hongzhang, curfman, keceli, avasan, balay, tisaac, le.chen, venkat }@anl.gov
Abstract —Generative AI, especially through large language
models (LLMs), is transforming how technical knowledge can
be accessed, reused, and extended. PETSc, a widely used nu-
merical library for high-performance scientific computing, has
accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation,
mailing lists, GitLab issues, Discord conversations, technical
papers, and more. Much of this knowledge remains informal
and inaccessible to users and new developers. To activate and
utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc
content with custom LLM tools—including retrieval-augmented
generation (RAG), reranking algorithms, and chatbots—to as-
sist users, support developers, and propose updates to formal
documentation. This paper presents initial experiences designing
and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation
methodologies for various LLMs and embedding models, and
user interface design. Leveraging the Argonne Leadership Com-
puting Facility resources, we analyze how LLM responses can
enhance the development and use of numerical software, with an
initial focus on scalable Krylov solvers. Our goal is to establish
an extensible framework for knowledge-centered AI in scientific
software, enabling scalable support, enriched documentation, and
enhanced workflows for research and development. We conclude
by outlining directions for expanding this system into a robust,
evolving platform that advances software ecosystems to accelerate
scientific discovery.
Index Terms —PETSc, AI, LLM, RAG, knowledge base
I. I NTRODUCTION
High-performance scientific computing—which underpins
discovery and innovation in all areas of science, engineering,
technology, and society—relies on complex, evolving soft-
ware libraries and tools that demand deep domain knowledge
to develop, use, and maintain across continual advances in
science drivers and computing architectures. Generative AI
provides unprecedented opportunities to transform strategies
This material is based upon work supported by Laboratory Directed Re-
search and Development (LDRD) funding from Argonne National Laboratory,
provided by the Director, Office of Science, of the U.S. Department of Energy
(DOE) under Contract No. DE-AC02-06CH11357. Research was partially
supported by the U.S. DOE Office of Science Distinguished Scientist Fellows
Program. This research used resources of the Argonne Leadership Computing
Facility, a U.S. Department of Energy (DOE) Office of Science user facility at
Argonne National Laboratory, and is based on research supported by the U.S.
DOE Office of Science-Advanced Scientific Computing Research Program,
under Contract No. DE-AC02-06CH11357.for research and development (R&D) of scientific software,
with the ultimate goal of accelerating scientific insight [1]–
[3]. PETSc—the Portable, Extensible Toolkit for Scientific
Computation—has served the scientific computing community
for over three decades as a foundational software infrastructure
for scalable numerical solvers [4]–[6]. During this time, the
project has cultivated a sophisticated and robust code base and
a deep, distributed body of developer and user knowledge.
The community knowledge base includes source code and
documentation, email exchanges in public mailing lists, Git-
Lab issues, git merge request reviews, Discord chats, manual
pages and examples from the PETSc website, written materials
such as papers and presentations, and importantly, PETSc
developers and users’ understanding of PETSc. Among the
written material, the structured and curated subset constitutes
theofficial knowledge base: documentation, code, frequently
asked questions (FAQs), and other resources that have un-
dergone refinement and review, essentially everything that
has passed a git merge request, while the remaining is the
unofficial knowledge base. We call the subset of community
knowledge that is available only in a developer’s brain (i.e.,
not written in a machine-accessible format) the wetknowledge
base1, and the opposite the dryknowledge base, see Fig. 1.
Unofﬁcial knowledge base
(emails,  Gitlab issues,  chats,  ...)  
Ofﬁcial knowledge base
(docs, code, F AQs,  papers, books, ...)
Wet knowledge base  
(unwritten,  not machine-accessible )Dry knowledge bases
(written, machine-accessible)
Fig. 1: PETSc community knowledge base. AI assistants could
help the flow of knowledge in all directions.
Historically, the PETSc knowledge base has grown through
the daily work of developers and the continuous stream of
user interactions. Because this content is implicit—captured
1https://en.wikipedia.org/wiki/Wetware (brain)

in natural language, scattered across platforms, and often
inaccessible without deep familiarity—this knowledge is only
partially utilized. Developers may recall past issues vaguely,
struggle to search 20-year-old mailing list archives, or repeat-
edly answer similar questions without updating the documen-
tation. Similarly, users of PETSc may encounter steep learning
curves despite abundant (but fragmented) resources, since any
PETSc-related information and data can serve as a source of
knowledge for developers and users.
Recent advances in generative AI, particularly large lan-
guage models (LLMs), open a new chapter in leveraging
knowledge bases. With LLMs’ ability to retrieve, synthesize,
and contextualize natural language content, we can now utilize
knowledge bases in transformative ways. In this framing,
knowledge bases can be considered a reservoir, and LLM
components—retrieval-augmented generation (RAG), rerank-
ing algorithms, Discord-integrated bots, and more—act as the
pipes, filters, and faucets that deliver timely, accurate, and
contextualized information to users and developers.
These tools do not replace human contributors but act
as scalable collaborators—suggesting code, summarizing dis-
cussions, generating documentation updates, and responding
to user questions while adhering to PETSc’s collaborative
development process. These same tools and techniques can
and will be used to transfer material from the unofficial
knowledge base to the official knowledge base, for example,
dramatically improving manual pages with little developer
effort. Developer-user (as well as developer-developer) com-
munication is a common way that information is transferred
from the wet knowledge base to machine-accessible format.
Questions provoke developers to write down (often informally)
knowledge and conceptualizations that they have never written
down before. By having the LLM process these developer-user
interactions, we can effectively capture wet knowledge as the
developers perform their daily tasks without requiring them to
try to core-dump their wet knowledge actively.
This paper presents initial efforts to develop LLM-based
tools that enhance and exploit the PETSc knowledge base.
•To enhance the PETSc knowledge base, we are im-
proving the flow of knowledge (from wet to dry and
from unofficial to official)—resulting in more information
about effectively using PETSc being vetted and readily
accessible, both directly to users and also to LLMs.
•To exploit the PETSc knowledge base, we are creating
AI assistants that serve as knowledgeable, tireless part-
ners in the daily workflows of PETSc R&D and user
support—that is, a system where AI tools are dynamic
connectors between broad technical content and diverse
needs of users and developers.
Fig. 2 illustrates six key domains in which we envision
LLM-powered assistants to meaningfully assist PETSc R&D
and user support while also augmenting the PETSc knowledge
base for long-term community advances:
a)User support and community engagement: Assis-
tants embedded in tools such as Discord can answer user
▪Identify deficient 
documentation (with specificity ) 
▪Automated documentation 
generation and updates
▪Content summarization
▪Personalized tutorials▪Code suggestions
▪Code review
▪Performance analysis 
assistance
▪Refactoring assistance
▪Automated test generation
▪Error detection 
and debugging
▪Assistance for continuous 
integration testing
▪Meeting summaries
▪Task prioritization
▪Resource allocation▪Literature review assistance
▪Uncover potential new 
research directions
▪Algorithm suggestions
▪Chatbots and virtual assistants
▪Suggest solutions to 
common problems
▪Onboarding assistance
▪User feedback analysisPETSc  AI 
ASSISTANTSTesting and 
validation
Project 
managementDocumentation
and tutorialsUser support 
and community 
engagementCode
development 
and optimizationResearch 
agenda and 
projects
Fig. 2: Our goal is to create PETSc AI assistants capable of
understanding and executing scientific computing tasks at the
heart of numerical library research, providing the AI equivalent
of human PETSc developers using LLM technology, while
also continually advancing the PETSc knowledge base.
queries, recommend solutions, aid onboarding, and even an-
alyze recurring feedback to guide improvements. For PETSc,
these capabilities help address questions on the development of
applications that build on PETSc scalable linear, nonlinear, and
timestepping solvers as well as lower-level vector and matrix
data structures for heterogeneous computing architectures [7]
and outer-loop numerical optimization and analytics.
b)Documentation and tutorials: LLMs can detect
outdated or unclear documentation, generate summaries, and
draft updates. Personalized tutorials tailored to a user’s needs
or application area could dramatically lower the barrier to
entry for new users. For the PETSc community, such custom
assistance could meet users where they are (e.g., helping to
translate between the terminology used in scientific application
areas and that employed by numerical algorithms researchers).
c)Code development and optimization: AI assistants
can help in code review, suggest code improvements, aid
in API design reviews, offer performance tuning strategies,
assist with refactoring complex code, and develop completely
new functionality. Such assistants could help with continual
advances needed in the PETSc code base to achieve efficient
performance and address next-generation science challenges
on ever-changing computer architectures.
d)Testing and validation: LLMs can help generate
automated tests, detect bugs, and assist in debugging or
improving continuous integration practices. Such capabilities
would help open-source libraries such as PETSc achieve com-
prehensive, multilevel testing to ensure a robust and reliable
code base as part of the broader scientific software ecosystem.
e)Research agenda and projects: Beyond software
development and maintenance, LLMs can support scientific
discovery itself—by aiding in literature reviews [8], surfacing
unexplored research directions [9], and suggesting algorithms
for novel problems. Research in numerical libraries incor-
porates advances in algorithms and software, as driven by
the needs of next-generation scientific applications and the
realities of continually changing computer architectures.
f)Project management: Through meeting summariza-
tion, task prioritization, and resource tracking, AI tools can re-
duce cognitive load and coordination overhead among contrib-
utors. For PETSc developers, such capabilities help with com-

munication and synchronization across a multi-institutional
distributed team.
We do not aim to replace human developers, but rather to
extend their capabilities and improve productivity. These AI
assistants will operate within the collaborative infrastructure
of PETSc—subject to the same review and git merge request
processes as human contributors—to ensure that any changes
to the official knowledge base are carefully vetted. Importantly,
we recognize that LLMs are not infallible. Just as human
developers occasionally submit flawed code or advice, so too
will AI assistants. However, the PETSc community culture has
robustly accommodated such imperfections through iterative
improvement for many years. Thus, it is well suited to integrate
these new tools while maintaining quality and trust.
Leveraging LLMs effectively within scientific comput-
ing requires more than generic chatbot capabilities. We
need specialized techniques for corpus preparation, retrieval-
augmented generation, fine-tuning, ranking, and continuous
evaluation—tailored to scientific software development’s tech-
nical depth, precision, and collaborative workflows.
Nonetheless, this endeavor is not without risk. The LLM
landscape is dominated by proprietary systems with rapidly
evolving capabilities. We aim to provide the best help for
the PETSc user and developer community over time, enabling
advanced scientific simulations that leverage emerging LLM
capabilities. Consequently, our emphasis is on developing a
framework for exploring PETSc AI assistants, which will
provide practical capabilities for our community and enable us
to focus on benchmarks and evaluation efforts that leverage our
expert knowledge in numerical computing while contributing
to improvements in the overall LLM-powered PETSc knowl-
edge base.
We have outlined a vision of how LLMs can help the
PETSc community; it is impossible to fully describe all aspects
of our current and future work in this short manuscript.
This paper presents two of our initial steps toward realizing
this vision: (1) a RAG pipeline enhanced with reranking to
improve retrieval from the PETSc knowledge base and (2)
a PETSc chatbot embedded in Discord that enables real-
time, AI-assisted conversations between users and developers
on specific PETSc user situations. These components are
supported by processing scripts, prompt libraries, and agentic
memory systems that track and evaluate interactions over time.
Together, they represent our first steps toward a sustainable,
AI-driven platform for capturing and using the collective
knowledge embedded in the PETSc ecosystem.
II. PETS CUSERSUPPORT
User support, which we have consistently provided for
over thirty years, is a fundamental responsibility of the
PETSc team—as it is for any open-source scientific software
project—to disseminate knowledge and realize the full impact
of the software. Consequently, user support is our first focus
when exploiting the PETSc knowledge base with AI assistants.
Like many scientific computing libraries, PETSc has been
funded primarily by the U.S. government. However, not onlyhas this funding been limited, but it has also largely focused
on supporting research in numerical computing, leaving few
resources available for software maintenance and user support.
Over the years, PETSc has developed a large and diverse
user base, with tens of thousands of users across dozens
of scientific domains. As a result, user support has become
a significant undertaking. For instance, in May 2025 alone,
there were over 300 messages across PETSc mailing lists,
Discord channels, and GitLab issues. While PETSc developers
have remained deeply committed to supporting users, the
volume and complexity of user inquiries can often overwhelm
a small team, particularly when resolving issues that involve
extended, multiday, or multiweek discussions. Thus, to ensure
that this process remains sustainable in the long term, we are
particularly interested in leveraging recent advances in AI to
assist with user support.
Although PETSc users can employ general-purpose AI tools
such as ChatGPT, these tools can produce incorrect or hallu-
cinated responses when answering technical questions about
PETSc. The reason is primarily that mainstream LLMs do not
have access to sufficient PETSc information. For example, in
January 2025, we posed a PETSc-specific question to a paid
version of ChatGPT and observed such limitations firsthand.
Question : What does KSPBurb do?
ChatGPT : KSPBurb is an implementation of a Krylov sub-
space method in PETSc used to solve systems of linear
equations. Specifically, KSPBurb is a block version of the
unpreconditioned Richardson iterative method ...
We scored this response as 0: it was a hallucination to a
question about “KSPBurb” – a fictitious term that follows the
naming convention of PETSc Krylov solvers (see Table I for
the rubric used in the assessment). Although later versions
of ChatGPT gave better results, this example shows the risk
of relying solely on mainstream LLMs. The example also
reminds us of the dangers of incorporating AI into user
support. As noted in [10], a prominent platform experienced
backlash when its AI agents, masquerading as human support
staff, fabricated a nonexistent login policy in response to
user inquiries that led to widespread account cancellations. To
safeguard the integrity and reputation of PETSc, we initially
have adopted a cautious approach, and to date we have avoided
full automation in user support interactions.
A typical interaction between PETSc users and developers is
highly interactive and rarely concludes with a single question-
and-answer exchange. A user’s initial question may be vague,
lack context, or use incorrect PETSc terminology. In response,
a developer will often ask for clarification. Other developers
join the conversation, contributing new perspectives or iden-
tifying misunderstandings. As the discussion progresses, the
user may pose follow-up questions that are now more precise
and well formulated. Code snippets or test cases are sometimes
shared, which should ideally be tried by the participants. This
process can be likened to a treasure hunt, where context
and clues gradually emerge and occasional experimentation is
required to determine what works. Both users and developers
may inadvertently provide incorrect information, so our pro-

cess must be robust to identify and overcome these mistakes.
To address the above challenges, we propose in Section
III an augmented PETSc LLM workflow that can effectively
process and deliver PETSc-centric information, and in Section
IV we present an integrated LLMs and PETSc Discord bots
system that can reduce the risk of hallucinated responses
reaching users.
III. T HEPETS CAUGMENTED LLM W ORKFLOW
We have developed a preliminary PETSc LLM workflow,
illustrated in Fig. 3, which can be used in multiple settings.
For example, we could use the LLM to answer questions from
the PETSc Discord server, from mailing lists, or from a chat
box on the PETSc homepage. For developers, we could even
provide command line tools and integrated development envi-
ronment (IDE) extensions to facilitate various use cases. Our
approach builds on standard LLM tools, such as LangChain
[11], while incorporating practical adaptations for PETSc.
Specifically, we do the following:
•We use embedding methods, RAG, and reranking to tai-
lor general-purpose LLM capabilities to PETSc-specific
content.
•We maintain a structured and detailed database of inter-
actions, facilitating systematic improvements and com-
parisons between different LLMs and embedding ap-
proaches.
•We provide developers with real-time access and control
over the information presented to the LLMs, helping to
ensure utility and accuracy.
The workflow is compatible with various continuation and
embedding models, including state-of-the-art variants. In the
following subsections we describe PETSc-specific processing
components in detail.
PETSc -specific LLM processing
PETSc knowledge base 
(manual pages, source code, papers, 
emails, meeting transcripts, etc.)Open -source LLM infrastructures
(LangChain, OpenAI Python bindings, etc.)
Partially processed data 
FINE TUNINGFine-tuned LLM (Large Language Model)
Developer data flow
Python scripts to process data
RAG  = Retrieval Augmented Generation
 Embedding 
generator
User and developer data flow
preliminary prompt data
LLM prompt
LLM response
   RAG databases 
Processed data Shared 
histories1 4
processed response
embedding vectors2 3
Fig. 3: Our approach for creating PETSc AI assistants builds
on standard LLM ecosystem tools. The boxes 1, 2, 3, and 4
in the top bar indicate the sequence of processing for PETSc-
specific LLM interactions.
A. Generating the RAG databases
Retrieval-augmented generation algorithms ensure that users
receive the most contextually appropriate and accurate infor-
mation. RAG consists of two phases: the generation phaseand the retrieval phase. Here, we discuss the generation phase.
PETSc documentation is maintained in Markdown format and
processed via Sphinx [12] into the website format. To construct
a practical RAG database using this documentation, we use the
following steps (see Fig. 3, beginning with “PETSc knowledge
base”, following the blue arrows to “Embedding generator”
and then the green arrow to “RAG databases”):
•Employ LangChain’s DirectoryLoader, Unstructured-
MarkdownLoader, and RecursiveCharacterTextSplitter.
•Input these results into Chroma.from documents to gen-
erate the vector database for a particular embedding (mul-
tiple databases can be built for different embeddings).
These steps allow us to remove irrelevant content and embed
direct reference links to relevant documentation, facilitating
quick user references. Similar processes will be used for
PETSc publications and the open PETSc mailing lists. Devel-
opers and users will be able to choose which vector databases
to use.
B. Utilizing the documentation RAG database
During retrieval, one encodes the user query into a vector
numerical representation and searches in the RAG vector
database for similar documents. Then, the original user query
is augmented by context information from the retrieved docu-
ments and sent to the LLM. In Fig. 3, from box 1 follow the
blue dotted arrow to the embedding generator, which returns
the embedding vectors to box 1, where the RAG databases are
searched and material is prepared to be passed to box 2.
C. Augmenting RAG searches with PETSc-specific keyword
searches
We have augmented the RAG searches with PETSc-specific
keyword searches. Whenever a word in the query has a PETSc
manual page associated with it, for example, KSPSolve, the
manual page is added to the material that RAG has found. In
Fig. 3, not explicitly displayed, box 1 locates material from
“Processed data.”
D. Reranking enhanced RAG
The embedding-based retrieval and keyword search priori-
tizes speed over accuracy since the knowledge databases can
be vast and complex. Through vector search, the retriever
quickly returns a few top results, which may include both rel-
evant and tangential information. Reranking enhances retrieval
by filtering and reordering retrieved documents according to
refined relevance scores, reducing noise and irrelevant infor-
mation. Reranking ensures that the LLM receives the most
contextually appropriate information. The reranker refines the
initial list of candidates, using scoring models based on, for
example, cross-encoders and specialized LLMs, moving the
most contextually relevant documents to the top and possibly
removing less relevant material completely. In Fig. 3 this
process occurs inside box 2 after the initial documents have
been located in box 1; Fig. 4 depicts this process in detail. In
particular, in our studies discussed below, we generate K= 8
candidate documents in the first pass and refine them down to
L= 4 documents with the reranker in the second pass.

Embedding-
based retriever
Semantic-
based rerankerUser query Knowledge base
Top-L relevant documentsReranking model
Context
generation
     LLM Response
......
question-passage pairsTop-K relevant documentsFig. 4: Reranking-enhanced RAG workflow.
E. Postprocessing LLM output for users
The LLM output, depicted in Fig. 3 with the orange arrow, is
generally provided in Markdown. We (in box 4) provide tools
that postprocess the Markdown before displaying it to users,
such as converting it to HTML for display on a webpage.
Itemized lists can be automatically detected and parsed. In
addition, we automatically detect blocks of code and can pass
them to a compiler to verify that they work. We note that
LLMs are now making it possible to return their output in
JSON, making postprocessing easier since we do not have to
“reverse engineer” the LLM output.
F . Storing and tracking interaction history
In addition to visually presenting the LLM output, we keep a
detailed, manipulatable, searchable database of all interactions
with all the LLMs by the PETSc developers (and eventually
also users). In Fig. 3 this is depicted with the dotted orange
arrow from box 4 to “Shared histories”; the blue dotted line to
box 1 indicates that material from the shared history will also
eventually be included in the RAG and reranking processing
and passed to the LLM. Questions, responses, dates when
done, and the continuation and embedding model used for each
interaction will be included, as well as the generated prompts
in the interaction. We provide an easy way for “scorers” to
“blind-score” the responses to questions and indicate correct
and incorrect portions of the responses. We can also score
answers from PETSc developers stored in the same database,
establishing high quality and trust. Currently, we use a bespoke
Python dictionary to manage the database, but we are pursuing
using emerging agentic memory systems [13].
IV. PETS CAI B OTS IN DISCORD
Discord [14] is an instant messaging platform where com-
munication happens in virtual communities called “servers.”
We have set up a PETSc Discord server, and hundreds of users
10. send if
ok'ed
1. new email
notiﬁcation
3. repost in
threads
/reply
LLM response
4. invoke
chat bot command5. bot 
command
8. repost
7. response 9. send /
revise /  discard
LLMsubscribe
6. conversation
contextDiscord
forum channel
...Original email
PETSc developers2. fetchpetscbot@gmail.com
PETSc
email botPETSc
chatbotdirect messagePETSc userpetsc-users@ mcs.anl.gov
Follow-up email
read /
contribute
PETSc knowledge baseread 
contribute
Fig. 5: Bots in Discord that connect a PETSc mailing list and
an LLM. Numbered arcs show a typical sequence of events in
the workflow.
have joined since its debut. We integrated augmented PETSc
LLMs with the Discord server and set up a chatbot that has
two usage modes: (1) one can directly chat with it, or (2)
the chatbot can answer questions from emails, while PETSc
developers can monitor and vet answers on back channels.
In this section we first briefly introduce Discord’s integration
mechanisms, then describe the workflow that connects a
PETSc public mailing list and LLMs; see Fig. 5.
Discord provides integrations via webhooks and applica-
tions (called apps or bots). Webhooks are user-defined HTTP
callbacks. One can create webhooks and associate them with
specific Discord channels. With the URL provided by Discord
for a webhook, one can, for example, make an HTTP request
to post a message to the associated channel. Discord users
can set up Discord apps and associated bots. In turn, they are
responsible for programming in Discord APIs (e.g., through
Python) and eventually deploying their apps locally or in
the cloud. Apps could access messages, channels, and user
accounts on Discord. They can provide commands for users
to invoke or act as bots on Discord.
PETSc has three public mailing lists: {petsc-users, petsc-
maint, petsc-dev }.petsc-dev is mainly for PETSc developers
and others interested in PETSc development. The other two are
for PETSc users with one distinction: petsc-maint is a private
maintenance email with controlled recipients and no public
archives, while petsc-users is fully public and has archives
from the past twenty years. In this study we targeted petsc-
users but didn’t touch its archives for RAG. We created a
Google Gmail account petscbot@gmail.com and subscribed
it to the petsc-users mailing list. Then, with Google Apps

Script services [15], we use JavaScript to periodically check
whether there are new (unread) emails from petsc-users in the
Gmail account. If there are, the script sends a message to a
webhook associated with a private channel named petsc-users-
notification in PETSc Discord. An email bot (application) was
added to the PETSc Discord server, which watches for new
messages in petsc-users-notification . Whenever there is a new
message, using Gmail’s Python API, the email bot fetches
unread emails from petsc-users in the Gmail account and
marks them read. The fetched emails are posted to a private
channel, visible only to PETSc developers, named petsc-users-
emails . This channel is a Discord Forum channel made of
posts. We can make each email thread a post, with its title
being the email thread’s subject and emails within the thread
follow up messages in the post. We also fetch and attach email
attachments to their corresponding messages in Discord. We
lightly parse email bodies to remove quotes commonly seen
in email replies and revert the url-defense protected URLs so
that messages are presented concisely.
From a post in petsc-users-emails , any PETSc developer
can invoke a command /reply on the chatbot, which will
then build a conversation context with the title, messages, and
their attachments of the post and present it to the LLM. The
Discord to LLM connection code was based on llmcord [16]
but was heavily adapted to our needs. The response from the
LLM is added as a message to the post. All PETSc developers
can see the message so that anyone can chime in. The message
has three buttons: send, discard, and revise. If the response is
fine, the developer clicks the send button to send the response
topetsc-users with a signature of the name of the developer
who clicked the button.2Also, the message in Discord will
be time-stamped and tagged with the developer’s name. If
the response is nonsense, developers can click the discard
button to delete the message. Otherwise, if the response is
partially correct and a developer clicks the revise button, the
chatbot will ask the developer to guide a follow-up message.
Developers can chat with the bot until a satisfying response is
generated and ready to be sent. The Javascript for the Gmail
account ignores emails from the chatbot and marks them as
read, so that it doesn’t repost messages in the channel. In
addition, users can do “direct messages” with the chatbot,
keeping their conversation private; however, this may expose
the user to unvetted hallucinations of the LLM.
The above workflow leverages LLMs to help but not replace
PETSc developers answering user questions from the mailing
list. The negative example [10] alerts us not to let misleading
or hallucinated answers damage PETSc’s reputation in the
community.
V. P RELIMINARY EVALUATION AND INSIGHTS
A. Evaluation strategy
To evaluate the quality of answers, we leverage human
expertise to classify the LLM responses using a blind-review
2The chatbot is still in private testing, and we haven’t sent any LLM-
generated responses to the petsc-users mailing list.workflow. Reviewers assigned a single score for each answer,
based on correctness and completeness, without knowledge of
the process used to generate it. The scoring rubric is presented
in Table I. We compiled a benchmark dataset comprising
37 questions on the use of Krylov methods within PETSc,
widely used by diverse scientific applications as a key part
of composable algebraic solvers [17] and representative of a
subtopic within the broader PETSc library.
0: Nonsensical answer
1: Incorrect or inaccurate statements (hallucinations) in the answer
2: Correct material with only minor inaccuracies
3: Answer is clear and correct
4: Ideal answer, close to what an expert would respond
TABLE I: Rubric for LLM responses (higher is better).
B. Comparison with LLM baseline
We conducted experiments with several popular LLMs, in-
cluding OpenAI’s GPT-4 variants and Meta’s Llama3 variants,
alongside various embedding models. Our analysis identified
OpenAI’s GPT-4o and text-embedding-3-large as providing the
best overall performance. Consequently, we report results for
this specific LLM and embedding model combination.
We have explored the NVIDIA reranker (commercial) and
the Flashrank reranker (free) through the LangChain interface.
The NVIDIA reranker provides GPU-accelerated transformer-
based encoder models. Flashrank provides lightweight mod-
els running on the CPU, including encoder (BERT-based)
rerankers and non-encoder rerank models. Both rerankers yield
a similar level of accuracy for our database. We selected
Flashrank in this study because of its speed.
Providing PETSc-specific information (via pretraining and
RAG) enables us to overcome limitations in mainstream
LLMs, which typically lack sufficient training data on PETSc
to provide accurate and reliable technical responses. For
instance, the example below demonstrates how our RAG
system prevents the LLM hallucination regarding “KSPBurb”
presented above.
Question : What does KSPBurb do?
ChatGPT : It appears there may be a typo or misunderstand-
ing, as there’s no PETSc function or object named KSPBurb.
In PETSc, the KSP (Krylov Subspace) module is used to solve
linear systems iteratively and includes a variety of solver types
such as GMRES, CG, BiCG, among others ...
Figs. 6a and 6b present the evaluation scores for the
benchmark questions, demonstrating an improvement over
the baseline achieved by using RAG (Fig. 6a) or by using
rerank-enhanced RAG (Fig. 6b). The baseline in both figures
corresponds to using GPT-4o without RAG. As indicated by
the orange bars in Fig. 6a, employing the LLM with RAG
improves scores for 20 questions while negatively impacting
scores for three questions. Fig. 6b demonstrates that utilizing
the LLM with reranking-enhanced RAG improved scores
for 25 questions, with no negative impact observed on any
question’s score. This configuration achieved a perfect score

1 6 11 16 21 26 31 36
Questions1
01234ScoreScoring improvement using RAG
PETSc AI score (RAG)
Improvement from baseline (no RAG)(a)
1 6 11 16 21 26 31 36
Questions01234ScoreScoring improvement using reranking-enhanced RAG
PETSc AI score (RAG+Rerank)
Improvement from baseline (no RAG)
(b)
1 6 11 16 21 26 31 36
Questions01234ScoreScoring improvement due to reranking
PETSc AI score (RAG+Rerank)
Improvement from LLM with RAG
(c)
Fig. 6: Scoring comparison for LLMs.
(4) for 33 of 37 questions and a score of 3 for the remaining
four questions.
C. Impact of reranking
Fig. 6c, which can also be inferred from Figs. 6a and 6b,
illustrates the significant impact of the reranking algorithm on
the RAG process. The scores for 11 questions were improved
by integrating reranking into the RAG process. Two of these
questions saw a score increase of 3 points!
Figs. 7 and 8 present the LLM responses and their scoring
justifications for two illustrative case studies. In the first
case study, the RAG-enabled LLM failed to suggest the
specific PETSc KSP solver for non-square systems. However,
the reranking-enhanced RAG successfully resolved this issue.
A closer investigation revealed that the reranking algorithm
identified the following critical context, which was missed by
the basic RAG algorithm:
KSP can also be used to solve least squares
problems, using, for example, KSPLSQR...
In the second case study, the LLM hallucinated an imaginary
PETSc runtime option. A comparison of contexts retrieved by
the two RAG methods revealed only one common context,
with the other three contexts being distinct. The reranking-
enhanced RAG, however, successfully retrieved a paragraph
discussing the -info option:
Question:Can I use KSP to solve a system where the matrix is not square, only rectangular? Must it be invertible too or does that depend on how you're using KSP?Response:In PETSc, the KSP (Krylov Subspace) solvers are primarily designed for square matrices, which correspond to solving linear systems where the coefficient matrix is of the form𝐴𝑥=𝑏with 𝐴being square. However, KSP can be used in combination with certain strategiesto solve rectangular matrices, typically in contexts involving least squares problems or overdetermined/underdetermined systems…Score:①Justification: The response does not recommend KSPTypeoptions suitable for non-square systems, and instead emphasized more complex approaches, such as the use of normal equations.Response: Yes, you can use KSP to solve systems where the matrix is rectangular, which typically arise in least squares problems. The pivotal solver for such cases in PETScis KSPLSQR, which can handle over-determined or under-determined systems…Score: ④Justification:The response recommends the least-square solver KSPLSQRin PETScaccurately.LLM with RAGLLM with reranking-enhanced RAGFig. 7: Case Study 1: Comparison of RAG and reranking-
enhanced RAG.
Question:Is there an option in PETScthat allows to check at run time if a sparse matrix has been preallocatedcorrectly?Response:Yes, PETScprovides a way to check if a sparse matrix has been preallocatedcorrectly using runtime options. You can use the -mat_view, -info, and -info_excludeoptions to gather information about the matrix allocation during runtime.…An example of how you might run your PETScapplication with these options is as follows:mpiexec -n <numprocs> ./your_petsc_program -mat_check_preallocation -info…Score:①Justification: Theresponsecreates an imaginary option -mat_check_preallocation.Response: Yes, PETScprovides a runtime option to help you check if a sparse matrix has been preallocatedcorrectly. You can use the -info option when running your PETScapplication. This option will print detailed information about the program's operations, including feedback on matrix preallocation.…Score: ④Justification: Thereisno misleading information in the response.LLM with RAGLLM with reranking-enhanced RAG
Fig. 8: Case Study 2: Comparison of RAG and reranking-
enhanced RAG.
As described above, the option -info will print
information about the success of preallocation
during matrix assembly...
whereas the plain RAG algorithm did not find this information.
D. Inference latency
To evaluate inference latency, we measured separately the
running time for the RAG process and the LLM response.
This approach enabled us to quantify the overhead introduced
by the RAG systems and assess their overall efficiency. As
presented in Table II, adding the reranking stage increased
the average RAG processing time by approximately 2.4times.
However, the average running time for the reranking-enhanced
RAG remained less than 11% of the average LLM response
time.
VI. R ELATED WORK
Recent advances in generative artificial intelligence, particu-
larly in large language models, have significantly transformed

RAG RAG+reranking
Min Max Avg Min Max Avg
RAG time 0.16 3.11 0.44 0.48 5.71 1.05
LLM response 2.74 16.47 9.56 2.28 15.62 9.63
TABLE II: Run time for RAG and the LLM (in seconds) on
an Intel i7-11700KF 3.6GHz CPU.
software engineering. These models have demonstrated im-
pressive capabilities in tasks such as code generation [18], bug
detection [19], and automated code review [20]. Building on
these capabilities, RAG systems [21] have extended the use
of LLMs by integrating external knowledge sources during
inference. Several studies ( [22], [23]) have explored using
RAG systems to support the development, maintenance, and
transformation of legacy code bases.
As a mature scientific computing library, PETSc contains
much valuable but unstructured information, such as developer
discussions and user Q&As, that general-purpose LLMs are
unlikely to have seen during training. This situation makes
retrieval and formatting essential for effective use. Moreover,
PETSc serves a diverse user base, so usability is critical.
Furthermore, its focus on domain-specific, high-performance
numerical computation requires knowledge that general LLMs
typically lack, limiting their effectiveness without adaptation.
VII. C ONCLUSION AND FUTURE WORK
Our preliminary results underscore the feasibility and sig-
nificant potential of integrating LLMs (with PETSc-specific
processing) to enhance the PETSc knowledge base and exploit
it through AI assistants in daily workflows of PETSc research,
development, and support. Future plans focus on advancing
LLM-based tools specialized for PETSc and deepening their
integration with PETSc activities to contribute to more robust
and accessible scientific software environments, including ex-
ploring agentic approaches for complex tasks such as code
generation and optimization. We also want to incorporate
additional information as part of PETSc-specific RAG and
to put the PETSc Discord chatbot in production use. The
end result will be LLM assistants acting as and interacting
daily with other PETSc developers, with goals of enhancing
productivity and advancing scientific discovery.
ACKNOWLEDGMENT
We thank the PETSc developers for discussing this work
as well as for their many contributions to the PETSc code
base and community knowledge base, without which this work
would not be possible.
REFERENCES
[1] J. Carter, J. Feddema, D. Kothe, R. Neely, J. Pruet, R. Stevens etal.,
“Advanced Research Directions on AI for Science, Energy, and Security:
Report on Summer 2022 Workshops,” 2023, doi:10.2172/1986455.[2] M. A. Heroux, D. E. Bernholdt, L. C. McInnes, J. R. Cary, D. S. Katz,
E. M. Raybourn, and D. Rouson, “Basic Research Needs in The Science
of Scientific Software Development and Use: Investment in Software is
Investment in Science,” 2023, report of the DOE Advanced Scientific
Computing Research Workshop, doi:10.2172/1846009.
[3] M. E. Papka, D. K. Panda, I. Altintas, W. Bhimji, E. Deelman, M. Emani,
N. Ferrier, D. S. Katz, L. C. McInnes, A. Nikolich, F. Wang, and
J. Willenbring, “Final Report of the 2024 NSF/DOE Workshop on
NAIRR Software,” 2025.
[4] S. Balay etal., “PETSc users manual,” Argonne National Laboratory,
Tech. Rep. ANL-21/39 - Revision 3.23.3, 2025. [Online]. Available:
https://petsc.org
[5] R. T. Mills etal., “PETSc/TAO developments for GPU-based early
exascale systems,” The International Journal ofHigh Performance
Computing Applications, vol. 39, no. 2, pp. 306–325, 2025,
doi:10.1177/10943420241303710.
[6] M. Adams etal., “The PETSc community as infrastructure,” Computing
inScience &Engineering, vol. 24, no. 03, pp. 6–15, May 2022,
doi:10.1109/MCSE.2022.3169974.
[7] J. Zhang etal., “The PetscSF scalable communication layer,” IEEE
Transactions onParallel and Distributed Systems, vol. 33, no. 4, pp.
842–853, 2022, doi:10.1109/TPDS.2021.3084070.
[8] S. A. Antu, H. Chen, and C. K. Richards, “Using LLM (large language
model) to improve efficiency in literature review for undergraduate
research,” LLM@ AIED, pp. 8–16, 2023.
[9] S. Schmidgall, Y . Su, Z. Wang, X. Sun, J. Wu, X. Yu, J. Liu, Z. Liu,
and E. Barsoum, “Agent laboratory: Using LLM agents as research
assistants,” arXiv preprint arXiv:2501.04227, 2025.
[10] Ycombinator, “Cursor IDE support hallucinates lockout policy,
causes user cancellations,” https://https://news.ycombinator.com/item?
id=43683012, 2025, [Online; accessed 12-June-2025].
[11] Langchain.com, “Langchain,” https://www.langchain.com, 2025, [On-
line; accessed 12-June-2025].
[12] The Sphinx Developers, “Sphinx Documentation,” https://www.
sphinx-doc.org/en/master/, 2024, [Online; accessed 12-June-2025].
[13] K. Hatalis, D. Christou, J. Myers, S. Jones, K. Lambert, A. Amos-
Binks, Z. Dannenhauer, and D. Dannenhauer, “Memory matters: The
need to improve long-term memory in LLM-agents,” in Proceedings of
theAAAI Symposium Series, vol. 2, no. 1, 2023, pp. 277–280.
[14] Discord, “Discord,” https://discord.com, 2025, [Online; accessed 12-
June-2025].
[15] Google, “Google apps script,” https://developers.google.com/apps-script,
2025, [Online; accessed 12-June-2025].
[16] Jakobdylanc, “llmcord,” https://github.com/jakobdylanc/llmcord, 2025,
[Online; accessed 12-June-2025].
[17] J. Brown, M. G. Knepley, D. A. May, L. C. McInnes, and B. Smith,
“Composable linear solvers for multiphysics,” in 2012 11th International
Symposium onParallel and Distributed Computing, 2012, pp. 55–62,
doi:10.1109/ISPDC.2012.16.
[18] L. Chen, A. Bhattacharjee, N. Ahmed, N. Hasabnis, G. Oren, V . V o, and
A. Jannesari, “OMPGPT: A generative pre-trained transformer model for
OpenMP,” in European Conference onParallel Processing. Springer,
2024, pp. 121–134.
[19] H. Li, Y . Hao, Y . Zhai, and Z. Qian, “Enhancing static analysis for
practical bug detection: An LLM-integrated approach,” Proceedings of
theACM onProgramming Languages, vol. 8, no. OOPSLA1, pp. 474–
499, 2024.
[20] J. Lu, L. Yu, X. Li, L. Yang, and C. Zuo, “Llama-reviewer: Advancing
code review automation with large language models through parameter-
efficient fine-tuning,” in 2023 IEEE 34th International Symposium on
Software Reliability Engineering (ISSRE). IEEE, 2023, pp. 647–658.
[21] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,
H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel etal., “Retrieval-
augmented generation for knowledge-intensive NLP tasks,” Advances in
Neural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.
[22] S. J. Ali, V . Naganathan, and D. Bork, “Establishing traceability between
natural language requirements and software artifacts by combining
RAG and LLMs,” in International Conference onConceptual Modeling.
Springer, 2024, pp. 295–314.
[23] L. Chen, P.-H. Lin, T. Vanderbruggen, C. Liao, M. Emani, and
B. De Supinski, “LM4HPC: Towards effective language model appli-
cation in high-performance computing,” in International Workshop on
OpenMP. Springer, 2023, pp. 18–33.

The submitted manuscript has been created by UChicago Argonne,
LLC, Operator of Argonne National Laboratory (“Argonne”). Argonne,
a U.S. Department of Energy Office of Science laboratory, is operated
under Contract No. DE-AC02-06CH11357. The U.S. Government re-
tains for itself, and others acting on its behalf, a paid-up nonexclusive,
irrevocable worldwide license in said article to reproduce, prepare
derivative works, distribute copies to the public, and perform publicly
and display publicly, by or on behalf of the Government. The Depart-
ment of Energy will provide public access to these results of federally
sponsored research in accordance with the DOE Public Access Plan.
http://energy.gov/downloads/doe-public-accessplan