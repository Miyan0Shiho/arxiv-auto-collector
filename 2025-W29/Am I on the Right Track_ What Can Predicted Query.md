# Am I on the Right Track? What Can Predicted Query Performance Tell Us about the Search Behaviour of Agentic RAG

**Authors**: Fangzheng Tian, Jinyuan Fang, Debasis Ganguly, Zaiqiao Meng, Craig Macdonald

**Published**: 2025-07-14 15:54:50

**PDF URL**: [http://arxiv.org/pdf/2507.10411v1](http://arxiv.org/pdf/2507.10411v1)

## Abstract
Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the
reasoning model decides when to invoke a retriever (as a "tool") when answering
a question. This paradigm, exemplified by recent research works such as
Search-R1, enables the model to decide when to search and obtain external
information. However, the queries generated by such Agentic RAG models and the
role of the retriever in obtaining high-quality answers remain understudied. To
this end, this initial study examines the applicability of query performance
prediction (QPP) within the recent Agentic RAG models Search-R1 and
R1-Searcher. We find that applying effective retrievers can achieve higher
answer quality within a shorter reasoning process. Moreover, the QPP estimates
of the generated queries, used as an approximation of their retrieval quality,
are positively correlated with the quality of the final answer. Ultimately, our
work is a step towards adaptive retrieval within Agentic RAG, where QPP is used
to inform the model if the retrieved results are likely to be useful.

## Full Text


<!-- PDF content starts -->

Am I on the Right Track? What Can Predicted Query
Performance Tell Us about the Search Behaviour of Agentic RAG
Fangzheng Tian
f.tian.1@research.gla.ac.uk
University of Glasgow
Glasgow, United KingdomJinyuan Fang
j.fang.2@research.gla.ac.uk
University of Glasgow
Glasgow, United KingdomDebasis Ganguly
First.Last@glasgow.ac.uk
University of Glasgow
Glasgow, United Kingdom
Zaiqiao Meng
First.Last@glasgow.ac.uk
University of Glasgow
Glasgow, United KingdomCraig Macdonald
First.Last@glasgow.ac.uk
University of Glasgow
Glasgow, United Kingdom
ABSTRACT
Agentic Retrieval-Augmented Generation (RAG) is a new paradigm
where the reasoning model decides when to invoke a retriever (as
a “tool”) when answering a question. This paradigm, exemplified
by recent research works such as Search-R1, enables the model to
decide when to search and obtain external information. However,
the queries generated by such Agentic RAG models and the role
of the retriever in obtaining high-quality answers remain under-
studied. To this end, this initial study examines the applicability of
query performance prediction (QPP) within the recent Agentic RAG
models Search-R1 and R1-Searcher. We find that applying effec-
tive retrievers can achieve higher answer quality within a shorter
reasoning process. Moreover, the QPP estimates of the generated
queries, used as an approximation of their retrieval quality, are pos-
itively correlated with the quality of the final answer. Ultimately,
our work is a step towards adaptive retrieval within Agentic RAG,
where QPP is used to inform the model if the retrieved results are
likely to be useful.
CCS CONCEPTS
•Information systems →Information retrieval query pro-
cessing .
KEYWORDS
Retrieval Augmented Generation, Agentic RAG, Query Performance
Prediction, Adaptive Search
ACM Reference Format:
Fangzheng Tian, Jinyuan Fang, Debasis Ganguly, Zaiqiao Meng, and Craig
Macdonald. 2025. Am I on the Right Track? What Can Predicted Query
Performance Tell Us about the Search Behaviour of Agentic RAG. In Pro-
ceedings of Make sure to enter the correct conference title from your rights
confirmation email (IR-RAG Workshop at SIGIR ’25). ACM, New York, NY,
USA, 13 pages. https://doi.org/XXXXXXX.XXXXXXX
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy
©2025 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX1 INTRODUCTION
Retrieval-Augmented Generation (RAG) is a technique that incorpo-
rates retrieved external knowledge into the generative process of a
Large Language Model (LLM) [ 15]. By doing so, RAG alleviates the
hallucination problem often observed in LLMs [ 11] and compen-
sates for their limitations in accessing up-to-date or domain-specific
knowledge [ 26], which is essential for fulfilling downstream tasks
such as Question Answering (QA). The basic standard form of the
RAG pipeline involves a single retrieval stage, based solely on an
input question. However, standard RAG has limitations in its flex-
ibility and adaptability, as it applies a uniform retrieval strategy
to all input questions, regardless of whether the LLM already pos-
sesses sufficient internal knowledge to answer them. In addition,
it is not well-suited for tasks that require complex or multi-step
information gathering [ 22]. As a result, the potential benefits of
integrating retrieval and generation are not fully realised, which
can lead to suboptimal performance on complex downstream tasks.
To address the limitation of single retrieval in standard RAG,
an alternative approach is to delegate the LLM itself to determine
when to invoke searches and what queries to search for. This para-
digm is termed as iterative orAgentic RAG . Instead of relying on a
static, single-step retrieval, the LLM in Agentic RAG can iteratively
formulate and issue queries, and ingest the results of the search
before continuing its reasoning process. This allows the reasoning
process to adapt in light of the retrieved information. Agentic RAG
models such as Search-R1 [ 23] and R1-Searcher [ 39] have shown
improved performance in question answering tasks, such as Natural
Questions (NQ) [25] and HotpotQA [49].
In the workflow of Agentic RAG, an LLM begins answering a
question by reasoning. During the reasoning process, if external
knowledge is needed, the LLM can automatically generate inter-
mediate queries that interrupt generation and trigger the search
engine (acting as an agent). The top-ranked retrieved documents are
then appended to the generated tokens, serving as the context for
reasoning in the next iteration. These reasoning-retrieval iterations
continue until the LLM decides to yield the final answer after rea-
soning or reaching the output token limit. While existing Agentic
RAG models have shown effective empirical results in question-
answering tasks [ 23,39], there remains a limited understanding of
the properties of the generated queries and their retrieval results,
as well as how the retrieval component contributes to enhancingarXiv:2507.10411v1  [cs.IR]  14 Jul 2025

IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy Tian et al.
answer quality. To further enhance the performance of Agentic
RAG by improving the reasoning-retrieval process, it is important
to analyse the generated queries and their likelihood of retrieving
relevant information. This motivates us to study Agentic RAG’s
retrieval component and its downstream effects on answer quality.
While the role of the retrieval component in standard RAG sys-
tems has been extensively studied, its role within Agentic RAG
remains under-explored. In standard RAG, factors such as retriever
effectiveness, the cutoff of top-retrieved documents and the order of
documents when they are appended into RAG prompts can all influ-
ence the answer quality. However, Agentic RAG systems introduce
additional complexity by involving multiple retrievals throughout
the reasoning process. Therefore, in Agentic RAG, answer quality
may be affected by the cumulative influence of all the retrievals con-
ducted during the reasoning iterations, presenting new challenges
for analysing it.
A significant challenge in evaluating the impact of retrieval qual-
ity within the Agentic RAG framework is the absence of ground
truth relevance judgements for the intermediate generated queries
in each reasoning-retrieval iteration. To tackle this challenge, we
resort to Query Performance Prediction (QPP) as a possible esti-
mator of the retrieval quality for generated queries. QPP methods
aim to predict the effectiveness of the retrieval for a query without
human-annotated relevance judgements, typically by analysing
features of the query and the retrieved documents [ 8,37,45]. Given
that both the generated queries and their corresponding retrieval
results are available within the Agentic RAG (implemented by Py-
Terrier-RAG [ 29]), QPP offers a viable approach for approximating
retrieval quality and investigating the role of retrieval in the system.
To this end, in this paper, we conduct experiments examining
answer quality for two recent Agentic RAG models using three
different retrieval models. For each retriever, we study how esti-
mated query performance, estimated by several unsupervised QPP
models, varies throughout the iterations in the reasoning process.
Furthermore, we investigate how the estimated query performance
correlates with answer quality.
In summary, this paper contributes a first study of the utility of
QPP in the querying behaviour of two typical agentic RAG models,
Search-R1 [ 23] and R1-Searcher [ 39]. Moreover, we investigate the
impact of retriever effectiveness on the iterative reasoning-retrieval
process in Agentic RAG. We conduct experiments on the 3610 test
queries of the Natural Questions (NQ) QA dataset [ 25]. Based on
our experiment results, we observe that (1) more effective retrieval
models can reduce the length of Agentic RAG’s reasoning process
and enhance the answer quality; (2) QPP estimation about the
generated queries is a potential signal for predicting the quality of
the final answers.
The remainder of the paper is organised as follows. In the next
section, we review related work about RAG and the role of the re-
trieval component in it. We then provide a systematic description of
Agentic RAG system in Section 3. In Section 4 we state our research
questions and introduce the setting of Agentic RAG pipelines and
the experimented QPP methods. According to the experiment re-
sults, we answer the research questions in Section 5 and conclude
the paper with perspectives on applying QPP in facilitating Agentic
RAG in Section 6. Appendix A demonstrates a case study on two
GeneratorRetriever
Q r
reasoningQ'D Generator r
r= *QPPEvaluatorAReasoning -Retrieval Iteration
× LFigure 1: A conceptual overview of a search-enhanced reasoner based
RAG, which we simply refer to as “Agentic RAG”. Given an input
question𝑄, the model performs initial reasoning 𝑟, following which
it iteratively generates queries 𝑄′. For each such generated query,
the retriever component incorporates the top-ranked documents
into the ongoing reasoning process. This process continues until the
model produces a final answer 𝐴after𝐼𝑡𝑒𝑟 iterations (the number of
iterations required depends on the input question). A post-retrieval
query performance prediction (QPP) model is applied to the interme-
diate retrieval results. An evaluator then assesses the quality of the
generated answer. In this paper, we seek to find correlations between
the QPP estimates and the answer quality.
outputs - one with a correct answer and one with a wrong answer -
to investigate the topical changes in the intermediate queries.
2 RELATED WORK
We position our work within the existing literature, in terms of
retrievers in RAG (Section 2.1), iterative RAG models (Section 2.2)
and query performance prediction (Section 2.3).
2.1 The Role of Retriever in RAG
RAG models enhance an LLM’s generative process by providing
access to external knowledge [ 26], which helps mitigate hallucina-
tions [ 14] and improve overall performance. In RAG, particularly for
the decoder-only LLMs [ 4,10,48], retrieved documents are directly
concatenated with the input question [ 15] as part of the prompt.
RAG has been applied to a number of downstream tasks, including
QA [20, 26], fact verification [21, 34] and summarisation [33].
Unlike traditional information systems, where users consume
the retrieval results, RAG systems pass the retrieval results directly
to a generator (i.e. an LLM). In RAG systems, it is the LLM, rather
than the user, that consumes the retrieved documents to generate
the final response. Traditional IR metrics, such as nDCG, may not
precisely measure the usefulness of retrieved documents in this con-
text, due to the difference between human-defined relevance and
the way LLMs interpret and leverage information [ 1]. Therefore,
the quality of retrieved documents in RAG should be evaluated not
by topical relevance but rather by their impact on the downstream
task performance [ 1,35,41]. To better understand this relationship,
recent studies have examined how different characteristics of the
retrieved documents affect RAG performance, such as the number
of documents [ 41], the position of relevant documents [ 28] and the
relative position of the query [ 7]. Despite these findings, existing
studies about the role of retrieval in RAG focus on single-retrieval

Am I on the Right Track? IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy
Prompt
Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After
reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and it will return the top
searched results between <information> and </information>. You can search as many times as your want. If you find no further
external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For
example, <answer> Beijing </answer>.
Question: who will take the throne after the queen dies?
Output - Iteration 1
<think>I need to find out who will take the throne after the queen dies. I’ll search for it.</think> <search> who will take the
throne after the queen dies </search>
<information>Doc 1(Title: “Succession to the British throne") Catholics are eligible .... Doc 2 .... Doc 3 ....</information>
Output - Iteration 2
<think>I found out that after the queen dies, her heir apparent will take the throne. I need to find out who the heir apparent is for
Queen Elizabeth II.</think> <search> heir apparent to Queen Elizabeth II </search>
<information>Doc 1(Title: “Heir apparent") rank behind her brothers regardless of their ages .... Doc 2 .... Doc 3 .... </information>
Output - Answer
<think>I found out that the heir apparent to Queen Elizabeth II is her eldest son, Charles, Prince of Wales. Now I can provide the
answer.</think> <answer> Charles, Prince of Wales </answer>’
Figure 2: Example prompt for Search-R1 and its output over two reasoning-retrieval iterations for a query from the Natural Questions dataset.
In each iteration, the top three documents retrieved by E5 are used to inform the reasoning process. In this example, the generator is tasked
with identifying the successor to the late Queen. Search-R1 performs two iterations with generated queries before producing the correct answer.
RAG pipelines. There remains a gap in understanding how re-
trieval influences performance in RAG pipelines that involve
multiple retrievals.
2.2 Iterative/Agentic RAG
In standard RAG models, external knowledge is retrieved and pro-
vided to the LLM only once [ 26]. However, a single retrieval might
not be enough to answer some difficult queries [ 17,43]. To ad-
dress this issue, recent studies, such as IRCoT [ 42], KiRAG [ 13]
and Search-O1 [ 27], have explored RAG frameworks that incor-
porate multiple (or iterative) retrievals. The number of retrievals
can be determined dynamically using techniques such as question
decomposition [ 22] or by iteratively detecting uncertain tokens in
the generated answer [ 47]. The queries executed in these dynamic
retrievals are often refined versions of the original query [ 24] or
are generated to resolve ambiguities in the current answer [22].
More recent RAG frameworks integrate dynamic retrieval di-
rectly into the LLM’s reasoning process [ 23,27,39], letting LLM it-
self autonomously conduct searches. This is a new paradigm known
as Agentic RAG [ 38]. Search-O1 achieves this integration by prompt
engineering [ 27], whereas more advanced models such as Search-
R1 [23] and R1-Searcher [ 39] employ reinforcement learning to train
the LLM to autonomously invoke retrievals during the reasoning
process. These methods have demonstrated improved performance
on QA benchmarks [ 25,49]. In this paper, we investigate thereasoning-retrieval iterations in RL-trained Agentic RAG
models, focusing on the role of the retriever and its impact
on answer quality.
2.3 Query Performance Prediction
Query performance prediction (QPP) is a technique used to esti-
mate the quality of retrieval results for an input query without
human-assessed relevance judgements. QPP methods are gener-
ally categorised into pre-retrieval andpost-retrieval , based on
whether the prediction is made before or after the retrieval process.
Pre-retrieval QPP methods solely rely on the features of the query
itself [ 16] – such as inverse document frequency (IDF) [ 18], query
length – to infer the query’s specificity and predict the retrieval
performance. Because they do not consider retrieval results, their
predictions are independent of the retriever used and are generally
less accurate [31].
In contrast, post-retrieval QPP methods can use the features of
the retrieval results, including retrieval score [ 6,36,37,40], dense
embeddings [ 12,45] and the document texts [ 3]. Among numer-
ous post-retrieval QPP methods, supervised methods directly train
models to predict specific target IR metrics [ 3,9]. Unsupervised
post-retrieval QPP models analyse the retrieval quality by analysing
certain properties of the retrieval result, such as the distinctiveness
of the top-retrieved document [ 6,37], semantic concentration [ 12],
coherence [ 2,45], etc. However, QPP methods have not been

IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy Tian et al.
applied in examining the query generation process in Agen-
tic RAG. Given the lack of ground truth for evaluating the queries
generated during reasoning-retrieval iterations, QPP provides a
promising technique for investigating the role of retrieval in Agen-
tic RAG systems.
3 AGENTIC RAG
In this section, we describe how we incorporate a query perfor-
mance prediction (QPP) component in a search-enhanced reasoning-
based RAG workflow, which, for simplicity, we simply refer to as
Agentic RAG , as the LLM uses the retriever as an agent.
3.1 Overview
Figure 1 shows the workflow of a Search-Enhanced Reasoning
based RAG, which we simply refer to as “Agentic RAG” throughout
the paper. In this system, an LLM-decoder, serving as the gener-
ator, automatically drives a retriever for searching information.
Given an input question 𝑄, the generator performs reasoning to-
wards deriving an answer. During the reasoning process, if the
generator perceives that additional external information is neces-
sary, it generates a query 𝑄′, enclosed within the tags <search>
and</search> .1The retriever then executes this query to retrieve
potentially relevant documents, which are incorporated into the
ongoing reasoning process. This may prompt the generator to initi-
ate further retrievals. Each cycle of this ‘reasoning’ and ‘retrieval’
constitutes an iteration in agentic RAG. This iterative process con-
tinues until the generator decides to yield the final answer 𝐴. The
total number of reasoning-retrieval iterations undertaken to reach
the final answer for the input query is termed as length of the
reasoning process , denoted as 𝐼𝑡𝑒𝑟in this paper.
To exemplify the reasoning process of agentic RAG, Figure 2
presents a sample prompt and output from Search-R1, which is a
commonly used search-enhanced reasoning based RAG model [ 23].
The top-part of the figure shows the prompt that explicitly instructs
the model to invoke searches when more knowledge is required
for answering the input question. In this example, the generator
invokes the retriever twice with progressively refined queries before
yielding the final answer. In this example, the answer generated
after two steps ( 𝐼𝑡𝑒𝑟=2) correctly answers the input question about
the successor of the late Queen.
3.2 Retrieval
When configuring a RAG model, various retrieval pipelines can
be integrated with the generator. They can range from an effi-
cient sparse retriever, such as BM25, to more complex retrieval-
re-ranking pipelines. Using PyTerrier-style notation, we can de-
note any retrieval pipeline for RAG. For instance, a MonoT5 cross-
encoder re-ranking the output of BM25 as: BM25 %𝑘≫MonoT52.
Alternatively, a dense retrieval pipeline may be formulated with
a query-encoder and an existing vector database of pre-encoded
passage vectors (e.g. E5 ≫VectorIndex).
1For clarity, in this paper, we always refer to the user’s input as a question, and the
search queries generated by the model as queries.
2In PyTerrier [ 30],𝑎≫𝑏denotes a pipeline composition operation: the output of 𝑎is
passed to𝑏;%𝑘denotes applying a rank cutoff of 𝑘per query.This flexibility in configuring retrieval pipeline allows us to easily
instantiate Agentic RAG models with various retrievers3, which
can result in different downstream RAG performances. Intuitively,
the top-𝑘documents (where 𝑘is typically small in RAG) retrieved
by more effective retrievers are more likely to contain relevant
information. Such documents can better complement the LLM’s
internal knowledge, potentially reducing the reasoning-retrieval
iterations required to generate an accurate answer. However, since
retrieval is only an intermediate step in the Agentic RAG, the degree
to which the retrieval pipeline impacts answer quality remains an
open question—one that we investigate in our research.
3.3 QPP of Generated Queries
To investigate the influence of the retrieval component in Agentic
RAG models, we need to first estimate the quality of the retrieved
documents. In this context, retrieval quality refers to how relevant
the retrieved documents are to the generated sub-query, and how
useful these documents will be to ‘guide’ the generator towards a
high-quality (relevant and factually correct) answer.
Since the retrieval component receives queries that are dynami-
cally generated by the LLM’s reasoning process, there is no rele-
vance ground truth for explicitly evaluating retrieval quality. To
address this problem, in this initial research, we investigate whether
a Query Performance Predictor (QPP) estimate may be substituted
as a reasonable approximation for the relevance or the usefulness
of these documents retrieved during the reasoning phase.
In this work, we focus on post-retrieval QPP models, which
estimate retrieval quality based on properties of the retrieved docu-
ments, such as their score distribution or embedded representations,
instead of pre-retrieval ones, where the prediction depends only
on the input query characteristics. A post-retrieval QPP model is a
more appropriate choice for this task of estimating the usefulness of
the retrieved documents in guiding the reasoning process towards
the correct answer.
A QPP model can be integrated within the Agentic RAG pipeline
to dynamically estimate the quality of retrievals in each reasoning-
retrieval iteration. This component is depicted as the red box con-
nected to the retrieval result in Figure 1.
Inclusion of a QPP model into the agentic RAG workflow is a
step towards better understanding the quality of the intermediate
queries generated during the reasoning-retrieval iterations. This is
also a step towards developing an adaptive agentic RAG workflow
that is capable of dynamically deciding to leverage the top-retrieved
documents for modifying the reasoning process only if their esti-
mated performance is high. In this initial work, we focus only on
the analysis of the QPP estimates and its relation with with the
downstream answer quality, leaving the exploration of an adaptive
QPP based workflow for agentic RAG as future work.
Finally, it is worth mentioning that the pragmatic applicability of
QPP methods depends on the retrieval model type used in RAG. For
instance, it is not a pragmatic choice to apply dense QPP approaches,
such as [ 12,45], to retrievers using sparse representations (e.g.,
BM25). That is because if document embeddings are available, we
3An example of RAG pipeline with BM25 ≫MonoT5 as the retriever can be found
here: https://github.com/terrierteam/pyterrier_rag/tree/main/examples.

Am I on the Right Track? IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy
may as well use them to improve the retriever itself, e.g., bi-encoder
retriever, instead of solely utilising them for QPP [45].
4 EXPERIMENTAL SETUP
We now elicit our research questions (Section 4.1), our experimental
setup in terms of retrievers and agentic rag models (Section 4.2 and
query performance predicts (Section 4.3).
4.1 Research Questions
In this work, we focus on the iterative query generation process
in two agentic RAG models, namely Search-R1 [ 23] and R1-Sear-
cher [ 39], and specifically the quality of queries generated at each
iteration. Indeed, if the effectiveness of the retriever used by the
RAG system changes, the number of iterations required to reach a
final answer may change. During their iterative reasoning–retrieval
processes, the quality of the generated queries may shift – becom-
ing more specific or more generic – which, in turn, can influence the
final answer quality. Since human-assessed relevance judgments
are not available for these dynamically generated queries, we esti-
mate their effectiveness using query performance predictors. Based
on this, we formulate our research questions as follows:
•RQ-1 : What is the effect of different retriever configurations on
answer quality and number of iterations?
•RQ-2 : How does the estimated performance of generated queries
vary as the number of iterations increases?
•RQ-3 : How does the estimated performance of generated queries
correlate with the quality of the generated answers?
4.2 Agentic RAG Pipeline Configurations
Query set .We use the Natural Questions (NQ) test set [ 25],
which contains a total of 3,610 questions. Each question is accom-
panied by gold (ground-truth) answers, as found from Wikipedia
by manual annotation. These ground-truth answers are then used
to evaluate the quality of the answers generated by Agentic RAG
models. Following common practices, we use F1 and Exact Match
% (EM) to measure answer quality [5].
Document corpus .A snapshot of English Wikipedia from 2018,
as used with the NQ dataset.
Generator .We experiment with two recent Agentic RAG mod-
els trained with Reinforcement Learning (RL) from QWEN2.5-
7B [48]:
•Search-R1 [23]: This model is trained by single-stage outcome-
based RL on NQ and HotpotQA training sets [49]4.
•R1-Searcher [39]: This model is trained via two-stage outcome-
based RL on HotpotQA and 2WikiMultiHopQA [ 19]. Since NQ
was not used in the training of R1-Searcher model, our experi-
ments for the R1-Searcher correspond to that of an out-of-domain
evaluation.
Both Search-R1 and R1-Searcher have demonstrated improved an-
swer quality over standard RAG formulations and other recent
iterative RAG formulations such as IRCoT [42].
4https://huggingface.co/PeterJinGo/SearchR1-nq_hotpotqa_train-qwen2.5-7b-em-
ppoRetriever .We address both sparse/lexical retrieval, as well as
neural re-ranking and dense retrieval:
(1)BM25 : A classic lexical retrieval model;
(2)MonoT5 : A retrieval-re-ranking pipeline with cross-encoder
BM25≫MonoT5. The top-20 documents retrieved by BM25 are
re-ranked by MonoT5 [32].
(3)E5: The bi-encoder model E5 [ 46] for general purpose retrieval.
It is commonly used as the retriever in RAG pipelines, including
in the original papers for Search-R1 and R1-Searcher.
The top-3 documents retrieved by each retrieval model are in-
corporated into the reasoning process. These models span a range
of retrieval strategies with varying effectiveness. In general, re-
ranking BM25 results with MonoT5 improves relevance, as the
cross-encoder can better model semantic similarity. However, the
utility of MonoT5 may be hindered by the limited recall offered by
BM25 (relevant documents not retrieved in the top 20 by BM25 can-
not be retrieved by MonoT5). As a semantic dense retrieval model,
E5 may retrieve passages that do not match any of the terms in the
queries - therefore, we expect that E5 should be more effective than
MonoT5 (and BM25).
All of the above RAG and retrieval pipelines are implemented
using PyTerrier [30]5and PyTerrier-RAG [29]6.
4.3 Query Performance Predictors
In our experiments, we apply QPP methods on the generated queries
to estimate how relevant the retrieval results are to those generated
queries, which may be related to the usefulness of those queries in
answering the original question. Specifically, we test:
Predictors based on Retrieval Scores .We experiment with
•NQC [37]: NQC is the classic QPP method based on the standard
deviation of retrieval scores for retrieved documents. It is appli-
cable to all three retrieval configurations. We use the retrieval
scores of the top-100 retrieved documents in NQC, which is a
reasonable setting to acquire accurate QPP estimates [45].
•Max(Score) [45]: For the neural retrievers, in which the retrieval
scores can more accurately reflect the relevance between a query-
document pair, there is a high likelihood that the score of the
top-ranked document likely acts as an effective estimator of query
performance. This predictor, denoted as Max(Score) [45], reflects
the estimated upper bound of the relevance of the retrieved con-
text, which is likely to be reliable due to a small cutoff on the
retrieved lists, as usually is the case in Agentic RAG.
Embedding-based Predictors .Additionally, for the bi-encoder
model (E5), which uses dense embeddings of the retrieved docu-
ments, we test two recent embedding-based QPP models:
•A-Pair-Ratio [45]: A coherence-based predictor that calculates
the ratio between the pairwise coherences of the top-ranked and
last-ranked documents in the retrieval result. In particular, we
compute this ratio among the first five and the last five documents
in the top-50 retrieved documents, as described in [45].
•Dense-QPP [12]: A recent geometric-based predictor that calcu-
lates the volume of the minimum hypercube encompassing the
5https://github.com/terrier-org/pyterrier
6https://github.com/terrierteam/pyterrier_rag

IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy Tian et al.
Table 1: Average quality (measured by EM & F1) of the answers
generated by Search-R1 and R1-Searcher on the Natural Questions
dataset, using various retrievers. We also report the average number
of reasoning/querying iterations (denoted 𝐼𝑡𝑒𝑟).
Model Search-R1 R1-Searcher
Retriever BM25 MonoT5 E5 BM25 MonoT5 E5
EM 0.3391 0.3873 0.4838 0.2089 0.2307 0.3075
F1 0.4185 0.4709 0.5687 0.2586 0.2865 0.3736
𝐼𝑡𝑒𝑟 2.52 2.19 2.00 2.20 2.03 1.99
Table 2: Spearman’s 𝜌correlation between number of iteration ( 𝐼𝑡𝑒𝑟)
and answer quality (F1).
Model Search-R1 R1-Searcher
Retriever BM25 MonoT5 E5 BM25 MonoT5 E5
𝜌(𝐼𝑡𝑒𝑟, F1) -0.2803 -0.2991 -0.3168 -0.1872 -0.1490 -0.1110
embeddings of the top- 𝑘retrieved documents and the query. We
calculate the hypercube based on the top 3 documents, as per
our RAG configuration.
Finally, considering that there can be multiple intermediate gen-
erated queries for answering an input question, when evaluating
the correlation between QPP estimates and the quality of generated
answers, in this initial study, we only consider the QPP estimates
for the first generated query.
5 RESULTS
We now report findings for each of our research questions.
5.1 RQ-1: Effect of different retriever
configurations on answer quality and
number of iterations
Table 1 presents the average answer quality (measured in terms of
EM and F1), and also the number of retrieval iterations ( 𝐼𝑡𝑒𝑟) ob-
tained with Search-R1 and R1-Searcher models using three different
retriever configurations. The results show that for both Search-R1
and R1-Searcher, more effective retrievers – MonoT5 and E5 – gen-
erally lead to fewer reasoning-retrieval iterations initiated by the
LLM before the final answer is generated.
Additionally, the answers generated using results from more ef-
fective retrievers tend to be of higher quality. For example, Searcher-
R1 achieves an F1 score of 0.4185 with an average of 2.52 iterations
when using BM25 as the retriever. Using the E5 retriever leads to
an increase in F1 score, to 0.5687, and this, in fact, is obtained with
a reduced average reasoning length of 2.0 iterations. These findings
suggest that retrieval effectiveness influences both the length of
the reasoning process and the quality of the generated answers.
Figure 3 provides a more detailed view on number of iterations,
across the 3610 NQ test queries for both Search-R1 (a) and R1-
Searcher (b). The dark part of each bar shows correct answers; the
light part shows wrong answers. From the plots of Figure 3, we
observe that, for both of the Agentic RAG models, using BM25 asthe retriever exhibit longer tails, indicating an extended reason-
ing process compared to models that use E5 and MonoT5 as the
retriever.
This observation raises a further question of whether the number
of iterations ( 𝐼𝑡𝑒𝑟) is correlated with average answer quality in
Agentic RAG. Table 2 reports the Spearman’s 𝜌correlation between
them within each RAG configuration. From the left side of the table,
we observe a moderate negative correlation between 𝐼𝑡𝑒𝑟and F1
score for Search-R1 (i.e. more iterations implies lower likelihood of
a good answer). For R1-Searcher, the correlations are weaker but
remain negative suggesting that shorter reasoning processes tend
to lead to higher answer quality.
Another observation for Search-R1 is that a more effective re-
triever shows a stronger negative correlation, e.g., the correlation
of both E5 and MonoT5 are lower than that of BM25. A plausible ex-
planation for this is that more effective retrieval models return doc-
uments that are likely to be more relevant to the generated queries
at each iteration, allowing the system to potentially generate better
follow-up queries, and reach the final answer with fewer iterations.
To conclude for RQ-1 , for both the Search-R1 and R1-Searcher
Agentic RAG models, adopting more effective retrievers can shorten
the reasoning process and lead to final answers of higher quality.
We now investigate whether QPP estimates of query performance
can provide any indication of answer quality.
5.2 RQ-2: Predicted Generated Query Quality
versus Iterations
Figures 4 and 5 show the average QPP values for queries generated
in the first five iterations across the test set. Each figure contains
three sub-plots, one for each retrieval model. Different QPP predic-
tors are applied for each retriever, based on their various applica-
bilities. Note that we normalise each predictor by z-score, to allow
different predictors (with different ranges) to be more easily shown
in a single plot. Finally, as noted earlier when analysing Figure 3,
few questions have more than 5 iterations, so we show QPP values
for iterations 1-5.
Overall, across both Figures 4 and 5, we observe a decreasing
trend in QPP estimates as the reasoning process progresses through
iterations. This suggests that the quality of retrieval, in terms of
relevance to the generated queries, generally deteriorates over iter-
ations. This overall downwards trend is consistent across the QPP
methods evaluated on both Search-R1 and R1-Searcher pipelines.
The only exception appears when NQC is applied to MonoT5, as
shown in Figure 4(b) and Figure 5(b).
However, this exception is explainable, as NQC is not an accurate
predictor for query performance in MonoT5. The score distribution
produced by MonoT5, as argued by Datta et al . [8], does not align
with the assumptions underlying the statistical QPP methods - such
as NQC. As a result of this, the NQC predictor may not reflect ac-
curately the relevance or the usefulness of the documents retrieved
for the purpose of LLM reasoning.
Additionally, these decreasing trends in the QPP estimates are
consistent for the better performing QPP models, e.g., Max(Score)
for Mono-T5. A likely explanation for this is that accurate QPP
estimates correlate with the usefulness of the intermediate re-
trieved documents in answering the generated query. For instance,

Am I on the Right Track? IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy
1 2 3 4 5 6 7
Number of Iterations0500100015002000Number of QueriesBM25
MonoT5
E5
(a) Search-R1.
1 2 3 4 5 6 7
Number of Iterations0500100015002000Number of QueriesBM25
MonoT5
E5 (b) R1-Searcher.
Figure 3: Distribution of the reasoning lengths (number of iterations) across 3610 NQ dataset questions for (a) Search-R1 and (b) R1-Searcher
agentic RAG models. For both these models, we show the number of iterations for the three retrieval models experimented with. In each bar,
the height of the part with a darker colour is the number of queries with a correct answer (EM=1). Notably, BM25 exhibits a longer tail with a
higher number of reasoning iterations compared to that of the neural retrieval models, MonoT5 and E5.
12345
iteration-0.200.2z-score
(a) BM2512345
iteration-0.8-0.6-0.4-0.200.2
(b) MonoT512345
iteration-0.8-0.6-0.4-0.200.2
(c) E5NQC Max(score) DenseQPP A-ratio
Figure 4: Average QPP estimates for the generated queries in each
reasoning-retrieval iteration for Search-R1. QPP estimates are nor-
malised by z-score.
12345
iteration-0.200.2z-score
(a) BM2512345
iteration-0.8-0.6-0.4-0.200.2
(b) MonoT512345
iteration-0.8-0.6-0.4-0.200.2
(c) E5
Figure 5: Average QPP estimates of generated queries in each
reasoning-retrieval iteration for R1-Searcher, legend as per Figure 4.
Max(Score) for Mono-T5 can be interpreted as an upper bound
on the topical relevance between the query and the retrieved doc-
uments. If this score is low, the retrieved content is unlikely to
satisfy the information need of the query – potentially prolonging
the overall reasoning process.
To conclude for RQ-2 , based on the predictions by the QPP
methods employed, it can be observed that the average quality of
queries generated by the Agentic RAG models tends to decrease
as the number of reasoning iterations increases. The most likely
reason for this is that the queries for which the answer generation
converges in a small number of iterations are likely to be more
central to the information need of the input question (and hence
expected to be of higher quality as estimated by a QPP model). On
the other hand, for questions that end up taking a larger number
of iterations, the generated answers are not of high quality (recall
the decreasing EM along iterations shown in Figure 3, and theTable 3: Spearman’s 𝜌correlation between QPP estimation (for the
query generated in the first iteration) and answer quality (measured
by F1), for both models with various retrievers. A ‘-’ value in the
table denotes that a predictor is not applicable for a given retriever
(as discussed in Section 4.3). All observed correlations are significant.
Agentic RAG Models
Search-R1 R1-Searcher
QPP BM25 MonoT5 E5 BM25 MonoT5 E5
NQC .1297 .0524 .2394 .1205 .0096 .0515
Max(Score) - .2383 .2369 - .1735 .0919
Dense-QPP - - .1871 - - .0834
A-Pair-Ratio - - .2497 - - .0297
negative correlations between the number of iterations and F1-score
from Table 2). It is likely that the lower answer quality associated
with an increase in the number of iterations is due to more noise
being added to the generated text for reasoning itself by the low-
quality retrievals. To better illustrate this phenomenon, we study
the outputs of two example queries of Search-R1 in Appendix A,
which shows that low-quality retrievals for off-topic intermediate
queries lead to a wrong answer with more iterations..
5.3 RQ-3: Correlation between Predicted Query
Performance and Answer Quality
We now to turn to examine if the QPP values of the queries are
correlated to the final answer quality. In particular, we postulate
that with high number of queries being answered in a single itera-
tion (see Section 5.1), the retrieval quality of the first query is most
likely to be indicative of the answer quality. Hence, we focus on
the correlation between the QPP score of the first generated query
and the F1 score of the final output answer. Table 3 reports the
observed correlations. From the table, we note that all correlations
are consistently positive, suggesting that a higher QPP estimate for
the first generated query tends to be associated with higher-quality
answers. However, these correlations are weak, and less than what
has been achievable for standard retrieval tasks (which can be as
high as Spearman’s 𝜌>0.5) [8,45]. This emphasises that QPPs are
not entirely capturing final answer quality.

IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy Tian et al.
Comparing the two Agentic RAG models, the observed correla-
tions are higher for Search-R1 than for R1-Searcher. Considering
the answer quality of these two Agentic RAG frameworks in Table 1,
QPP estimates may be better correlated with the quality of answers
generated by more effective RAG systems, in that Search-R1 makes
better use of the retrieved documents, and hence is more susceptible
to poor retrieval, leading to higher correlations.
When BM25 is used as the retriever, both pipelines show a weak
but positive correlation using the NQC predictor, around 0.12–0.13.
When the retrieval results are re-ranked by MonoT5, the correlation
for NQC decreases (as expected because NQC is not an effective
QPP model for neural re-rankers [ 8]). In contrast, the QPP method
Max(Score) achieves higher correlations, around 0.2. When E5 is
used for retrieval, all QPP predictors in the Search-R1 pipeline show
stronger correlations, particularly the coherence-based predictor
A-ratio [45], achieving the highest correlation at 0.25.
These observations indicate that QPP can serve as a useful signal
in predicting the answer quality in the setting of Agentic RAG.
However, in this initial study, we focus on correlating the answer
quality with the QPP about the first generated query; higher corre-
lation may be observed with improved aggregation methods of the
QPP estimates in the multi-iteration reasoning process [44].
To conclude for RQ-3 , we found that QPP estimates of the
queries generated in the first iteration of both Agentic RAG mod-
els are positively correlated with the quality of the final answer.
Although the correlations are weak, they indicate promise worth
further investigation.
6 CONCLUSIONS
In this work, we analysed the retrieval component in Agentic RAG
models, using QPP as a tool to estimate the retrieval quality for
queries generated by the Search-R1 and R1-Searcher models. Our
experiments show that, for both these models, more effective re-
trieval models (e.g., E5) tend to yield higher-quality answers with
fewer reasoning-retrieval iterations. Moreover, by evaluating QPP
estimates of the generated queries at each iteration, we observe that
retrieval quality tends to decrease over the course of the reasoning
process. A case study in Appendix A suggests that the ambiguity
of the original query may be a reason leading to this decrease in
retrieval quality. Finally, we observe a weak positive correlation
between the QPP estimations of the first generated query and the
final answer quality. While the correlations are weak, it, nonethe-
less, provides a potentially useful signal in predicting the quality of
generated answers. Overall, our results emphasise the importance
of the first retrieval iteration in final answer quality.
In terms of limitations, this study focused on just two models:
Search-R1 and R1-Searcher, while other Agentic RAG models, such
as Search-O1 [ 27] may exhibit different behaviours. Furthermore,
our experiments are conducted only on the NQ dataset, while Agen-
tic RAG models may be better suited to multi-hop QA questions,
such as HotpotQA [ 49]. We plan to address these limitations in a
future extension.
This work opens the doors to using IR techniques, including QPP,
to improve the searching behaviour of Agentic RAG models. For in-
stance, QPP may be used to adaptively decide whether the retrieval
results are useful enough to show to the reasoning model; alter-
natively, QPP estimates could serve as a reward signal during thetraining time to help the model learn to execute better queries. This
goes beyond the reward signals used in the training of R1-Searcher,
which only reward the model for making search requests and gen-
erating responses that are validly formatted [ 39]. We believe that,
in the era of RAG, applying QPP to enhance the LLM’s ability to
better utilise IR retrieval models is a promising research direction.
REFERENCES
[1]Marwah Alaofi, Negar Arabzadeh, Charles L. A. Clarke, and Mark Sanderson.
2024. Generative Information Retrieval Evaluation. arXiv:2404.08137 [cs.IR]
https://arxiv.org/abs/2404.08137
[2]Negar Arabzadeh, Amin Bigdeli, Morteza Zihayat, and Ebrahim Bagheri. 2021.
Query Performance Prediction Through Retrieval Coherency. In Advances in
Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual
Event, March 28 – April 1, 2021, Proceedings, Part II . Springer-Verlag, Berlin,
Heidelberg, 193–200. https://doi.org/10.1007/978-3-030-72240-1_15
[3]Negar Arabzadeh, Maryam Khodabakhsh, and Ebrahim Bagheri. 2021. BERT-QPP:
Contextualized Pre-trained transformers for Query Performance Prediction. In
Proceedings of the 30th ACM International Conference on Information & Knowledge
Management (Virtual Event, Queensland, Australia) (CIKM ’21) . Association for
Computing Machinery, New York, NY, USA, 2857–2861. https://doi.org/10.1145/
3459637.3482063
[4]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In
Proceedings of the 34th International Conference on Neural Information Processing
Systems (NIPS ’20) . Curran Associates Inc., Red Hook, NY, USA, Article 159.
[5]Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Eval-
uating Question Answering Evaluation. In Proceedings of the 2nd Workshop on
Machine Reading for Question Answering . Association for Computational Linguis-
tics, Hong Kong, China, 119–124. https://aclanthology.org/D19-5817/
[6]Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2002. Predicting query
performance (SIGIR ’02) . Association for Computing Machinery, New York, NY,
USA, 299–306. https://doi.org/10.1145/564376.564429
[7]Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare
Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The
Power of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (Washington DC, USA) (SIGIR ’24) . Association for Computing Machin-
ery, New York, NY, USA, 719–729. https://doi.org/10.1145/3626772.3657834
[8]Suchana Datta, Debasis Ganguly, Mandar Mitra, and Derek Greene. 2022. A
Relative Information Gain-based Query Performance Prediction Framework with
Generated Query Variants. ACM Trans. Inf. Syst. 41, 2, Article 38 (dec 2022),
31 pages. https://doi.org/10.1145/3545112
[9]Suchana Datta, Sean MacAvaney, Debasis Ganguly, and Derek Greene. 2022.
A ’Pointwise-Query, Listwise-Document’ based Query Performance Prediction
Approach. In Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval (Madrid, Spain) (SIGIR ’22) .
Association for Computing Machinery, New York, NY, USA, 2148–2153. https:
//doi.org/10.1145/3477495.3531821
[10] Abhimanyu Dubey et al .2024. The Llama 3 Herd of Models.
arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783
[11] Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy. 2022. On
the Origin of Hallucinations in Conversational Models: Is it the Datasets or the
Models?. In Proceedings of the 2022 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies .
Association for Computational Linguistics, Seattle, United States, 5271–5285.
https://aclanthology.org/2022.naacl-main.387/
[12] Guglielmo Faggioli, Nicola Ferro, Cristina Ioana Muntean, Raffaele Perego, and
Nicola Tonellotto. 2023. A Geometric Framework for Query Performance Pre-
diction in Conversational Search. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in Information Retrieval (Taipei,
Taiwan) (SIGIR ’23) . Association for Computing Machinery, New York, NY, USA,
1355–1365. https://doi.org/10.1145/3539618.3591625
[13] Jinyuan Fang, Zaiqiao Meng, and Craig Macdonald. 2025. KiRAG: Knowledge-
Driven Iterative Retriever for Enhancing Retrieval-Augmented Generation. arXiv
preprint arXiv:2502.18397 (2025).
[14] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting
hallucinations in large language models using semantic entropy. Nature (London)
630, 8017 (2024), 625–630.

Am I on the Right Track? IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy
[15] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi
Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented
Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL]
https://arxiv.org/abs/2312.10997
[16] Claudia Hauff, Djoerd Hiemstra, and Franciska de Jong. 2008. A survey of pre-
retrieval query performance predictors. In Proceedings of the 17th ACM Conference
on Information and Knowledge Management (Napa Valley, California, USA) (CIKM
’08). 1419–1420. https://doi.org/10.1145/1458082.1458311
[17] Bolei He, Nuo Chen, Xinran He, Lingyong Yan, Zhenkai Wei, Jinchang Luo,
and Zhen-Hua Ling. 2024. Retrieving, Rethinking and Revising: The Chain-of-
Verification Can Improve Retrieval Augmented Generation. In Findings of the
Association for Computational Linguistics: EMNLP 2024 . Association for Computa-
tional Linguistics, Miami, Florida, USA, 10371–10393. https://aclanthology.org/
2024.findings-emnlp.607/
[18] Ben He and Iadh Ounis. 2004. Inferring Query Performance Using Pre-retrieval
Predictors. In String Processing and Information Retrieval . 43–54.
[19] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.
Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reason-
ing Steps. In Proceedings of the 28th International Conference on Computational
Linguistics . 6609–6625. https://aclanthology.org/2020.coling-main.580/
[20] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with
Generative Models for Open Domain Question Answering. In Proceedings of the
16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume . Association for Computational Linguistics, Online,
874–880. https://aclanthology.org/2021.eacl-main.74/
[21] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo
Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.
2023. Atlas: few-shot learning with retrieval augmented language models. J.
Mach. Learn. Res. 24, 1, Article 251 (Jan. 2023), 43 pages.
[22] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-
Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval
Augmented Generation. In Proceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing . Association for Computational Linguistics,
Singapore, 7969–7992. https://doi.org/10.18653/v1/2023.emnlp-main.495
[23] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang,
Hamed Zamani, and Jiawei Han. 2025. Search-R1: Training LLMs to Reason and
Leverage Search Engines with Reinforcement Learning. arXiv:2503.09516 [cs.CL]
https://arxiv.org/abs/2503.09516
[24] Ashutosh Joshi, Sheikh Muhammad Sarwar, Samarth Varshney, Sreyashi Nag,
Shrivats Agrawal, and Juhi Naik. 2024. REAPER: Reasoning based Retrieval
Planning for Complex RAG Systems. In Proceedings of the 33rd ACM International
Conference on Information and Knowledge Management (Boise, ID, USA) (CIKM
’24). Association for Computing Machinery, New York, NY, USA, 4621–4628.
https://doi.org/10.1145/3627673.3680087
[25] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,
Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.
Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A
Benchmark for Question Answering Research. Transactions of the Association for
Computational Linguistics 7 (2019), 452–466. https://aclanthology.org/Q19-1026/
[26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Proceedings of the 34th International Conference
on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS ’20) .
Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages. https://arxiv.
org/abs/2005.11401
[27] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian
Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic Search-Enhanced Large
Reasoning Models. arXiv:2501.05366 [cs.AI] https://arxiv.org/abs/2501.05366
[28] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models
Use Long Contexts. Transactions of the Association for Computational Linguistics
12 (2024), 157–173. https://doi.org/10.1162/tacl_a_00638
[29] Craig Macdonald, Jinyuan Fang, Andrew Parry, and Zaiqiao Meng. 2025. Con-
structing and Evaluating Declarative RAG Pipelines in PyTerrier. In Proceedings
of SIGIR 2025 .
[30] Craig Macdonald, Nicola Tonellotto, Sean MacAvaney, and Iadh Ounis. 2021.
PyTerrier: Declarative Experimentation in Python from BM25 to Dense Retrieval.
InProceedings of the 30th ACM International Conference on Information & Knowl-
edge Management (Virtual Event, Queensland, Australia) (CIKM ’21) . 4526–4533.
https://doi.org/10.1145/3459637.3482013
[31] Chuan Meng, Negar Arabzadeh, Mohammad Aliannejadi, and Maarten de Rijke.
2023. Query Performance Prediction: From Ad-hoc to Conversational Search.
InProceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval (Taipei, Taiwan) (SIGIR ’23) . Association
for Computing Machinery, New York, NY, USA, 2583–2593. https://doi.org/10.
1145/3539618.3591919[32] Rodrigo Frassetto Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020.
Document Ranking with a Pretrained Sequence-to-Sequence Model. In Findings
of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20
November 2020 (Findings of ACL, Vol. EMNLP 2020) . Association for Computational
Linguistics, 708–718.
[33] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-
Wei Chang. 2021. Retrieval Augmented Code Generation and Summarization. In
Findings of the Association for Computational Linguistics: EMNLP 2021 . Association
for Computational Linguistics, Punta Cana, Dominican Republic, 2719–2734.
https://aclanthology.org/2021.findings-emnlp.232/
[34] Nirmal Roy, Leonardo F. R. Ribeiro, Rexhina Blloshmi, and Kevin Small. 2024.
Learning When to Retrieve, What to Rewrite, and How to Respond in Conversa-
tional QA. In Findings of the Association for Computational Linguistics: EMNLP
2024. Association for Computational Linguistics, Miami, Florida, USA, 10604–
10625. https://aclanthology.org/2024.findings-emnlp.622/
[35] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in
Retrieval-Augmented Generation. In Proceedings of the 47th International ACM
SIGIR Conference on Research and Development in Information Retrieval (Wash-
ington DC, USA) (SIGIR ’24) . Association for Computing Machinery, New York,
NY, USA, 2395–2400. https://doi.org/10.1145/3626772.3657957
[36] Anna Shtok, Oren Kurland, and David Carmel. 2010. Using statistical decision
theory and relevance models for query-performance prediction. In Proceedings
of the 33rd International ACM SIGIR Conference on Research and Development in
Information Retrieval (Geneva, Switzerland) (SIGIR ’10) . 259–266. https://doi.org/
10.1145/1835449.1835494
[37] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. 2012.
Predicting Query Performance by Query-Drift Estimation. ACM Trans. Inf. Syst.
30, 2, Article 11 (May 2012), 35 pages. https://doi.org/10.1145/2180868.2180873
[38] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. 2025.
Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG.
arXiv:2501.09136 [cs.AI] https://arxiv.org/abs/2501.09136
[39] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin
Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-Searcher: Incentivizing the Search
Capability in LLMs via Reinforcement Learning. arXiv:2503.05592 [cs.AI] https:
//arxiv.org/abs/2503.05592
[40] Yongquan Tao and Shengli Wu. 2014. Query Performance Prediction By Consid-
ering Score Magnitude and Variance Together. In Proceedings of the 23rd ACM
International Conference on Conference on Information and Knowledge Manage-
ment (Shanghai, China) (CIKM ’14) . Association for Computing Machinery, New
York, NY, USA, 1891–1894. https://doi.org/10.1145/2661829.2661906
[41] Fangzheng Tian, Debasis Ganguly, and Craig Macdonald. 2025. Is Relevance
Propagated from Retriever to Generator in RAG?. In Advances in Information
Retrieval: 47th European Conference on Information Retrieval, ECIR 2025, Lucca,
Italy (Lucca, Italy). 18 pages.
[42] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-
Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) . 10014–10037.
[43] Neeraj Varshney, Man Luo, and Chitta Baral. 2022. Can Open-Domain QA Reader
Utilize External Knowledge Efficiently like Humans? arXiv:2211.12707 [cs.CL]
https://arxiv.org/abs/2211.12707
[44] Maria Vlachou. 2025. Predicting Retrieval Failures in Conversational Recommen-
dation Systems . Ph.D. thesis. University of Glasgow, Glasgow, UK.
[45] Maria Vlachou and Craig Macdonald. 2024. Coherence-based Query Performance
Measures for Dense Retrieval. In Proceedings of the 2024 ACM SIGIR International
Conference on Theory of Information Retrieval (Washington DC, USA) (ICTIR
’24). Association for Computing Machinery, New York, NY, USA, 15–24. https:
//doi.org/10.1145/3664190.3672518
[46] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,
Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised
contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).
[47] Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. 2024.
RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-
Horizon Generation. arXiv:arXiv:2403.05313
[48] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang,
Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang
Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue,
Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia,
Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,
Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical
Report. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115
[49] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for
Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing . Association for
Computational Linguistics, Brussels, Belgium, 2369–2380. https://aclanthology.
org/D18-1259/

IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy Tian et al.
A APPENDIX
In this Appendix, we study one negative example (A.1) and one positive example (A.2) output from Search -R1 with retriever E5. The
purpose is to investigate the reasons leading to the degradation of average retrieval quality in later iterations. A.3 presents a brief insight
from the analysis of the examples.
A.1 Negative Example: Ambiguous Question Causes Drifted Sub-Queries and Low-Quality Answer
Figure 6 shows the studied negative example. In this example, the input question “chris cornell she’s going to change the world” asks for
the name of the Chris Cornell song that contains the lyric “She’s going to change the world.” The query is formulated very ambiguously;
consequently, it confuses the Agentic RAG model. The correct answer should be “Can’t Change Me”, but the RAG model instead yields Eric
Clapton, a singer known for the song “Change the World”.
To answer this question, Search -R1 invokes the retriever (E5) three times. In the first retrieval, the intermediate query is identical to the
input. From the reasoning preceding the search, we observe that the LLM treats “She’s going to change the world” as a song title rather than
a lyric excerpt. In the second retrieval, the intermediate query omits “Chris Cornell”, one of the two key entities from the original question.
As a result, the topic drifts significantly. In the third search, the retrieved documents focus on a song titled “Changed the World”, which is
only weakly related to the intermediate query, indicating a lower-quality retrieval.
The QPP estimates (using Max(Score)) for the three iterations are 0.8370,0.7870 and 0.8511, which are lower than those in the positive
examples discussed in the next section (Max(Score) >0.9). It is worth noting that retrieval quality does not necessarily decline monotonically.
In this case, the ambiguity in the original question causes the LLM to misinterpret the user’s information need, ultimately leading to an
incorrect answer.
A.2 Positive Example: Specific Question with High Quality Retrievals
Figure 7 shows the positive example under study. In this case, the input question: “when was the last time villanova won the ncaa
basketball championship?” represents a very clear information need. The Search -R1 model uses two reasoning-retrieval iterations to answer
this question. After the first retrieval, which reformulates the original query, the model initiates a second search — another reformulation of
the same question. After two rounds, the model correctly outputs 2018 as the answer.
The QPP estimates (using Max(Score)) for these two iterations are 0.9263 and 0.9268, respectively. Compared to the negative example
with three iterations, these high values indicate strong retrieval quality. Such high-quality retrievals enable the RAG model to arrive at the
correct answer. Since questions requiring fewer iterations tend to rely on more effective intermediate retrievals, their average estimated
query performance is generally higher in earlier iterations.
However, this example also reveals a persistent issue in Search -R1 and other agentic models: repetition of intermediate queries. With
E5 as the retriever and a cutoff of 3 iterations, 262out of 3610 queries generated identical sub-queries during reasoning. This indicates
insufficient utilisation of retrieved documents in current Agentic models. Incorporating QPP or other signals into the LLM could help achieve
more efficient use of retrieved information.
A.3 Insights from the Case Study
From the above analysis, we observe that questions requiring more iterations in their reasoning process tend to involve lower-quality
intermediate retrievals, which do not positively contribute to the correct final answer. Conversely, reasoning trajectories that lead to correct
answers generally involve fewer iterations, generally yielding higher-quality retrievals. When we compute the average (predicted) retrieval
quality by iteration, the early iterations — populated by questions with shorter reasoning chains - exhibit higher mean quality than later
iterations.
However, in the positive example (see Appendix A.2), we still observe the current Agentic RAG models repeating identical queries in
intermediate retrieval steps. This redundancy can reduce the efficiency of the system and underscores the need to improve the Agentic
workflow further.

Am I on the Right Track? IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy
Output - Iteration 1
<think>I need to determine if Chris Cornell sang the song “She’s Going to Change the World”. I’ll search for it.</think> <search>
chris cornell She’s Going to Change the World </search>
<information> Doc 1(Title: “Chris Cornell”) called “Chris Cornell”. According to lead singer Shirley Manson, she was obsessed
with Cornell at the time of the release of Soundgarden’s hit song “Black Hole Sun”. A demo version of “Fix Me Now” has Manson
singing Cornell’s name. The line “I’m looking California, and feeling Minnesota” from Soundgarden’s song “Outshined” (written by
Cornell), inspired the title of Steven Baigelman’s 1996 film “Feeling Minnesota”, starring Keanu Reeves and Cameron Diaz. Cornell’s
song “Preaching the End of the World” from his debut solo album “Euphoria Morning”, inspired the title of Lorene Scafaria’s 2012
film “Seeking a Friend for the End of
Doc 2(Title: “Chris Cornell”) the World”, starring Steve Carell and Keira Knightley. In 1985, Cornell started dating Susan Silver,
the manager of Alice in Chains, Soundgarden and Screaming Trees, and they got married in 1990. They had a daughter, Lillian Jean,
born in June 2000. He and Silver divorced in 2004. In December 2008, Cornell reported via his official website that he had finally
won back his collection of 15 guitars after a four-year court battle with Silver. In 2004, he married Vicky Karayiannis, a Paris-based
American publicist of Greek heritage. The couple had a daughter together, Toni, in September 2004, and a son,
Doc 3(Title: “Chris Cornell”) Chris Cornell Christopher John Cornell (né Boyle; July 20, 1964 – May 18, 2017) was an American
musician, singer and songwriter. He was best known as the lead vocalist for the rock bands Soundgarden and Audioslave. Cornell
was also known for his numerous solo works and soundtrack contributions since 1991, and as the founder and frontman for Temple
of the Dog, the one-off tribute band dedicated to his late friend Andrew Wood. Cornell is considered one of the chief architects of the
1990s grunge movement, and is well known for his extensive catalog as a songwriter, his nearly four-octave vocal </information>
Output - Iteration 2
<think>I found out that Chris Cornell sang for the rock bands Soundgarden and Audioslave. Now I need to check if he sang the
song “She’s Going to Change the World”. I’ll search for it.</think> <search> She’s Going to Change the World </search>
<information> Doc 1(Title: “How to Change the World”) It was very difficult to start a private organization at first, since Hungary
was a communist country. So she started with a small group of disabled individuals and taught them simple skills that they could
use to work a job, and that grew larger and changed over the years. It eventually became a functional and beautiful community
where disabled individuals could come and take up a residency, learn a skill and make a living, and learn how to live independently
if possible. There were many times where she failed, or her efforts seemed to be going nowhere. But she never
Doc 2(Title: “How to Change the World”) method, by stern discipline, by rigid attention to detail, by ceaseless labor, by the fixed
determination of an indomitable will.” She knew she had the ability to make an extremely beneficial impact, and even though it
reversed society’s norm, she succeeded in fulfilling her dreams. Florence Nightingale will forever be remembered as a great social
entrepreneur for her heroic work in the field of nursing. Nightingale’s “indomitable will” may have been her most instrumental trait
in accomplishing her radical ideas and plans. Beginning with strong resistance from her parents, and continuing to the point of
defying army officers, her calling
Doc 3(Title: “How to Change the World”) was a constant struggle against society’s standards. Like Nightingale, every social
entrepreneur will face opposition at some point in their career. It comes with the very definition of an entrepreneur. Their job is to
defy cultural norms to find revolutionary, though often seemingly insane, ideas to solve the problems of life. “It takes concentrated
focus, practical creativity, and a long-term source of energy to advance a system change and to ensure that the change becomes well
rooted in institutions and cultures.” Social entrepreneurs must have the courage and indomitable will of Florence Nightingale to
succeed in a world so resistant </information>

IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy Tian et al.
Output - Iteration 3
<think>I found out that “She’s Going to Change the World” is a song, but I still need to check if Chris Cornell sang it. I’ll search for
it.</think> <search> who sang She’s Going to Change the World </search>
<information> Doc 1(Title: “Change the World”) Change the World “Change the World” is a song written by Tommy Sims,
Gordon Kennedy, and Wayne Kirkpatrick whose best-known version was recorded by the British recording artist Eric Clapton for
the soundtrack of the 1996 film, “Phenomenon”. The track was produced by R&B record producer Kenneth “Babyface” Edmonds.
The single release, Clapton recorded for Reprise and Warner Bros. Records, reached the Top 40 in twenty countries and topped the
charts in Canada as well as “Billboard” magazine’s Adult Contemporary and Adult Top 40 charts in the United States. The single
was prized with eight awards, among them three Grammy
Doc 2(Title: “Gordon Kennedy (musician)”) 20 years of friendship, Kennedy remains Frampton’s collaborator and performs with
him often on tour. As of 2017, Kennedy has composed for or recorded with Bonnie Raitt (five songs, including “Gypsy in Me”), Reba
McEntire, Tim McGraw, Wynonna, Charlie Daniels, Trisha Yearwood, George Strait, Faith Hill (“It Will Be Me”), Carrie Underwood
(“The More Boys I Meet”), and others. The enduring worldwide hit song, “Change the World”, earned Kennedy a Grammy for “Song
of the Year” in 1996. The song was first recorded in 1995 by Wynonna Judd and later Eric Clapton through the release of the John
Travolta film,
Doc 3(Title: “Change the World”) the line “If I could change the world...”, leading to the second link, which is repeated three
times, before the song’s outro, which is identical to the song’s intro and interlude. Before Clapton’s hit version was released, the
song was recorded by country artist Wynonna Judd for her February 1996 album, “Revelations” (MCA Records). Her three-minute,
nineteen-second take on the song is more of a neo-traditionalist country music track. Judd did not release her version as a single.
Judd’s interpretation was also released on the compilation album “Best of America, Volume 2” for Curb Records on June 3, 2003. Eric
</information>
Output - Answer
<think>I found out that the song “She’s Going to Change the World” was sung by Eric Clapton. Now I can provide the answer.</think>
<answer> Eric Clapton </answer>
Figure 6: The output from Search-R1 for the negative example “chris cornell she’s going to change the world”. In this example, the intermediate
queries gradually drift away from the original information need, eventually leading to the wrong answer.

Am I on the Right Track? IR-RAG Workshop at SIGIR ’25, July 17, 2025, Padua, Italy
Output - Iteration 1
<think>I need to find the last time Villanova won the NCAA basketball championship. I’ll search for it.</think> <search> when
was the last time Villanova won the NCAA basketball championship </search>
<information> Doc 1(Title: “Villanova Wildcats men’s basketball”) a team for the game. Their 2016 NCAA Championship, is
referred to as “The Perfect Ending” and is the only NCAA Men’s Championship game to be won on a buzzer beater, as Kris Jenkins
drained a shot as time expired. They made the Final Four in 1939, 1971, 1985, 2009, 2016 and 2018; their six Final Four appearances
are 13th most all-time. As of 2018, they have an NCAA Tournament record of 64–36 (). Villanova has defeated six No. 1 seeds in the
NCAA tournament (Michigan and Georgetown in 1985, Pittsburgh in 2009, and Kansas and North Carolina in 2016,
Doc 2(Title: “Villanova Wildcats men’s basketball”) and Kansas in 2018), which is sixth most all-time. The Villanova Wildcats
have appeared in the NCAA Tournament 37 times, the eighth highest total in NCAA history. They have won the Big East regular
season championship eight times, most recently winning four straight from 2014 to 2017. They won the Big East Tournament
in 1995, 2015, 2017, and 2018. Villanova entered the 2016–2017 season with an all-time winning percentage of (.648), placing the
Wildcats tied for 13th among all NCAA Division I basketball programs. Through 2018, Villanova has 1,753 wins, which is 23rd
among Division I men’s basketball teams. Villanova
Doc 3(Title: “Villanova Wildcats men’s basketball”) and were the national champions in 1985, 2016, and 2018. Villanova has
won as the underdog (based on Tournament seeding) 16 times, more than any other program. Villanova is one of only two programs
(the other being Ohio State) that has played in the NCAA Tournament in every decade since the 1930s. “The NCAA began seeding
the tournament with the 1979 edition.” The Wildcats have appeared in the National Invitation Tournament (NIT) 17 times. Their
combined record is 24–17. They were NIT Champions in 1994. The Wildcats appeared in the only National Campus Basketball
Tournament. Their record is 0–1. </information>
Output - Iteration 2
<think>I found out that Villanova won the NCAA basketball championship in 1985, 2016, and 2018. Now I need to find the last time
they won the championship.</think> <search> last time Villanova won the NCAA basketball championship </search>
<information> Doc 1(Title: “Villanova Wildcats men’s basketball”) a team for the game. Their 2016 NCAA Championship, is
referred to as “The Perfect Ending” and is the only NCAA Men’s Championship game to be won on a buzzer beater, as Kris Jenkins
drained a shot as time expired. They made the Final Four in 1939, 1971, 1985, 2009, 2016 and 2018; their six Final Four appearances
are 13th most all-time. As of 2018, they have an NCAA Tournament record of 64–36 (). Villanova has defeated six No. 1 seeds in the
NCAA tournament (Michigan and Georgetown in 1985, Pittsburgh in 2009, and Kansas and North Carolina in 2016,
Doc 2(Title: “Villanova Wildcats men’s basketball”) and Kansas in 2018), which is sixth most all-time. The Villanova Wildcats
have appeared in the NCAA Tournament 37 times, the eighth highest total in NCAA history. They have won the Big East regular
season championship eight times, most recently winning four straight from 2014 to 2017. They won the Big East Tournament
in 1995, 2015, 2017, and 2018. Villanova entered the 2016–2017 season with an all-time winning percentage of (.648), placing the
Wildcats tied for 13th among all NCAA Division I basketball programs. Through 2018, Villanova has 1,753 wins, which is 23rd
among Division I men’s basketball teams. Villanova
Doc 3(Title: “Villanova Wildcats men’s basketball”) and were the national champions in 1985, 2016, and 2018. Villanova has
won as the underdog (based on Tournament seeding) 16 times, more than any other program. Villanova is one of only two programs
(the other being Ohio State) that has played in the NCAA Tournament in every decade since the 1930s. “The NCAA began seeding
the tournament with the 1979 edition.” The Wildcats have appeared in the National Invitation Tournament (NIT) 17 times. Their
combined record is 24–17. They were NIT Champions in 1994. The Wildcats appeared in the only National Campus Basketball
Tournament. Their record is 0–1. </information>
Output - Answer
<think>I found out that Villanova won the NCAA basketball championship in 1985, 2016, and 2018. The last time Villanova won the
championship was in 2018.</think> <answer> 2018 </answer>
Figure 7: The output from Search-R1 for the positive example “when was the last time villanova won the ncaa basketball championship”.
In this example, through multiple retrievals based on reformulations of the original query, Search-R1 model reaches the correct answer.