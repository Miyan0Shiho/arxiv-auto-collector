# HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation

**Authors**: Wen-Sheng Lien, Yu-Kai Chan, Hao-Lung Hsiao, Bo-Kai Ruan, Meng-Fen Chiang, Chien-An Chen, Yi-Ren Yeh, Hong-Han Shuai

**Published**: 2026-02-16 05:15:55

**PDF URL**: [https://arxiv.org/pdf/2602.14470v1](https://arxiv.org/pdf/2602.14470v1)

## Abstract
Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.

## Full Text


<!-- PDF content starts -->

HyperRAG: Reasoning N-ary Facts over Hypergraphs for
Retrieval Augmented Generation
Wen-Sheng Lien
National Yang Ming Chiao Tung
University
Hsinchu, Taiwan
vincentlien.ii13@nycu.edu.twYu-Kai Chan
National Yang Ming Chiao Tung
University
Hsinchu, Taiwan
ctw33888.ee13@nycu.edu.twHao-Lung Hsiao
National Yang Ming Chiao Tung
University
Hsinchu, Taiwan
hlhsiao.cs13@nycu.edu.tw
Bo-Kai Ruan
National Yang Ming Chiao Tung
University
Hsinchu, Taiwan
bkruan.ee11@nycu.edu.twMeng-Fen Chiang
National Yang Ming Chiao Tung
University
Hsinchu, Taiwan
meng.chiang@nycu.edu.twChien-An Chen
E.SUN Bank
Taipei, Taiwan
lukechen-15953@esunbank.com
Yi-Ren Yeh
National Kaohsiung Normal
University
Kaohsiung, Taiwan
yryeh@nknu.edu.twHong-Han Shuai
National Yang Ming Chiao Tung
University
Hsinchu, Taiwan
hhshuai@nycu.edu.tw
Abstract
Graph-based Retrieval-Augmented Generation (RAG) typically op-
erates on binary Knowledge Graphs (KGs). However, decomposing
complex facts into binary triples often leads to semantic fragmenta-
tion and longer reasoning paths, increasing the risk of retrieval drift
and computational overhead. In contrast, ğ‘›-ary hypergraphs pre-
serve high-order relational integrity, enabling shallower and more
semantically cohesive inference. To exploit this topology, we pro-
poseHyperRAG, a framework tailored for ğ‘›-ary hypergraphs fea-
turing two complementary retrieval paradigms: (i) HyperRetriever
learns structural-semantic reasoning over ğ‘›-ary facts to construct
query-conditioned relational chains. It enables accurate factual
tracking, adaptive high-order traversal, and interpretable multi-hop
reasoning under context constraints. (ii) HyperMemory leverages
the LLMâ€™s parametric memory to guide beam search, dynamically
scoringğ‘›-ary facts and entities for query-aware path expansion.
Extensive evaluations on WikiTopics (11 closed-domain datasets)
and three open-domain QA benchmarks (HotpotQA, MuSiQue,
and 2WikiMultiHopQA) validate HyperRAGâ€™s effectiveness. Hy-
perRetriever achieves the highest answer accuracy overall, with
average gains of 2.95% in MRR and 1.23% in Hits@10 over the
strongest baseline. Qualitative analysis further shows that Hyper-
Retriever bridges reasoning gaps through adaptive and interpretable
ğ‘›-ary chain construction, benefiting both open and closed-domain
QA. Our codes are publicly available at https://github.com/Vincent-
Lien/HyperRAG.git.
This work is licensed under a Creative Commons Attribution 4.0 International License.
WWW â€™26, Dubai, United Arab Emirates.
Â©2026 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-2307-0/2026/04
https://doi.org/10.1145/3774904.3792710CCS Concepts
â€¢Information systems â†’Retrieval models and ranking;Lan-
guage models;Question answering.
Keywords
Hypergraph-based Retrieval-Augmented Generation, N-ary Rela-
tional Knowledge Graphs, Multi-hop Question Answering, Memory-
Guided Adaptive Retrieval
ACM Reference Format:
Wen-Sheng Lien, Yu-Kai Chan, Hao-Lung Hsiao, Bo-Kai Ruan, Meng-Fen
Chiang, Chien-An Chen, Yi-Ren Yeh, and Hong-Han Shuai. 2026. Hyper-
RAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented
Generation. InProceedings of the ACM Web Conference 2026 (WWW â€™26),
April 13â€“17, 2026, Dubai, United Arab Emirates.ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3774904.3792710
1 Introduction
Retrieval-Augmented Generation (RAG) has established itself as a
critical mechanism for augmenting Large Language Models (LLMs)
with non-parametric external knowledge during inference [12, 17,
19,20]. By dynamically retrieving verifiable information from ex-
ternal corpora without the need for extensive fine-tuning, RAG
effectively mitigates intrinsic LLM limitations such as hallucina-
tions and temporal obsolescence. This paradigm has proven par-
ticularly transformative for knowledge-intensive tasks, including
open-domain question answering (QA), fact verification, and com-
plex information extraction, driving significant innovation across
both academia and industry.
Current RAG methodologies broadly fall into three categories:
document-based, graph-based, and hybrid approaches. Document-
based methods utilize dense vector retrieval to match queries with
textual segments, offering scalability but often failing to capture
complex structural dependencies [ 5,6]. Conversely, graph-basedarXiv:2602.14470v1  [cs.CL]  16 Feb 2026

WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates. Wen-Sheng Lien et al.
TV 101
Sam
Pillsbury
Eric
Laneuville
English
California(b) Hyper graph
Bruce  
Seth Gr een
Sam
WeismanBruce Seth Green, Sam
Weisman, Sam Pillsbury, and
Eric Laneuville directs TV 101 in
English in California.N-ary relation
Relational chain
EntityHyperedge Question: What other shows or movies were directed by directors
who also directed shows that Bruce Seth Green directed?
(a) Kno wledge Gr aph
Bruce 
Seth GreenTV 101
Unchained
Hear t
Sam
Weisman
Sam
Pillsbur y
Born  
Into ExileDickie
Rober ts
Eric
Laneuville
TV 101Film Film DirectorBinary relation
Relational chain
Entity
Figure 1: Structural Comparison of (a) Knowledge Graphs
and (b) Hypergraphs. For a given question ğ‘, (a) requires
3-hop reasoning over binary facts, while (b) enables single-
hop inference via an ğ‘›-ary relational fact, yielding a more
compact and expressive multi-entity representation.
methods leverage Knowledge Graphs (KGs) to explicitly model re-
lationships, enabling multi-hop reasoning over structured data [ 15,
31]. Hybrid approaches attempt to bridge these paradigms, bal-
ancing comprehensiveness with efficiency. However, despite the
reasoning potential of graph-based methods, the prevailing reliance
on binary KGs presents fundamental topological limitations.
Traditional graph-based RAG methods predominantly rely on bi-
nary knowledge graphs, which suffer from notable limitations when
applied to closed-domain question-answering scenarios. Specifi-
cally, binary KG approaches encounter two fundamental structural
limitations. First,Semantic Fragmentationarises because binary
relations limit the expressiveness required to capture complex multi-
entity interactions, forcing the decomposition of holistic facts into
disjoint triples that fail to represent intricate semantic nuances.
Second, this fragmentation leads toPath Explosion, where con-
ventional approaches incur significant computational costs due to
the need for deep traversals over the vast binary relation space to
reconnect these facts, enabling error propagation and undermin-
ing real-world practicality [ 18,37]. To address these limitations,
recent work advocates hypergraphs for structured retrieval in RAG.
Hypergraphs natively encode higher-order ( ğ‘›-ary) relations that
bind multiple entities and roles, providing a richer semantic sub-
strate than binary graphs [ 26]. As illustrated in Figure 1, the Path
Explosion issue is evident when answering a question grounded on
the topic entity â€œBruce Seth Green,â€ which requires a 3-hop binary
traversal on a standard KG. In contrast, this reduces to a single
hop through an ğ‘›-ary relation in a hypergraph, yielding a more
compact representation. Hypergraphs enable the direct modelingof higher-order relational chains, effectively mitigating Semantic
Fragmentation and reducing the reasoning steps required to capture
complex dependencies.
Motivated by these insights, we introduceHyperRAG, an inno-
vative retrieval-augmented generation framework designed explic-
itly for reasoning over ğ‘›-ary hypergraphs. HyperRAG integrates
two novel adaptive retrieval variants: (i)HyperRetriever, which uses
a multilayer perceptron (MLP) to fuse structural and semantic em-
beddings, constructing query-conditioned relational chains that
enable accurate and interpretable evidence aggregation within con-
text and token constraints; and (ii)HyperMemory, which leverages
the parametric memory of an LLM to guide beam search, dynam-
ically scoring ğ‘›-ary facts and entities for query-adaptive path ex-
pansion. By combining higher-order reasoning with shallower yet
more expressive chains that locate key evidence without multi-hop
traversal. Replacement of the ğ‘›-ary structure with a binary reduces
the average MRR from36 .45%to34.15%and the average Hits@10
from40.59%to36.82%(Table 3), indicating gains in response quality.
Our key contributions are summarized as follows.
â€¢We propose HyperRAG, a pioneering framework that shifts the
graph-RAG paradigm from binary triples to ğ‘›-ary hypergraphs,
tackling the issues of semantic fragmentation and path explosion.
â€¢We introduce HyperRetriever, a trainable MLP-based retrieval
module that fuses structural and semantic signals to extract pre-
cise, interpretable evidence chains with low latency.
â€¢We develop HyperMemory, a synergistic retrieval approach that
utilizes LLM parametric knowledge to guide symbolic beam
search over hypergraphs for complex query adaptive reasoning.
â€¢Extensive evaluation across closed-domain and open-domain
benchmarks demonstrates that HyperRAG consistently outper-
forms strong baselines, offering a superior trade-off between
retrieval accuracy, reasoning interpretability, and system latency.
2 Preliminaries
2.1 Background
Definition 2.1( ğ‘›-ary Relational Knowledge Graph).An ğ‘›-ary
relational knowledge graph, or hypergraph, represents relational
facts involving two or more entities and one or more relations.
Formally, following the definition in [ 43], a hypergraph is defined
asG=(E ,R,F), whereEdenotes the set of entities, Rdenotes the
set of relations, and Fthe set ofğ‘›-ary relational facts (hyperedges).
Eachğ‘›-ary factğ‘“ğ‘›âˆˆF, which consists of two or more entities, is
represented as: ğ‘“ğ‘›={ğ‘’ğ‘–}ğ‘›
ğ‘–=1, where{ğ‘’ğ‘–}ğ‘›
ğ‘–=1âŠ†E is a set ofğ‘›entities
withğ‘›â‰¥2.
Unlike binary knowledge graphs, ğ‘›-ary representation inher-
ently captures higher-order relational dependencies among multi-
ple entities. ğ‘›-ary relations cannot be faithfully decomposed into
combinations of binary relations without losing structural integrity
or introducing ambiguity in semantic interpretation [ 1,9,35]. We
formalize faithful reduction and show that any straightforward bi-
nary scheme violates at least one of: (i) recoverability of the original
tuples, (ii) role preservation, or (iii) multiplicity of co-participations.
Please refer to Appendix A for more details on the recoveryability
of role-preserving hypergraph reduction, roles, and multiplicity.

HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates.
2.2 Problem Formulation
Problem(Hypergraph-based RAG).Given a question ğ‘, a hyper-
graphGrepresenting ğ‘›-ary relational structures, and a collection
of source documents D, the goal of hypergraph-based retrieval-
augmented generation (RAG) is to generate faithful and contextu-
ally grounded answers ğ‘by leveraging salient multi-hop relational
chains fromGand extracting relevant textual evidence fromD.
Complexity: Native ğ‘›-ary Hypergraph Retrieval.Let ğ‘ğ‘’=|E|,
ğ‘ğ‘“=|F| , and Â¯ğ‘›be the average arity. A query binds ğ‘˜role-typed
arguments, ğ‘={(ğ‘Ÿğ‘–:ğ‘ğ‘–)}ğ‘˜
ğ‘–=1, and asks for the remaining ğ‘›âˆ’ğ‘˜ roles.
We maintain sorted posting lists over role incidences, P(ğ‘Ÿ:ğ‘)=
{ğ‘“âˆˆF :(ğ‘Ÿ:ğ‘)âˆˆğ‘“} , with length ğ‘‘(ğ‘Ÿ:ğ‘) . To answer ğ‘, theğ‘›-ary
based retriever intersects the ğ‘˜posting listsby hyperedge IDsand
reads the missing roles from each surviving hyperedge. Let ğ‘›â˜…be
the (max/avg) arity among matches. The running time is given by:
ğ‘‡HYP(ğ‘)=Oğ‘˜âˆ‘ï¸
ğ‘–=1ğ‘‘(ğ‘Ÿğ‘–:ğ‘ğ‘–) +out
,(1)
where outis the number of matching facts. In typical schemas, the
relation arity is often bounded by a small constant (e.g., triadic,
ğ‘›â‰¤3). As a result, for each match the retriever touches exactly
one hyperedge record to materialize the unbound roles, yielding
per-outputoverheadO(1).
Complexity: Standard Binary KG Retrieval.Suppose each ğ‘›-
ary factğ‘“is reified as an event node ğ‘’ğ‘“withğ‘›role-typed binary
edges (e.g., roleğ‘—(ğ‘’ğ‘“,ğ‘ğ‘—)). For each binding (ğ‘Ÿğ‘–:ğ‘ğ‘–), use the list of
event IDs postedPevent(ğ‘Ÿğ‘–:ğ‘ğ‘–)and intersect the ğ‘˜lists to obtain
candidate events to mirror the hypergraph intersection. For each
survivingğ‘’ğ‘“, follow its remaining (ğ‘›âˆ’ğ‘˜) role-edges to materialize
unbound arguments. Let ğ‘‘event(ğ‘Ÿ:ğ‘)=|P event(ğ‘Ÿ:ğ‘)| and letğ‘›â˜…be
the (max/avg) arity over matches. The running time is given by:
ğ‘‡BIN(ğ‘)=Oğ‘˜âˆ‘ï¸
ğ‘–=1ğ‘‘event(ğ‘Ÿğ‘–:ğ‘ğ‘–) +outÂ·(ğ‘›â˜…âˆ’ğ‘˜)
.(2)
Under a schema-bounded arity, theper-resultoverhead is up to Â¯ğ‘›
role lookups to materialize the remaining arguments. In contrast,
the hypergraph returns them from a single record.
Complexity Gap.In a native hypergraph, all arguments of anğ‘›-
ary fact co-reside in asinglehyperedge record, thus materializing a
hit, is one read, i.e., O(1)per result under bounded arity. In contrast,
in an event-reified binary KG, the fact is split across ğ‘›role-typed
edges, reachable only via the intermediate event node ğ‘’ğ‘“. As a result,
materializing requires up to (ğ‘›âˆ’ğ‘˜) pointer chases, yielding outÂ· Â¯ğ‘›
term, and usually incurs extra indirections/cache misses.
3 Methodology
We proposeHyperRAG, a novel framework that enhances answer
fidelity by integrating reasoning over condensed ğ‘›-ary relational
facts with textual evidence. As depicted in Figure 2, HyperRAG
features two retrieval paradigms: (i)HyperRetriever, which per-
forms adaptive structural-semantic traversal to build interpretable,
query-conditioned relational chains; (ii)HyperMemory, which uti-
lizes the parametric knowledge of the LLM to guide symbolic beam
search. Both variants ground the generation process in hypergraph
structures, ensuring faithful and accurate multi-hop reasoning.
HyperRetriever HyperMemory
MLPFrontier Entities
Entities
Hyperedges
Chunks
ContextFrontier Entities
Entities
Hyperedges
Chunks
Context
LLM
Memory-Guided 
Beam Retriever
Budget-aware Contextualized GeneratorSubgraphAdapted SearchHypergraphDocuments
Relational
ChainsSubgraph
Relational
ChainsBeam SearchWhat other shows or movies were directed by di-
rectors who also directed shows that Bruce Seth
Green directed?Question
Answer: TV 101Bruce Seth
Green
Generator
LLMFigure 2: The overall framework of HyperRAG.
3.1 HyperRetriever: Relational Chains Learning
The motivation behind learning to extract fine-grained ğ‘›-ary re-
lational chains over hypergraph structures stems from two key
challenges: (i) the well-documented tendency of LLMs to halluci-
nate factual content and (ii) the vast combinatorial search space
of hypergraphs under limited token and context budgets [ 25]. To
mitigate these challenges, we introduce a lightweight yet expres-
sive retriever that integrates structural and semantic cues to rank
salientğ‘›-ary facts aligned with query intent.
3.1.1 Topic Entity Extraction.The purpose of obtaining the topic
entity is to ground the query semantics onto hypergraphs G. For-
mally, given a query ğ‘, we request an LLM with prompt ğ‘topicto
identify a set of topic entities that appear in ğ‘in an LLM as follows:
Eğ‘=LLM ğ‘topic,ğ‘,
whereEğ‘denotes the set of extracted entities in the queryğ‘.
3.1.2 Hyperedge Retrieval and Triple Formation.For each extracted
topic entity ğ‘’ğ‘ âˆˆEğ‘, we retrieve its incident hyperedges from F,
formally defined as follows:
Fğ‘’ğ‘ ={ğ‘“ğ‘›âˆˆF:ğ‘’ğ‘ âˆˆğ‘“ğ‘›}.
Each hyperedge ğ‘“ğ‘›âˆˆFğ‘’ğ‘ defines anğ‘›-ary relation over a subset of
ğ‘›entities. To enable pairwise reasoning, we derive a set of pseudo-
binary triples by enumerating ordered entity pairs within each
hyperedge for queryğ‘as follows:
Tğ‘={(ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡)|ğ‘“ğ‘›âˆˆFğ‘’ğ‘ , ğ‘’â„âˆˆğ‘“ğ‘›,ğ‘’ğ‘¡âˆˆğ‘“ğ‘›},(3)

WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates. Wen-Sheng Lien et al.
where each pseudo-binary triple (ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡)consists of a head entity,
the originating hyperedge, and a tail entity.
3.1.3 Structural Proximity Encoding.To capture the structural prox-
imity between entities in the hypergraph, we adapt the directional
distance encoding (DDE) mechanism from SubGraphRAG [ 21], ex-
tending it from binary relations to ğ‘›-ary hyperedges. Formally, for
each candidate triple (ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡)âˆˆTğ‘, we compute its directional
encoding in the following steps:
â€¢One-Hot Initialization:For each entity (ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡), we initialize
a one-hot indicator for the head entity:
ğ‘ (0)
ğ‘’=1,ifâˆƒ(ğ‘’ â„,ğ‘“ğ‘›,ğ‘’ğ‘¡)âˆˆTğ‘such thatğ‘’=ğ‘’ â„,
0,otherwise.(4)
â€¢Bi-directional Feature Propagation:For each layer ğ‘™=0,...,ğ¿ ,
we propagate features over the set of derived triples Tğ‘. Forward
propagation simulates how the head entity ğ‘’â„reaches out to the
tail entityğ‘’ ğ‘¡as follows:
ğ‘ (ğ‘™+1)
ğ‘’=1
|{ğ‘’â€²|(ğ‘’â€²,Â·,ğ‘’)âˆˆTğ‘}|âˆ‘ï¸
(ğ‘’â€²,Â·,ğ‘’)âˆˆTğ‘ğ‘ (ğ‘™)
ğ‘’â€².(5)
In contrast, backward propagation updates head encodings based
on tail-to-head influence:
ğ‘ (ğ‘Ÿ,ğ‘™+1)
ğ‘’ =1
|{ğ‘’â€²|(ğ‘’,Â·,ğ‘’â€²)âˆˆTğ‘}|âˆ‘ï¸
(ğ‘’,Â·,ğ‘’â€²)âˆˆTğ‘ğ‘ (ğ‘Ÿ,ğ‘™)
ğ‘’â€².(6)
â€¢Bi-directional Encoding:After ğ¿rounds of propagation, we
concatenate the forward and backward encodings to obtain the
final vector for each entityğ‘’as follows:
ğ‘ ğ‘’=[ğ‘ (0)
ğ‘’âˆ¥ğ‘ (1)
ğ‘’âˆ¥Â·Â·Â·âˆ¥ğ‘ (ğ¿)
ğ‘’âˆ¥ğ‘ (ğ‘Ÿ,1)
ğ‘’âˆ¥Â·Â·Â·âˆ¥ğ‘ (ğ‘Ÿ,ğ¿)
ğ‘’],(7)
whereâˆ¥denotes vector concatenation. Note that the backward
propagation starts from ğ‘™=1, asğ‘™=0is shared in both directions.
â€¢Triple Encoding:For each candidate triple (ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡), we define
its structural proximity encoding as follows:
ğ›¿(ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡)=
ğ‘ ğ‘’â„âˆ¥ğ‘ ğ‘’ğ‘¡
,(8)
which is passed to a lightweight parametric neural function to
compute the plausibility score for each candidate triple (ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡)
given queryğ‘.
3.1.4 Contrastive Plausibility Scoring.To reduce the search space in
the hypergraph structure, we address the challenge that similarity-
based retrieval often introduces noisy or irrelevant triples. To miti-
gate this, we train a lightweight MLP classifier ğ‘“ğœƒto estimate the
plausibility of each triple candidate and prune uninformative ones.
To this end, the training set is prepared with positive and nega-
tive samples. Let ğ‘ƒâˆ—
ğ‘denote the shortest path of triples connecting
the topic entity to a correct answer in the hypergraph G. The
positive samplesT+
ğ‘–at hopğ‘–consist of triples in ğ‘ƒâˆ—
ğ‘, denoted as
T+
ğ‘–={(ğ‘’â„,ğ‘–,ğ‘“ğ‘›
ğ‘–,ğ‘’ğ‘¡,ğ‘–)}. Negative samples ğ‘‡âˆ’
ğ‘–consist of all other
triples incident to the head entity ğ‘’ğ‘–at hopğ‘–that are not in ğ‘ƒâˆ—
ğ‘. At
each exploration step, only positive triples are expanded at each
hop, while negative ones are excluded. Each triple (ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡)is
encoded in a feature vector by concatenating its contextual and
structural encodings:
x=
ğœ‘(ğ‘)âˆ¥ğœ‘(ğ‘’ â„)âˆ¥ğœ‘(ğ‘“ğ‘›)âˆ¥ğœ‘(ğ‘’ğ‘¡)âˆ¥ğ›¿(ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡)
,(9)whereğœ‘denotes an embedding model that maps the textual content
of the query ( ğ‘), head entity ( ğ‘’â„), hyperedge ( ğ‘“ğ‘›), and tail entity
(ğ‘’ğ‘¡), into vector representations, forming the candidate pseudo-
binary triple(ğ‘’â„,ğ‘“ğ‘›,ğ‘’ğ‘¡). The classifier outputs a plausibility score
ğ‘“ğœƒ(x)âˆˆ[0,1], trained using binary cross-entropy as follows:
L=âˆ’1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1h
ğ‘¦ğ‘–log ğ‘“ğœƒ(xğ‘–)+ (1âˆ’ğ‘¦ğ‘–)log 1âˆ’ğ‘“ğœƒ(xğ‘–)i
.(10)
3.1.5 Adaptive Search.At inference time, we initiate the retrieval
process with initial triples of topic entities and compute their plau-
sibility scores using the trained MLP, ğ‘“ğœƒ(x). Triples exceeding a
plausibility threshold ğœare retained, and their tail entities are used
as frontier entities in the next hop. This expansionâ€“filtering cy-
cle continues until no new triples satisfy the threshold. However,
using a fixed threshold ğœcan be problematic: it may be too strict
in sparse hypergraphs, limiting retrieval, or too lenient in dense
hypergraphs, leading to an overload of irrelevant triples. To mit-
igate this, we implement an adaptive thresholding strategy. We
initialize with ğœ0=0.5, allow a maximum of ğ‘max=5threshold
reductions, and define ğ‘€= 50as the minimum acceptable num-
ber of hyperedges per hop. At hop ğ‘–, we retrieve the set of triples,
Tğ‘,â‰¥ğœğ‘—={(ğ‘’â„,h,ğ‘’ğ‘¡)|ğ‘“ğœƒ(ğ‘¥)â‰¥ğœğ‘—}under the current threshold ğœğ‘—. If
|Tğ‘,â‰¥ğœğ‘—|<ğ‘€, we iteratively reduce the threshold as follows:
ğœğ‘—+1=ğœğ‘—âˆ’ğ‘, ğ‘—=0,...,ğ‘ maxâˆ’1,(11)
whereğ‘= 0.1is the decay factor. This process continues until
||Tğ‘,â‰¥ğœğ‘—||â‰¥ğ‘€ or the reduction limit is reached. To further adapt to
structural variations in the hypergraph, we incorporate a density-
aware thresholding policy. Given the density of the hypergraph
Î”(G) and the predefined lower and upper bounds Î”loandÎ”up,
we classify the hypergraph and adjust ğœ0accordingly to balance
coverage and precision as follows:
MG=ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³Mlow,Î”(G)â‰¤Î” lo,
Mmid,Î” lo<Î”(G)â‰¤Î” up,
Mhigh,Î”(G)>Î” up(12)
After convergence or exhaustion of threshold reduction attempts,
the retrieval strategy is adjusted based on the assigned graph density
category. For low-density graphs ( Mlow), the retriever selects from
previously discarded triples those that satisfy the final plausibility
threshold. For medium and high-density graphs ( MmidandMhigh),
the strategy additionally expands from the tail entities of these
newly accepted triples to increase the depth of reasoning. This
density-aware adjustment prevents over-retrieval in sparse graphs
while enabling more profound and broader exploration in dense
graphs. To further control expansion in high-density settings, where
the number of candidate hyperedges may become excessive, we
impose an upper bound on the number of retrieved triples per
hop. This constraint effectively limits entity expansion, accelerates
retrieval, and reduces the inclusion of low-utility information.
3.1.6 Budget-aware Contextualized Generator.After completion
of the retrieval process, we organize the selected elements into a
structured input for the generator. Following the context layout
protocol of HyperGraphRAG [ 25], we include (i) entities and their
associated descriptions, (ii) hyperedges along with their participat-
ing entities, and (iii) supporting source text chunks linked to each

HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates.
entity or hyperedge. Due to input length constraints, we prioritize
components based on their utility. As shown in the ablation study
of HyperGraphRAG, ğ‘›-ary relational facts (i.e., hyperedges) con-
tribute the most to reasoning performance, followed by entities
and then source text. We therefore allocate the token budget ac-
cordingly: 50% for hyperedges, 30% for entities, and 20% for source
chunks. To further maximize informativeness, we order hyperedges
and entities according to their plausibility scores ğ‘“ğœƒ(Â·), with graph
connectivity as a secondary criterion. The selected components
are then sequentially filled in the order: hyperedges, entities, and
source chunks. Components are filled in priority order and any
unused budget is passed to the next category. The contextualized
evidence resulting context , together with the original query ğ‘, is
then passed to the LLM to generate the final answerAnsweras:
Answer:=LLM(Context,ğ‘).(13)
3.2 HyperMemory: Relational Chain Extraction
To improve interpretability and context awareness in path retrieval,
we avoid naive top- ğ‘˜heuristics with LLM-guided scoring that lever-
ages the modelâ€™s parametric memory to assess the salience of hyper-
edges and entities. This enables retrieval to be guided by contextual
priors and query intent, facilitating more targeted and meaningful
relational exploration.
3.2.1 Memory-Guided Beam Retriever.Specifically, we design beam
search with width ğ‘¤= 3and depth ğ‘‘= 3, whereğ‘¤denotes the
number of paths ranked in the top order retained at each iteration,
andğ‘‘specifies the maximum number of expansion steps. Following
the process of theLearnable Relational Chain Retriever, we begin by
identifying the set of topic entities Eğ‘from the input query ğ‘using
an LLM-based entity extractor. For each topic entity ğ‘’ğ‘ âˆˆEğ‘, we
retrieve its incident hyperedge set Fğ‘’ğ‘ . Each hyperedge ğ‘“ğ‘›âˆˆFğ‘’ğ‘ is
scored for relevance to bothğ‘’ ğ‘ andğ‘using a promptğ‘ edge:
SF(ğ‘“ğ‘›|ğ‘’ğ‘ ,ğ‘)âˆ¼LLM(ğ‘ edge,ğ‘’ğ‘ ,ğ‘“ğ‘›,ğ‘).(14)
We retain the top- ğ‘¤hyperedges, denoted ğ»+
ğ‘’ğ‘ , based on the score
SF(Â·). Next, for each ğ‘“ğ‘›âˆˆF+
ğ‘’ğ‘ , we identify unvisited tail entities
ğ‘’ğ‘¡and score their relevance using a second promptğ‘ entity:
SE(ğ‘’ğ‘¡|ğ‘“ğ‘›,ğ‘) âˆ¼LLM(ğ‘ entity, ğ‘“ğ‘›, ğ‘’ğ‘¡, ğ‘).(15)
Next, each resulting candidate triple (ğ‘’ğ‘ ,ğ‘“ğ‘›,ğ‘’ğ‘¡)receives a weighted
composite score as follows:
S(ğ‘’ğ‘ ,ğ‘“ğ‘›,ğ‘’ğ‘¡)=SF(ğ‘“ğ‘›|ğ‘’ğ‘ ,ğ‘) Â·SE(ğ‘’ğ‘¡|ğ‘“ğ‘›,ğ‘).(16)
From the current set of candidate triples, we retain the top- ğ‘¤based
on the final triple scorer S(Â·). The tail entities of these selected paths
define the next expansion frontier. At each depth ğ‘–, we evaluate
whether the accumulated evidence suffices to answer the query. All
retrieved triples are assembled into a contextualized component ğ¶ğ‘–,
which is passed to the LLM for an evidence sufficiency check:
LLM(ğ‘ ctx,ğ¶ğ‘–,ğ‘) âˆ’â†’ {yes,no},Reason.(17)
If the result is yes, terminate the search and proceed to generation.
Otherwise, ifğ‘–<ğ‘‘, the search continues until the next iteration.3.2.2 Contextualized Generator.The entities and hyperedges re-
trieved are organized in a fixed format context, as defined in Eq.(13).
This contextualized evidence Context , combined with the original
queryğ‘, is then passed to the LLM to generate the finalAnswer.
4 Experiments
We quantitatively evaluate the effectiveness and efficiency of Hyper-
Retriever against RAG baselines both in-domain and cross-domain
settings. Ablation studies highlight the benefits of adaptive expan-
sion andğ‘›-ary relational chain learning, complemented by qual-
itative analyzes that illustrate the precision and efficiency of the
adaptive retrieval process.
4.1 Experimental Setup
4.1.1 Datasets.We conduct experiments under both open-domain
and closed-domain multi-hop question answering (QA) settings.
For in-domain evaluation, we use three widely adopted bench-
mark datasets: HotpotQA [ 42], MuSiQue [ 38], and 2WikiMulti-
HopQA [ 16]. To evaluate cross-domain generalization, we adopt
the WikiTopics-CLQA dataset [ 11], which tests zero-shot induc-
tive reasoning over unseen entities and relations at inference time.
Comprehensive dataset statistics are summarized in Appendix B.2.
4.1.2 Evaluation Metrics.We employ four standard metrics to as-
sess performance, aligning with established protocols for each
benchmark type. For open-domain QA datasets, where the objective
is precise answer generation, we report Exact Match (EM) and F1
scores. For WikiTopics-CLQA, which involves ranking correct enti-
ties from a candidate list, we utilize Mean Reciprocal Rank (MRR)
and Hits@k to evaluate retrieval fidelity. All metrics are reported as
percentages (%), with higher values indicating better performance.
4.1.3 Baselines.To evaluate the effectiveness of our approach, we
compare HyperRAG with RAG baselines with varying retrieval
granularities, enabling a systematic analysis of how evidence struc-
ture affects retrieval effectiveness and answer generation in both
open- and closed-domain settings. Specifically, we include: RAP-
TOR [ 33], which retrieves tree-structured nodes; HippoRAG [ 14],
which retrieves free-text chunks; ToG [ 37], which retrieves rela-
tional subgraphs; and HyperGraphRAG [ 25], which retrieves a
heterogeneous mixture of entities, relations, and textual spans.
4.1.4 Implementation Details.All baselines and our proposed meth-
ods utilize gpt-4o-mini as the core model for both graph construc-
tion and question answering. For HyperRetriever, we additionally
employ the pretrained text encoder gte-large-en-v1.5 to pro-
duce dense embeddings for entities, relations, and queries. With
434M parameters, this GTE-family model achieves strong perfor-
mance on English retrieval benchmarks, such as MTEB, and of-
fers an efficient balance between inference speed and embedding
quality, making it well-suited for semantic subgraph retrieval. All
experiments were implemented in Python 3.11.13 with CUDA 12.8
and conducted on a single NVIDIA RTX 3090 (24 GB). Peak GPU
memory usage remained within 24 GB due to dynamic allocation.
4.2 Open-domain Answering Performance
4.2.1 Setup.ForHyperRetriever, a lightweight MLP ğ‘“ğœƒscores the
plausibility of candidate hyperedges, enabling aggressive pruning

WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates. Wen-Sheng Lien et al.
TopicRAPTOR HippoRAG ToG HyperGraphRAG HyperRetriever HyperMemory Rel. Gain (%)
MRR Hits@10 MRR Hits@10 MRR Hits@10 MRR Hits@10 MRR Hits@10 MRR Hits@10 MRR Hits@10
art3.44 4.13 8.42 9.77 2.99 3.20 17.18 21.68 19.31 24.3115.63 19.17 12.40 12.13
award20.57 25.13 32.80 38.65 8.70 9.35 51.64 63.43 52.66 65.2847.34 56.98 1.98 2.93
edu4.94 5.90 23.82 26.37 9.09 9.49 43.44 50.05 44.79 51.6341.68 46.95 3.11 3.16
health18.85 22.04 25.72 29.59 7.14 7.95 31.46 37.94 32.68 39.2627.48 33.13 3.88 3.48
infra10.95 12.79 23.88 27.11 9.87 10.67 37.18 44.82 38.92 45.7735.77 41.69 4.68 2.12
loc16.55 18.68 19.88 23.08 3.45 3.83 29.92 34.38 31.80 36.8530.73 35.95 6.28 7.18
org12.00 14.54 36.20 41.70 6.61 7.3364.68 74.89 62.87 71.21 52.26 59.84 -2.80 -4.91
people10.74 13.10 15.39 18.28 3.90 4.40 20.67 28.10 21.62 28.4818.96 25.29 4.60 1.35
sci6.84 8.66 15.62 18.86 6.87 7.2825.92 34.54 25.15 32.30 21.50 27.53 -2.97 -6.49
sport11.31 13.28 22.78 26.01 7.51 8.53 37.40 44.91 39.37 45.5633.64 39.72 5.27 1.45
tax10.48 11.08 24.77 26.65 6.22 6.50 35.15 40.94 37.20 40.9833.65 38.19 5.83 0.10
AVG11.52 13.58 22.66 26.01 6.58 7.14 35.88 43.24 36.94 43.7832.60 38.59 2.95 1.23
Table 1: Performance comparison of domain generalization across 11 diverse topics. The â€œRel. Gainâ€ column highlights the
substantial relative improvement of our approach over the best baseline, averaged across all domains (metrics in %).
ModelHotpotQA MuSiQue 2WikiMultiHopQA
EM(%) F1(%) EM(%) F1(%) EM(%) F1(%)
RAPTOR 35.50 41.56 15.00 16.31 22.50 22.95
HippoRAG 49.50 55.8714.50 17.43 30.00 30.44
ToG 10.08 11.00 2.70 2.69 5.20 5.34
HyperGraphRAG51.0042.6922.00 20.02 42.5030.17
HyperRetriever 42.50 43.65 13.50 14.15 34.00 34.06
HyperMemory 35.50 41.51 8.00 12.96 31.50 32.56
Rel. Gain (%) -16.67 -21.87 -38.64 -29.32 -20.00 11.89
Table 2: Performance comparison on HotpotQA, MuSiQue,
and 2WikiMultiHopQA. Rel. Gain (%) indicates the relative
performance gains achieved by our model compared with
the best baselines. The best results are bolded, and the second
best are underlined .
that reduces traversal complexity without compromising reason-
ing quality. ForHyperMemory, we set beam width ğ‘¤= 3and
depthğ‘‘=3to balance retrieval coverage against computational
cost. Comprehensive prompt definitions for edge scoring ( ğ‘edge),
entity ranking ( ğ‘entity), context evaluation ( ğ‘ctx), and generation
are provided in the Appendix.
4.2.2 Results.Table 2 details the Exact Match (EM) and F1 scores
across three open-domain QA benchmarks. HyperRetriever consis-
tently outperforms the HyperMemory variant on HotpotQA and
MuSiQue, demonstrating superior capability in identifying eviden-
tial relational chains. This advantage is attributed to its learnable
MLP-based plausibility scorer and density-aware expansion strat-
egy, which affords precise control over retrieval depth. In contrast,
HyperMemory relies on the fixed parametric memory of the LLM,
rendering it less adaptable to domain-specific relational patterns.
When compared to external KG-based RAG baselines, we observe
a performance divergence based on graph topology. On HotpotQA
and MuSiQue, HyperRetriever exhibits a performance gap (e.g.,38.64% lower EM on MuSiQue), likely because these datasets re-
quire the rigid structural guidance of explicit KG priors for cross-
document navigation. However, on 2WikiMultiHopQA, HyperRe-
triever reverses this trend, achieving an 11.89% relative F1 improve-
ment. This suggests that while KG priors aid in sparse settings,
HyperRetriever is uniquely effective at exploiting the denser, com-
plex relational contexts found in 2WikiMultiHopQA.
4.3 Closed-domain Generalization Performance
To evaluate adaptability to closed-domain ğ‘›-ary knowledge graphs,
we evaluate the performance ofHyperRAGon the WikiTopics-
CLQA dataset (Table 1). The results demonstrate a strong gener-
alization across diverse topic-specific hypergraphs. In particular,
our learnable variant, HyperRetriever, achieved the highest over-
all answer precision, with average improvements of 2.95% (MRR)
and 1.23% (Hits@10) compared to the second-best baseline, Hyper-
GraphRAG. These gains are statistically significant ( ğ‘â‰ª 0.001),
withğ‘¡-test values of1 .46Ã—10âˆ’17for MRR and2 .41Ã—10âˆ’6for Hits@10,
suggesting the empirical reliability of our approach. HyperRetriever
secures top performance in 9 out of the 11 categoriesâ€”for instance,
achieving relative gains of 12.40% (MRR) and 12.13% (Hits@10) in
theArtdomainâ€”and consistently ranks second in the remaining
two. This broad efficacy highlights the robustness of HyperRe-
trieverâ€™s adaptive retrieval mechanism. Unlike baselines that are
sensitive to domain-specific graph density, HyperRetrieverâ€™s learn-
able MLP scorer dynamically calibrates its expansion strategy to suit
varyingğ‘›-ary topologies, ensuring high precision even in complex
reasoning tasks. In contrast, our memory-guided variant,Hyper-
Memory, consistently underperforms against to HyperRetriever.
This variant serves as a critical ablation to probe the limitations of
an LLMâ€™s intrinsic parametric memory for ğ‘›-ary retrieval. The re-
sults confirm that prompt-based scoring alone, without the explicit
structural learning provided by HyperRetriever, is insufficient for
multi-hop reasoning in closed domains.

HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates.
TopicFull w/o Entities w/o Hyperedges w/o Chunks w/o Adaptive Search w Binary KG
MRR Hits@10 MRR Hits@10 MRR Hits@10 MRR Hits@10 MRR Hits@10 MRR Hits@10
art26.03 31.00 27.2831.00 24.03 27.00 24.17 27.00 26.33 31.00 14.00 15.00
award56.9170.00 43.22 61.00 55.95 69.00 55.01 66.00 52.98 66.00 48.92 53.00
edu49.0056.00 43.24 52.00 47.93 52.00 42.67 47.00 47.53 53.00 38.20 42.00
health41.2547.00 37.17 43.00 37.70 40.00 39.33 47.00 39.20 46.00 36.17 39.00
infra34.85 43.00 35.17 43.00 30.87 39.0038.7544.00 35.50 45.00 30.50 32.00
loc38.75 42.5044.5847.50 37.50 40.00 33.13 37.50 41.67 47.50 39.58 42.50
org46.79 58.9758.7565.00 45.92 55.00 53.00 60.00 38.07 45.00 47.50 47.50
people14.20 22.0021.2328.00 13.73 19.00 20.03 26.00 13.37 20.00 19.33 22.00
sci25.91 36.00 18.67 22.00 24.53 32.0026.0938.00 21.14 32.00 24.00 27.00
sport31.04 40.00 35.83 40.00 35.00 45.50 29.58 40.00 33.33 37.5042.0847.50
tax36.25 40.00 29.17 35.00 33.54 36.25 33.13 36.2536.8840.00 35.42 37.50
AVG36.4540.59 35.85 42.50 35.15 41.34 35.90 42.61 35.64 42.91 34.15 36.82
Table 3: Ablation on the Contribution of Context Formation and Adaptive Search. The full model incorporates all components
essential for context formation, including entities, hyperedges involved in learnable relational chains, and retrieved chunks.
The best results in MRR are bolded, and the best in Hits@10 are underlined .
Dimension RAPTOR [33] HippoRAG [14] ToG [37] HyperGraphRAG [25] OG-RAG [34] HyperRetriever / Memory
Structure type Doc tree (summ.) KG (binary) KG (binary) Hypergraph (ğ‘›-ary) Object graph (mostly bin.) Hypergraph (ğ‘›-ary)
Unit of fact Passage / summary Entity-entity edge Step / subgoal Hyperedge (ğ‘›-ary fact) Object-object edge Hyperedge (ğ‘›-ary fact)
Candidate growth Additive (levels) Additive on edge LLM-var. Additive on hyperedges Additive on objects Additive on hyperedges
Per-query overhead Tokens onlyO(ğ‘›âˆ’ğ‘˜)Var.O(1)â€ O(1) O(1)â€ 
Depth for reasoning chain Deep Deep (pairwise) LLM-var. Shallow (ğ‘›-ary edges) Deep (pairwise) Shallow (ğ‘›-ary edges)
Retrieval strategy Dense tree search Graph walk + dense LLM on graph Static Object-centric walk Adaptive / LLM on graph
LLM at retrieval Low-Med Low Med-High (LLM) Low Low Low / Med (LLM)
Ontologyâœ— âœ— âœ— âœ— âœ“ âœ—
Table 4: Method Comparison. HyperRetriever utilizes adaptive search on ğ‘›ary hyperedges, enabling higher-order reasoning
with shallow chains and near constant per-query retrieval overhead O(1). In contrast, static or object-centric walks on binary
graphs entail deeper pairwise chains and materialization cost. â€ denotes bounded arity; âœ“indicates an ontology requirement.
4.4 Ablation Study
To evaluate the effectiveness of our approach, we conduct a series
of ablation studies targeting two key aspects: (i) the contribution
of individual components to context formation, and (ii) the impact
of the adaptive search policy on retrieval performance.
4.4.1 Higher-Order Reasoning Chains.Compared with binary KG
RAG,HyperRAGsupports higher-order reasoning on ğ‘›-ary hyper-
graphs. Anğ‘›-ary hyperedge jointly binds multiple entities and roles,
capturing fine-grained dependencies beyond pairwise links. Exploit-
ing this structure yields shallower yet more expressive reasoning
chains, enabling the model to surface key evidence without multi-
hop traversal. Empirically (Table 3), replacing the ğ‘›-ary structure
with a binary one lowers average MRR from36 .45%to34.15%(-2.3%)
and the average Hits @ 10 from40 .59%to36.82%(-3.77%), indicat-
ing gains in both accuracy and efficiency. Additional qualitative
examples appear in Appendix C.
4.4.2 Impact of Context Formation.Table 3 presents a component-
wise ablation study conducted on a representative 1% subset to
isolate the contributions of (i) entities, (ii) structural relations (hy-
peredges), and (iii) textual context. We observe that removing any
component consistently degrades Mean Reciprocal Rank (MRR),though Hits@10 exhibits higher variance. This divergence high-
lights the distinction between ranking fidelity (MRR) and candidate
inclusion (Hits@10). For instance, in theorgandlocdomains,
certain ablated variants maintain competitive Hits@10 scores but
suffer sharp declines in MRR. This indicates that while the correct
answer remains within the top candidates, the loss of structural or
semantic signals causes it to drift down the ranking list, degrading
precision. Crucially, hyperedges emerge as the dominant factor in
effective context formation. Their exclusion precipitates the most
significant performance drops across both metrics, underscoring
the necessity of high-order topological structure for reasoning. In
contrast, removing entities yields less severe degradation, as enti-
ties primarily provide node-level descriptions, whereas hyperedges
capture the joint dependencies between them. Text chunks offer
complementary unstructured semantics but lack the relational preci-
sion of the graph structure. Ultimately, the superior performance of
the full model validates the synergistic integration of entity-aware
signals, hypergraph topology, and adaptive textual evidence.
4.4.3 Impact of Adaptive Search.Removing the adaptive search
component results in a noticeable decline in MRR across most cate-
gories, whereas its impact on Hit@10 is minimal and in some cases

WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates. Wen-Sheng Lien et al.
0 100 200 300 400
Average Retrieval Time (s)01020304050Average Hits@10 (%)
Figure 3: The visualization shows the efficiency-effectiveness
tradeoff in multi-hop QA: retrieval time ( ğ‘¥-axis), answer qual-
ity (Hits@10, ğ‘¦-axis), and context volume (bubble size, log-
scaled by retrieved tokens).
(e.g.,infra,loc), even marginally positive. This pattern suggests
that while correct answers remain retrievable among the top 10
candidates, they tend to be ranked lower in the absence of adaptive
search, resulting in a reduced overall ranking precision.
4.5 Efficiency Study
4.5.1 Setup.To assess retrieval efficiency, we draw a stratified
1% from each WikiTopics-CLQA category, yielding approximately
1,000 questions evenly distributed across 11 topic domains, and
evaluate all baselines on this set. Figure 3 depicts the three-way
trade off among retrieval time ( ğ‘¥-axis), Hits@10 accuracy ( ğ‘¦-axis),
and context volume (bubble size, logarithmically scaled by retrieved
tokens). Models in the upper left quadrant achieve the best balance
between efficiency and effectiveness, combining low latency with
high Hits@10 while retrieving compact contexts.
4.5.2 Empirical Evidence.HyperRetriever achieves the shortest
retrieval time and the highest Hits@10. Although it retrieves more
tokens than some baselines, top performers consistently rely on
larger contexts, highlighting a common trade-off between answer
quality and retrieval volume. Our empirical findings align with
the theoretical analysis in Â§2.2.HyperRetrieveremploys adaptive
search over ğ‘›-ary hyperedges, enabling higher-order reasoning
with shallow chains and nearly constant per query overhead O(1).
In contrast, static or object-centric walks in binary graphs require
deeper pairwise chains and incur an event materialization cost
O(ğ‘›âˆ’ğ‘˜) . We further benchmark our approach against five publicly
available graph-based RAG systems, covering both ğ‘›-ary and binary
KG designs, and summarize in Table 4.
5 Related Work
Retrieval-Augmented Generation.RAG fundamentally aug-
ments the parametric memory of LLMs with external data, serving
as a critical countermeasure against hallucination in knowledge-
intensive tasks. The standard pipeline operates by retrieving top- ğ‘˜
document chunks via dense similarity search before conditioning
generation on this augmented context [ 2,12,17]. However, conven-
tional dense retrieval methods [ 6,20] treat data as flat text, often
overlooking the complex structural and relational signals requiredfor deep reasoning. To address this, iterative multi-step retrieval
approaches have been proposed [ 18,36,39]. Yet, these methods of-
ten suffer from diminishing returns: they increase inference latency
and retrieve redundant information that dilutes the context signal.
This noise contributes to the â€œlost-in-the-middleâ€ effect, where fi-
nite context windows prevent the LLM from effectively attending
to dispersed evidence [24, 41].
Graph-based RAG.Graph-based RAG frameworks incorporate
inter-document and inter-entity relationships into retrieval to en-
hance coverage and contextual relevance [ 3,15,31,32]. Early ap-
proaches queried curated KGs (e.g., WikiData, Freebase) for factual
triples or reasoning chains [ 4,22,27,40], while recent methods fuse
KGs with unstructured text [ 8,23] or build task-specific graphs from
raw corpora [ 7]. To improve efficiency, LightRAG [ 13], HippoRAG
[14], and MiniRAG [ 10] adopt graph indexing via entity links, per-
sonalized PageRank, or incremental updates [ 28,29]. However,
KG-based RAGs often face a trade-off between breadth and pre-
cision: broader retrieval increases noise, while narrower retrieval
risks omitting key evidence. Methods using fixed substructures (e.g.,
paths, chunks) simplify reasoning [ 33,44] but may miss global con-
text, and challenges are amplified by LLM context window limits,
vast KG search spaces [ 18,30,37], and the high latency of iterative
queries [ 37]. Moreover, most graph-based RAG methods rely on
binary relational facts, limiting the expressiveness and coverage
of knowledge. Hypergraph-based representations capture richer ğ‘›-
ary relational structures [ 26]. HyperGraphRAG [ 25] advances this
line by leveraging ğ‘›-ary hypergraphs, outperforming conventional
KG-based RAGs, yet suffers from noisy retrieval and reliance on
dense retrievers. OG-RAG [ 34] addresses these issues by grounding
hyperedge construction and retrieval in domain-specific ontologies,
enabling more accurate and interpretable evidence aggregation.
However, its dependence on high-quality ontologies constrains
scalability in fast-changing or low-resource domains. Most graph-
based and hypergraph-based RAG methods still face challenges,
particularly due to the use of static or object-centric walks on binary
graphs, which entail deeper pairwise chains and higher material-
ization costs. Table 4 compares existing methods withHyperRAG.
6 Conclusion
We introduced HyperRAG, a novel framework that advances multi-
hop Question Answering by shifting the retrieval paradigm from
binary triples to ğ‘›-ary hypergraphs featuring two strategies: Hyper-
Retriever, designed for precise, structure-aware evidential reason-
ing, and HyperMemory, which leverages dynamic, memory-guided
path expansion. Empirical results demonstrate that HyperRAG
effectively bridges reasoning gaps by enabling shallower, more
semantically complete retrieval chains. Notably, HyperRetriever
consistently outperforms strong baselines across diverse open- and
closed-domain datasets, proving that modeling high-order depen-
dencies is crucial for accurate and interpretable RAG systems.
Acknowledgments
This work is partially supported by the National Science and Tech-
nology Council (NSTC), Taiwan (Grants: NSTC-112-2221-E-A49-
059-MY3, NSTC-112-2221-E-A49-094-MY3, 114-2222-E-A49-004, and
114-2639-E-A49-001-ASP).

HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates.
References
[1]Serge Abiteboul, Richard Hull, and Victor Vianu. 1995.Foundations of Databases.
Addison-Wesley.
[2]Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning.
2015. Leveraging Linguistic Structure For Open Domain Information Extraction.
InProceedings of the Annual Meeting of the Association for Computational Linguis-
tics and the 7th Intâ€™l Joint Conference on Natural Language Processing, Chengqing
Zong and Michael Strube (Eds.). 344â€“354.
[3]Mariam Barry, Gaetan Caillaut, Pierre Halftermeyer, Raheel Qader, Mehdi
Mouayad, Fabrice Le Deit, Dimitri Cariolaro, and Joseph Gesnouin. 2025.
GraphRAG: Leveraging Graph-Based Efficiency to Minimize Hallucinations in
LLM-Driven RAG for Finance Data. InProceedings of the Workshop on Generative
AI and Knowledge Graphs (GenAIK). 54â€“65.
[4]Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph database for structuring human
knowledge. InProceedings of the ACM SIGMOD Intâ€™l Conf. on Management of
Data (SIGMOD â€™08). 1247â€“1250.
[5]Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.
2024. M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity
Text Embeddings Through Self-Knowledge Distillation. InFindings of the Associ-
ation for Computational Linguistics: ACL 2024. 2318â€“2335.
[6]Gabriel de Souza P. Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt
Schifferer, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. 2024.
NV-Retriever: Improving text embedding models with effective hard-negative
mining. arXiv:2407.15831
[7]Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, and Anton Tsitsulin. 2024.
Donâ€™t Forget to Connect! Improving RAG with Graph-based Reranking.arXiv
preprint arXiv: 2405.18414(2024).
[8]Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva
Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan
Larson. 2025. From Local to Global: A Graph RAG Approach to Query-Focused
Summarization. arXiv:2404.16130
[9]Ronald Fagin. 1977. Multivalued Dependencies and a New Normal Form for
Relational Databases.ACM Transactions on Database Systems2, 3 (Sept. 1977),
262â€“278. doi:10.1145/320557.320571
[10] Tianyu Fan, Jingyuan Wang, Xubin Ren, and Chao Huang. 2025. MiniRAG:
Towards Extremely Simple Retrieval-Augmented Generation.arXiv preprint
arXiv: 2501.06713(2025).
[11] Jianfei Gao, Yangze Zhou, Jincheng Zhou, and Bruno Ribeiro. 2023. Double
Equivariance for Inductive Link Prediction for Both New Nodes and New Relation
Types. InNeurIPS 2023 Workshop: New Frontiers in Graph Learning.
[12] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Ji-
awei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generation
for Large Language Models: A Survey. arXiv:2312.10997
[13] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. LightRAG:
Simple and Fast Retrieval-Augmented Generation.arXiv preprint arXiv: 2410.05779
(2024).
[14] Bernal JimÃ©nez GutiÃ©rrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su.
2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large
Language Models. InThe Annual Conf. on Neural Information Processing Systems.
[15] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Ma-
hantesh Halappanavar, Ryan A. Rossi, Subhabrata Mukherjee, Xianfeng Tang, Qi
He, Zhigang Hua, Bo Long, Tong Zhao, Neil Shah, Amin Javari, Yinglong Xia, and
Jiliang Tang. 2025. Retrieval-Augmented Generation with Graphs (GraphRAG).
arXiv:2501.00309
[16] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.
Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reason-
ing Steps. InProceedings of the Intâ€™l Conf. on Computational Linguistics. 6609â€“6625.
[17] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian
Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2025. A Survey on Hallucination in Large Language Models: Principles,
Taxonomy, Challenges, and Open Questions.ACM Transactions on Information
Systems43, 2 (Jan. 2025), 1â€“55.
[18] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-
Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval
Augmented Generation. InProceedings of the Conf. on Empirical Methods in
Natural Language Processing. 7969â€“7992.
[19] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. InProceedings of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP). 6769â€“6781.
[20] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi,
Bryan Catanzaro, and Wei Ping. 2025. NV-Embed: Improved Techniques for
Training LLMs as Generalist Embedding Models. InIntâ€™l Conf. on Learning Repre-
sentations.
[21] Mufei Li, Siqi Miao, and Pan Li. 2025. Simple is Effective: The Roles of Graphs
and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented
Generation. InIntâ€™l Conf. on Learning Representations.[22] Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao
Zhang, and Bing Yin. 2023. Graph Reasoning for Question Answering with
Triplet Retrieval. InFindings of the Association for Computational Linguistics: ACL
2023. 3366â€“3375.
[23] Lei Liang, Zhongpu Bo, Zhengke Gui, Zhongshu Zhu, Ling Zhong, Peilong
Zhao, Mengshu Sun, Zhiqiang Zhang, Jun Zhou, Wenguang Chen, Wen Zhang,
and Huajun Chen. 2025. KAG: Boosting LLMs in Professional Domains via
Knowledge Augmented Generation. InCompanion Proceedings of the ACM on
Web Conf.334â€“343.
[24] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models
Use Long Contexts.Transactions of the Association for Computational Linguistics
12 (2024), 157â€“173.
[25] Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo,
Qika Lin, Yu Feng, Zemin Kuang, Meina Song, Yifan Zhu, and Luu Anh Tuan. 2025.
HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured
Knowledge Representation. arXiv:2503.21322
[26] Haoran Luo, Haihong E, Yuhao Yang, Tianyu Yao, Yikai Guo, Zichen Tang,
Wentai Zhang, Shiyao Peng, Kaiyang Wan, Meina Song, Wei Lin, Yifan Zhu,
and Anh Tuan Luu. 2024. Text2NKG: Fine-Grained N-ary Relation Extraction
for N-ary relational Knowledge Graph Construction. InAdvances in Neural
Information Processing Systems, Vol. 37. Curran Associates, Inc., 27417â€“27439.
[27] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2024. Reasoning
on Graphs: Faithful and Interpretable Large Language Model Reasoning. InIntâ€™l
Conf. on Learning Representations.
[28] Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Dinh Phung, Chen Gong, and
Shirui Pan. 2025. GFM-RAG: Graph Foundation Model for Retrieval Augmented
Generation.arXiv preprint arXiv:2502.01113(2025).
[29] Costas Mavromatis and George Karypis. 2025. GNN-RAG: Graph Neural Retrieval
for Efficient Large Language Model Reasoning on Knowledge Graphs. InFindings
of the Association for Computational Linguistics: ACL 2025. 16682â€“16699.
[30] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.
2024. Unifying Large Language Models and Knowledge Graphs: A Roadmap.
IEEE Transactions on Knowledge and Data Engineering36, 7 (2024), 3580â€“3599.
[31] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong,
Yan Zhang, and Siliang Tang. 2024. Graph Retrieval-Augmented Generation: A
Survey. arXiv:2408.08921
[32] Ian Robinson, Jim Webber, and Emil Eifrem. 2015.Graph Databases: New Oppor-
tunities for Connected Data(2nd ed.). Oâ€™Reilly Media, Inc.
[33] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and
Christopher D Manning. 2024. RAPTOR: Recursive Abstractive Processing for
Tree-Organized Retrieval. InThe Twelfth Intâ€™l Conf. on Learning Representations.
[34] Kartik Sharma, Peeyush Kumar, and Yunqing Li. 2024. OG-RAG: Ontology-
Grounded Retrieval-Augmented Generation For Large Language Models.
arXiv:2412.15235
[35] Abraham Silberschatz, Henry F. Korth, and S. Sudarshan. 2010.Database System
Concepts(6 ed.). McGraw-Hill.
[36] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN:
Dynamic Retrieval Augmented Generation based on the Real-time Information
Needs of Large Language Models. InProceedings of the Annual Meeting of the
Association for Computational Linguistics (Vol. 1: Long Papers). 12991â€“13013.
[37] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun
Gong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. 2024. Think-on-Graph: Deep
and Responsible Reasoning of Large Language Model on Knowledge Graph. In
The Twelfth Intâ€™l Conf. on Learning Representations.
[38] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
2022. MuSiQue: Multihop Questions via Single-hop Question Composition.
Transactions of the Association for Computational Linguistics10 (2022), 539â€“554.
[39] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-
Intensive Multi-Step Questions. InProceedings of the Annual Meeting of the
Association for Computational Linguistics (Vol. 1: Long Papers). 10014â€“10037.
[40] Denny VrandeÄiÄ‡ and Markus KrÃ¶tzsch. 2014. Wikidata: a free collaborative
knowledgebase.Commun. ACM57, 10 (Sept. 2014), 78â€“85.
[41] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu,
Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
Catanzaro. 2024. Retrieval meets Long Context Large Language Models. InIntâ€™l
Conf. on Learning Representations.
[42] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for
Diverse, Explainable Multi-hop Question Answering. InProceedings of the Conf.
on Empirical Methods in Natural Language Processing. 2369â€“2380.
[43] Dengyong Zhou, Jiayuan Huang, and Bernhard SchÃ¶lkopf. 2006. Learning with
Hypergraphs: Clustering, Classification, and Embedding. InAdvances in Neural
Information Processing Systems.
[44] Yingli Zhou, Yaodong Su, Youran Sun, Shu Wang, Taotao Wang, Runyuan He,
Yongwei Zhang, Sicong Liang, Xilin Liu, Yuchi Ma, and Yixiang Fang. 2025. In-
depth Analysis of Graph-based RAG in a Unified Framework. arXiv:2503.04338

WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates. Wen-Sheng Lien et al.
A Reduction to Binary Knowledge Graphs
Definition A.1(Faithful Reduction to Binaries).Let Fbe a set of
ğ‘›-ary facts(ğ‘›â‰¥ 3)over entitiesEwith role-typed arguments. A
reductionis a mapping Î¦:P(F)â†’P(EÃ—E) that introduces no
new auxiliary nodes and satisfies, for allğ¹,ğ¹â€²âŠ†F:
(1)Recoverability: ğ¹is uniquely determined by Î¦(ğ¹) without
spurious or missing tuples; and
(2)Role preservation:argument roles in ğ¹are recoverable from
Î¦(ğ¹); and
(3)Multiplicity:distinct co-participation instances remain distin-
guishable (no accidental merging).
The three conditions cannot be met under a binary-only schema
Î¦. Intuitively, triadic and higher-arity facts imposejointconstraints
across all arguments, whereas binaries encode onlypairwiseco-
occurrence. Removing the joint carrier hyperedge either obscures
â€œwho did what with which roleâ€ or merges parallel events. Therefore,
an auxiliary event node (or equivalent mechanism) is necessary to
preserve tuple identity and roles. An illustrative example follows.
Example(Role Ambiguity).
ğ¹1={give(Alice,Bob,Book),give(Alice,Carol,Pen)},
ğ¹2={give(Alice,Bob,Pen),give(Alice,Carol,Book)}.
Naive pairwise projection (no event node):
Î¦(ğ¹)=gaveTo(Alice,Bob),gaveTo(Alice,Carol),
gaveItem(Alice,Book),gaveItem(Alice,Pen)
.
Then Î¦(ğ¹ 1)=Î¦(ğ¹ 2): the (receiver, item) pairing is unrecoverable,
violatingrecoverabilityandrole preservation.
Prompt for Entity Salience Scoring (             )
Please score the entities' contribution to the
question on a scale from 0 to 1 (the sum of the
scores of all entities is 1).
Example:
Q: Who directed the movie that won Best Picture in
1998?
Hyperedge: Titanic, directed by James Cameron, won
the Academy Award for Best Picture in 1998.
Entities: Titanic; James Cameron; 1998; Academy Award
Score: 0.3, 0.6, 0.05, 0.05
"James Cameron" is the director of Titanic, the movie
that won Best Picture in 1998. Therefore, "James
Cameron" receives the highest score. "Titanic" is the
movie in question and gets a moderate score. "1998"
and "Academy Award" provide context and get lower
scores.
---
Q: {query}
Hyperedge: {hyperedge}
Entities: {entities}
Score: 
entity
Figure 4: Prompt for Entity Salience Scoring (ğ‘ entity ).
B Reproducibility Details
B.1 Hyperparameter Setting
HyperRetriever Hyperparameters.HyperRetriever is trained
using nn.BCEWithLogitsLoss with a batch size of 32, learning rateof1Ã—10âˆ’4, and early stopping (patience = 10) over 50 epochs. For the
retrieval phase of HyperRetriever, we followed the hyperparameters
specified in the methodology: initial plausibility threshold ğœ0=0.5,
maximum threshold reductions ğ‘max=5, minimum number of
hyperedges per questionğ‘€=50, and decay coefficientğ‘=0.1. To
further adapt retrieval behavior based on the graph structure, we
design hypergraphâ€™s density lower and upper bounds Î”lo=2.35
andğ·ğ‘’ğ‘™ğ‘¡ğ‘ up=5.
HyperMemory Hyperparameters.For HyperMemory, we set
the beam width ğ‘¤= 3and the maximum search depth ğ‘‘= 3.
This approach prevents the retriever from managing an excessive
number of paths while still providing sufficient information for
effective retrieval.
B.2 Dataset Statistics
Comprehensive statistics for open-domain and closed-domain QA
benchmarks, including dataset splits, are presented in Table 5.
Dataset Train Validation Test Total
Wikitopics 89815 89726 89749 269290
HotpotQA 640 160 200 1,000
MuSiQue 640 160 200 1,000
2WikiMultiHopQA 640 160 200 1,000
Table 5: Statistics of QA benchmarks across domain settings
B.3 Github Repository
Our anonymized code is available at https://github.com/Vincent-
Lien/HyperRAG.git.
C Additional Qualitative Results
Figure 6 provides a qualitative comparison of evidential ğ‘›-ary re-
lational chains extracted by the strong baseline, ToG, versus our
proposed HyperRetriever, alongside the Ground Truth (GT). The
analysis reveals that HyperRetriever exploits hypergraph topology
to preserve the semantic integrity of dense ğ‘›-ary facts, resulting
in structurally concise reasoning paths. Conversely, ToG is con-
strained by binary graph decomposition, necessitating longer, more
fragmented traversal paths to capture equivalent dependencies.
D Prompt Templates
Edge Plausibility Scoring ( ğ‘edge).The template for ğ‘edgeis de-
picted in Figure 7a.
Entity Salience Scoring ( ğ‘entity).The template for ğ‘edgeis de-
picted in Figure 4.
Context Relevance Evaluation ( ğ‘ctx).The template for ğ‘ctxis
depicted in Figure 7b.
Question Answering.We generate the final answers for both Hy-
perRetriever and HyperMemory using the same prompt and dataset.
For open-domain QA benchmarks such as HotpotQA, MuSiQue
and 2WikiMultiHopQA, the answer is usually a single entity or
sentence. Therefore, we design the prompt to guide the model to-
ward a clear, single factual reply. In contrast, the closed-domain
WikiTopics-CLQA dataset expects a list of multiple entities. In this

HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates.
Prompt for Open-Domain Question Answering
---Role---
You are a helpful assistant responding to questions
about data in the tables provided.
---Goal---
Generate a response that lists exactly one entity
that can answer the user's question.
If you don't know the answer, just say so. Do not
make anything up.
Do not include information where the supporting
evidence for it is not provided.
---Target response length and format---
A JSON array containing exactly one entity name (no
other text). 
Example:
["The Romantic Englishwoman"]
---Data tables---
{context_data}
Add sections and commentary to the response as
appropriate for the length and format. Style the
response in markdown.
(a) Open-Domain Question Answering
Prompt for Closed-Domain Question Answering
---Role---
You are a helpful assistant responding to questions
about data in the tables provided.
---Goal---
Generate a response that lists exactly which entities
can answer the user's question.
If you don't know the answer, just say so. Do not
make anything up.
Do not include information where the supporting
evidence for it is not provided.
---Target response length and format---
A JSON array of entity names (no other text). 
Example:
["The Romantic Englishwoman", "Love in the
Wilderness", Brave", "Ring of Bright Water"]
---Data tables---
{context_data}
Add sections and commentary to the response as
appropriate for the length and format. Style the
response in markdown.
(b) Closed-Domain Question Answering
Figure 5: Prompt templates for (a) Open-Domain Question
Answering, and (b) Closed-Domain Question Answering.
case, we shape the prompt to ensure the model produces a list of
all relevant entities, thus ensuring the output matches the required
multi-item format.
Prompt for Open-Domain Question Answering:The template
for open-domain question answering is illustrated in Figure 5a.Prompt for Closed-Domain Question Answering:The tem-
plate for closed-domain question answering is given in Figure 5b.
Question: Which stations are connected by the same line as the line
that connects Sawajiri Stationâ€™s adjacent station?
HyperRetriever
Hanawa
Line
Hanawa
LineSawajiri
Station
Sawajiri Station is adjacent to Dobukai Station and
JÅ«nisho Station on the Hanawa Line.
The Hanawa Line connects various stations including
RikuchÅ«-ÅŒsato Station, Hachimantai Station, and
Kazunohanawa Station.
Kazunohanawa
Station
Question: Which genes are associated with multiple sclerosis?
GT
ToG
HyperRetriever
chst12
multiple sclerosis
Genetic association chst12 has been linked to
multiple sclerosis.
chst12
Question: Who received an award from the University of Florida
Athletic Hall of Fame?
GT
ToG
HyperRetriever
Doug dickey
was honored by
was inducted into
College Football
Hall of FameUniversity of Florida
Athletic Hall of Fame
Doug Dickey received an award from the
University of Florida Athletic Hall of Fame and
College Football Hall of Fame.
award
 Doug dickey
Doug dickey
athletic hall of fame
university of florida
university of florida
is associated with
is linked to
multiple sclerosis
chst12
genes
multiple sclerosis
GT
ToG
Kazunohanawa Station
is adjacent to
is connected byis on
Dobukai Station
Kazunohanawa StationHanawa Line
Sawajiri Station
Hanawa LineDobukai Station
Figure 6: Comparison of evidential ğ‘›-ary relational chains.
We contrast Ground-Truth (GT) answers with reasoning
paths derived by ToG and HyperRetriever. While ToG op-
erates on standard knowledge graphs restricted to binary
relations, HyperRetriever leverages hypergraphs to preserve
the semantic integrity of denseğ‘›-ary facts.

WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates. Wen-Sheng Lien et al.
Prompt for Edge Plausibility Scoring (           )
Please retrieve %s hyperedges (each hyperedge is a passage) that contribute to answering the question and rate
their contribution on a scale from 0 to 1 (the sum of the scores of the %s hyperedges must equal 1).
Example:
Q: Where did Albert Einstein publish his paper on general relativity?
Topic Entity: Albert Einstein
Hyperedges: 
1. "In 1905, Einstein published four groundbreaking papers on the photoelectric effect, Brownian motion, special
relativity, and massâ€“energy equivalence in the journal Annalen der Physik." 
2. "In November 1915, Einstein presented the field equations of general relativity to the Prussian Academy of
Sciences in Berlin." 
3. "Einstein received the 1921 Nobel Prize in Physics for his explanation of the photoelectric effect." 
4. "During World War I, scientific exchange in Europe was severely limited." 
A: 
1. {{2. "In November 1915, Einstein presented the field equations of general relativity to the Prussian Academy
of Sciences in Berlin." (Score: 0.70)}}: This passage directly states where his general relativity work was
presented, making it the most relevant. 
2. {{1. "In 1905, Einstein published four groundbreaking papers on the photoelectric effect, Brownian motion,
special relativity, and massâ€“energy equivalence in the journal Annalen der Physik." (Score: 0.20)}}: Although
this lists multiple papers, it mentions the same journal which provides context on Einstein's publication
venues. 
3. {{4. "During World War I, scientific exchange in Europe was severely limited." (Score: 0.10)}}: Offers
historical context but does not directly answer the publication venue. 
---
Q: {query}
Topic Entity: {topic_entity}
Hyperedges: 
{hyperedges} 
A:edge
(a) Edge Plausibility Scoring (ğ‘ edge)
Prompt for Context Relevance Evaluation (        )
You are given a question and a set of related knowledge statements (hyperedges), where each statement connects
multiple entities. You are also given descriptions of the involved entities. Your task is to judge whether the
provided information is sufficient to answer the question, considering your own knowledge and the given context.
Answer with either {{Yes}} or {{No}}, and explain your reasoning briefly.
Example:
Q: Who is the spouse of the person who played Hermione Granger in Harry Potter?
Entity Descriptions:
Emma Watson: British actress known for her role as Hermione Granger in Harry Potter. 
Hermione Granger: A fictional character from the Harry Potter series. 
Harry Potter: A fantasy film and book series featuring a young wizard. 
Hyperedges:
1. "Emma Watson played the role of Hermione Granger in the Harry Potter film series." 
 Connected Entities: [Emma Watson, Hermione Granger, Harry Potter] 
2. "Emma Watson is a British actress born in 1990." 
 Connected Entities: [Emma Watson] 
3. "Emma Watson has been involved in various humanitarian activities." 
 Connected Entities: [Emma Watson]
A: {{No}}. The provided statements confirm that Emma Watson played Hermione Granger, but they do not include any
information about her spouse. Additional data is needed to answer the question.
---
Q: {query}
Entity Descriptions:
{entity_descriptions}
Hyperedges:
{hyperedges}
A: 
ctx
(b) Context Relevance Evaluation (ğ‘ ctx)
Figure 7: Prompt for (a) Edge Plausibility Scoring, and (b) Context Relevance Evaluation.