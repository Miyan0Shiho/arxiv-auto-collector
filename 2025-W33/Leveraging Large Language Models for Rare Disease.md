# Leveraging Large Language Models for Rare Disease Named Entity Recognition

**Authors**: Nan Miles Xi, Yu Deng, Lin Wang

**Published**: 2025-08-12 20:16:31

**PDF URL**: [http://arxiv.org/pdf/2508.09323v1](http://arxiv.org/pdf/2508.09323v1)

## Abstract
Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

## Full Text


<!-- PDF content starts -->

 1 Leveraging Large Language Models for Rare Disease Named Entity Recognition Nan Miles Xi 1, Yu Deng 1, Lin Wang 2*  1 Data and Statistical Sciences, AbbVie Inc., North Chicago, IL 60064, USA 2 Department of Statistics, Purdue University, West Lafayette, IN 47907, USA * Correspondence: linwang@purdue.edu    Abstract Named Entity Recognition (NER) in the rare disease domain poses unique challenges due to limited labeled data, semantic ambiguity between entity types, and long-tail distributions. In this study, we evaluate the capabilities of GPT-4o for rare disease NER under low-resource settings, using a range of prompt-based strategies including zero-shot prompting, few-shot in-context learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We design a structured prompting framework that encodes domain-specific knowledge and disambiguation rules for four entity types. We further introduce two semantically guided few-shot example selection methods to improve in-context performance while reducing labeling effort. Experiments on the RareDis Corpus show that GPT-4o achieves competitive or superior performance compared to BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art (SOTA) results. Cost-performance analysis reveals that few-shot prompting delivers high returns at low token budgets, while RAG offers marginal additional benefit. An error taxonomy highlights common failure modes such as boundary drift and type confusion, suggesting opportunities for post-processing and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can serve as effective, scalable alternatives to traditional supervised models in biomedical NER, particularly in rare disease applications where annotated data is scarce.  Keywords: Rare disease; Named Entity Recognition; Large language model; Prompt engineering; In-context learning; Retrieval-augmented generation   

 2 1 Instruction Rare diseases are individually rare but collectively common, with over 6,000 distinct conditions affecting an estimated 300 million people worldwide 1. Their low prevalence means that general practitioners have little experience with any given rare disease, while the clinical heterogeneity across conditions further complicates diagnosis 2. As a result, patients often face prolonged processes before receiving a correct diagnosis and appropriate treatment 3. This diagnostic gap has elevated rare diseases to a global health priority and highlights the urgent need for scalable methods to extract and disseminate rare disease knowledge. Automated information extraction, particularly named entity recognition (NER), can play a pivotal role in addressing this gap. NER enables the construction of biomedical knowledge graphs linking diseases to phenotypes, supports clinical decision-making, and assists patient care by surfacing relevant findings in medical narratives 4–6. Recent work has also demonstrated the utility of NER for symptom surveillance in social-media platforms 7. However, extracting such information from unstructured text poses several challenges. Foremost among these is the low-resource setting. Few annotated corpora exist for rare disease NER, as expert labeling is costly and time-consuming. In addition, rare disease terminology is often semantically ambiguous, which creates overlapping entity boundaries and introduces high annotation variability. Distinguishing between such entities requires nuanced domain understanding that even advanced models struggle to achieve 8. Compounding these issues is the long-tail distribution of rare diseases. The vast majority occur with low frequency, often below one case per million individuals 9. Consequently, most rare disease mentions appear infrequently in existing data, and language models may lack sufficient exposure to ultra-rare conditions. Any robust solution must contend with both data scarcity and domain-specific ambiguity to succeed in this setting. Conventional biomedical NER systems rely on supervised learning with domain-specific models. Transformer-based architectures such as BioClinicalBERT and BioBERT have achieved strong performance on medical NER tasks when trained on large-scale corpora 10,11. However, these supervised approaches are inherently constrained by their dependence on large and high-quality annotated datasets, which remain scarce in the rare disease domain. Even when such data are available, generalization to novel or ultra-rare entities remains difficult due to the long-tail distribution of biomedical concepts 12. In parallel, generative large language models (LLMs) have enabled a shift toward prompt-based learning through natural language instructions. In general-domain applications, generative LLMs have demonstrated impressive zero-shot and few-shot capabilities, substantially reducing the need for task-specific labeled data 13. Several recent studies have begun to test prompt-engineering for rare-disease extraction 3,14–16, yet systematic evaluation and broader generalization remain open questions.  Prompt-based NER in biomedical text introduces several open questions. Early evaluations indicate that general-purpose LLMs underperform compared to fine-tuned biomedical models on 

 3 clinical NER tasks 17,18. Moreover, prompt designs tailored to rare disease extraction are still in their infancy. It remains unclear whether off-the-shelf LLMs can reliably disambiguate the subtle semantic distinctions in rare disease contexts. Beyond basic prompting, two complementary approaches, retrieval-augmented generation (RAG) and in-context learning, offer potential solutions. RAG enables an LLM to access external information at inference time by retrieving and incorporating supporting documents 19. In rare disease NER, RAG can allow the model to consult definitions or explanations from curated biomedical knowledge databases. Similarly, the effectiveness of in-context learning relies on the choice of labeled learning exemplars. Recent studies have shown that selecting semantically similar examples can substantially improve few-shot learning in biomedical Natural Language Processing (NLP) tasks 20. Yet, it remains unclear how example selection strategies affect LLM performance in disambiguating complex rare disease entities. Given these challenges, we aim to answer the following question: Can generative LLMs accurately and cost-effectively perform NER in the rare disease domain using prompt-based methods, fine-tuning on domain-specific prompts, or retrieval-augmented context? We focus on OpenAI’s GPT-4o as a representative LLM 21 and evaluate its ability to identify rare disease-related entities under various low-resource settings. We benchmark GPT-4o against a supervised baseline BioClinicalBERT to quantify the strengths and limitations of prompt-based LLMs in specialized biomedical tasks. Our goal is to assess whether prompt-only, in-context learning, and RAG can approach state-of-the-art (SOTA) models without large and annotated datasets, and to understand their trade-offs relative to traditional supervised learning approaches. Our contributions in this paper are summarized as follows. First, we design a prompt template that encodes domain knowledge for semantically overlapping entity types. This framework guides GPT-4o to perform entity recognition with nuanced semantic boundaries. Second, we evaluate GPT-4o under multiple prompting regimes and compare its performance against the SOTA. This comparison quantifies the effectiveness of prompt-based LLMs relative to conventional NER systems. Third, we investigate two context-aware strategies for selecting learning examples. We show that these methods outperform random selection and enhance GPT-4o’s ability to resolve ambiguous entity mentions. Another contribution is that we implement an RAG approach and let GPT-4o retrieve contextual snippets from a knowledge base. We assess the utility of this external biomedical context and highlight when RAG provides meaningful performance gains. We also evaluate the inference cost across different prompting strategies. This analysis provides insights into the deployment feasibility of prompt-based LLMs in real-world applications. Finally, we introduce a taxonomy of error types and perform an error analysis to identify common failures in GPT-4o’s output. In our experiments, GPT-4o demonstrates strong performance on rare disease NER under minimal supervision. With a small number of in-context learning examples, GPT-4o’s performance approaches that of the fine-tuned BioClinicalBERT. Importantly, we find that the quality of selected examples plays a critical role in this success. Semantic selection strategies 

 4 consistently outperform random selection by enabling the model to resolve ambiguous entity boundaries and improve recall. In contrast, RAG provides only marginal benefits. Overall, our findings indicate that prompt-engineered LLMs can deliver competitive NER performance in the rare disease domain. However, challenges remain in disambiguating closely related entity types and addressing edge cases with low frequency. Our error analysis reveals systematic failure modes, with most errors stemming from span boundary mismatches. These insights highlight specific areas for future refinement, such as post-processing heuristics and hybrid LLM rule-based systems to improve boundary resolution and type specificity 22,23. The remainder of this paper is structured as follows. Section 2 describes the methodology, including the rare disease dataset, prompt design, learning example selection, and RAG components. Section 3 presents the experimental results and evaluation, performance comparisons, ablation studies, and error analysis. Section 4 discusses the implications of these findings and concludes the paper with future directions.  2 Methods 2.1 RareDis Corpus Dataset We utilize the RareDis Corpus, a domain-specific dataset developed to support NLP applications in the rare disease domain 24. Let the dataset be denoted as: 𝐃={(𝑥!,𝑌!)}!"#$ where 𝑥!∈𝐗 is a biomedical document and 𝑌!=,-𝑠!%,𝑐!%01%"#&!⊂𝐘 is the set of annotated entities, with 𝑠!% denoting a surface text span, 𝑛! being the number of annotated entities in document 𝑥!, and 𝑐!%∈𝐂 representing the entity type. The entity space is defined as: 𝐂={rare	disease,disease,sign,symptom} The corpus contains 𝑁=1,041 documents sourced from the National Organization for Rare Disorders (NORD) database 25. Each document is structured into multiple clinically relevant sections, including general discussion, signs and symptoms, causes, diagnosis, related disorders, affected populations, and therapies. Entity annotations are performed manually by domain experts and contain 5,221 rare disease mentions, 2,348 general disease mentions, 5,333 signs, and 396 symptoms. The corpus is split into training (70%), validation (10%), and test (20%) subsets: 𝐃=𝐃'()*+∪𝐃,)-∪𝐃'./' The RareDis Corpus reports an Inter-Annotator Agreement (IAA) with an average F1 score of 83.5% for entity recognition, reflecting a high degree of annotation consistency 24. The corpus makes fine-grained distinctions between semantically related entity types: disease vs. rare 

 5 disease (based on prevalence thresholds) and sign vs. symptom (objective vs. subjective clinical presentation). These subtle boundaries introduce substantial challenges for LLMs in entity recognition. A detailed breakdown of the entity statistics and representative examples is provided in Table 1.  2.2 Model and Prompt Design We utilize OpenAI’s pretrained large language model GPT-4o to perform NER in the rare disease domain, treating the task as conditional sequence generation. For each test input 𝑥*+01*(2∈𝐗'./', the model is provided with a prompt 𝜋∈𝐏, constructed from five structured components designed to instruct the model on entity recognition without labeled training examples: 𝜋=task	description∥output	format∥task	guidance∥disambiguation	rule∥𝑥*+01*(2 Here, the components are defined as follows: • Task description specifies the recognition objective, denoted by a label 𝑐∈𝐂. For instance: “Identify the names of rare diseases from the following text”. • Output format enforces a standardized, comma-separated list of identified entities 𝑦S*+01*(2∈𝐘, enabling exact-match evaluation. For example: “Output only the exact disease names without any additional changes. If there are multiple diseases, separate their names with commas. If there is no disease, output none”. • Task guidance provides formal definitions for each entity type 𝑐, assisting the model to distinguish between semantically overlapping categories. For example: “Symptoms are subjective experiences reported by the patient, which cannot be directly observed or measured by others. They reflect what the patient feels, such as pain, fatigue, or nausea. Symptoms are experienced internally and rely on the patient’s description”. • Disambiguation rule offers meta-instructions highlighting frequent errors observed during validation. These discourage undesirable behaviors such as misclassifying general diseases as rare diseases or merging distinct entities. For example: “Treat abbreviations as separate rare disease names. Do not identify regular diseases as rare diseases.” • Input text (𝑥*+01*(2) is the raw contents from which entities are to be identified. A prefix marks its beginning, such as: “The text from which you need to exact the signs of rare diseases is: …” We define the basic prompt components as the combination of the task description, output format, and 𝑥*+01*(2. Basic prompt contains the core instruction and context. Advanced components include task guidance and disambiguation rules, which encode domain knowledge and observed failure modes. All prompts are constructed without including any labeled examples (i.e., zero-shot learning), ensuring that the model’s performance is attributable solely to prompt 

 6 content and pretrained knowledge. To quantify the contribution of each prompt category, we vary the presence of basic and advanced components in the complete prompt π and evaluate the zero-shot performance of GPT-4o under each configuration using the evaluation framework described in Section 2.6. A complete set of prompt templates by entity type is summarized in Table 2.  2.3 In-Context Learning and Example Selection Strategies In-context learning refers to providing demonstration examples directly in the prompt to guide the model’s response, without gradient-based parameter updates 13,26. Formally, let {(𝑥!,𝑦!)}!"#3⊂𝐃'()*+ denote a set of 𝑘 in-context learning examples, where 𝑥!∈𝐗'()*+ and	𝑦!∈𝐘'()*+. Here, 𝑦! is a flattened, comma-separated list of entities derived from the structured annotations 𝑌!. LLM receives a prompt of the form: 𝜋=basic	components∥advanced	components∥,-𝑥%,𝑦%01%"#3∥𝑥*+01*(2 The model then generates output 𝑦S*+01*(2=𝑀(𝜋), where 𝑀 is the LLM conditioned on the full prompt. Depending on 𝑘, the setup is referred to as one-shot (𝑘=1) or few-shot (𝑘>1) learning. To assess how different configurations of in-context learning examples affect model performance, we explore a set of example selection methods by leveraging semantic similarity between input texts. Each 𝑥*+01*(2∈𝐗'./' is mapped to an embedding vector 𝑓(𝑥)∈𝐑4567 using OpenAI’s text-embedding-3-large model. Given two texts 𝑥 and 𝑥′, semantic similarity is quantified via the Euclidean distance: 𝑑(𝑥,𝑥8)=‖𝑓(𝑥)−𝑓(𝑥8)‖7 We then consider the following three selection strategies: • Inquiry-Random – For each 𝑥*+01*(2, select 𝑘 learning examples uniformly at random from 𝐗'()*+ independent of semantic similarity.  • Inquiry-KNN – For each 𝑥*+01*(2, compute 𝑑(𝑥*+01*(2,𝑥!) for all 𝑥!∈𝐗'()*+, and select the top 𝑘 learning examples with the smallest distances. This yields context-specific, nearest-neighbor demonstrations. • Cluster-KNN – Partition the test set 𝐗'./' into 𝐶 clusters using k-means clustering in the embedding space. Let 𝐂%⊂𝐗'./' denote the set of inquiry texts in cluster 𝑗. For each training example 𝑥!∈𝐗'()*+, define its average distance to cluster 𝑗 as: 𝑑̅%(𝑥!)=1a𝐂%ab𝑑(𝑥,𝑥!)9∈𝐂" 

 7 Then, for every 𝑥*+01*(2∈𝐂%, select the 𝑘 training examples with the smallest 𝑑̅%(𝑥!). This approach selects examples that are collectively representative for all members of a cluster, rather than individually optimized per inquiry. The number of clusters 𝐶 is treated as a hyperparameter, with values 32 or 64 explored in our analysis. Note that this clustering is applied only at evaluation time to guide example selection.  To study the impact of demonstration count, we vary 𝑘∈{1,2,4,6,8,10,12,14,16} across all selection methods. The learning examples (𝑥!,𝑦!) start with a prefix “Here are demonstration shots:” Model performance is evaluated for each 𝑘 and selection method combination across the four entity types. A representative summary of the learning examples and prompt configurations evaluated is presented in Table 3.  2.4 Task-Level Fine-Tuning Prompt engineering and in-context learning do not force the model to internalize domain-specific regularities in rare disease NER. We therefore investigate a complementary strategy: task-level fine-tuning. Unlike BioClinicalBERT and BioBERT pretrained on general-domain biomedical corpora, task-level fine-tuning updates the parameters 𝛉 of a pretrained LLM 𝑀𝛉 using the training set of RareDis Copus, enabling it to learn task-specific patterns rather than relying solely on prompts 27. For each training pair (𝑥!,𝑦!)∈𝐃'()*+, the prompt 𝜋! is constructed by:  𝜋!=basic	components∥advanced	components∥𝑥! The objective of task-level fine-tuning is to minimize the empirical loss: min𝛉1𝑁b𝐿(𝑀𝛉(𝜋!),𝑦!)$!"# where 𝐿 is a token-level cross-entropy loss between the identified entity and the ground-truth 𝑦!. In this study, we fine-tune the GPT-4o-mini-2024-07-18 model on the RareDis Corpus. Training is conducted using OpenAI’s API interface, with hyperparameters batch size, learning rate multiplier, and number of epochs set to “auto”. The held-out validation set 𝐃,)-	is used for early stopping to mitigate overfitting. Training and validation examples are formatted as JSONL records, each containing both the inquiry input 𝑥! and the corresponding entity labels 𝑦!, along with the full prompt structure. Unlike in-context learning, no additional examples are prepended at inference time. After fine-tuning, model performance is evaluated on the test set 𝐃'./' using the same five-component prompt structure but without any in-context demonstrations.  2.5 Retrieval-Augmented Generation Analysis 

 8 To augment prompt-based inference with external domain knowledge, we implement a retrieval-augmented generation (RAG) approach in which external reference is dynamically incorporated into the prompt at inference time 28. This enables the model to access semantically relevant background context without requiring gradient-based parameter updates, contrasting with task-level fine-tuning. We construct a domain-specific knowledge corpus from the Orphanet rare disease alignments database 29. Alternative biomedical-QA RAG systems have reported only marginal gains when retrieval snippets overlap the prompt content 30. Let 𝐊𝐂={(𝑑!,𝑧!)}!"#= denote the resulting corpus, where each entry consists of a disease name 𝑑! and corresponding definition snippet 𝑧!. The final RAG corpus contains 𝑇=6,860 entries, each tokenized to a length ℓ!∈[8,196], with a median of 53 tokens. Each entry 𝑧!∈𝐊𝐂 is mapped to a semantic embedding 𝑓(𝑧!)∈𝐑4567 using OpenAI’s text-embedding-3-large model. Likewise, the inquiry text 𝑥*+01*(2 is embedded as 𝑓(𝑥*+01*(2). We define the retrieval score as the Euclidean distance: 𝑑-𝑥*+01*(2,𝑧!0=o𝑓-𝑥*+01*(20−𝑓(𝑧!)o7 For a given 𝑥*+01*(2, the top-𝐾 retrieved knowledge snippets are selected: 𝑅-𝑥*+01*(20=argmin𝐒⊂𝐊𝐂|𝐒|"Bb𝑑-𝑥*+01*(2,𝑧!0C!⊂𝐒 These retrieved snippets are concatenated into a prefix segment of the prompt, “Here are knowledge snippets:”, followed by the prompts described in previous sections. Two retrieval-augmented prompting strategies are tested: • Zero-shot + RAG – The full prompt consists of only the RAG knowledge prefix and the inquiry input and no labeled learning examples are included: 𝜋D.(EFGHI=basic	components∥advanced	components∥𝑅-𝑥*+01*(20∥𝑥*+01*(2 • Few-shot + RAG – In this setting, 𝑘 labeled learning examples ,-𝑥%,𝑦%01%"#3⊂𝐃'()*+ are included using the Inquiry-KNN strategy described in the Section 2.3. The full prompt becomes: 𝜋J.KFGHI=basic	components∥advanced	components∥ ,-𝑥%,𝑦%01%"#3∥	𝑅-𝑥*+01*(20∥𝑥*+01*(2 We vary 𝐾∈{1,2}, and 𝑘∈{1,2,4}, observing that larger values of 𝐾 often introduce semantic noise and lead to performance degradation. These two RAG-augmented strategies are evaluated against their non-RAG counterparts to quantify the incremental benefit of incorporating external biomedical knowledge at inference time.  

 9 2.6 Performance Evaluation Metrics We formulate rare disease NER as a text-to-entity sequence generation problem, where an LLM outputs a set of entity mentions based on a natural language input. Let the input text be denoted by a token sequence {𝑡#,𝑡7,…,𝑡&}, where 𝑛 is the total number of tokens. For any given entity type 𝑐, the corresponding ground-truth entity set is given by {𝑒#(M),𝑒7(M),...,𝑒O(M)}, where each 𝑒!(M)∈𝐄(M), and 𝐄(M) is the set of all valid entity strings. The model generates an identified set of entities 𝐄w(M)={𝑒̂#(M),𝑒̂7(M),...,𝑒̂3(M)}, where 𝑚 and 𝑘 may differ.  An entity recognition 𝑒̂%(M)∈𝐄w(M) is considered a true positive if there exists a 𝑒!(M)∈𝐄(M) such that 𝑒̂%(M)=𝑒!(M)  (i.e., exact string match). We denote the number of such correct matches as the true positive for entity type 𝑐: TP(M)=|}𝑒̂%(M)∈𝐄w(M):𝑒̂%(M)∈𝐄(M)| Accordingly, we define the model evaluation metrics for entity type 𝑐 as follows. Precision is the proportion of identified entities that are correct: Precision(M)=TP(M)|𝐄w(M)| Recall is the proportion of ground-truth entities that are correctly identified: Recall(M)=TP(M)|𝐘(M)| F1 score is the harmonic mean of precision and recall F1(M)=2×Precision(M)Precision(M)+Recall(M) These metrics are computed separately for each entity type 𝑐∈{rare disease, disease, sign, symptom} under varying prompt configurations and learning methods described in previous sections.  2.7 Error Taxonomy and Quantification To better understand model behavior, we perform a token-string error analysis on the test set for all four entity types. For each input text 𝑥, we consider its ground-truth entity set 𝐄(M) and the model-identified set 𝐄w(M) for entity class 𝑐. The recognitions are obtained using Inquiry-KNN method, with 𝑘 selected based on the highest observed F1 score (see Results and Figure 1). For any input 𝑥, if no ground-truth entities of class 𝑐 exist, then 𝐄(𝒄)=∅. Similarly, if the model 

 10 produces no output for class 𝑐, then 𝐄w(M)=∅. Each identified entity 𝑒̂ of type 𝑐̂ is compared to all ground-truth entities 𝑒(M)∈𝐄(M) using a case-insensitive token overlap metric: 𝑂-𝑒(M),𝑒̂0=atokens(𝑒(M))∩tokens(𝑒̂)a A greedy one-to-one alignment procedure is applied, where each recognition is matched to the first available ground-truth span with which it shares the highest token overlap 𝑂-𝑒(M),𝑒̂0>0. Remaining unmatched recognitions and ground-truth entities are retained as spurious and missed, respectively. Aligned entity pairs -𝑒(M),𝑒̂0 are classified into one of six mutually exclusive categories: • Correct – The identified span exactly matches the ground-truth span and the identified entity type matches the true annotation: 𝑒̂=𝑒(M),𝑐̂=𝑐 • Boundary – The identified and ground-truth spans have non-zero token overlap but are not identical, with the correct entity type: 𝑒̂≠𝑒(M),𝑂-𝑒(M),𝑒̂0>0,𝑐̂=𝑐 • Type – The identified span exactly matches the ground-truth span, but the entity type is incorrect: 𝑒̂=𝑒(M),𝑒̂≠𝑒(M) • Boundary + Type – The identified and ground-truth spans overlap but are not identical, and the identified type is incorrect: 𝑒̂≠𝑒(M),𝑂-𝑒(M),𝑒̂0>0,𝑐̂≠𝑐 • Spurious – The identified entity 𝑒̂ cannot be aligned to any ground-truth entity of type 𝑐 (i.e., no overlapping span), representing a false positive. • Missed – A ground-truth entity 𝑒(M) cannot be aligned to any recognition, representing a false negative.  2.8 Performance-Cost Analysis We conduct a performance-cost analysis to quantify how each 𝑘-shot configuration trades off F1 score against the monetary cost incurred per query using the OpenAI API. Pricing is based on the April 2025 OpenAI pricing sheet, which charges $5 per 1 million input tokens. We compute the number of input tokens for each query under both zero-shot and few-shot settings, ignoring output tokens due to their negligible length in the NER task (typically 10-20 tokens). For each entity type, we compute the average per-query cost for 𝑘=0,1,2,…,16. We then regress F1 score against cost to obtain smooth performance-cost curves, aiming to characterize the cost-

 11 efficiency of different prompt configurations. Two distinct regression models are adopted based on the empirical shape of the F1-cost relationship for each entity type: • Asymptotic-exponential regression – For entity types exhibiting a monotonic and saturating increase in F1 score (rare disease, disease, and sign), we model the performance-cost curve using a one-phase asymptotic exponential function 31: 𝐹(𝑥)=𝐹Q−𝑅5∙exp(−λ𝑥),𝑥>0 Here, 𝐹Q∈(0,1) is the maximum attainable F1 score given unbounded cost; 𝑅5=𝐹Q−𝐹(0) is the performance gain achievable over the zero-shot baseline; λ>0 controls the rate of saturation, where higher values imply faster convergence. Model fitting is performed using nonlinear least squares with a Gauss-Newton optimizer, implemented by nls function in R programming language. We also derive the half-rise cost: 𝑐5.S=	ln2λ which reflects the expenditure required to achieve 50% of the total attainable improvement 𝑅5. • Local polynomial regression – The symptom entity does not conform to the monotonic rise assumption; instead, its performance curve is non-monotonic and lacks a well-defined plateau. We therefore conduct local polynomial regression using a Locally Estimated Scatterplot Smoothing (LOESS) method with a span of 0.75 32. The smoother fits a first-order local regression at each evaluation point 𝑥 with tri-cube kernel weighting: 𝑤%(𝑥)=1−𝑑%𝑑OT944 where 𝑑% is the distance between the evaluation point 𝑥 and training point 𝑐%, and 𝑑OT9	is the maximum distance within the local neighborhood defined by the span. The fitted value 𝐹w(𝑥) is obtained by minimizing the locally weighted least squares criterion: b𝑤%(𝑥),𝐹%−𝛽5(𝑥)−𝛽#(𝑥)(𝑥−𝑥%)17% where 𝐹% is the observed F1 score at cost value 𝑥%. The coefficients 𝛽5(𝑥), 𝛽#(𝑥) define a locally linear approximation of the performance-cost curve near 𝑥, with 𝐹w(𝑥)=𝛽5(𝑥)  as the locally fitted value. The span hyperparameter (0.75) is selected a priori to balance the bias-variance trade-off, given the relatively coarse granularity of the 𝑘-shot cost grid.  3 Results 

 12 3.1 Zero-Shot Learning To evaluate model performance in the zero-shot learning setting, we conduct NER analysis using three prompt configurations, each excluding in-context examples. The first design includes only the basic components: task description, output format, and inquiry text. The second design extends this by incorporating task guidance, while the third design adds disambiguation rule on top of the prior components. We compare them performance against BioClinicalBERT, a domain-specific variant of BERT pretrained on PubMed and MIMIC-III corpora. In a prior study, Shyr et al. tested BioClinicalBERT on the RareDis Corpus and achieved SOTA performance on rare disease NER 32. We adopt their reported results as the baseline in this comparison. Table 4 summarizes precision, recall, and F1 score across all four entity types for each zero-shot prompt configuration and the BioClinicalBERT baseline. BioClinicalBERT outperforms all zero-shot prompt designs in overall F1, confirming the advantage of supervised learning in this domain. Besides, several trends emerge from the zero-shot results. For rare disease, the basic prompt achieves the highest precision (0.914) across all methods including BioClinicalBERT, though at the cost of reduced recall. Incorporating task guidance and disambiguation rule notably improves recall (from 0.463 to 0.576) and lifts the F1 score from 0.614 to 0.702. This demonstrates that task-specific guidance can significantly enhance recall without severely compromising precision, bringing zero-shot performance closer to the SOTA benchmark (F1 = 0.837). For disease, a different pattern is observed. The prompt with all components achieves the highest precision (0.545), surpassing BioClinicalBERT (0.494). However, its recall remains low (0.221), leading to a relatively modest F1 score (0.314 vs. 0.491 for SOTA). This suggests that while disambiguation helps filter false positives, it may also suppress valid predictions in this entity type. In the case of sign, the basic prompt yields the highest F1 score (0.392) among zero-shot variants. Task guidance and disambiguation appear to reduce recall (from 0.362 to 0.221), without improving precision. This indicates that the pretrained LLM already captures sufficient contextual cues for sign recognition, and that additional prompt instructions may introduce constraints that hinder generalization. For symptom, precision and F1 score remain low across all configurations. Interestingly, the basic prompt achieves the highest recall (0.653), while the full prompt improves precision (0.142) and F1 score (0.230). Nevertheless, all zero-shot prompts fall well below the BioClinicalBERT (F1 = 0.648), highlighting the difficulty without explicit supervision. Among the three prompt configurations, the full prompt yields the highest F1 scores for three out of four entity types, suggesting that task-specific definitions and error-aware instructions enhance model performance even without labeled examples. For the sign entity type, the basic prompt alone achieves the best F1 score. This result implies that the LLM pretrained on large-scale general corpora may already encode sufficient knowledge of signs, and that further guidance could introduce redundancy or noise. Overall, while none of the zero-shot prompts 

 13 match the performance of BioClinicalBERT, they demonstrate competitive precision and recall in certain scenarios, indicating the potential of prompt engineering as a lightweight alternative in low-resource applications.   3.2 Few-Shot Learning Figure 1 summarizes the F1 scores of few-shot learning across four entity types, comparing different example selection strategies. In general, increasing the number of in-context learning examples 𝑘 improves model performance, though the degree and pattern of improvement vary by entity type and selection method. For rare disease (Figure 1A), all methods except Inquiry-Random show consistent gains as 𝑘 increases. The Inquiry-KNN strategy consistently outperforms all others and notably exceeds the SOTA (F1 = 0.704) across almost all 𝑘, with performance peaking around 𝑘=8. Interestingly, even a small number of well-selected examples (e.g., 𝑘=2) leads to substantial improvement over zero-shot (F1 = 0.702), highlighting the value of semantically aligned demonstrations. A recent multilingual study likewise found that properly selected few-shot cues can outperform fully supervised baselines in English, French, and Spanish clinical NER 33. For disease (Figure 1B), the model again benefits from increased 𝑘, but the gains plateau earlier, around 𝑘=4 to 8. Inquiry-KNN achieves the best results (F1 = 0.518), surpassing the SOTA (F1 = 0.491), and Cluster-KNN follows closely. In contrast, Inquiry-Random yields marginal improvement over zero-shot (F1 = 0.314), underscoring the importance of semantic relevance in example selection. The sign entity (Figure 1C) exhibits a slower performance climb, with gains tapering after 𝑘=8. Unlike rare disease and disease, Cluster-KNN-64 performs slightly better than Inquiry-KNN across most 𝑘, suggesting that collective similarity within clusters is more effective than pointwise similarity in this entity type. All three semantic-based methods outperform Inquiry-Random, reinforcing the previous trend. For symptom (Figure 1D), performance is underperformed compared to zero-shot (F1 = 0.230) across all methods and values of 𝑘. In some cases, adding examples degrades performance. This likely reflects the ambiguous nature of symptom annotations, or misalignment between training examples and the model’s pretrained representation of medical terms. Among different strategies, Inquiry-KNN yields the highest performance (F1 = 0.223 at 𝑘=14). Comparing selection strategies, we observe that Inquiry-KNN yields the highest F1 scores for rare disease, disease, and symptom, while Cluster-KNN-64 leads on sign. Across entity types, Inquiry-Random consistently underperforms, indicating that semantic similarity – either at the individual or cluster level – is crucial for effective in-context learning. Notably, Cluster-KNN’s strong performance demonstrates the potential of collective similarity, which may reduce overfitting to query-specific features that affect pointwise nearest-neighbor strategies like Inquiry-KNN. In addition, Cluster-KNN assigns the same exemplar set to all inputs within a 

 14 cluster and therefore requires fewer total labeled examples than Inquiry-KNN, which selects a distinct set of examples per query. This makes Cluster-KNN more scalable in scenarios where annotation cost is a bottleneck. Overall, few-shot learning with semantically aligned examples improves performance over zero-shot learning and surpasses SOTA in rare disease and disease entities. These findings underscore the effectiveness of few-shot learning as a low-resource alternative to supervised training when guided by appropriate example selection strategies.  3.3 Task-Level Fine-Tuning Achieves New SOTA Table 5 compares the NER performance of the task-level fine-tuned GPT-4o-mini model with zero-shot, few-shot, and BioClinicalBERT across four entity types. For zero-shot and few-shot settings, we report the best results across all prompt configurations and learning example selection strategies. Overall, fine-tuning GPT-4o-mini establishes a new SOTA in terms of both F1 score and recall for three entity types. For rare disease, fine-tuning achieves an F1 score of 0.837, exceeding both the zero-shot (0.702) and few-shot (0.776) variants, and outperforming BioClinicalBERT (0.704), despite its extensive pretraining on general biomedical corpus. The performance gain is especially pronounced in recall (0.822 vs. 0.702 for few-shot), indicating that model exposure to task-specific supervision improves entity coverage substantially. In the disease category, fine-tuning again leads in F1 (0.702), outperforming few-shot (0.518) and BioClinicalBERT (0.491). The fine-tuned model also achieves substantially higher precision (0.713 vs. 0.545) and recall (0.692 vs. 0.511) compared to the few-shot model. For sign, fine-tuning provides the highest F1 score (0.541), benefiting from the strongest recall (0.561) despite slightly lower precision than BioClinicalBERT (0.522 vs. 0.561). This highlights a recall-precision trade-off, where fine-tuning favors completeness over conservatism in recognition. Compared to other entity types, the symptom exhibits a slightly different trend. BioClinicalBERT achieves the highest F1 score (0.648) and the highest precision (0.667), outperforming the fine-tuned GPT-4o-mini, which attains an F1 of 0.614 with slightly higher recall (0.633 vs. 0.630). The performance gap is narrower here than in other entity types, and both models substantially outperform the zero-shot (F1 = 0.230) and few-shot (F1 = 0.223) models. These results suggest that the more ambiguous and context-sensitive symptom extraction benefits from broad biomedical pretraining and may require additional contextual reasoning beyond prompt-based learning. Across all four entity types, task-level fine-tuned GPT-4o-mini consistently yields balanced precision and recall, indicating robust generalization and reliability for NER tasks. In contrast, zero-shot prompting tends to favor precision at the expense of recall, while few-shot learning offers intermediate gains but does not consistently close the performance gap, particularly on high-recall tasks. These findings reinforce the value of full-model fine-tuning when task-specific labeled data is available. Unlike BioClinicalBERT, which is trained on general-purpose biomedical corpora (PubMed, MIMIC-III), GPT-4o-mini benefits from being directly fine-tuned 

 15 on the RareDis Corpus, allowing it to internalize domain-specific terminology, entity structure, and annotation conventions. However, it is important to acknowledge the resource-intensive nature of fine-tuning, which requires substantial annotation effort, model retraining, and validation infrastructure. In contrast, few-shot learning achieves near-SOTA results at a fraction of the labeling cost, particularly for rare disease and sign.  3.4 Impact of RAG on Zero- and Few-Shot Learning Table 6 presents the performance gains achieved by augmenting prompts with one or two knowledge snippets retrieved from the RAG corpus, across zero-shot and few-shot learning settings (with 1, 2, or 4 learning examples). Only metric-entity combinations that show improvement with RAG augmentation are reported. Among the 48 metric-entity combinations evaluated, 13 (27%) demonstrate measurable improvement with RAG. The degree of benefit varies across evaluation metrics: 7 out of 16 (44%) precision scores improve, compared to 3 out of 16 (19%) for recall and 3 out of 16 (19%) for F1 score. The average gains are modest, with 0.016 for precision, 0.045 for recall, and 0.013 for F1 score. By entity type, symptom recognition shows the greatest relative benefit (6 of 12 metrics improved, 50%), followed by sign (3 of 12, 25%), disease (2 of 12, 13%), and rare disease (2 of 12, 13%). These results suggest that, in the context of rare disease NER, RAG provides limited additional value when high-quality prompts and relevant learning examples are already available. Improvements in precision imply that RAG snippets may help suppress false positives in select cases. However, the gains are marginal and do not scale with the number of snippets included. Recall also shows small improvement, except for modest boosts in sign and symptom recognition, indicating that RAG does not substantially surface new entities beyond what is already captured by the base prompt. Given that each RAG snippet contains roughly 50 tokens and requires a separate embedding computation, the cost-benefit trade-off becomes unfavorable for scenarios constrained by token budget or inference latency. In such settings, allocating resources toward additional learning examples or lightweight fine-tuning may yield better returns. Two factors likely explain the limited benefit of RAG in this task. First, GPT-4o model already encodes substantial biomedical knowledge, including lexical variants and factual associations relevant to rare and common diseases. As such, short RAG snippets often add little new information. Second, overlap between few-shot learning examples and retrieved snippets, in terms of disease mentions and syntactic structure, further diminishes the incremental value of RAG. When retrieved content redundantly mirrors patterns already shown in the prompt, its utility drops to near zero.  3.5 Error Analysis 

 16 Figure 2 visualize model-identified entities into six mutually exclusive categories defined in Section 2.7, providing a fine-grained view of model behavior. Rare disease recognition emerges as the most robust, with over half of all mentions correctly identified with exact span and type agreement (Correct = 51%), roughly twice the rate observed for disease (30%) and sign (23%). Most remaining errors are relatively benign, comprising modest span deviations (Boundary = 20%) and low-severity false positives (Spurious = 15%). Omissions are comparatively infrequent (Missed = 14%), and the near absence of class confusion (Type + Boundary and Type < 1%) confirms that rare disease are rarely mislabeled. The disease and sign categories exhibit complementary error profiles. Disease recognition is primarily limited by recall, with nearly one-third of ground-truth mentions undetected (Missed = 29%). Sign recognition, on the other hand, suffers more from low precision: boundary drift affects 31% of recognitions, and a further 33% are spurious hallucinations, indicating substantial over-generation. Together, these two entity types account for a substantial portion of the overall error volume illustrated in Figure 2 and highlight the divergent sources of model failure. The symptom category presents a distinct pattern. While the model identifies a relatively large proportion of entities (Correct = 35%), nearly half of all outputs are unsupported by the ground truth (Spurious = 46%), often involving patient-reported terms like fatigue or nausea that annotators deem out of scope. These results suggest that overgeneration, rather than type confusion or omission, is the dominant failure mode for symptoms. The remediation may require refining entity boundaries or prompt specificity than on increasing the number of examples. Overall, Boundary, Spurious, and Missed collectively account for the majority of errors, with Spurious alone exceeding 30% in Sign and 45% in Symptom. This indicates that improvements in model performance may be more effectively achieved through post-processing heuristics, such as dictionary-based filtering to suppress unsupported outputs and head-noun alignment to correct span drift.  3.6 Performance-Cost Trade-off  Figure 3 illustrates the F1 scores achieved with zero- and few-shot learning as a function of the corresponding per-query cost. For rare disease, disease, and sign entities, the F1-cost relationship exhibits a smooth saturating trend, well-modeled by an asymptotic-exponential function. In contrast, the symptom entity displays pronounced non-monotonic behavior, for which a LOESS smoother is used. The estimated performance ceiling 𝐹Q and the half-rise cost 𝑐5.S=ln2/λ  for the exponential fits are summarized in Table 7. For rare disease recognition, F1 score increases from 0.702 at zero-shot (0.19¢) to 0.760 at 𝑘	=	4 (0.64¢), reaching 96% of the estimated ceiling. Beyond this point, each additional cent of inference cost contributes less than 0.003 F1 improvement, showing rapid saturation. For disease and sign, performance plateaues near F1 ≈ 0.50, with half of the total gain achieved at a cost of 

 17 approximately 0.10¢ (𝑘=2). However, an additional 0.9¢ is required to close the final 5% of the performance gap (𝑘	=	8). Past this threshold, marginal returns drop below 0.002 F1 per additional cent. For symptom recognition, the fitted LOESS curve fluctuates within ±0.021 of the baseline F1 ≈ 0.230 across the entire 0–2¢ cost range, revealing no meaningful benefit from increasing the number of learning examples. This result suggests that token budget is largely ineffective for improving model accuracy on this entity type. Overall, a uniform budget cap of ~1¢ per query, equivalent to up to 8-shot prompting, is sufficient to capture ≥ 95% of the attainable performance for rare disease, disease, and sign entities. Beyond this threshold, further spending results in negligible gains. For symptoms, improvements are likely better achieved through alternative strategies, such as data augmentation, label refinement, or fine-tuning, rather than through prompt expansion.  4 Discussion This study demonstrates that prompt-based LLMs can achieve competitive performance in rare disease NER without extensive task-specific training data. One key finding is the importance of prompt design and learning example selection. We observe that semantic selection of in-context examples consistently improves NER accuracy over random example selection. This is in line with recent reports that contextually relevant demonstrations boost medical NER performance 19. In contrast, incorporating external knowledge via RAG yields only marginal gains, suggesting that GPT-4o already possesses considerable domain knowledge. This result diverges from the large improvements RAG has shown on knowledge-intensive QA tasks 33, indicating that for rare disease NER, the bottleneck is less about world knowledge but more about recognizing precise spans in context. Our analysis also highlights the cost-efficiency of the prompt-based approach. With only a handful of well-chosen examples, GPT-4o achieves strong results at a fraction of the total cost than collecting and curating a large expert-annotated corpus. In essence, prompt-based GPT-4o offers high returns for low investment, making it an attractive solution in low-resource NER scenarios 34. Our NER pipeline represents a departure from traditional supervised approaches in biomedical NER. Historically, state-of-the-art results come from language models like BioBERT and BioClinicalBERT pretrained on general biomedical corpora, or earlier from statistical sequence taggers and LSTM-based models 35. Recent studies begin to explore the potential of LLM in using prompt engineering. For instance, Agrawal et al. show that GPT-3 could perform few-shot clinical information extraction comparably to fully trained models 36; Xi et al. apply GPT-based methods to Reddit posts to characterize patient-reported manifestations of sarcoidosis 37. The significance of our findings is underscored by comparisons to earlier work on rare disease text mining, which are often bottlenecked by data scarcity 38. We confirm these observations and demonstrate that a next-generation LLM can substantially close the performance gap with 

 18 domain-trained models. Our work builds upon and goes beyond prior insights, showing that prompt-based LLMs can achieve near-parity with SOTA in low-resource rare disease NER. The performance of GPT-4o in the few-shot learning context suggests that institutions can leverage a pretrained LLM to perform entity recognitions without large-scale data annotation. In settings where rapid deployment is valued over absolute peak performance, our prompt-based pipeline offers a compelling solution. LLM-powered NER system can also be integrated into electronic health records to label rare disease mentions in physicians’ notes and identify patients fit certain rare disease profiles. Another use case is biomedical knowledge curation, in which researchers could use GPT-4o to extract disease-phenotype associations from research papers or case reports. The system can also be easily repurposed for new subtasks by modifying the prompts, rather than retraining models. Because the NER logic resides in the prompt and model rather than custom code, deploying the system can be as simple as calling an API endpoint. This lowers the barrier for institutions that lack extensive machine learning infrastructure.  Our study has several limitations that warrant discussion. First, prompting still lags behind specialized models in certain NER tasks. These gaps echo observations by Hu et al. (2023) and Chen et al. (2023) that GPT-3.5/4 underperform domain-specific models on detailed biomedical NER 17,39. For scenarios requiring strict annotation fidelity, post-processing or alignment is needed to refine the raw outputs of the LLM. The second limitation is the marginal benefit observed from RAG in our experiments. One possible reason is that GPT-4o already encodes a wealth of medical knowledge from its pretraining, and additional snippets contribute limited new information. It is also plausible that our retrieval method does not select sufficiently targeted context. More sophisticated retrieval, such as grabbing example sentences of the exact entity usage, might yield a greater benefit 39. Finally, the reliance on API is a practical limitation. Using a closed-source model means that reproducibility and long-term deployment are not fully guaranteed. Sending sensitive patient data to an external API can also conflict with privacy regulations 40. Therefore, deploying a similar system in a hospital setting would require robust de-identification solutions 41,42. There are several avenues to extend current study. One direction is to combine the strengths of LLMs with rule-based systems. After generating candidate entities, a post-processing step could apply heuristic rules or dictionary matching to correct span boundaries and unify terminology 42. Even simple alignment rules, such as ensuring the output exactly matches a known rare disease name, could substantially increase precision without requiring model retraining. Another promising avenue is self-consistency decoding, which generates multiple outputs for the same input and then taking a majority vote among the answers 43. Similarly, incorporating chain-of-thought prompting may help the model internally reason about the text 44. By guiding the model through intermediate reasoning steps, it is possible to resolve ambiguities and improve the recognition of difficult entities. Lastly, a hybrid strategy worth exploring is to fine-tune a model on synthetic annotations generated by the LLM. Recent work suggests that LLMs can create high-fidelity synthetic data for training downstream models 45. To apply this strategy, GPT-4o 

 19 could annotate a large collection of unlabeled clinical texts, possibly with iterative refinement or human review. A compact model fine-tuned on this corpus might then serve as a cost-effective and privacy-preserving solution that approaches SOTA. In conclusion, our study shows that thoughtful prompt engineering and use of learning examples can serve as a powerful tool for rare disease NER. We have discussed how our findings both align with and extend prior knowledge, the practical trade-offs involved, and the limitations that temper the results. By addressing those limitations through the future directions outlined above, we anticipate that prompt-based LLM approaches will become even more accurate, interpretable, and integrated into real-world biomedical text mining pipelines.   Conflicts of Interest Nan Miles Xi and Yu Deng are full-time employees of AbbVie and may hold stock or stock options in the company. AbbVie had no role in the design, analysis, interpretation, or decision to publish this study. All other authors declare no conflicts of interest.  Code and Data Availability  All code needed to reproduce the results of this study are openly accessible at GitHub repository https://github.com/xnnba1984/Leveraging-Large-Language-Models-for-Rare-Disease-Named-Entity-Recognition/tree/main.  The RareDis corpus is downloaded from https://github.com/isegura/NLP4RARE-CM-UC3M.  The Orphanet database in the RAG is downloaded from https://www.orphadata.com/orphanet-scientific-knowledge/.  Funding Author Lin Wang is supported by the National Science Foundation (DMS-2413741) and the Central Indiana Corporate Partnership AnalytiXIN Initiative.  Author Contributions Nan Miles Xi performed prompt engineering framework, performance evaluation, and manuscript writing. Yu Deng contributed to data preprocessing, embedding generation, model implementation, and results analysis. Lin Wang led the conceptualization and study design, 

 20 provided guidance on statistical analysis and manuscript review. All authors reviewed and approved the final version of the manuscript.  Reference 1. Lee, C. E., Singleton, K. S., Wallin, M. & Faundez, V. Rare genetic diseases: Nature’s experiments on human development. iScience 23, 101123 (2020). 2. Segura-Bedmar, I., Camino-Perdones, D. & Guerrero-Aspizua, S. Exploring deep learning methods for recognizing rare diseases and their clinical manifestations from texts. BMC Bioinformatics 23, 263 (2022). 3. Shyr, C. et al. Identifying and Extracting Rare Diseases and Their Phenotypes with Large Language Models. Journal of Healthcare Informatics Research (2024) doi:10.1007/s41666-023-00155-0. 4. Thukral, A., Dhiman, S., Meher, R. & Bedi, P. Knowledge graph enrichment from clinical narratives using NLP, NER, and biomedical ontologies for healthcare applications. Int. J. Inf. Technol. (2023) doi:10.1007/s41870-022-01145-y. 5. Durango, M. C., Torres-Silva, E. A. & Orozco-Duque, A. Named entity recognition in Electronic Health Records: A methodological review. Healthc. Inform. Res. 29, 286–300 (2023). 6. Cao, L., Sun, J. & Cross, A. An automatic and end-to-end system for rare disease knowledge graph construction based on ontology-enhanced large language models: Development study. JMIR Med. Inform. 12, e60665 (2024). 7. Guo, M. et al. Identifying COVID-19 cases and extracting patient reported symptoms from Reddit using natural language processing. Sci. Rep. 13, 1–13 (2023). 8. Groza, T. et al. An evaluation of GPT models for phenotype concept recognition. BMC Med. Inform. Decis. Mak. 24, 30 (2024). 9. Nguengang Wakap, S. et al. Estimating cumulative point prevalence of rare diseases: analysis of the Orphanet database. Eur. J. Hum. Genet. 28, 165–173 (2020). 10. Lee, J. et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, 1234–1240 (2020). 11. Alsentzer, E. et al. Publicly available clinical BERT embeddings. arXiv [cs.CL] (2019). 12. Portelli, B. et al. Generalizing over long tail concepts for medical term normalization. in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (eds. Goldberg, Y., Kozareva, Z. & Zhang, Y.) 8580–8591 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2022). 13. Brown, T. B. et al. Language Models are Few-Shot Learners. Neural Inf Process Syst abs/2005.14165, 1877–1901 (2020). 14. Oniani, D. et al. Large Language Models Vote: Prompting for rare disease identification. arXiv [cs.CL] (2023). 15. Wu, J. et al. A hybrid framework with large language models for rare disease phenotyping. BMC Med. Inform. Decis. Mak. 24, 289 (2024). 16. Chen, P., Wang, J., Lin, H., Zhao, D. & Yang, Z. Few-shot biomedical named entity recognition via knowledge-guided instance generation and prompt contrastive learning. Bioinformatics 39, btad496 (2023). 17. Chen, Q. et al. Benchmarking large language models for biomedical natural language processing applications and recommendations. Nat. Commun. 16, 3280 (2025). 

 21 18. Lu, Q. et al. Large Language Models struggle in token-level clinical Named Entity Recognition. arXiv [cs.CL] (2024). 19. Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. Neural Inf Process Syst abs/2005.11401, 9459–9474 (2020). 20. Li, M., Zhou, H., Yang, H. & Zhang, R. RT: a Retrieving and Chain-of-Thought framework for few-shot medical named entity recognition. J. Am. Med. Inform. Assoc. 31, 1929–1938 (2024). 21. OpenAI et al. GPT-4 Technical Report. arXiv [cs.CL] (2023). 22. Kim, Y. & Riloff, E. Stacked generalization for medical concept extraction from clinical notes. in Proceedings of BioNLP 15 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2015). doi:10.18653/v1/w15-3807. 23. Kraljevic, Z. et al. Multi-domain clinical natural language processing with MedCAT: The Medical Concept Annotation Toolkit. Artif. Intell. Med. 117, 102083 (2021). 24. Martínez-deMiguel, C., Segura-Bedmar, I., Chacón-Solano, E. & Guerrero-Aspizua, S. The RareDis corpus: A corpus annotated with rare diseases, their signs and symptoms. J. Biomed. Inform. 125, 103961 (2022). 25. The National Organization for Rare Disorders (NORD). in The Grants Register 2019 545–545 (Palgrave Macmillan UK, London, 2019). 26. Mathur, Y. et al. SummQA at MEDIQA-chat 2023:In-context learning with GPT-4 for Medical Summarization. arXiv [cs.CL] (2023). 27. Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv [cs.LG] 1–67 (2019). 28. Ngo, N. T., Van Nguyen, C., Dernoncourt, F. & Nguyen, T. H. Comprehensive and practical evaluation of retrieval-augmented generation systems for medical question answering. arXiv [cs.CL] (2024). 29. Weinreich, S. S., Mangon, R., Sikkens, J. J., Teeuw, M. E. en & Cornel, M. C. Orphanet: a European database for rare diseases. Ned. Tijdschr. Geneeskd. 152, 518–519 (2008). 30. Merker, J. H., Bondarenko, A., Hagen, M. & Viehweger, A. MiBi at BioASQ 2024: Retrieval-augmented generation for answering biomedical questions. CLEF 176–187 (2024). 31. Vianna, L. S., Gonçalves, A. L. & Souza, J. A. Analysis of learning curves in predictive modeling using exponential curve fitting with an asymptotic approach. PLoS One 19, e0299811 (2024). 32. Cleveland, W. S. & Devlin, S. J. Locally weighted regression: An approach to regression analysis by local fitting. J. Am. Stat. Assoc. 83, 596–610 (1988). 33. Naguib, M., Tannier, X. & Névéol, A. Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting. arXiv [cs.CL] (2024). 34. Klang, E. et al. A strategy for cost-effective large language model use at health system-scale. NPJ Digit. Med. 7, 320 (2024). 35. Dang, T. H., Le, H.-Q., Nguyen, T. M. & Vu, S. T. D3NER: biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embeddings of various linguistic information. Bioinformatics 34, 3539–3546 (2018). 36. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large language models are few-shot clinical information extractors. arXiv [cs.CL] (2022). 37. Xi, N. M., Ji, H.-L. & Wang, L. Understanding sarcoidosis using large language models and social media data. J. Healthc. Inform. Res. 1–26 (2024). 38. Kariampuzha, W. Z. et al. Precision information extraction for rare disease epidemiology at scale. J. Transl. Med. 21, 157 (2023). 

 22 39. Hu, Y. et al. Improving large language models for clinical named entity recognition via prompt engineering. J. Am. Med. Inform. Assoc. 31, 1812–1820 (2024). 40. Ng, M. Y., Helzer, J., Pfeffer, M. A., Seto, T. & Hernandez-Boussard, T. Development of secure infrastructure for advancing generative artificial intelligence research in healthcare at an academic medical center. J. Am. Med. Inform. Assoc. 32, 586–588 (2025). 41. Kovačević, A., Bašaragin, B., Milošević, N. & Nenadić, G. De-identification of clinical free text using natural language processing: A systematic review of current approaches. Artif. Intell. Med. 151, 102845 (2024). 42. Dennstädt, F., Hastings, J., Putora, P. M., Schmerder, M. & Cihoric, N. Implementing large language models in healthcare while balancing control, collaboration, costs and security. NPJ Digit. Med. 8, 143 (2025). 43. Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. arXiv [cs.CL] (2022). 44. Wei, J. et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. in Advances in Neural Information Processing Systems (eds. Koyejo, S. et al.) vol. 35 24824–24837 (Curran Associates, Inc., 2022). 45. Šuvalov, H. et al. Using synthetic health care data to leverage large language models for named entity recognition: Development and validation study. J. Med. Internet Res. 27, e66279 (2025).     

 23 Tables and Figures  Table 1. Summary statistics of RareDis Corpus dataset and definitions of its named entities.  Training set Validation set Test set Total Number of documents 729 104 208 1,041 Named entity Rare disease 3,608 525 1,088 5,221 Disease 1,647 230 471 2,348 Sign 3,744 528 1,061 5,333 Symptom 319 24 53 396 
Definition Rare disease A rare disease is a health condition that affects a small percentage of the population. In the U.S., a disease is considered rare if it affects fewer than 200,000 people. In the European Union, a disease is considered rare if it affects fewer than 1 in 2,000 people. Disease A disease is a condition of the body or mind that impairs normal functioning and is characterized by specific signs and symptoms. Diseases can be caused by a variety of factors, including infections, genetic mutations, environmental factors, and lifestyle choices Sign A sign of a disease is objective evidence of disease that can be observed or detected by someone other than the individual affected by the disease. It includes measurable indicators such as physical findings, laboratory test results, and imaging studies, which provide concrete evidence of a medical condition. Symptom A symptom is the subjective experience reported by the patient, which cannot be directly observed or measured by others. They reflect what the patient feels, such as pain, fatigue, or nausea. Symptoms are experienced internally and rely on the patient’s description.    

 24 Table 2. Prompt design components and task-specific instructions used for extracting each entity type. Prompt component Content Task description Identify the names of [entity] from the following text. Output format Output only the exact [entity] names without any additional changes. If there are multiple [entity], separate their names with commas. If there is no [entity], output ‘none’. Input text The text from which you need to exact the names of [entity] is … 
Task guidance Rare disease: A rare disease is a health condition that affects a small percentage of the population. In the U.S., a disease is considered rare if it affects fewer than 200,000 people. In European Union, a disease is considered rare if it affects fewer than 1 in 2,000 people. Disease: A disease is a condition of the body or mind that impairs normal functioning and is characterized by specific signs and symptoms. Diseases can be caused by a variety of factors, including infections, genetic mutations, environmental factors, and lifestyle choices. Sign: A sign of a disease is the objective evidence of disease that can be observed or detected by someone other than the individual affected by the disease. It includes measurable indicators such as physical findings, laboratory test results, and imaging studies, which provide concrete evidence of a medical condition. Symptom: A symptom is the subjective experience reported by the patient, which cannot be directly observed or measured by others. They reflect what the patient feels, such as pain, fatigue, or nausea. Symptoms are experienced internally and rely on the patient’s description. Disambiguation rule Rare disease: Treat abbreviations as separate rare disease names. Do not identify regular diseases as rare diseases. Disease: Differentiate between rare diseases and diseases. A rare disease is a health condition that affects a small percentage of the population. Rare diseases are a subset of diseases. Only output diseases, not rare diseases. Sign: Differentiate between signs and symptoms. Symptoms are subjective experiences of disease reported by the patient and cannot be directly measured by healthcare providers. Only output signs, not symptoms. Symptom: Differentiate between symptoms and signs. Signs are objective indicators of a disease that can be observed, measured, or detected by someone other than the patient, such as a doctor or medical professional. Only output symptoms, not signs.   

 25 Table 3. Exemplary in-context learning examples contained in the prompts. Named entity Content Rare disease Input text: Myhre syndrome is an extremely rare inherited disorder that, in theory, affects males and females in equal numbers. More than 60 cases have been reported in medical literature. Because some cases of Myhre syndrome most likely go undiagnosed or misdiagnosed, determining the true frequency of the disorder in the general population is difficult.   Output: myhre syndrome. Disease Input text: May-Hegglin Anomaly is a rare, inherited, blood platelet disorder characterized by abnormally large and misshapen platelets (giant platelets) and defects of the white blood cells known as leukocytes. The defect of the white blood cells consists of the presence of very small (2-5 micrometers) rods, known as Dohle bodies, in the fluid portion of the cell (cytoplasm). Some people with this disorder may have no symptoms while others may have various bleeding abnormalities. In mild cases, treatment for May-Hegglin Anomaly is not usually necessary. In more severe cases, transfusions of blood platelets may be necessary. May-Hegglin Anomaly is a rare blood platelet disorder that affects males and females in equal numbers.  It occurs more often in people of Greek or Italian descent than among others.  As of about 10 years ago, only about 170 cases were reported in the literature.   Output: inherited, blood platelet disorder, blood platelet disorder. Sign Input text: The autonomic nervous system controls involuntary actions such as widening or narrowing of our blood vessels. Failure in this system can lead to orthostatic hypotension, which means a sudden drastic drop in blood pressure especially from a lying or sitting down position. The exact cause of pure autonomic failure (PAF) is not known, but is defined as autonomic failure without central nervous system (brain or spinal cord) involvement. PAF is caused by abnormal accumulation of a protein called alpha-synuclein in autonomic nerves. This protein helps nerve cells communicate, but its function is not fully understood. Patients with PAF have a loss of nerve cells (neurons) in the intermediolateral column of the spinal cord. The worldwide prevalence of PAF is not known. The age of onset is during adulthood usually in individuals over 60 years. It is more common in males than in females.   Output: orthostatic hypotension, sudden drastic drop in blood pressure, accumulation of a protein called alpha-synuclein in autonomic nerves. Symptom Input text: Carbamoyl phosphate synthetase I deficiency (CPSID) is a rare inherited disorder characterized by complete or partial lack of the carbamoyl phosphate synthetase (CPS) enzyme. This is one of five enzymes that play a role in the breakdown and removal of nitrogen from the body, a process known as the urea cycle. The lack of the CPSI enzyme results in excessive accumulation of nitrogen, in the form of ammonia (hyperammonemia), in the blood. Affected children may experience vomiting, refusal to eat, progressive lethargy, and coma. CPSID is inherited as an autosomal recessive genetic disorder. The estimated frequency of CPSID is 1 in 150-200,000 births. The estimated frequency of urea cycle disorders collectively is one in 30,000.  However, because urea cycle disorders like CPSID often go unrecognized, these disorders are under-diagnosed, making it difficult to determine the true frequency of urea cycle disorders in the general population.   Output: refusal to eat, progressive lethargy. 

 26 Table 4. NER performance of different prompt designs under zero-shot learning. The state-of-the-art (SOTA) results are based on BioClinicalBERT model fine-tuned on PubMed database and MIMIC-III dataset. The best performances among different prompt designs and BioClinicalBERT model are underscored for each task. Named entity Prompt and model Precision Recall F1 score Rare disease Basic 0.914 0.463 0.614 Basic + Task guidance 0.873 0.442 0.587 Basic + Task Guidance + Disambiguation rule 0.897 0.576 0.702 BioClinicalBERT (SOTA) 0.853 0.822 0.837 Disease Basic 0.230 0.282 0.253 Basic + Task guidance 0.252 0.297 0.273 Basic + Task Guidance + Disambiguation rule 0.545 0.221 0.314 BioClinicalBERT (SOTA) 0.494 0.488 0.491 Sign Basic 0.426 0.362 0.392 Basic + Task guidance 0.387 0.257 0.309 Basic + Task Guidance + Disambiguation rule 0.377 0.221 0.278 BioClinicalBERT (SOTA) 0.561 0.516 0.538 Symptom Basic 0.048 0.653 0.090 Basic + Task guidance 0.097 0.592 0.167 Basic + Task Guidance + Disambiguation rule 0.142 0.612 0.230 BioClinicalBERT (SOTA) 0.667 0.630 0.648    

 27 Table 5. NER performance of different models and learning methods. Each metric in zero-shot and few-shot learning is the best result across all prompt-example configurations. The best performances among different models are underscored for each task. Named entity Model Precision Recall F1 score Rare disease Fine-tuning  0.853 0.822 0.837 Zero-shot  0.914 0.576 0.702 Few-shot  0.920 0.702 0.776 BioClinicalBERT 0.689 0.720 0.704 Disease Fine-tuning  0.713 0.692 0.702 Zero-shot  0.545 0.297 0.314 Few-shot  0.545 0.511 0.518 BioClinicalBERT 0.494 0.488 0.491 Sign Fine-tuning  0.522 0.561 0.541 Zero-shot  0.426 0.362 0.392 Few-shot  0.463 0.494 0.478 BioClinicalBERT 0.561 0.516 0.538 Symptom Fine-tuning  0.596 0.633 0.614 Zero-shot  0.142 0.612 0.230 Few-shot  0.134 0.673 0.223 BioClinicalBERT 0.667 0.630 0.648    

 28 Table 6. NER performance gains from RAG relative to in-context learning. A 𝑘-shot model refers to one prompted with 𝑘 labeled learning examples selected using the Inquiry-KNN method. The “+ 𝑛-RAG” condition additionally prepends 𝑛 knowledge snippets retrieved from the RAG corpus. Only metrics showing performance improvement with RAG snippets are reported. A dash (-) indicates no observed benefit. The best-performing configuration for each task is underscored. Named entity Model Precision Recall F1 score Rare disease 2-shot 0.870 - - 2-shot + 1-RAG 0.871 - - 4-shot 0.855 - - 4-shot + 2-RAG 0.886 - - Disease 2-shot 0.525 - - 2-shot + 2-RAG 0.545 - - 4-shot 0.508 - - 4-shot + 2-RAG 0.534 - - Sign Zero-shot 0.377 0.221 0.278 Zero-shot + 1-RAG 0.405 0.254 0.312 Symptom Zero-shot - 0.612 - Zero-shot + 1-RAG - 0.673 - 2-shot 0.114 - 0.193 2-shot + 2-RAG 0.116 - 0.196 4-shot 0.115 0.612 0.194 4-shot + 1-RAG 0.117 0.653 0.198     

 29 Table 7. Asymptotic performance and cost-efficiency metrics across four named entity types. Named entity Plateau 𝑭$ Half-rise cost 𝒄𝟎.𝟓 Cost at 95 % of plateau Description Rare disease 0.763 0.07 ¢ 0.62 ¢ (k ≈ 4) Fastest and highest saturation Disease 0.495 0.11 ¢ 1.05 ¢ (k ≈ 8) Gains diminish beyond 8-shot Sign 0.465 0.08 ¢ 0.94 ¢ (k ≈ 8) Mirrors disease trend Symptom 0.230 — — No systematic cost response         

 30  Figure 1. F1 scores of few-shot learning across different numbers of learning examples. A. Rear disease. B. Disease. C. Sign. D. Symptom. Two dash lines indicate the state-of-the-art (SOTA) performances using BioClinicalBERT model reported by Shyr et al. 3 and the best performance of prompt designs without learning examples (zero-shot) reported in Table 4, respectively.   


 31  Figure 2. Error distribution for each entity type. Each bar represents the proportion of entity predictions falling into one of six mutually exclusive categories on the test set. Results are calculated using Inquiry-KNN methods with the best-performing 𝑘-shot configuration per entity as determined by F1 score in Figure 1.    


 32  Figure 3. Cost–performance curves for the four named entities. Each point corresponds to a 𝑘-shot prompt evaluated on the test set. Solid lines are entity-specific smoothers: an asymptotic-exponential fit for rare disease, disease and sign, as well as a LOESS smoother for symptom, whose non-monotonic pattern violates the exponential assumption.   
