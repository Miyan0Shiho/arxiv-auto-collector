# Can Large Language Models Identify Materials from Radar Signals?

**Authors**: Jiangyou Zhu, Hongyu Deng, He Chen

**Published**: 2025-08-05 06:07:49

**PDF URL**: [http://arxiv.org/pdf/2508.03120v1](http://arxiv.org/pdf/2508.03120v1)

## Abstract
Accurately identifying the material composition of objects is a critical
capability for AI robots powered by large language models (LLMs) to perform
context-aware manipulation. Radar technologies offer a promising sensing
modality for material recognition task. When combined with deep learning, radar
technologies have demonstrated strong potential in identifying the material of
various objects. However, existing radar-based solutions are often constrained
to closed-set object categories and typically require task-specific data
collection to train deep learning models, largely limiting their practical
applicability. This raises an important question: Can we leverage the powerful
reasoning capabilities of pre-trained LLMs to directly infer material
composition from raw radar signals? Answering this question is non-trivial due
to the inherent redundancy of radar signals and the fact that pre-trained LLMs
have no prior exposure to raw radar data during training. To address this, we
introduce LLMaterial, the first study to investigate the feasibility of using
LLM to identify materials directly from radar signals. First, we introduce a
physics-informed signal processing pipeline that distills high-redundancy radar
raw data into a set of compact intermediate parameters that encapsulate the
material's intrinsic characteristics. Second, we adopt a retrieval-augmented
generation (RAG) strategy to provide the LLM with domain-specific knowledge,
enabling it to interpret and reason over the extracted intermediate parameters.
Leveraging this integration, the LLM is empowered to perform step-by-step
reasoning on the condensed radar features, achieving open-set material
recognition directly from raw radar signals. Preliminary results show that
LLMaterial can effectively distinguish among a variety of common materials,
highlighting its strong potential for real-world material identification
applications.

## Full Text


<!-- PDF content starts -->

Can Large Language Models Identify Materials from Radar
Signals?
Jiangyou Zhu
The Chinese University of Hong Kong
Hong Kong SAR, China
zj124@ie.cuhk.edu.hkHongyu Deng
The Chinese University of Hong Kong
Hong Kong SAR, China
dh021@ie.cuhk.edu.hkHe Chenâˆ—
The Chinese University of Hong Kong
Hong Kong SAR, China
he.chen@ie.cuhk.edu.hk
Figure 1: Home robots often need to distinguish the material composition of visually similar objects during everyday tasks.
This figure illustrates such a scenario, where the robot needs to identify a glass cup to safely microwave a cup of milk, a task
made challenging by its visual similarity to a plastic cup, which may not be microwave-safe. Consequently, additional sensing
modalities or methods are necessary to achieve accurate material identification. This image is generated by the Doubao AI
model from a sketch input, with additional text overlaid on the generated image.
ABSTRACT
Accurately identifying the material composition of objects is a crit-
ical capability for AI robots powered by large language models
(LLMs) to perform context-aware manipulation. Radar technologies
(e.g., millimeter-wave radar) offer a promising sensing modality
for material recognition tasks, providing robustness to lighting
conditions and high spatial resolution. When combined with deep
learning, radar technologies have demonstrated strong potential in
identifying the material composition of various objects. However,
existing radar-based solutions are often constrained to closed-set
object categories and typically require task-specific data collec-
tion to train deep learning models, largely limiting their practical
applicability. This raises an important question: Can we leverage
the powerful reasoning capabilities of pre-trained LLMs to directly
infer material composition from raw radar signals ? Answering this
question is non-trivial due to the inherent redundancy of radar
signals and the fact that pre-trained LLMs have no prior exposure
to raw radar data during training. To address this, we introduce
LLMaterial, the first study to investigate the feasibility of using
âˆ—He Chen is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
UbiComp Companion â€™25, October 12â€“16, 2025, Espoo, Finland.
Â©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-1477-1/25/10. . . $15.00
https://doi.org/10.1145/3714394.3756289LLMs to identify materials directly from radar signals. First, we
introduce a physics-informed signal processing pipeline that distills
high-redundancy radar raw data into a set of compact intermediate
parameters that encapsulate the materialâ€™s intrinsic characteris-
tics. Second, we adopt a retrieval-augmented generation (RAG)
strategy to provide the LLM with domain-specific knowledge, en-
abling it to interpret and reason over the extracted intermediate
parameters. Leveraging this integration, the LLM is empowered to
perform step-by-step reasoning on the condensed radar features,
achieving open-set material recognition directly from raw radar
signals. Preliminary results show that LLMaterial can effectively
distinguish among a variety of common materials, highlighting its
strong potential for real-world material identification applications.
CCS CONCEPTS
â€¢Human-centered computing â†’Ubiquitous and mobile com-
puting systems and tools .
KEYWORDS
Material identification, radar sensing, large language model, retrieval-
augmented generation.
ACM Reference Format:
Jiangyou Zhu, Hongyu Deng, and He Chen. 2025. Can Large Language
Models Identify Materials from Radar Signals?. In Proceedings of Companion
of the 2025 ACM International Joint Conference on Pervasive and Ubiquitous
Computing (UbiComp Companion â€™25). ACM, New York, NY, USA, 8 pages.
https://doi.org/10.1145/3714394.3756289arXiv:2508.03120v1  [eess.SP]  5 Aug 2025

UbiComp Companion â€™25, October 12â€“16, 2025, Espoo, Finland. Jiangyou Zhu, Hongyu Deng, & He Chen
1 INTRODUCTION
Object recognition is a fundamental capability for autonomous sys-
tems, underpinning a wide range of applications in home robotics,
assistive technologies, and industrial automation [ 13]. In recent
years, the application of AI to vision-based object recognition has
made substantial strides. In particular, the rise of large language
models (LLMs) has led to remarkable results not only in object iden-
tification but also in recognizing object attributes such as material
composition [ 2,11,12]. This progress is largely attributed to a key
strength of LLMs: their deep semantic understanding, developed
through large-scale pretraining on diverse and extensive datasets.
However, our real-world experiments reveal a critical limitation:
LLMs relying solely on visual inputs often fail to distinguish be-
tween visually similar objects, which can lead to unsafe decisions.
For instance, a robot may be unable to differentiate between a glass
and a plastic cup, an error that poses a safety risk when heating
liquids, as only glass is microwave-safe (see Figure 1). This chal-
lenge has also been highlighted in prior work [ 1], emphasizing the
need for more robust, multimodal object recognition systems that
extend beyond vision alone.
As an emerging sensing modality, radar is increasingly recog-
nized for its noticeable potential in material recognition. Specifi-
cally, RadarCat [ 10] pioneered this direction by using time-domain
analysis of near-field millimeter-wave (mmWave) signals and ma-
chine learning to classify materials based on their unique reflective
signatures. mSense [ 7] quantitatively characterizes the materialâ€™s
reflectivity using the material reflection feature (MRF), and then
determines the material type by finding the best match against a
pre-trained database storing the MRFs of different materials. CRFU-
SION [ 8] combines camera and radar modalities to simultaneously
identify object categories and materials, using the energy reflection
factor (ERF) as a key feature within a modality fusion network.
However, a common limitation of these approaches is that they
often rely on extensive data collection to train end-to-end mate-
rial identification models. However, their effectiveness is typically
confined to a narrow range of materials, which noticeably limits
their practical utility in real-world applications. Given the profound
comprehension capabilities of LLMs, a promising direction is to
leverage them for interpreting radar signals in material recognition
tasks. This leads us to our research question: Can LLMs infer an
objectâ€™s electromagnetic properties and accurately identify its material
from radar signals in an open-set setting?
Answering this question is nontrivial, as it involves overcoming
challenges stemming from both the limitations of the LLMâ€™s prior
knowledge and the complex nature of raw radar data. First, because
radar raw data is rarely represented in the pretraining datasets of
LLMs, these models lack the inherent ability to directly interpret
the semantic content embedded in radar signals. This necessitates
a dedicated preprocessing step to translate raw radar inputs into a
representation that aligns with the LLMâ€™s modality and semantic
understanding. In addition, the high dimensionality and inherent
redundancy of raw radar data present significant challenges. Much
of the signal may be unrelated to the materialâ€™s intrinsic properties,
introducing noise that can obscure relevant information. As a result,
a key challenge lies in guiding the LLM to focus selectively on
material-relevant features within the radar signal.This paper introduces LLMaterial, the first system that can lever-
age pre-trained LLMs to directly identify object materials from
raw radar signals. Our contributions in developing LLMaterial are
threefold:
â€¢We develop a physics-informed signal processing pipeline de-
signed to distill a materialâ€™s intrinsic dielectric constant from raw
radar signals. This multi-stage process systematically reduces
data redundancy, transforming the high-dimensional signal into
a set of compact intermediate parameters of the materialâ€™s in-
trinsic properties. This approach provides more tractable and
interpretable input for subsequent LLM reasoning.
â€¢We incorporate a retrieval-augmented generation (RAG) frame-
work to equip the LLM with domain-specific knowledge, enabling
it to construct a chain-of-thought (CoT) that reasons about the in-
terplay of physical properties. For example, when presented with
an object made of a specialized material exhibiting a high dielec-
tric constant but a low radar cross section (RCS), a conventional
classifier may struggle to resolve the ambiguity and produce an
accurate prediction. In contrast, LLMaterial is designed to reason
through such scenarios step by step, inferring that this com-
bination of properties likely corresponds to a radar-absorbent
material. Rather than relying on a direct feature-to-label map-
ping, LLMaterial can interpret underlying physical characteristics
through semantic reasoning.
â€¢We perform an initial validation to assess the practical feasi-
bility of LLMaterial. The results indicate that LLMaterial can
reliably identify four representative materials, including metal,
ceramic, glass, and plastic, demonstrating its effectiveness in real-
world material recognition tasks. This validation underscores
the promise of our approach and provides a strong foundation
for future research toward more intelligent and context-aware
assistive technologies.
We note that recent studies have explored the use of LLMs to
interpret data from other non-visual sensors, such as inertial mea-
surement units (IMUs) [ 4,6] and ultra-wideband (UWB) sensors [ 9].
2 SYSTEM DESIGN
In this section, we introduce the architecture of LLMaterial, de-
signed to perform robust material identification through a new
integration of radar sensing and LLM-based reasoning. In our work,
we used an mmWave radar sensor (IWR6843ISK) manufactured by
Texas Instruments (Dallas, TX, USA). The overall workflow of LLMa-
terial is depicted in Figure 2. For visually similar objects, LLMaterial
first acquires the raw target echo signal using an mmWave radar.
Next, this raw radar data is preprocessed, followed by performing
electromagnetic parameter estimation to extract key parameters.
These parameters are then fed into an LLM-based system aug-
mented with specialized domain knowledge. Leveraging RAG, the
model then generates a CoT process to reason about all electromag-
netic properties, ultimately leading to robust material identification.
2.1 Radar Signal Processing
This part details our signal preprocessing pipeline. We first per-
form multichannel accumulation on the 3D radar echoes to yield a
high-resolution range-Doppler (RD) map. Subsequently, the range-
angle (RA) map is derived by processing the data in the slow-time

Can Large Language Models Identify Materials from Radar Signals? UbiComp Companion â€™25, October 12â€“16, 2025, Espoo, Finland.
Figure 2: Overview of LLMaterial.
-0.4 -0.2 0 0.2 0.4
X Position (m)00.10.20.30.4Y Position (m)
 51015105
Figure 3: (Left) RD map; (Right) RA map.
-60 -40 -20 0 20 40 60
Azimuth (Deg.)-40-30-20-100Gain (dB)-3 dB
-6.4Â° 6.4Â°
-10 0 10 20
Azimuth (Deg.)-25-20-15-10-50Normalized Amplitude (dB)metal 1
metal 2
glass 1
glass 2ceramic 1
ceramic 2
plastic 1
plastic 22 4 6 8-4-20
Figure 4: (Left) Beam Pattern of the Antenna Array; (Right)
Beam Direction After Beamforming.
dimension. The combination of these maps provides the targetâ€™s
distance (ğ‘…), velocity (ğ‘‰), and angle ( ğœƒ) relative to the radar, which
are crucial for its accurate localization. Representative RD and RA
maps are depicted in Figure 3.
To enhance the precision of target angle estimation, we employ
beamforming techniques to accurately resolve the targetâ€™s direction
of arrival (DoA). This step is crucial, as the antenna arrayâ€™s native
resolution is limited by its physical aperture; a smaller aperture
results in a wider half-power beamwidth (HPBW). As shown in
Figure 4, the HPBW of our mmWave radar array was measured
to be 12.8â—¦, and the target angle after beamforming can achieve
better accuracy.
The aforementioned steps constitute our signal preprocessing
stage. Once a target is detected and its signal is extracted, the sub-
sequent analysis focuses on electromagnetic parameter estimation,
as shown in Figure 5. First, for a target that meets far-field condi-
tions, the received echo power is determined by the classical radar
equation, expressed as:
ğ‘ƒğ‘Ÿ=ğ‘ƒğ‘¡ğºğ‘¡ğºğ‘Ÿğœ†2ğœ
(4ğœ‹)3ğ‘…4, (1)
Figure 5: Workflow of radar signal processing.
whereğ‘ƒğ‘Ÿis the received power, ğ‘ƒğ‘¡is the transmitted power, ğºğ‘¡
andğºğ‘Ÿare the gains of the transmitting and receiving antennas,
respectively, ğœ†is the signal wavelength, ğœis the targetâ€™s RCS, and
ğ‘…is the distance of the target relative to the mmWave radar. Based
on the expression of ğ‘ƒğ‘Ÿ, the signal-to-noise ratio (SNR) of the target
echo signal can be expressed as:
ğ‘†ğ‘ğ‘…=ğ‘ƒğ‘Ÿ
ğ‘ƒğ‘›=ğ‘ƒğ‘¡ğºğ‘¡ğºğ‘Ÿğœ†2ğœ
(4ğœ‹)3ğ‘˜ğ‘‡ğ‘›ğµğ‘…4=ğ¾ğœ
ğ‘…4, (2)
whereğ‘ƒğ‘›is the total noise power of the receiver. For thermal noise,
ğ‘ƒğ‘›=ğ‘˜ğ‘‡ğ‘›ğµ, whereğ‘˜is the Boltzmann constant, ğ‘‡ğ‘›is the effective
system noise temperature, and ğµis the bandwidth. The constant
ğ¾combines all system-specific parameters, highlighting that the
SNR is primarily dependent on the targetâ€™s RCS and its distance.
Constantğ¾can be expressed as:
ğ¾=ğ‘ƒğ‘¡ğºğ‘¡ğºğ‘Ÿğœ†2
(4ğœ‹)3ğ‘˜ğ‘‡ğ‘›ğµ. (3)
To determine the system constant ğ¾, we performed a calibration
using a metal sphere ( ğ‘‘=63mm) as a standard target. For a sphere
with a diameter significantly exceeding the signal wavelength ( ğœ†=5
mm), the sphereâ€™s RCS in the optical scattering regime equals its
physical cross-sectional area, which can be calculated as:
ğœğ‘=ğœ‹ğ‘‘
22
â‰ˆ0.0031 m2. (4)

UbiComp Companion â€™25, October 12â€“16, 2025, Espoo, Finland. Jiangyou Zhu, Hongyu Deng, & He Chen
0 0.05 0.1
X Position (m)0.20.220.240.260.28Y Position (m)
 51015105
Figure 6: PRCA (red box).
 Figure 7: Antenna array.
After measuring the SNR of the sphere, which serves as a known
reference, we computed ğ¾by inversion of Equation (2). This cali-
brated constant enables the direct calculation of an unknown tar-
getâ€™s RCS, which is given by:
ğœ=ğ‘†ğ‘ğ‘…ğ‘…4
ğ¾. (5)
However, the RCS itself is a composite parameter, integrating the
intrinsic electromagnetic properties of the targetâ€™s material with its
overall geometric shape. To disentangle these attributes and analyze
the material characteristics more directly, we relate the RCS to the
backscattering coefficient. In the field of radar, the backscattering
coefficient, denoted as ğœ0, is defined as the average RCS per unit
area, where ğ´ğ‘denotes the area on the ground associated with that
object. The RCS of a specific object can be expressed as:
ğœ=ğœ0ğ´ğ‘. (6)
Inspired by this principle, we shift our focus from the total RCS
to the dominant reflection center that generates the signal peak.
We define a power reflection coefficient ğœŒto represent the strong
reflection signal originating from a small but highly reflective re-
gion, which we term as peak reflection cell area (PRCA) ğ´ğ‘Ÿ. The
extracted PRCA is shown in Figure 6. By approximating PRCA as a
local specular reflector, ğœŒbecomes analogous to ğœ0. Therefore, RCS
can be further expressed as:
ğœ=ğœŒğ´ğ‘Ÿ. (7)
Theoretically, the power reflection coefficient ğœŒis described by
the Fresnel equations, which define it as a function of the relative
permittivity ( ğœ€ğ‘Ÿ) of the material, the polarization of the wave, and
the angle of incidence. For the positive ğœŒthat we calculate, we per-
form a normalization to obtain the Fresnel reflection coefficient Î“ğ‘“
for vertical polarization. This step is justified by our core assump-
tions: specular reflection from a locally smooth surface combined
with the known vertical polarization of our mmWave radar, as
shown in Figure 7 (as solving for ğœ€ğ‘Ÿwith horizontal polarization is
mathematically intractable, resulting in a high-order polynomial,
and our approach is consistent with commercial radars, which are
commonly vertically polarized, thereby focusing our analysis to
the vertical polarization case). Consequently, Î“ğ‘“is modeled as:
Î“ğ‘“=ğœ€ğ‘Ÿcosğœƒâˆ’âˆšï¸
ğœ€ğ‘Ÿâˆ’sin2ğœƒ
ğœ€ğ‘Ÿcosğœƒ+âˆšï¸
ğœ€ğ‘Ÿâˆ’sin2ğœƒ, (8)
whereğœ€ğ‘Ÿ=ğœ€â€²âˆ’ğ‘—ğœ€â€²â€²,ğœ€â€²is the real part, and ğœ€â€²â€²is the imaginary part.
We define the variables as follows:
ğ´=ğœ€ğ‘Ÿcosğœƒ, ğµ =âˆšï¸ƒ
ğœ€ğ‘Ÿâˆ’sin2ğœƒ. (9)Then, we get:
Î“ğ‘“=ğ´âˆ’ğµ
ğ´+ğµ. (10)
We can further rewrite it as:
Î“ğ‘“(ğ´+ğµ)=ğ´âˆ’ğµ. (11)
Next, we expand and rearrange to obtain:
Î“ğ‘“ğ´+Î“ğ‘“ğµ=ğ´âˆ’ğµ,
Î“ğ‘“ğ´âˆ’ğ´=âˆ’Î“ğ‘“ğµâˆ’ğµ,
ğ´(Î“ğ‘“âˆ’1)=âˆ’ğµ(Î“ğ‘“+1).(12)
Substituting A and B back into Equation (12), we have:
ğœ€ğ‘Ÿcosğœƒ(Î“ğ‘“âˆ’1)=âˆ’âˆšï¸ƒ
ğœ€ğ‘Ÿâˆ’sin2ğœƒ(Î“ğ‘“+1). (13)
To proceed, we square both sides, expand and rearrange, and
move all terms to the left side, getting:
ğœ€2
ğ‘Ÿcos2ğœƒ(Î“2
ğ‘“âˆ’2Î“ğ‘“+1)âˆ’ğœ€ğ‘Ÿ(Î“ğ‘“+1)2+sin2ğœƒ(Î“ğ‘“+1)2=0.(14)
We treat Equation (14) as a quadratic equation in terms of ğœ€ğ‘Ÿ,
and then we can solve it using the quadratic formula. Due to space
constraints, we do not provide an extensive derivation here. The
final relationship between ğœ€ğ‘Ÿ,Î“ğ‘“andğœƒis given by:
ğœ€ğ‘Ÿ=(Î“ğ‘“+1)2"
1Â±âˆšï¸‚
1âˆ’sin 2ğœƒ(Î“ğ‘“âˆ’1)
Î“ğ‘“+12#
2 cos2ğœƒ[Î“ğ‘“âˆ’1]2. (15)
According to the principles of physics, for ğœ€ğ‘Ÿ, which is a complex
number, we choose the positive root. Specifically, when ğœƒ=0(i.e.,
the target is located directly in front of the radar), we can get:
ğœ€ğ‘Ÿ= 
1+Î“ğ‘“
1âˆ’Î“ğ‘“!2
. (16)
In summary, we have systematically traced the signal processing
workflow from initial raw signal to SNR, then through the analysis
of RCS (ğœ), power reflection coefficient ( ğœŒ), and Fresnel reflection
coefficient ( Î“ğ‘“), ultimately leading to the extraction of the targetâ€™s
fundamental material property: the relative permittivity ( ğœ€ğ‘Ÿ). This
signal processing pipeline facilitates the extraction of essential
physical characteristics of an object directly from raw radar signals.
2.2 RAG-Enhanced LLM
In this section, we present our LLM-based framework for mate-
rial inference. As illustrated in Figure 8, the LLM interprets eight
parameters extracted from the radar signal processing pipeline to
predict the material composition of the target object. These include
the targetâ€™s basic information ( ğ‘…,ğ‘‰,ğœƒ) and SNR, along with four
key electromagnetic features: ğœ,ğœŒ,Î“ğ‘“, andğœ€ğ‘Ÿ.
As shown in Figure 8, we apply the RAG technique to enhance
the LLMâ€™s capability in material identification. The RAG process
establishes a foundation for the LLM within a domain-specific
knowledge base through a structured workflow. Initially, a doc
subset is created by segmenting documents into chunks, which
are then embedded and indexed in a specialized data storage (i.e.,
vector database). When the database is ready, LLMaterial performs a
semantic search to retrieve the top-k chunks that are most relevant
from this storage. These retrieved chunks, along with engineered

Can Large Language Models Identify Materials from Radar Signals? UbiComp Companion â€™25, October 12â€“16, 2025, Espoo, Finland.
Figure 8: Workflow of the RAG-enhanced LLM for Radar-based material identification system.
Table 1: Radar Parameter Settings.
Parameter Value Explanation
ğ‘“0 60 GHz Carrier Frequency
ğ‘† 66 MHz/ğœ‡sModulation Slope
ğµ 3.96 GHz Bandwidth
ğ‘“ğ‘  10 MHz Sampling Rate
prompts, are then used to inform the LLM, which generates an
accurate response supported by the provided information. This
process makes the modelâ€™s reasoning transparent, as the output can
be traced back to specific source documents. As a result, LLMaterial
provides not only a correct recognition result but also a trustworthy
and verifiable one.
As shown in Figure 9, the online identification system of our
proposed LLMaterial is divided into two main panels:
â€¢Left Panel (Radar Measurement Inputs): This panel displays
the eight key parameters derived from the mmWave radar signal
processing workflow. These parameters serve as the quantitative
input for the following analysis.
â€¢Right Panel (Online analysis of LLM): This panel provides a
guided two-step analytical process.
(1)Step 1: Upload Knowledge Base. The user first uploads
a set of domain-specific documents to serve as the knowledge
base for the RAG system. Upon successful upload, the interface
visualizes the internal RAG workflow, providing transparency
into the retrieval and augmentation process. This allows users to
monitor operations easily.
(2)Step 2: Start Analysis. Once the knowledge base is loaded,
the user can initiate the analysis. The system then processes the
eight parameters, allowing the LLM to use RAG and CoT for
comprehensive analysis and inference, ultimately generating the
final material identification result of a real-world object.
Moreover, Figure 9 shows the noticeable impact of RAGâ€™s knowl-
edge base on the LLMâ€™s reasoning. As an initial effort to provide
domain-specific context, our knowledge base was constructed from
primary sources on key topics, such as RCS [ 3] and the relationship
between dielectric constants and material properties [5].
3 PRELIMINARY EXPERIMENTAL RESULTS
AND ANALYSIS
The parameters of mmWave radar are listed in Table 1. Our signal
processing pipeline was implemented in MATLAB R2023a. The 14-
billion-parameter DeepSeek LLM (deepseek-r1:14b) was employed.All experiments were conducted on a Windows desktop equipped
with a 2.60 GHz Intel Core i7-9750H CPU and 16 GB of RAM.
Experimental results : To evaluate LLMaterialâ€™s performance,
we tested seven common objects made from four distinct material
categories, with results summarized in Figure 10. The preliminary
findings show that the electromagnetic parameters have unique
signatures for each category, allowing the RAG-enhanced LLM to
conduct a step-by-step CoT reasoning process. The final recogni-
tion results are accurate and consistent with the actual materials,
validating LLMaterial and confirming that it is indeed feasible to
apply LLMs to identify materials from raw radar signals.
Analysis : To validate the effectiveness of our proposed LLMa-
terial, we performed an ablation study. As detailed in Table 2, we
compared our full system (LLMaterial w/ RAG) against an ablated
version without this module (LLMaterial w/o RAG). The results
reveal a dramatic performance gap. The ablated version (LLMaterial
w/o RAG) struggled in correctly identifying only two of the seven
objects. In contrast, the LLMaterial system, empowered by RAG,
achieved a perfect score, correctly identifying all seven objects. This
demonstrates that the RAG module is not only beneficial but also
indispensable, as it provides the essential context for the LLM to ac-
curately interpret radar signals, thus enabling precise identification
of object materials.
4 CONCLUSION AND FUTURE WORK
In conclusion, this paper introduces LLMaterial, the first frame-
work to successfully leverage LLMs for radar-based material iden-
tification. We tackle this task through two key innovations: (1) a
physics-informed signal processing pipeline that transforms raw
radar data into meaningful electromagnetic parameters, and (2)
a RAG-enhanced LLM that interprets these parameters to infer
the objectâ€™s material composition. Our preliminary experiments
confirm that LLMaterial achieves accurate material recognition,
demonstrating for the first time that LLMs can effectively interpret
object materials from raw radar signals.
We acknowledge several limitations in the current study. This
work serves as a preliminary validation, focusing on only four ma-
terial types. Future research should pursue three key directions: (1)
broadening the diversity of evaluated materials to verify generaliz-
ability; (2) enhancing the RAG knowledge base and incorporating
advanced CoT reasoning strategies to boost interpretability and
accuracy; and (3) developing robust radar-vision fusion techniques
for multimodal material recognition in real-world scenarios.

UbiComp Companion â€™25, October 12â€“16, 2025, Espoo, Finland. Jiangyou Zhu, Hongyu Deng, & He Chen
Figure 9: Online material identification from radar signals using our LLM-based system. (Up) Step 1; (Down) Step 2.
Table 2: Ablation study.
Method Metal bottle Metal box Glass bottle Glass cup Ceramic cup Ceramic mug Plastic bottle
LLMaterial w/o RAG Ã—âˆšÃ— Ã— Ã— Ã—âˆš
LLMaterial w RAGâˆšâˆšâˆšâˆšâˆšâˆšâˆš
5 ACKNOWLEDGMENTS
The work of He Chen is supported in part by the CUHK Strategic
Seed Funding for Collaborative Research Scheme under Project
3136053 and the CUHK Direct Grant for Research under Project
4055229.The research work described in this paper was conducted in the
JC STEM Lab of Advanced Wireless Networks for Mission-Critical
Automation and Intelligence funded by The Hong Kong Jockey
Club Charities Trust.

Can Large Language Models Identify Materials from Radar Signals? UbiComp Companion â€™25, October 12â€“16, 2025, Espoo, Finland.
Figure 10: Summary of radar signal processing and recognition results: 7 common objects and 4 material types.

UbiComp Companion â€™25, October 12â€“16, 2025, Espoo, Finland. Jiangyou Zhu, Hongyu Deng, & He Chen
REFERENCES
[1]Qihang Cao and Huangxun Chen. 2025. ObjVariantEnsemble: Advancing Point
Cloud LLM Evaluation in Challenging Scenes with Subtly Distinguished Objects.
InProceedings of the AAAI Conference on Artificial Intelligence , Vol. 39. 1944â€“1952.
[2]Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis, Theodore
Patkos, Antonis Argyros, and Dimitris Plexousakis. 2024. LLM-aided Knowledge
Graph construction for Zero-Shot Visual Object State Classification. In 2024 14th
International Conference on Pattern Recognition Systems (ICPRS) . IEEE, 1â€“7.
[3]Eugene F Knott, John F Schaeffer, and Michael T Tulley. 2004. Radar cross section .
SciTech Publishing.
[4]Zikang Leng, Amitrajit Bhattacharjee, Hrudhai Rajasekhar, Lizhe Zhang, Eliza-
beth Bruda, Hyeokhyen Kwon, and Thomas PlÃ¶tz. 2024. Imugpt 2.0: Language-
based cross modality transfer for sensor-based human activity recognition. Pro-
ceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
8, 3 (2024), 1â€“32.
[5]Rajenda Singh and Richard K Ulrich. 1999. High and low dielectric constant
materials. The Electrochemical Society Interface 8, 2 (1999), 26.
[6]Jian Wang, Rishabh Dabral, Diogo Luvizon, Zhe Cao, Lingjie Liu, Thabo Beeler,
and Christian Theobalt. 2025. Ego4o: Egocentric Human Motion Capture and
Understanding from Multi-Modal Input. In Proceedings of the Computer Vision
and Pattern Recognition Conference . 22668â€“22679.
[7]Chenshu Wu, Feng Zhang, Beibei Wang, and KJ Ray Liu. 2020. mSense: Towards
mobile material sensing with a single millimeter-wave radio. Proceedings of the
ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 3 (2020),
1â€“20.
[8]Liyang Xiao, Yanni Yang, Zhe Chen, Gao Yue, Prasant Mohapatra, and Pengfei Hu.
2025. CRFUSION: Fine-grained Object Identification Using RF-image Modality
Fusion. IEEE Transactions on Mobile Computing (2025).
[9]Hongchao Yang, Yunjia Wang, Chee Kiat Seow, Zengke Li, Meng Sun, Cedric
De Cock, Jingxue Bi, Wout Joseph, and David Plets. 2025. NLOS Identification
and Ranging Trustworthiness for Indoor Positioning with LLM-based UWB-IMU
Fusion. IEEE Transactions on Instrumentation and Measurement (2025).
[10] Hui-Shyong Yeo, Gergely Flamich, Patrick Schrempf, David Harris-Birtill, and
Aaron Quigley. 2016. Radarcat: Radar categorization for input & interaction.
InProceedings of the 29th Annual Symposium on User Interface Software and
Technology . 833â€“841.
[11] Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. 2024. Towards Open-Ended
Visual Recognition with Large Language Models. In European Conference onComputer Vision . Springer, 359â€“376.
[12] Chuyang Zhao, YuXin Song, Junru Chen, Kang Rong, Haocheng Feng, Gang
Zhang, Shufan Ji, Jingdong Wang, Errui Ding, and Yifan Sun. 2024. Octopus: A
multi-modal llm with parallel recognition and sequential understanding. Ad-
vances in Neural Information Processing Systems 37 (2024), 90009â€“90029.
[13] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. 2023.
Object detection in 20 years: A survey. Proc. IEEE 111, 3 (2023), 257â€“276.