# Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory

**Authors**: Guowei Lan, Kaixian Qu, René Zurbrügg, Changan Chen, Christopher E. Mower, Haitham Bou-Ammar, Marco Hutter

**Published**: 2025-07-22 15:48:49

**PDF URL**: [http://arxiv.org/pdf/2507.16713v1](http://arxiv.org/pdf/2507.16713v1)

## Abstract
Vision-language models (VLMs) have been widely adopted in robotics to enable
autonomous planning. However, grounding VLMs, originally trained on internet
data, to diverse real-world robots remains a challenge. This paper presents
ExpTeach, a framework that grounds VLMs to physical robots by building a
self-generated memory of real-world experiences. In ExpTeach, the VLM
autonomously plans actions, verifies outcomes, reflects on failures, and adapts
robot behaviors in a closed loop. The self-generated experiences during this
process are then summarized into a long-term memory, enabling retrieval of
learned knowledge to guide future tasks via retrieval-augmented generation
(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with
an on-demand image annotation module. In experiments, we show that reflection
improves success rates from 36% to 84% on four challenging robotic tasks and
observe the emergence of intelligent object interactions, including creative
tool use. Across extensive tests on 12 real-world scenarios (including eight
unseen ones), we find that grounding with long-term memory boosts single-trial
success rates from 22% to 80%, demonstrating the effectiveness and
generalizability of ExpTeach.

## Full Text


<!-- PDF content starts -->

Experience is the Best Teacher: Grounding VLMs for
Robotics through Self-Generated Memory
Guowei Lan1 *Kaixian Qu1 *Ren´e Zurbr ¨ugg1 2Changan Chen1
Christopher E. Mower3Haitham Bou-Ammar3 4Marco Hutter1
1Robotic Systems Lab, ETH Zurich2ETH AI Center3Huawei Noah’s Ark Lab4UCL Centre for AI
Figure 1: Robot completes a new task guided by previously self-generated memory. We propose
grounding VLMs through memory generated by the robot itself from direct hardware experiments,
establishing awareness of the robot’s own capability. When executing a novel task, the robot keeps
a short-term memory that helps the robot reflect and complete the task (illustrated in the grey clip).
The experience is then stored as long-term memory and retrieved to guide the VLM’s task planning
whenever a similar scenario is encountered (illustrated in the main figure).
Abstract: Vision-language models (VLMs) have been widely adopted in robotics
to enable autonomous planning. However, grounding VLMs, originally trained on
internet data, to diverse real-world robots remains a challenge. This paper presents
EXPTEACH , a framework that grounds VLMs to physical robots by building a
self-generated memory of real-world experiences. In E XPTEACH , the VLM au-
tonomously plans actions, verifies outcomes, reflects on failures, and adapts robot
behaviors in a closed loop. The self-generated experiences during this process are
then summarized into a long-term memory, enabling retrieval of learned knowl-
edge to guide future tasks via retrieval-augmented generation (RAG). Addition-
ally, E XPTEACH enhances the spatial understanding of VLMs with an on-demand
image annotation module. In experiments, we show that reflection improves suc-
cess rates from 36% to84% on four challenging robotic tasks and observe the
emergence of intelligent object interactions, including creative tool use. Across
extensive tests on 12 real-world scenarios (including eight unseen ones), we find
that grounding with long-term memory boosts single-trial success rates from 22%
to80%, demonstrating the effectiveness and generalizability of E XPTEACH .
Keywords: robot memory, vision-language models, task planning
*These authors contributed equally to this work. Correspondence to Kaixian Qu <kaixqu@ethz.ch >.arXiv:2507.16713v1  [cs.RO]  22 Jul 2025

1 Introduction
Recently, large language models (LLMs) have demonstrated near-human performance across a range
of reasoning tasks, showcasing emergent capabilities in diverse domains such as mathematics and
programming [1, 2, 3, 4, 5, 6]. These broad competencies have enabled LLMs to move beyond
traditional language tasks and play an increasingly important role in robotics. In particular, they are
now widely used in task planning, where LLMs interpret natural language instructions and generate
feasible action plans with common-sense reasoning [7, 8, 9, 10, 11].
To address the limitations of text-only input, research has increasingly shifted toward multimodal
models, especially vision-language models (VLMs) that jointly process visual and textual data. Re-
cent VLMs [12, 13, 14, 15] exhibit strong multimodal reasoning and high-resolution visual process-
ing. Building on these capabilities, recent work has leveraged VLMs to enable robots to reason about
visual inputs and develop closed-loop, autonomous systems [16, 17, 18, 19, 20]. This approach re-
duces reliance on manually designed components such as explicit scene descriptors [9, 10, 21, 22].
However, grounding VLMs, originally trained on internet data, to diverse real-world robots remains
a challenge. For example, in the scenario depicted in Figure 1, when seeing a tennis ball partially
occluded by a fan, a VLM often confidently instructs the robot to pick up the ball. While this aligns
with human intuition–since humans can often act successfully based on partial visibility–robots
typically struggle to grasp the ball due to imperfect object perception. This raises a critical question:
how can we make the VLM aware of the specific capabilities of the robot it is assisting? How can
we effectively ground VLMs for robotics?
One promising direction lies in augmenting VLMs with memory, a core brain-based capability un-
derpinning human cognition [23, 24, 25, 26]. Incorporating memory of past experience into LLM
agents has shown potential for improving decision-making in complex tasks [27, 28, 29]. In robotics,
memory has been used to help agents retain contextual information to support tasks such as navi-
gating familiar environments [30, 31, 32], and to enable more natural and effective human–robot
interaction [21, 33, 34, 35, 36]. Building on these developments, we investigate whether VLMs,
when deployed on real robots, can generate their own memory to ground themselves in the specific
capabilities and limitations of the robot.
This paper presents E XPTEACH , which grounds VLMs for robotics through a self-generated mem-
ory of past experiences. The central idea behind E XPTEACH is that, even if initially poorly
grounded, a VLM can autonomously complete instructions efficiently and use the resulting self-
generated experiences to progressively ground itself via a memory mechanism. To realize this, E X-
PTEACH hinges on three pivotal components: (i) a VLM success detector for autonomous feedback;
(ii) a short-term memory (STM) that enables intelligent adaptation; and (iii) a long-term mem-
ory (LTM) that stores past experiences. Upon receiving a new instruction, E XPTEACH retrieves
relevant experiences from the LTM to ground the VLM through retrieval-augmented generation
(RAG) [37, 38]. Additionally, E XPTEACH enhances the spatial understanding of VLMs with an
on-demand image annotation module. We evaluate our approach on multiple challenging real-world
tasks and show that this grounding technique significantly strengthens the robot’s performance.
This paper presents the following key contributions:
• A self-generated memory framework combining short-term and long-term memory to
ground VLMs in robotic planning.
• A memory retrieval strategy using RAG to access task-relevant prior experiences from
LTM, enabling the robot to act correctly in future tasks with similar instructions and scenes.
• An on-demand image annotation module that enhances spatial reasoning across multiple
skills, leading to more accurate and robust action execution with VLMs.
• Extensive real-world evaluations demonstrating that E XPTEACH significantly improves
success rates through both STM and LTM, and generalizes effectively to unseen tasks.
2

2 Related Work
VLMs in Robotics. LLMs have demonstrated strong capabilities in robotics [8, 7, 10, 9]. Recent
developments in VLMs [12, 14] have further propelled the field by integrating visual perception into
reasoning [39, 16, 11]. Prior research has explored the use of VLMs in various robotics tasks, such
as generating action sequences from visual and language inputs [40, 18], assessing action outcomes
and failure recovery [41, 18, 42, 20]. In this work, we build on these approaches by using a VLM as
both a task planner and a success detector, enabling closed-loop task execution.
Image Annotation for Action Planning. Annotating images is an effective approach to help ground
VLMs for precise action planning [43, 44, 45]. Recent works combines object-centric annotations
(masks aligned directly with objects) with grasping tools to support more challenging tasks such as
semantic object grasping [46, 47, 48]. However, these approaches are typically limited to planar
tabletop manipulation or grasping alone. In contrast, E XPTEACH introduces an on-demand image
annotation tool that enables grounded actions in 3D space for multiple skills beyond 6-DoF grasping.
Self-Reflection and Robot Memory. Recent work has explored ways for LLM-enabled robots to
reflect on their failures [22, 49, 50] and to learn from a diverse range of experiences [51]. However,
these systems typically require external supervision signals, as there is no visual feedback from the
LLM itself. Recent advances in VLMs have enabled robots to plan, verify, and self-reflect using a
single VLM model with raw sensor input [18, 20]. Our approach, E XPTEACH , investigates whether
reflection can lead to emergent intelligent object interactions (including tool use) and how such
experiences enhance future performance. While aligning with recent work on autonomous memory
generation for VLM agents [52], E XPTEACH specifically focuses on grounding VLMs to robots
through real-world experiments. Table 1 summarizes the comparison with related approaches.
Table 1: Comparison of E XPTEACH with prior LLM/VLM task planning work. LVLM stands
for large vision-language model. Interactive Replan refers to interacting with non-target objects after
a failure to aid task completion. Emergent Tool Use denotes the robot autonomously deciding to use
an unmentioned tool and is a subset of interactive replanning. Short-Term Memory uses the current
action log, while Long-Term Memory leverages past summarized experiences.
Robot
ReflectLVLM
FeedbackInteractive
ReplanEmergent
Tool UseShort-Term
MemoryLong-Term
Memory
CaP [7] ✗ ✗ ✗ ✗ ✗ ✗
SayCan [8] ✗ ✗ ✗ ✗ ✓ ✗
Inner Mono. [10] ✗ ✗ ✗ ✗ ✓ ✗
RoboTool [53] ✗ ✗ ✗ ✓ ✗ ✗
DROC [21] ✗ ✗ ✗ ✗ ✓ ✓
REFLECT [22] ✓ ✗ ✓ ✗ ✓ ✗
ComeRobot [18] ✓ ✓ ✗ ✗ ✓ ✗
ReplanVLM [20] ✓ ✓ ✓ ✗ ✓ ✗
EXPTEACH (ours) ✓ ✓ ✓ ✓ ✓ ✓
3 Problem Formulation
An embodied robotic agent is given a high-level natural language instruction I, such as “Move the
screws to the toolbox. ” The robot is equipped with a predefined library of parameterized skills πk∈
Πand operates in a closed-loop manner, executing actions step by step while receiving egocentric
RGB-D observations. The task planner T, implemented as a pretrained VLM, is responsible for
planning a sequence of actions (skill instantiations at=πk(arg)) that fulfill the instruction I. At
each timestep t, the planner observes the environment otthrough RGB-D input and selects an action
atfor the robot to execute. In addition, Tcan leverage contextual information Cto guide planning.
Our work investigates how the planner T, a VLM originally trained on internet data and unaware
of the specific embodiment, can be effectively and efficiently grounded to the robot’s capabilities
through this contextual information C. Furthermore, we explore whether such grounding can be
achieved autonomously–that is, whether the VLM can generate Con its own by self-correcting its
behaviors and using that to ground itself.
3

Figure 2: EXPTEACH pipeline overview. At the start of each task, the system takes the user instruc-
tionIand egocentric observation o0, which the VLM summarizes into a scenario. RAG retrieves
relevant experiences from long-term memory M(section 4.3) and, together with the instruction and
observation, feeds them into the VLM task planner T(section 4.1) to generate the next action a.
After execution, success is checked by the VLM. If the task is not completed, the action aand its
feedback rare accumulated into short-term memory m(section 4.2) and fed back into planning.
Once the task is completed, the short-term memory mis summarized and stored in long-term mem-
oryMfor future use.
4 Method
EXPTEACH leverages the VLM task planner Tand success detector Das the core components for
closed-loop planning (section 4.1). The VLM task planner Tis enhanced with a memory mecha-
nism comprising a short-term memory m(section 4.2), which records actions taken for the current
instruction I, and a long-term memory M(section 4.3), which stores past experiences. In addi-
tion, the VLM planner Tmay activate an on-demand image annotation tool to enhance its spatial
reasoning capabilities (section 4.4). An overview of the framework is provided in Figure 2.
4.1 VLM Task Planning
We use the same VLM for both the task planner Tand the success detector D. Both
components use the function-calling interface to enforce a structured output format, such as
push(object="apple") . The VLM task planner Tinterprets user instructions Iand generates
action plans from RGB images. After each action ais executed, the VLM success detector Deval-
uates the feedback signal rfrom visual observation o. This feedback includes a scene description,
action outcome (success or failure), failure analysis for recovery, and suggestions for the next action.
The feedback is then passed to the task planner Tvia the STM.
4.2 Short-Term Memory
Short-term memory min our framework plays a critical role in grounding VLMs in robots to dy-
namically reflect, replan, and recover from failures during task execution. As depicted in Figure 2,
the short-term memory moperates in an iterative pipeline, interacting closely with the task planner
Tand success detector D. It stores the generated action log until time t. More precisely, each entry
records the action aand its associated feedback rfromD,
m={(aτ,rτ+1)}t−1
τ=0={(aτ,D(aτ,oτ+1))}t−1
τ=0 (1)
In cases of action failure, the short-term memory enables the robot to identify better strategies for
achieving the task—for example, interacting with non-target objects to facilitate grasping or using
4

tools to manipulate small objects. STM can significantly improve success rates within the allotted
attempts for challenging tasks.
4.3 Long-Term Memory
The LTM Mplays a pivotal role in grounding VLMs for robotics by autonomously accumulating
knowledge and experience over time. Specifically, upon successful completion of a task, the robot’s
STMmcontents are summarized by a VLM experience summarizer Eand stored into the LTM
as a key-value pair (K,E)where the key Kis the scenario description (which consists of the user
instruction Iand the initial scene description from the RGB image o0), and the value is the associated
summarized experience E, as illustrated in Figure 2.
Algorithm 1 EXPTEACH
Given: Instruction I, initial obs. o0
Internal: Long-term memory M
1:K′←EMBED (I,o0)
2:{(Ki,Ei)}k
i=1←RAG(K′,M)
3:t←0,m← ∅
4:repeat
5: at← T (I,ot,m,{(Ki,Ei)}k
i=1)
6: Execute atand receive ot+1
7: rt+1← D (at,ot+1)
8: m←m∪(at,rt+1)
9: t←t+ 1
10:untilrt.completed
11:M←M∪ {(K′,E(m))}When presented with a new task, the task planner
Tretrieves relevant prior experiences Efrom the
LTMMthrough a RAG process. A new retrieval
key (scenario) is formed by combining the current
user instruction Iand scene description o. This key
is embedded into a real-valued vector using Ope-
nAI’s text-embedding-3-large model [54]. In
the LTM, each stored scenario maintains a cached
embedding from the same model. RAG then uses
cosine similarity to retrieve the top- kmost rele-
vant past experiences E, which are incorporated into
the task planning prompt. Through this retrieval
and augmentation mechanism, the robot gains ac-
cess to relevant prior experiences, allowing it to act
correctly from the outset. In summary, the LTM
grounds VLMs in robotic experience, providing relevant knowledge that enables the robot to plan
correctly and even generalize to unseen but similar situations. Algorithm 1 illustrates this process.
4.4 Enhanced Skillset with Image Annotations
Pick up  drumstick.
✅
✅
Push grape to banana.
Figure 3: Illustration of image annotation tools.We focus on three manipulation skills—pick,
place, and push. Identifying a suitable location
for each action is non-trivial: purely geometric
grasp planning may seize an undesirable part
(e.g., the meat on a skewer or the ice-cream top
rather than its cone), and effective placing or
pushing likewise demands semantic scene un-
derstanding. To address these challenges, we
introduce an on-demand image-annotation tool
that is shared across all skills. Given a user in-
struction Iand the current RGB frame ot, VLM first selects the skill to execute with its parameters
(object name, whether image annotation is needed, etc.). The robot then performs open-vocabulary
segmentation with Grounded SAM [55], which integrates Grounding DINO [56] and SAM [57],
producing an initial object mask. If the VLM requests a second annotation pass, our image overlays
a set of candidate location masks on the image, allowing the VLM to choose the most appropriate
location for the current action a. Figure 3 illustrates this procedure for the pick and push actions.
For placing, we apply farthest-point sampling (FPS) [58] on the segmented mask to generate candi-
date placement locations, whereas for pushing, we draw candidate goal masks that denote the end
points. In both cases, the VLM evaluates the annotated options and selects the most suitable location,
as illustrated in Figure 3. For grasping, the segmented point cloud is first passed to AnyGrasp [59],
which returns a set of grasp hypotheses accompanied by confidence scores sconf∈[0,1]. Grasp
poses that violate kinematic constraints are filtered out through inverse-kinematics checks using
5

Pinocchio [60], yielding the feasible subset G. If the VLM determines that image annotation is
beneficial (typically not the case for simple objects like an apple), our annotation tool similarly per-
forms FPS within the object mask to generate a collection of numbered location masks for the VLM
to select from. The final grasp is selected by maximizing the product of two scores:
g∗= arg maxg∈Gsconf(g)·sloc(g), (2)
where sloc(g)∈[0,1]denotes the location score, computed based on the normalized Euclidean
distance between grasp gand the chosen location.
5 Results
5.1 Experiment Setup
In our experiments, we use a legged manipulator that combines ANYmal [61], a quadrupedal robot,
with a 6-DoF arm. The arm is equipped with a Robotiq 2F-140 gripper for object manipulation and
a ZED X Mini Stereo Camera mounted on the elbow. For the VLM, we employ gpt-4o [12], with
the prompt details provided in the Appendix. If the robot fails an action and alters the environment,
a human operator intervenes to reset the scene, after which the robot resumes execution.
5.2 Evaluation of Short-Term Memory and Self-Reflection
To evaluate the effectiveness of the STM and reflection module at efficiently generating successful
episodes, even after initial failures, we designed a series of challenging object manipulation tasks.
For the baseline, we use CaP-V , which enhances CaP [7] by incorporating visual feedback. This
baseline does not have STM and selects the next action solely based on the current image and user
instruction, without the ability to reflect on failed actions. Experimental results in Table 2 highlight
the critical role of STM and reflection in achieving successful task completion. Without STM, the
robot tends to repeat the same failures without adapting, leading to poor task performance.
Table 2: Effect of STM on success
rates. Each task is tested 5–10 times
with two attempts allowed.
Task CaP-V E XPTEACH
Put apple on plate
(container obstructs)50% 86%
Move tiny candy
(sponge/towel nearby)22% 67%
Move egg
(open view)40% 100%
Pick up bowl
(apple inside)33% 83%Table 3: Effect of LTM on single-trial success rates.
Task ComeRobot [18] E XPTEACH
Put apple on plate ( container obstructs ) 29% 100%
Move tiny candy ( towel nearby ) 11% 78%
Move egg ( open view ) 20% 100%
Pick up bowl ( apple inside ) 17% 83%
Put tennis ball in box ( mug obstructs ) 29% 71%
Put orange/ball on plate ( fan blocks ) 10% 80%
Move crumpled paper ( brush nearby ) 25% 63%
Move screw ( towel nearby ) 0% 86%
Move sushi ( open view ) 14% 71%
Move grape/cherry ( open view ) 20% 70%
Pick up box ( apple on top ) 43% 86%
Pick up towel ( orange on top ) 50% 75%
With STM, the robot can successfully reflect on its failures, leading to emergent intelligent object
interactions (including tool use) and ultimately task success. For example, when instructed to “put
the apple on the plate,” and faced with a partially obstructing container, the robot initially fails to
grasp the apple due to poor perception. After detecting the failure, the VLM decides to push away
the container and successfully retries the grasp (see Figure 1). Similarly, when told to “move the
candy to the banana,” the robot fails to push the candy with its gripper due to insufficient contact.
Upon reflection, the VLM autonomously chooses to use a sponge as a tool to push more effectively
(see Figure 4). After cracking an egg while grasping, the VLM similarly learns to push rather than
grasp fragile objects. In another case, when asked to “collect the bowl,” the robot initially drops an
apple from the bowl during execution because it overlooked the object. After the user intervenes
and rebuilds the scene, the robot reflects and modifies its strategy: “I should first move the apple to
the table before picking up the bowl” (see Figure 4). Note that this is the only instance where we
let a human judge success, as the robot technically picked up the bowl, but the apple dropping was
unacceptable and should be treated as a failure.
6

Figure 4: Examples of E XPTEACH on different scenarios . In the first STM example (top row),
after failing to push the candy with its gripper, the robot reflects and uses a sponge as a tool. In
the second STM example (middle row), after dropping the apple during bowl collection, the robot
reflects with user feedback and learns to move the apple first. These experiences are then summa-
rized and stored in the LTM, allowing the robot to generalize its learning to similar future scenarios
(bottom row). For example, when asked to “Move the screw to the toolbox,” the robot immediately
decides to use a towel to push the screw successfully. Likewise, when tasked with “Pick up the milk
carton,” it remembers to reposition the apple first before retrieving the target item.
5.3 Evaluation of Grounding VLMs through Self-Generated Memory
Upon completing the task in section 5.2, the STM is autonomously transferred to the LTM for future
use. Together with 96 unrelated experiences from simple tasks, we build an LTM containing 100
entries. For the baseline, we use ComeRobot [18], which does not have access to the LTM. We first
evaluate the effectiveness of the LTM by reconstructing the exact same scene with the same objects.
As shown in Table 3, the robot successfully recalls the correct action from the LTM with RAG,
demonstrating improved performance compared to ComeRobot. We further investigate its ability
to generalize. To do so, we modify the scene to create new but structurally similar scenarios, as
illustrated in Figure 4. The results in Table 3 show that experience gained from one task successfully
transfers to related tasks under similar conditions and significantly improves the success rate.
5.4 Ablation Study of Memory Retrieval
To evaluate the effectiveness of our memory retrieval strategy, we measure the VLM’s task planning
success rate (without execution) across 12 tasks using three retrieval settings, as shown in Table 4.
We re-use the self-generated LTM described in section 5.3. Randomly selecting kmemories (with
k= 5) yields the worst performance, with a success rate of 27%. This is because the likelihood
of retrieving task-relevant context is relatively low. Providing the entire LTM leads to better perfor-
mance (success rate = 67% ); however, unfiltered retrieval can overwhelm the model with irrelevant
or distracting information, introducing unstable behavior. This aligns with prior findings that exces-
sively long or noisy contexts can degrade model performance, as LLMs may struggle to focus on the
most relevant content [62, 63]. In contrast, our retrieval strategy with RAG consistently improves
accuracy, achieving a success rate of 89%. Most remaining failures occur when the VLM judges the
7

retrieved memory to be misaligned with the current visual scene and thus chooses to rely less on the
past experience stored in LTM.
Table 4: Ablation study of memory retrieval module on task planning success rates. The success
rate here only considers the correct action sequence (without execution).
User instructions Entire LTM Random k Topkvia RAG
Put apple on the plate ( container obstructs ) 100% 43% 100%
Put tennis ball in the box ( mug obstructs ) 43% 29% 71%
Put tennis ball/orange in the plate ( fan blocks ) 60% 20% 80%
Move tiny candy to banana ( sponge/towel nearby ) 89% 22% 89%
Move tiny paper trash next to the box/banana ( brush nearby ) 38% 25% 86%
Move small screw next to the toolbox/banana ( towel nearby ) 71% 0% 86%
Move egg to sushi/banana ( open view ) 80% 20% 100%
Move sushi next to the plate ( open view ) 43% 29% 86%
Move grape/cherry close to banana ( open view ) 50% 20% 80%
Pick up the bowl ( apple inside ) 83% 17% 100%
Pick up the box ( apple on top ) 71% 43% 86%
Pick up the towel ( orange on top ) 75% 50% 100%
5.5 Ablation Study of Image Annotation Module
To evaluate the effectiveness of image annotation on grasping tasks, we compared success rates
with and without it across 7 objects. A grasp was considered successful if it targeted the correct
object section (e.g., the stick of a meat skewer). As shown in Figure 5, annotation significantly
improves success rates for objects with complex shapes that require grasping specific sections (e.g.,
drumsticks, skewers). Without annotation, AnyGrasp [59] often favors larger surfaces due to its
reliance on geometric cues. The failures with annotation are mostly due to inaccurate 3D point
clouds. We also evaluated pushing by measuring the distance error to the target location. As shown
in the right panel of Figure 5, image annotation consistently reduces pushing errors, highlighting its
benefits beyond grasping.
box mug banana drum-
stickmeat
skewerice
creambrush0255075100Success rate [%]Success rate for picking objects ()
w/o annotation
w/ annotation
egg
to
sushisushi
to
platecherry
to
bananagrape
to
bananascrew
to
toolboxcandy
to
bananapaper
to
box051015Distance error [cm]Distance to target for pushing objects ()
w/o annotation
w/ annotation
Figure 5: Ablation study of the image annotation module. Left: success rates for picking objects
(higher is better). Right: distance errors in pushing one object to another (lower is better). The
results demonstrate that incorporating image annotation consistently enhances performance.
6 Conclusion
This work introduces E XPTEACH , a novel method that equips VLMs with self-generated memory
to better ground them in real-world robotic systems. Empirical results show that E XPTEACH en-
ables robots to autonomously reflect and adapt through short-term memory, leading to substantial
improvements in task success rates and enabling intelligent object interactions (such as creative tool
use). Additionally, extensive experimental results show that aggregating these self-generated short-
term memories grounds the VLM in its own past experience, yielding significant improvements over
the state of the art in task planning when faced with new scenarios. Our work can serve as a general
framework for integrating VLMs with robotic systems and is a step further towards the development
of general-purpose robotic systems.
8

7 Limitations and Future Work
While E XPTEACH demonstrates strong potential in grounding VLMs within robotic systems by
autonomously generating memory, it still presents several limitations. First, E XPTEACH currently
focuses exclusively on manipulation tasks. However, its core approach is not inherently limited to
this domain, and we plan to extend it to mobile manipulation tasks in future work [18]. Second,
our current setup assumes that visual feedback alone is sufficient for understanding and decision-
making. In practice, some real-world scenarios require feedback that cannot be captured by vision
alone. Integrating additional modalities–such as tactile or auditory signals–could greatly improve
the system’s capacity to interpret complex, multimodal feedback [22, 49]. Third, while prior work
has explored aligning robotic memory with user preferences, E XPTEACH does not yet support this
capability. We intend to incorporate user-aligned memory into our framework, enabling more per-
sonalized and context-aware behavior [64, 21]. Finally, inspired by Self-Instruct [65], we are also
interested in enabling VLMs to autonomously propose tasks, generate instructions, and learn from
the outcomes, further reducing the amount of human effort required.
Acknowledgments
This research was supported by the Swiss National Science Foundation through the National Centre
of Competence in Digital Fabrication (NCCR dfab), by Huawei Tech R&D (UK) through a research
funding agreement, by an ETH RobotX research grant funded through the ETH Zurich Foundation,
and partially by the ETH AI Center. This work was also conducted as part of ANYmal Research, a
community to advance legged robotics. We would also like to thank Cesar Cadena for his support
and helpful discussions.
References
[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances
in neural information processing systems , 33:1877–1901, 2020.
[2] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur,
A. Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 ,
2024.
[3] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,
D. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682 , 2022.
[4] J. Ahn, R. Verma, R. Lou, D. Liu, R. Zhang, and W. Yin. Large language models for mathe-
matical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157 , 2024.
[5] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y . Burda,
N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374 , 2021.
[6] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le, D. Zhou, et al. Chain-of-
thought prompting elicits reasoning in large language models. Advances in neural information
processing systems , 35:24824–24837, 2022.
[7] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code
as policies: Language model programs for embodied control. In 2023 IEEE International
Conference on Robotics and Automation (ICRA) , pages 9493–9500. IEEE, 2023.
[8] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakr-
ishnan, K. Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances.
arXiv preprint arXiv:2204.01691 , 2022.
9

[9] C. E. Mower, Y . Wan, H. Yu, A. Grosnit, J. Gonzalez-Billandon, M. Zimmer, J. Wang,
X. Zhang, Y . Zhao, A. Zhai, et al. Ros-llm: A ros framework for embodied ai with task
feedback and structured reasoning. arXiv preprint arXiv:2406.19741 , 2024.
[10] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,
Y . Chebotar, et al. Inner monologue: Embodied reasoning through planning with language
models. In Conference on Robot Learning , pages 1769–1782. PMLR, 2023.
[11] W. Huang, C. Wang, R. Zhang, Y . Li, J. Wu, and L. Fei-Fei. V oxposer: Composable 3d value
maps for robotic manipulation with language models. In Conference on Robot Learning , pages
540–562. PMLR, 2023.
[12] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 , 2023.
[13] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y . T. Lee,
Y . Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
arXiv preprint arXiv:2303.12712 , 2023.
[14] G. Team, P. Georgiev, V . I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan,
S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of
context. arXiv preprint arXiv:2403.05530 , 2024.
[15] Z. Wu, X. Chen, Z. Pan, X. Liu, W. Liu, D. Dai, H. Gao, Y . Ma, C. Wu, B. Wang, et al.
Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal under-
standing. arXiv preprint arXiv:2412.10302 , 2024.
[16] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,
Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. In International
Conference on Machine Learning , pages 8469–8488. PMLR, 2023.
[17] A. Brohan, N. Brown, J. Carbajal, Y . Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess,
A. Dubey, C. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to
robotic control. arXiv preprint arXiv:2307.15818 , 2023.
[18] P. Zhi, Z. Zhang, Y . Zhao, M. Han, Z. Zhang, Z. Li, Z. Jiao, B. Jia, and S. Huang. Closed-loop
open-vocabulary mobile manipulation with gpt-4v. arXiv preprint arXiv:2404.10220 , 2024.
[19] G. R. Team, S. Abeyruwan, J. Ainslie, J.-B. Alayrac, M. G. Arenas, T. Armstrong, A. Balakr-
ishna, R. Baruch, M. Bauza, M. Blokzijl, et al. Gemini robotics: Bringing ai into the physical
world. arXiv preprint arXiv:2503.20020 , 2025.
[20] A. Mei, G.-N. Zhu, H. Zhang, and Z. Gan. Replanvlm: Replanning robotic tasks with visual
language models. IEEE Robotics and Automation Letters , 2024.
[21] L. Zha, Y . Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia, and D. Sadigh. Distill-
ing and retrieving generalizable knowledge for robot manipulation via language corrections.
In2024 IEEE International Conference on Robotics and Automation (ICRA) , pages 15172–
15179. IEEE, 2024.
[22] Z. Liu, A. Bahety, and S. Song. Reflect: Summarizing robot experiences for failure explanation
and correction. In Conference on Robot Learning , pages 3468–3484. PMLR, 2023.
[23] J. Zhang. Cognitive functions of the brain: perception, attention and memory. arXiv preprint
arXiv:1907.02863 , 2019.
[24] L. R. Squire. Memory and the hippocampus: a synthesis from findings with rats, monkeys,
and humans. Psychological review , 99(2):195, 1992.
10

[25] E. Tulving. Episodic memory: From mind to brain. Annual review of psychology , 53(1):1–25,
2002.
[26] J. R. Anderson. The architecture of cognition . Psychology Press, 2013.
[27] A. Madaan, N. Tandon, P. Clark, and Y . Yang. Memory-assisted prompt editing to improve gpt-
3 after deployment. In ACL 2022 Workshop on Commonsense Representation and Reasoning ,
2022.
[28] X. Tang, T. Hu, M. Ye, Y . Shao, X. Yin, S. Ouyang, W. Zhou, P. Lu, Z. Zhang, Y . Zhao,
et al. Chemagent: Self-updating library in large language models improves chemical reasoning.
arXiv preprint arXiv:2501.06590 , 2025.
[29] J. Zheng, C. Shi, X. Cai, Q. Li, D. Zhang, C. Li, D. Yu, and Q. Ma. Lifelong learning of large
language model based agents: A roadmap. arXiv preprint arXiv:2501.07278 , 2025.
[30] Z. Wang, B. Yu, J. Zhao, W. Sun, S. Hou, S. Liang, X. Hu, Y . Han, and Y . Gan. Karma:
Augmenting embodied ai agents with long-and-short term memory systems. arXiv preprint
arXiv:2409.14908 , 2024.
[31] Q. Xie, S. Y . Min, P. Ji, Y . Yang, T. Zhang, K. Xu, A. Bajaj, R. Salakhutdinov, M. Johnson-
Roberson, and Y . Bisk. Embodied-rag: General non-parametric embodied memory for retrieval
and generation. arXiv preprint arXiv:2409.18313 , 2024.
[32] M. F. Ginting, D.-K. Kim, S.-K. Kim, B. J. Krishna, M. J. Kochenderfer, S. Omidshafiei, and
A.-a. Agha-mohammadi. Saycomply: Grounding field robotic tasks in operational compliance
through retrieval-based language models. arXiv preprint arXiv:2411.11323 , 2024.
[33] R. Arora, N. Narendranath, A. Tambi, S. S. Zachariah, S. Chakraborty, and R. Paul. G2tr:
Generalized grounded temporal reasoning for robot instruction following by combining large
pre-trained models. arXiv preprint arXiv:2410.07494 , 2024.
[34] S. Paplu, R. F. Navarro, and K. Berns. Harnessing long-term memory for personalized human-
robot interactions. In 2022 IEEE-RAS 21st International Conference on Humanoid Robots
(Humanoids) , pages 377–382. IEEE, 2022.
[35] I. Idrees, S. P. Reiss, and S. Tellex. Robomem: Giving long term memory to robots. arXiv
preprint arXiv:2003.10553 , 2020.
[36] L. B ¨armann, R. Kartmann, F. Peller-Konrad, J. Niehues, A. Waibel, and T. Asfour. Incremen-
tal learning of humanoid robot behavior from natural interaction and large language models.
Frontiers in Robotics and AI , 11:1455375, 2024.
[37] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K ¨uttler, M. Lewis, W.-t.
Yih, T. Rockt ¨aschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.
Advances in neural information processing systems , 33:9459–9474, 2020.
[38] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, H. Wang, and H. Wang. Retrieval-
augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 ,
2, 2023.
[39] R. Firoozi, J. Tucker, S. Tian, A. Majumdar, J. Sun, W. Liu, Y . Zhu, S. Song, A. Kapoor,
K. Hausman, et al. Foundation models in robotics: Applications, challenges, and the future.
The International Journal of Robotics Research , page 02783649241281508, 2023.
[40] Y . Hu, F. Lin, T. Zhang, L. Yi, and Y . Gao. Look before you leap: Unveiling the power of
gpt-4v in robotic vision-language planning. arXiv preprint arXiv:2311.17842 , 2023.
11

[41] Y . Guo, Y .-J. Wang, L. Zha, and J. Chen. Doremi: Grounding language model by detecting and
recovering from plan-execution misalignment. In 2024 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) , pages 12124–12131. IEEE, 2024.
[42] M. Ahn, M. G. Arenas, M. Bennice, N. Brown, C. Chan, B. David, A. Francis, G. Gonzalez,
R. Hessmer, T. Jackson, et al. Vader: Visual affordance detection and error recovery for multi
robot human collaboration. arXiv preprint arXiv:2405.16021 , 2024.
[43] J. Yang, H. Zhang, F. Li, X. Zou, C. Li, and J. Gao. Set-of-mark prompting unleashes extraor-
dinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 , 2023.
[44] H. Liu, R. Yao, W. Liu, Z. Huang, S. Shen, and J. Ma. Codrivevlm: Vlm-enhanced urban coop-
erative dispatching and motion planning for future autonomous mobility on demand systems.
arXiv preprint arXiv:2501.06132 , 2025.
[45] S. Nasiriany, F. Xia, W. Yu, T. Xiao, J. Liang, I. Dasgupta, A. Xie, D. Driess, A. Wahid, Z. Xu,
et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. In International
Conference on Machine Learning , pages 37321–37341. PMLR, 2024.
[46] K. Fang, F. Liu, P. Abbeel, and S. Levine. Moka: Open-world robotic manipulation through
mark-based visual prompting. Robotics: Science and Systems (RSS) , 2024.
[47] G. Tziafas and H. Kasaei. Towards open-world grasping with large vision-language models.
In8th Annual Conference on Robot Learning , 2024.
[48] Y . Qian, X. Zhu, O. Biza, S. Jiang, L. Zhao, H. Huang, Y . Qi, and R. Platt. Thinkgrasp: A
vision-language system for strategic part grasping in clutter. arXiv preprint arXiv:2407.11298 ,
2024.
[49] Z. Wang, B. Liang, V . Dhat, Z. Brumbaugh, N. Walker, R. Krishna, and M. Cakmak. I can tell
what i am doing: Toward real-world natural language grounding of robot experiences. In 8th
Annual Conference on Robot Learning , 2024.
[50] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents
with verbal reinforcement learning. Advances in Neural Information Processing Systems , 36:
8634–8652, 2023.
[51] T. Kagaya, T. J. Yuan, Y . Lou, J. Karlekar, S. Pranata, A. Kinose, K. Oguri, F. Wick, and Y . You.
Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. In
NeurIPS 2024 Workshop on Open-World Agents , 2024.
[52] G. Sarch, L. Jang, M. Tarr, W. W. Cohen, K. Marino, and K. Fragkiadaki. Vlm agents generate
their own memories: Distilling experience into embodied programs of thought. Advances in
Neural Information Processing Systems , 37:75942–75985, 2024.
[53] M. Xu, P. Huang, W. Yu, S. Liu, X. Zhang, Y . Niu, T. Zhang, F. Xia, J. Tan, and D. Zhao.
Creative robot tool use with large language models. arXiv preprint arXiv:2310.13065 , 2023.
[54] OpenAI. New embedding models and api updates, 2024. URL https://openai.com/
index/new-embedding-models-and-api-updates/ . Accessed: 2025-05-01.
[55] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y . Chen, F. Yan, et al.
Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint
arXiv:2401.14159 , 2024.
[56] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, et al.
Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In
European Conference on Computer Vision , pages 38–55. Springer, 2024.
12

[57] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead,
A. C. Berg, W.-Y . Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 4015–4026, 2023.
[58] Y . Eldar, M. Lindenbaum, M. Porat, and Y . Y . Zeevi. The farthest point strategy for progressive
image sampling. IEEE transactions on image processing , 6(9):1305–1315, 1997.
[59] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y . Xie, and C. Lu. Anygrasp:
Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on
Robotics , 39(5):3929–3945, 2023.
[60] J. Carpentier, G. Saurel, G. Buondonno, J. Mirabel, F. Lamiraux, O. Stasse, and N. Mansard.
The pinocchio c++ library – a fast and flexible implementation of rigid body dynamics algo-
rithms and their analytical derivatives. In IEEE International Symposium on System Integra-
tions (SII) , 2019.
[61] M. Hutter, C. Gehring, D. Jud, A. Lauber, C. D. Bellicoso, V . Tsounis, J. Hwangbo, K. Bodie,
P. Fankhauser, M. Bloesch, et al. Anymal-a highly mobile and dynamic quadrupedal robot.
In2016 IEEE/RSJ international conference on intelligent robots and systems (IROS) , pages
38–44. IEEE, 2016.
[62] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost
in the middle: How language models use long contexts. Transactions of the Association for
Computational Linguistics , 12:157–173, 2024.
[63] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston. Retrieval augmentation reduces hallu-
cination in conversation. arXiv preprint arXiv:2104.07567 , 2021.
[64] M. Abugurain and S. Park. Integrating disambiguation and user preferences into large language
models for robot motion planning. arXiv preprint arXiv:2404.14547 , 2024.
[65] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi.
Self-instruct: Aligning language models with self-generated instructions. arXiv preprint
arXiv:2212.10560 , 2022.
13

Appendix
A Prompts for the VLM Modules
A.1 VLM Scene Describer
The VLM Scene Describer generates an initial scene description, combining the user’s instruction
as a scenario, which is then used as a key for memory retrival.
Prompt:
You are a helpful assistant for a legged robot equipped with a single arm and a two-finger gripper.
You specialize in generating accurate scene descriptions.
You always apply chain-of-thought reasoning to ensure accurate and comprehensive scene understanding.
The user’s instruction: {instruction }
Based on your observation of the image, provide a short scene description focusing on the spatial
relationships between the target object and the nearby objects the robot may need to interact with.
Function Definition:
1{
2 " type ": " function ",
3 " function ": {
4 " name ": " describe_scene ",
5 " description ": " Provide a brief description of the environment surrounding the
target object .",
6 " strict ": True ,
7 " parameters ": {
8 " type ": " object ",
9 " properties ": {
10 " reasoning ": {
11 " type ": " string ",
12 " description ": " Describe what was observed in the image to generate
the scene description .",
13 },
14 " scene_description ": {
15 " type ": " string ",
16 " description ": "A brief summary of the scene , focusing on relevant
spatial relationships .",
17 },
18 },
19 " required ": [" reasoning ", " scene_description "],
20 " additionalProperties ": False ,
21 },
22 },
23 }
A.2 VLM Action Planner
The VLM Action Planner leverages the user’s instructions, current scene information, and memory
to generate the next necessary action, such as picking, placing, or pushing an object, to successfully
complete the given task.
Prompt:
You are a helpful assistant for a legged robot equipped with a single arm and a two-finger gripper.
You specialize in task planning and can learn or adapt from the previous experience.
Always apply chain-of-thought reasoning and think step by step before making any final decision.
The robot received this instruction from the user: {instruction }. Considering the given image, along
with the robot’s capabilities and experience, what is the most appropriate next action to efficiently
fulfill the user’s instruction?
Here is the short-term memory for the current task so far for reference: {short term memory }. Please
learn from this experience history, especially the suggestions for next action, to plan the next action.
The following lifelong memories represent the robot’s previous activities and are intended to showcase its
capabilities and experience. Please first identify similar scenarios and learn from them to avoid similar
failures: {long term memory }
14

For the push action, choose the most efficient direction to push. For example, if the target is on the
left in the image, the robot should prefer to push the object to the left if both directions are viable.
Conversely, If the object is on the right side, the robot should prefer pushing it rightward if both
directions are viable.
For pick or place actions, indicate whether the object must be grasped at a specific section to ensure
a stable and proper grasp. This will enable the image annotation tool and trigger a follow-up query to
achieve more precise grasping. Note that due to imperfect part segmentation, this should be activated
if the object needs to be held by a specific part, for example, to avoid contaminating food or to prevent
damage to the object.
If you plan to use a tool, first check whether it’s ready to use. For example, if you intend to use the
axes on the table, you may need to grasp it first.
For all actions, always pay attention to the spatial relationships between objects, and ensure the robot
interacts with only one object at a time. Avoid giving or parameters that could cause the robot to
unintentionally interact with the wrong object.
Function Definition:
1{
2 " type ": " function ",
3 " function ": {
4 " name ": " pick_object ",
5 " description ": " Pick a specified object , providing details about the grasping
area and surrounding environment .",
6 " strict ": true ,
7 " parameters ": {
8 " type ": " object ",
9 " properties ": {
10 " scene_description ": {
11 " type ": " string ",
12 " description ": " Short description of the object ’s surroundings ,
especially the spatial relationships with nearby objects ."
13 },
14 " reasoning ": {
15 " type ": " string ",
16 " description ": " Provide reasoning for each parameter choice ."
17 },
18
19 " target_object ": {
20 " type ": " string ",
21 " description ": " Specify the object the robot should pick ."
22 },
23 " grasp_part ": {
24 " type ": " string ",
25 " description ": " Specify the part of the object to be grasped . Leave
blank if no commonly recognized specific part is relevant to the
action ."
26 },
27 " specific_grasp_required ": {
28 " type ": " boolean ",
29 " description ": " Indicate whether the object must be grasped at a
specific section to ensure a stable and proper grasp ."
30 },
31 " action_description ": {
32 " type ": " string ",
33 " description ": " Briefly describe the action to be performed , focusing
only on what the robot should do."
34 }
35 },
36 " required ": [" scene_description ", " reasoning ", " target_object ", " grasp_part ",
" specific_grasp_required ", " action_description "],
37 " additionalProperties ": false
38 }
39 }
40 },
41 {
42 " type ": " function ",
43 " function ": {
44 " name ": " place_object ",
45 " description ": " Place a specified object at a designated location , including
context about positioning and the surrounding environment .",
46 " strict ": true ,
47 " parameters ": {
48 " type ": " object ",
49 " properties ": {
50 " scene_description ": {
51 " type ": " string ",
15

52 " description ": " Detailed description of the surroundings where the
object will be placed , including nearby objects and obstacles ."
53 },
54 " reasoning ": {
55 " type ": " string ",
56 " description ": " Provide reasoning for each parameter choice ."
57 },
58 " target_object ": {
59 " type ": " string ",
60 " description ": " Specify the name or type of the object that the robot
should place ."
61 },
62 " placement_location ": {
63 " type ": " string ",
64 " description ": "The specific name of the location where the robot
should place the object ."
65 },
66 " precise_placement_spot_required ": {
67 " type ": " boolean ",
68 " description ": " Indicate whether the object must be placed in a
specific spot within the placement area ."
69 },
70 " action_description ": {
71 " type ": " string ",
72 " description ": " Briefly describe the action to be performed , focusing
only on what the robot should do."
73 }
74 },
75 " required ": [
76 " scene_description ", " reasoning ", " target_object ", " placement_location ",
" precise_placement_spot_required ", " action_description "],
77 " additionalProperties ": false
78 }
79 }
80 },
81 {
82 " type ": " function ",
83 " function ": {
84 " name ": " push_object ",
85 " description ": " Push the specified object by the minimum required distance .",
86 " strict ": true ,
87 " parameters ": {
88 " type ": " object ",
89 " properties ": {
90 " scene_description ": {
91 " type ": " string ",
92 " description ": " Detailed description of the scene , including spatial
relationships with nearby objects . Also describe the object ’s
location in the image frame (e.g., left or right side )."
93 },
94 " reasoning ": {
95 " type ": " string ",
96 " description ": " Provide reasoning for each parameter choice ."
97 },
98
99 " object_to_push ": {
100 " type ": " string ",
101 " description ": " Specify the object to be pushed by the robot ’s
gripper ."
102 },
103 " push_direction ": {
104 " type ": " string ",
105 " enum ": [" left ", " right "],
106 " description ": "The direction in which to push the object in the
image view ."
107 },
108 " action_description ": {
109 " type ": " string ",
110 " description ": " Briefly describe the action to be performed , focusing
only on what the robot should do."
111 }
112 },
113 " required ": [" scene_description ", " reasoning ", " object_to_push ", "
push_direction ", " action_description "],
114 " additionalProperties ": false
115 }
116 }
117 }
16

A.3 VLM Grasping Selector
The VLM Grasping Selector determines the most stable and effective section of the object to grasp,
providing reasoning and descriptions for each section to identify the optimal grasp section for the
performing action.
Prompt:
You are a helpful assistant for a legged robot equipped with a single arm and a two-finger gripper.
You specialize in semantic object manipulation.
You always apply chain-of-thought reasoning to thoroughly analyze each situation before making a final
decision.
Based on your observation of the image, determine the optimal grasping section of the object to ensure
stable handling and successfully fulfill the given action: {action }. Avoid contaminating food, damaging
the object, or compromising safety. Select the most appropriate grasping section (by number) that best
fulfills the action requirements, as a human would.
Function Definition:
1{
2 " type ": " function ",
3 " function ": {
4 " name ": " choose_section ",
5 " description ": " Select the most stable and effective section of the object to
grasp in order to perform the action .",
6 " strict ": True ,
7 " parameters ": {
8 " type ": " object ",
9 " properties ": {
10 " reasoning ": {
11 " type ": " string ",
12 " description ": " Explain the rationale behind selecting the chosen
section .",
13 },
14 " object_part_description ": {
15 " type ": " string ",
16 " description ": " Describe each numbered section and the corresponding
part of the object .",
17 },
18 " grasp_section_number ": {
19 " type ": " integer ",
20 " enum ": [i + 1 for i in range ( len( section_list ))],
21 " description ": " Choose the number corresponding to the best section
for grasping the target object .",
22 },
23 },
24 " required ": [" reasoning ", " object_part_description ", " grasp_section_number "],
25 " additionalProperties ": False ,
26 },
27 },
28 }
A.4 VLM Placement Selector
The VLM Placement Selector chooses the most stable and effective location for object placement,
providing reasoning and a description of each option to identify the best spot for the performing
action.
Prompt:
You are a helpful assistant for a legged robot equipped with a single arm and a two-finger gripper.
You specialize in spatial analysis and object placement.
You always apply chain-of-thought reasoning to thoroughly analyze each situation before making a final
decision.
Based on your observation of the given image, select the optimal placement location for the object that
ensures both stability and accessibility for performing the action: {action }.
Function Definition:
17

1{
2 " type ": " function ",
3 " function ": {
4 " name ": " choose_location ",
5 " description ": " Select the most stable and effective location to place the object
in order to fulfill the task .",
6 " strict ": True ,
7 " parameters ": {
8 " type ": " object ",
9 " properties ": {
10 " reasoning ": {
11 " type ": " string ",
12 " description ": " Explain the rationale behind selecting each location ,
focusing on stability , accessibility , and suitability for the
task .",
13 },
14 " placement_spot_description ": {
15 " type ": " string ",
16 " description ": " Describe each numbered option and its corresponding
placement spot .",
17 },
18 " best_placement_location ": {
19 " type ": " integer ",
20 " enum ": [i + 1 for i in range ( len( placement_spot_list ))],
21 " description ": " Choose the number corresponding to the most suitable
placement location for the object .",
22 },
23 },
24 " required ": [" reasoning ", " placement_spot_description ", "
best_placement_location "],
25 " additionalProperties ": False ,
26 },
27 },
28 }
A.5 VLM Pushing Spot Selector
The VLM Pushing Spot Selector identifies the most effective final gripper position for completing
the task, minimizing unnecessary movement while providing reasoning and descriptions for each
potential position.
Prompt:
You are a helpful assistant for a legged robot equipped with a single arm and a two-finger gripper.
You specialize in semantic object pushing.
You always apply chain-of-thought reasoning to thoroughly analyze each situation before making a final
decision.
Based on the given image, determine the optimal final position of the gripper to complete the following
action efficiently, ensuring stability and minimizing unnecessary pushing distance: {action }.
The initial position (0) represents the gripper’s position before pushing. The provided numbers indicate
the possible final gripper positions after the push. These positions refer specifically to the gripper’s
movement, not the object’s.
Assume that the relative position between the gripper and the object remains unchanged before and after
the push.
Select the optimal final gripper position number.
Function Definition:
1{
2 " type ": " function ",
3 " function ": {
4 " name ": " select_position ",
5 " description ": " Select the most effective final gripper position to successfully
complete the action while minimizing unnecessary movement .",
6 " strict ": True ,
7 " parameters ": {
8 " type ": " object ",
9 " properties ": {
10 " reasoning ": {
11 " type ": " string ",
12 " description ": " Explain the rationale behind selecting the final
gripper position .",
18

13 },
14 " gripper_position_description ": {
15 " type ": " string ",
16 " description ": "A list of descriptions corresponding to each possible
gripper position .",
17 },
18 " gripper_position_number ": {
19 " type ": " integer ",
20 " enum ": [i + 1 for i in range ( len( position_list ))],
21 " description ": " Select the number corresponding to the optimal final
gripper position to complete the task .",
22 },
23 },
24 " required ": [
25 " reasoning ",
26 " gripper_position_description ",
27 " gripper_position_number ",
28 ],
29 " additionalProperties ": False ,
30 },
31 },
32 }
A.6 VLM Success Detector
The VLM Success Detector determines whether the current action or task has been successfully
completed, diagnosing potential failure causes, and suggest next steps.
Prompt:
You are a helpful assistant for a legged robot equipped with a single arm and a two-finger gripper.
You specialize in detecting whether an action or task has been successfully completed, and you provide
clear, constructive feedback or alternatives when needed.
You always apply chain-of-thought reasoning to thoroughly analyze each situation before reaching a
conclusion.
Please analyze the provided image to determine whether the given action and current task have been
successfully completed.
If an action has failed, analyze the spatial relationship between the target object and its surrounding
objects to identify the cause of failure.
Learn from the failure cause and consider another way to achieve the goal. This may involve interacting
with other relevant objects in the environment if necessary.
Considering the robot only has a two-finger gripper, it might not be able to interact with things very
precisely.
If there are any tools or objects in the image that could help achieve the goal, please consider using
them. Be creative! Everything on the table could potentially be used as a tool.
Action to detect: {action }
The user’s instruction: {instruction }
Function Definition:
1{
2 " type ": " function ",
3 " function ": {
4 " name ": " evaluate_action_status_and_issues ",
5 " description ": " Evaluate whether the current action or task was successfully
completed , and identify any issues that may impact the task ’s overall
feasibility .",
6 " strict ": true ,
7 " parameters ": {
8 " type ": " object ",
9 " properties ": {
10 " reasoning ": {
11 " type ": " string ",
12 " description ": " Provide a brief explanation of the reasoning behind
the action status and issue identification ."
13 },
14 " action_status ": {
15 " type ": " string ",
16 " enum ": [" successful ", " uncertain ", " failed "],
17 " description ": " Indicate whether the current action was completed
successfully , failed , or had an uncertain outcome ."
19

18 },
19 " failure_cause ": {
20 " type ": " string ",
21 " description ": " Provide one short , specific reason for the action ’s
outcome ."
22 },
23 " next_step_suggestions ": {
24 " type ": " string ",
25 " description ": " Provide one short and specific suggestion for the next
action needed to fulfill the task ."
26 },
27 " is_task_completed ": {
28 " type ": " boolean ",
29 " description ": "Set to true if the task has been successfully completed ,
that is , if the intended result has been achieved ."
30 }
31 },
32 " required ": [
33 " reasoning ", " action_status ", " failure_cause ", " next_step_suggestions ", "
is_task_completed "],
34 " additionalProperties ": false
35 }
36 }
37 }
A.7 VLM Experience Summarizer
The VLM Experience Summarizer generates a lifelong memory summarizing the robot’s recent ac-
tions based on short-term memory logs. It captures key events such as successes, failures, behavioral
adjustments.
Prompt:
You are a helpful assistant for a legged robot equipped with a single arm and a two-finger gripper.
You specialize in converting the robot’s experiences into concise task summaries.
You always apply chain-of-thought reasoning to thoroughly analyze each situation before performing the
conversion.
Please convert the following robot short-term memory into a single, concise paragraph summary.
Robot short-term memory: {short term memory }
Function Definition:
1{
2 " type ": " function ",
3 " function ": {
4 " name ": " summarize_robot_experience ",
5 " description ": (
6 " Summarize the robot ’s short - term experience in a paragraph using the given
memory logs . "
7 " Include reflections if the robot learned or adjusted its behavior , or any
human observations ."
8 ),
9 " strict ": True ,
10 " parameters ": {
11 " type ": " object ",
12 " properties ": {
13 " summary ": {
14 " type ": " string ",
15 " description ": (
16 "A paragraph summarizing the sequence of actions from the given
memory log , including any failures , adjustments made by the
robot , or observations by the operator ."
17 ),
18 }
19 },
20 " required ": [" summary "],
21 " additionalProperties ": False ,
22 },
23 },
24 }
20

B Additional Results
B.1 Short-Term Memory
We evaluated robot memory generation across four challenging real-world scenarios. Each task was
tested 5–10 times, with two attempts allowed per trial. Table 5 presents the complete response from
the VLM modules for one task illustrated in Figure 4, showcasing memory generation through the
robot’s self-reflection during the task.
Table 5: Illustration of self-reflection for the instruction: “Pick up the bowl.”
Scene VLM Response
Response from VLM Task Planner: {"name":
"pick object", "arguments": {"scene description":
"A bowl with an apple inside is placed on a table.
The bowl is centered on the table with no other
objects nearby.", "reasoning": "The bowl is centered
and unobstructed, allowing for a straightforward
approach. Grasping the bowl from the middle ensures
stability and avoids disturbing the apple inside.",
"target object": "bowl", "grasp part": "middle",
"specific grasp required": true, "action description":
"Approach the bowl from the middle and grasp it
securely to pick it up." } }
Observation from human: The robot failed to pick up
the bowl cause the apple was dropped on the floor.
Response from VLM Task Planner: {"name":
"pick object", "arguments": {"scene description":
"A bowl with an apple inside is on a table.
The robot’s gripper is positioned in front of
the bowl.", "reasoning": "The apple inside
the bowl may cause instability if the bowl is
picked directly. To avoid dropping the apple, it
should be removed first. The apple is centrally
placed, allowing for a straightforward grasp
from the middle.", "target object": "apple",
"grasp part": "", "specific grasp required": false,
"action description": "Grasp the apple from the
middle of the bowl and lift it out." } }
Response from VLM Success Detector: {"name":
"evaluate action status andissues", "arguments":
{"reasoning": "The robot’s gripper is holding
the apple, indicating that the action of grasping
the apple was successful.", "action status":
"successful", "failure cause": "None",
"next step suggestions": "Release the apple and
reposition the gripper to grasp the bowl directly.",
"istask completed": false } }
21

Scene VLM Response
Response from VLM Task Planner: {"name":
"place object", "arguments": {"scene description":
"The robot is holding an apple in its gripper, and
there is a bowl on the table in front of it. The
table is clear of other objects.", "reasoning": "The
robot needs to place the apple down before it can
pick up the bowl. The table is clear, so placing the
apple directly on the table is safe and efficient.",
"target object": "apple", "placement location":
"table", "precise placement spot required": false,
"action description": "Place the apple on the table."
} }
Response from VLM Success Detector: {"name":
"evaluate action status andissues", "arguments":
{"reasoning": "The image shows the apple placed
on the table, which means the action of placing
the apple is successful. The robot’s gripper is
open and not holding the bowl, indicating that
the task of picking up the bowl has not been
completed. The bowl is positioned directly in
front of the robot, making it accessible for the
next attempt.", "action status": "successful",
"failure cause": "None", "next step suggestions":
"Move the gripper forward to grasp the bowl and lift
it.", "is task completed": false } }
Response from VLM Task Planner: {"name":
"pick object", "arguments": {"scene description":
"The bowl is on the table, centered in front of
the robot. There is an apple on the left side of
the table, but it is not obstructing the bowl.",
"reasoning": "The bowl is directly in front of
the robot, making it accessible for a direct
approach. The apple is not in the way, so the
robot can grasp the bowl from the middle without
any obstruction.", "target object": "bowl",
"grasp part": "middle", "specific grasp required":
false, "action description": "Move the gripper
forward to grasp the bowl and lift it." } }
Response from VLM Success Detector: {"name":
"evaluate action status andissues", "arguments": {
"reasoning": "The robot’s two-finger gripper is
positioned above the edge of the bowl and appears
to be holding it securely. There are no visible
obstructions or issues with the spatial relationship
between the bowl and its surroundings. The bowl is
not obstructed by other objects, and the gripper
is not holding any other object.", "action status":
"successful", "failure cause": "N/A - The action was
successful.", "next step suggestions": "No further
action is needed as the bowl has been successfully
picked up.", "is task completed": true } }
Once the task is marked as complete by the VLM Success Detector, E XPTEACH activates the VLM
Experience Summarizer to summarize the self-generated short-term memory accumulated during
the task, which is then stored in long-term memory. We show one example in Table 6.
22

Table 6: STM summarization for the instruction: “Pick up the bowl.” Left: Sequence of STM
entries. Right : A summary generated by the VLM Experience Summarizer.
STM Logs STM Summary
At time step 0, A bowl with an apple
inside is placed on a table. The bowl
is centered on the table with no other
objects nearby.
At time step 1, the human stated that
the robot failed to pick up the bowl
because the apple was dropped on the
floor.
At time step 2, VLM detected that the
robot successfully grasped the apple
from the middle of the bowl and lifted
it out. Suggestions for next action:
Release the apple and reposition the
gripper to grasp the bowl directly.
At time step 3, VLM detected that the
robot placed the apple on the table.
Suggestions for next action: Move the
gripper forward to grasp the bowl and
lift it.
At time step 4, VLM detected that the
robot successfully moved the gripper
forward to grasp the bowl and lift it.
Suggestions for next action: No further
action is needed as the bowl has been
successfully picked up.Response from VLM Experience Summarizer:
{
"name": "summarize robot experience",
"arguments": {
"summary": "The robot initially
attempted to pick up a bowl containing
an apple from a cluttered table but
failed, resulting in the apple being
dropped on the floor. After receiving
feedback, the robot successfully grasped
the apple from the bowl and placed it
on the table. Subsequently, the robot
repositioned its gripper to directly
grasp and lift the bowl. The task was
completed successfully with the bowl
being picked up, and no further actions
were needed. The robot adjusted its
approach based on visual feedback and
suggestions, demonstrating adaptability
in handling the objects."
}
}
B.2 Long-Term Memory
Upon receiving the user’s instruction, the VLM Scene Describer first generates a description of the
current scene. This scene description is then combined with the instruction to form a retrieval key,
which is used to retrieve relevant experiences from the long-term memory to guide this task. We
show an example of the retrieval in Table 7.
23

Table 7: Past experiences retrieved via RAG.
Scenario Retrieved LTM
Instruction: Pick up the milk
carton.
Scene:
long-term memory 1
Instruction: Pick up the bowl.
Scene: A bowl with an apple inside is placed on a table.
The bowl is centered on the table with no other objects
nearby.
Experience: The robot initially attempted to pick up
a bowl containing an apple from a cluttered table but
failed, resulting in the apple being dropped on the
floor. After receiving feedback, the robot successfully
grasped the apple from the bowl and placed it on the
table. Subsequently, the robot repositioned its gripper
to directly grasp and lift the bowl. The task was
completed successfully with the bowl being picked up,
and no further actions were needed. The robot adjusted
its approach based on visual feedback and suggestions,
demonstrating adaptability in handling the objects.
long-term memory 2
Instruction: Place the spoon next to the plate.
Scene: The spoon was already in the robot’s gripper, and
the plate was centered on the table.
Experience: The robot placed the spoon in the middle of
the plate.
long-term memory 3
Instruction: Pick up the water bottle.
Scene: The bottle was upright and centered on a flat
surface.
Experience: The robot wrapped its gripper around the
middle of the bottle and lifted it successfully.
long-term memory 4
Instruction: Put the apple on the plate.
Scene: The apple is next to a can.
Experience: The robot initially attempted to grasp an
apple that was positioned next to a can on a cluttered
table. The first attempt to pick up the apple failed
due to the can obstructing access. Recognizing this,
the robot adjusted its approach by pushing the can to
the right, successfully clearing the path to the apple.
With the obstruction removed, the robot was able to pick
up the apple from the top without disturbing the nearby
items. Finally, the robot placed the apple gently onto
the plate, completing the task without any further issues
or need for human intervention.
long-term memory 5
Instruction: Place the remote on the table.
Scene: The robot was already holding the remote, and the
table surface was empty.
Experience: The robot moved its gripper over the table
and released the remote onto the surface.
We evaluated the generalization capability of the LTM using a set of challenging real-world scenar-
ios. Each task is tested 5–10 times, with only one attempt allowed per trial. The following example
illustrates the full response of the VLM modules, demonstrating how long-term memory could help
ground VLMs to act according to past experiences.
24

Table 8: Illustration of the grounding with retrieved LTM based on the instruction: “Pick up
the milk carton.”
Scene Output
Response from VLM Task Planner: {"name":
"pick object", "arguments": {"scene description":
"A milk carton with an apple on top is placed on a
table. The carton is centered on the table with no
other objects nearby.", "reasoning": "The apple on
top of the milk carton needs to be removed first to
avoid dropping it when picking up the carton. This is
similar to the previous experience where the robot had
to remove an apple from a bowl before picking up the
bowl.", "target object": "apple", "grasp part": "top",
"specific grasp required": false, "action description":
"Grasp the apple from the top and lift it off the milk
carton." } }
Response from VLM Success Detector: {"name":
"evaluate action status andissues", "arguments": {
"reasoning": "The robot’s two-finger gripper is
currently grasping the apple, which is positioned on top
of the milk carton. The action specified was to grasp
the apple from the top and lift it off the milk carton.
However, the image shows the apple still in the gripper
and above the milk carton, suggesting the apple has been
successfully grasped and lifted. The task, however, is
to pick up the milk carton, which cannot be done while
the apple is still in the gripper. The apple must be
placed elsewhere before the robot can attempt to grasp
and lift the milk carton. There are no other objects on
the table that could be used as tools, so the next step
should be to place the apple on the table, freeing the
gripper to pick up the milk carton.", "action status":
"successful", "failure cause": "Apple is still in the
gripper, preventing the robot from grasping the milk
carton.", "next step suggestions": "Place the apple on
the table to free the gripper, then attempt to grasp and
lift the milk carton.", "is task completed": false } }
Response from VLM Task Planner: {"name":
"place object", "arguments": {"scene description":
"The robot is holding an apple in its gripper, and
there is a milk carton on the table in front of it.
The table is clear, providing space to place the
apple down.", "reasoning": "The robot’s gripper
is currently occupied with an apple, preventing it
from picking up the milk carton. Placing the apple
on the table will free the gripper, allowing the
robot to proceed with picking up the milk carton.",
"target object": "apple", "placement location":
"table", "precise placement spot required": false,
"action description": "Place the apple on the table
to free the gripper." } }
25

Scene Output
Response from VLM Success Detector: {"name":
"evaluate action status andissues", "arguments": {
"reasoning": "The image shows the robot’s gripper
is open and empty, with the apple now placed on the
table to the left and the milk carton in the center.
The table is clear, and the apple is no longer in
the gripper, indicating the action to place the apple
down has been completed. The gripper is now free to
interact with the milk carton, which is the next step for
the task. There are no spatial obstructions between
the gripper and the milk carton, so the robot can
proceed to pick up the milk carton.", "action status":
"successful", "failure cause": "No failure; the
apple was placed on the table, freeing the gripper.",
"next step suggestions": "Proceed to move the gripper
towards the milk carton and attempt to grasp it.",
"istask completed": false } }
Response from VLM Task Planner: {"name":
"pick object", "arguments": {"scene description":
"The milk carton is on the table, directly in front of
the robot’s gripper. There is an apple to the left of
the milk carton, but it is not obstructing the path to
the milk carton.", "reasoning": "The milk carton is
directly in front of the robot, making it accessible
for a direct approach. There are no obstacles in
the way, so the robot can grasp it from the middle.
The milk carton should be grasped from the sides to
ensure a stable grip.", "target object": "milk carton",
"grasp part": "sides", "specific grasp required": true,
"action description": "Move the gripper towards the milk
carton and grasp it from the sides to lift it." } }
Response from VLM Success Detector: {"name":
"evaluate action status andissues", "arguments": {
"reasoning": "The robot’s gripper is currently holding
a green carton labeled ’beleaf,’ which appears to be a
plant-based drink, possibly oat milk. The action was
to pick up the milk carton, and the gripper is securely
around the carton, lifting it from the table. There
are no visible obstructions or issues with the grasp.
The only other object on the table is an apple, which
is not interfering with the action. The two-finger
gripper seems to have successfully grasped the carton,
and the task appears to be completed.", "action status":
"successful", "failure cause": "N/A - The action was
successful.", "next step suggestions": "No further
action needed; the milk carton has been picked up.",
"istask completed": true } }
B.3 Image Annotation Module
To evaluate the image annotation module, we conducted 5 trials per scenario across 7 different
scenarios for both picking and pushing actions. The evaluation measures the success rate of selecting
the correct object sections for picking and analyzes the distance error between the object and the
target position after pushing, as shown in Figure 5.
The following figures (Figure 6, Figure 7, Figure 8) show testing scenarios in a real-world environ-
ment, where the image annotation module plays a key role in handling challenging objects that are
difficult to grasp due to their complex shapes or strict grasping constraints. A grasp is considered
successful if the object is picked up from the correct region, similar to how a human would grasp it.
26

Figure 6: Left: The robot picked up the meat section without VLM guidance through image an-
notation. Right : With image annotation, the robot successfully picked up the bone section of the
drumstick.
Figure 7: Left: The robot picked up the meat of the skewer without VLM guidance through image
annotation. Right : With image annotation, the robot successfully picked up the stick of the skewer.
Figure 8: Left: The robot picked up the body of the brush without VLM guidance through image
annotation. Right : With image annotation, the robot successfully picked up the handle of the brush.
27