# SINDI: an Efficient Index for Approximate Maximum Inner Product Search on Sparse Vectors

**Authors**: Ruoxuan Li, Xiaoyao Zhong, Jiabao Jin, Peng Cheng, Wangze Ni, Lei Chen, Zhitao Shen, Wei Jia, Xiangyu Wang, Xuemin Lin, Heng Tao Shen, Jingkuan Song

**Published**: 2025-09-10 08:38:32

**PDF URL**: [http://arxiv.org/pdf/2509.08395v1](http://arxiv.org/pdf/2509.08395v1)

## Abstract
Sparse vector Maximum Inner Product Search (MIPS) is crucial in multi-path
retrieval for Retrieval-Augmented Generation (RAG). Recent inverted index-based
and graph-based algorithms have achieved high search accuracy with practical
efficiency. However, their performance in production environments is often
limited by redundant distance computations and frequent random memory accesses.
Furthermore, the compressed storage format of sparse vectors hinders the use of
SIMD acceleration. In this paper, we propose the sparse inverted non-redundant
distance index (SINDI), which incorporates three key optimizations: (i)
Efficient Inner Product Computation: SINDI leverages SIMD acceleration and
eliminates redundant identifier lookups, enabling batched inner product
computation; (ii) Memory-Friendly Design: SINDI replaces random memory accesses
to original vectors with sequential accesses to inverted lists, substantially
reducing memory-bound latency. (iii) Vector Pruning: SINDI retains only the
high-magnitude non-zero entries of vectors, improving query throughput while
maintaining accuracy. We evaluate SINDI on multiple real-world datasets.
Experimental results show that SINDI achieves state-of-the-art performance
across datasets of varying scales, languages, and models. On the MsMarco
dataset, when Recall@50 exceeds 99%, SINDI delivers single-thread
query-per-second (QPS) improvements ranging from 4.2 to 26.4 times compared
with SEISMIC and PyANNs. Notably, SINDI has been integrated into Ant Group's
open-source vector search library, VSAG.

## Full Text


<!-- PDF content starts -->

SINDI: an Efficient Index for Approximate Maximum Inner
Product Search on Sparse Vectors
Ruoxuan Li
ECNU, Shanghai, China
rxlee@stu.ecnu.edu.cnXiaoyao Zhong
Jiabao Jin
Ant Group, Shanghai, China
zhongxiaoyao.zxy@antgroup.com
jinjiabao.jjb@antgroup.comPeng Cheng
Tongji University & ECNU
Shanghai, China
cspcheng@tongji.edu.cn
Wangze Ni
Zhejiang University
Hangzhou, China
niwangze@zju.edu.cnLei Chen
HKUST (GZ) & HKUST
Guangzhou & HK SAR, China
leichen@cse.ust.hkZhitao Shen
Ant Group, Shanghai, China
zhitao.szt@antgroup.com
Wei Jia
Xiangyu Wang
Ant Group, Shanghai, China
jw94525@antgroup.com
wxy407827@antgroup.comXuemin Lin
Shanghai Jiaotong University
Shanghai, China
xuemin.lin@gmail.comHeng Tao Shen
Jingkuan Song
Tongji University, Shanghai, China
shenhengtao@hotmail.com
jingkuan.song@gmail.com
ABSTRACT
Sparse vector Maximum Inner Product Search (MIPS) is crucial in
multi-path retrieval for Retrieval-Augmented Generation (RAG). Re-
cent inverted index-based and graph-based algorithms have achieved
high search accuracy with practical efficiency. However, their per-
formance in production environments is often limited by redun-
dant distance computations and frequent random memory accesses.
Furthermore, the compressed storage format of sparse vectors hin-
ders the use of SIMD acceleration. In this paper, we propose the
sparse inverted non-redundant distance index(SINDI), which incor-
porates three key optimizations: (i) Efficient Inner Product Computa-
tion: SINDIleverages SIMD acceleration and eliminates redundant
identifier lookups, enabling batched inner product computation; (ii)
Memory-Friendly Design: SINDIreplaces random memory accesses
to original vectors with sequential accesses to inverted lists, substan-
tially reducing memory-bound latency. (iii) Vector Pruning: SINDIre-
tains only the high-magnitude non-zero entries of vectors, improving
query throughput while maintaining accuracy. We evaluate SINDIon
multiple real-world datasets. Experimental results show that SINDI
achieves state-of-the-art performance across datasets of varying
scales, languages, and models. On the MSMARCOdataset, when
Recall@50 exceeds 99%, SINDIdelivers single-thread query-per-
second (QPS) improvements ranging from4.2√óto26.4√ócompared
with SEISMICand PYANNS. Notably, SINDIhas been integrated
into Ant Group‚Äôs open-source vector search library,VSAG.
PVLDB Reference Format:
Ruoxuan Li, Xiaoyao Zhong, Jiabao Jin, Peng Cheng, Wangze Ni, Lei Chen,
Zhitao Shen, Wei Jia, Xiangyu Wang, Xuemin Lin, Heng Tao Shen,
and Jingkuan Song. SINDI: an Efficient Index for Approximate Maximum
Inner Product Search on Sparse Vectors. PVLDB, 14(1): XXX-XXX, 2026.
doi:XX.XX/XXX.XX
This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by1 INTRODUCTION
Recently, retrieval-augmented generation (RAG) [ 7,11,12] become
one of most successful information retrieval framework attracting
attention from research communities and industry. Usually, texts are
embedded into dense vectors (i.e., no dimension of the vector is zero
entry) in RAG, then retrieved through approximate nearest neighbor
search (ANNS) on their corresponding dense vectors.
To enhance the RAG framework, researchers find that using sparse
vectors retrieval to complement dense vector based RAG can yield
better overall accuracy and recall performance. Different from dense
vectors, sparse vectors (i.e., only a very small portion of dimensions
of spare vectors are non-zero entries) are generated by specific mod-
els (e.g., SPLADE[ 4‚Äì6]) to preserve semantic information while
enabling precise lexical matching [ 12]. In the enhanced RAG frame-
work, dense vectors capture the holistic semantic similarity between
texts and sparse vectors ensure exact term recall, therefore resulting
in better overall performance. We show the process of the enhanced
RAG in the following example:
Example 1.Precise lexical matching.In the retriever stage of
RAG, queries and documents are compared to select top- ùëòcan-
didates. Dense vectors capture semantic similarity, while sparse
vectors support exact term matching. For example, ‚ÄúI love black
cats‚Äù is tokenized into ‚Äúi‚Äù, ‚Äúlove‚Äù, ‚Äúblack‚Äù, and ‚Äúcats‚Äù, with ‚Äúcats‚Äù
assigned the highest weight (0.8). A query containing ‚Äúcats‚Äù will
precisely match documents where this token has a high weight.Chal-
lenges in inner-product computation.Dense vectors are stored con-
tiguously, enabling parallel dot-product over consecutive dimensions
via SIMD. Sparse vectors typically have very high dimensionality
but store only their non-zero entries in a compact format, which
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, V ol. 14, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX

Doc:Iloveblackcats.BERTSPLADEQuery:Doyoukeepcats?
[0.6,0.3,0.8,‚Ä¶,0.5,0.1][I:0.2,love:0.4,black:0.6,cats:0.8]0.60.30.8‚Ä¶0.10.50.10.50.90.20.40.60.80129997399989999[you:0.1,keep:0.5,cats:0.9]0.70.10.8‚Ä¶0.20.4012‚Ä¶767766[0.7,0.1,0.8,‚Ä¶,0.4,0.2]TextModel
InnerProductCalculationDensevectorSparsevectorNon-zeroentriesMultiplicationoperator‚ûä‚ù∑‚ùπ‚ù∏Zeroentries‚ù∫
‚Ä¶Figure 1: Example of Dense and Sparse Vector Representations and
Inner Product Calculations.
leads to two bottlenecks: (1)ID lookup overhead: Matching common
non-zero dimensions requires traversing all non-zero entries. Even if
only one dimension (e.g., 9999) matches, all entries must be scanned.
(2)No SIMD acceleration: Their storage is not dimension-aligned,
preventing parallel SIMD processing.
The similarity retrieval problem for sparse vectors is formally
known as the Maximum Inner Product Search (MIPS) [ 10,15‚Äì17,
19], which aims to identify the top- ùëòvectors in a dataset that have
the largest inner product value with a given query vector. However,
due to the curse of dimensionality [ 9], performing exact MIPS in
high-dimensional spaces is computationally prohibitive. To mitigate
this issue, we focus on Approximate Maximum Inner Product Search
(AMIPS), which trades a small amount of recall for significantly
improved search efficiency.
Many algorithms [ 2,14,18] have been proposed for AMIPS,
employing techniques such as inverted index, proximity graphs,
and hash-based partitioning. They improve efficiency by grouping
similar vectors into the same partition, thereby reducing the number
of candidates examined during query processing.
Despite reducing the search space, existing approaches still face
two major performance bottlenecks: (i)Distance computation cost:
Matching non-zero dimensions between a query and a document
incurs substantial identifier lookup overhead, and the inner product
computation cannot be effectively accelerated using SIMD instruc-
tions. (ii)Random memory access cost: During query processing,
data are accessed in a random manner, and the variable lengths of
sparse vectors further complicate direct access in memory.
To address the aforementioned challenges, we propose SINDI, a
Sparse Inverted Non-redundant Distance-calculation Indexfor effi-
cient sparse vector search. The main contributions of this paper are
as follows: (i)Value-Storing Inverted Index: SINDIstores both vectorTable 1: Comparison to Existing Algorithms.
SINDI(ours)SEISMICPYANNs
Distance ComplexityO(Ô∏Ç‚à•ùëû‚à•
ùë†)Ô∏Ç
O(‚à•ùëû‚à•+‚à•ùë•‚à•) O(‚à•ùëû‚à•+‚à•ùë•‚à•)
Memory Friendly‚úì‚úó ‚úó
SIMD Support‚úì‚úó ‚úó
QPS (Recall@50=99%) 241 58 24
Construction Time(s) 63 220 4163
identifiers and their corresponding values in the inverted index, en-
abling direct access during query processing; (ii)Efficient Inner Prod-
uct Computation: SINDIeliminates redundant overhead in identify-
ing common dimensions and fully exploits SIMD acceleration. By
grouping values under the same dimension, SINDIenables batched
inner product computation during queries; (iii)Cache-Friendly De-
sign: SINDIreduces random memory accesses by avoiding fetches
of original vectors. Instead, it sequentially accesses inverted lists for
specific dimensions, thereby lowering cache miss rates. (iv)Vector
Mass Pruning: SINDIretains only high-value non-zero entries in
vectors, effectively reducing the search space and improving query
throughput while preserving accuracy.
We compare SINDIwith several state-of-the-art methods on the
MSMARCOdataset (8.8M scale) in Table 1. The time complexity
of computing the inner product between a query vector ùëûand a
document vector ùë•using SINDIisO(Ô∏Ç
‚à•ùëû‚à•
ùë†)Ô∏Ç
, where the involved
symbols are defined in Table 2. In contrast, inverted index and graph-
based algorithms have a time complexity of O(‚à•ùëû‚à•+‚à•ùë•‚à•) . The
detailed derivation is given in ¬ß 3.2.
In summary, the contributions of this paper are as follows:
‚Ä¢We present SINDI, a novel value-storing inverted index described
in ¬ß3.1 and ¬ß3.2, which reduces redundant distance computation
and random memory accesses. We further introduce aWindow
Switchstrategy in ¬ß3.3 to support large-scale datasets.
‚Ä¢We proposeVector Mass Pruningin ¬ß4 to decrease the search
space and improve query speed while maintaining accuracy.
‚Ä¢We evaluate SINDIon multi-scale, multilingual datasets in ¬ß5,
demonstrating 4√ó‚àº26√ó higher single-thread QPS than PYANNs
and SEISMICat over 99% Recall@50, and achieving 8.8M-scale
index construction in 60 seconds with minimal cost.
2 PRELIMINARIES
2.1 Problem Definition
Sparse vectors differ from dense vectors in that most of their dimen-
sions have zero values. By storing only the non-zero entries, they
significantly reduce storage and computation costs. We formalize
the definition as follows.
Definition 1(Sparse Vector and Non-zero Entries).Let D‚äÜRùëë
be a dataset of ùëë-dimensional sparse vectors. For any ‚Éóùë•‚ààD , let
ùë•denote its sparse representation, defined as the set of non-zero
entries:
ùë•={ùë•ùëó|ùë•ùëó‚â†0, ùëó‚àà[0,ùëë‚àí1]}.
Here,ùë•ùëódenotes the value of ‚Éóùë•in dimension ùëó. The notation‚à•ùë•‚à•
denotes the number of non-zero entries inùë•.
2

Table 2: Summary of Symbols
Symbol Description
Dbase dataset
ùëëdimension ofD
‚Éóùë•,‚Éóùëûbase vector, query vector
ùë•,‚à•ùë•‚à•sparse format of‚Éóùë•; number of non-zero entries inùë•
‚Éóùë•ùëñ,ùë•ùëñùëñ-th base vector and its sparse format
ùë•ùëó
ùëñvalue of‚Éóùë• ùëñin dimensionùëó
ùë†SIMD width (elements per SIMD operation)
ùúÜwindow size
ùúénumber of windows
ùõºbase vector pruning ratio
ùõΩquery vector pruning ratio
ùõæreorder pool size
ùêº, ùêºùëó inverted index; inverted list for dimensionùëó
ùêºùëó,ùë§ùë§-th window of inverted listùêº ùëó
ùëáùëó,ùëáùëó[ùë°]temporary product array on dimensionùëó; value at indexùë°
ùê¥,ùê¥[ùëö]distance array; value at indexùëö
Œ©(‚Éóùë• 1,‚Éóùë•2)set of common non-zero dimensions of‚Éóùë• 1and‚Éóùë• 2
ùõø(‚Éóùë• 1,‚Éóùë•2)inner product of‚Éóùë• 1and‚Éóùë• 2
To avoid confusion, we illustrate sparse vectors with an example
in Figure 1.
Example 2.Consider the document ‚ÄúI love black cats‚Äù encoded
into a sparse embedding: [I: 0.2,love: 0.4,black: 0.6,cats: 0.8] .
The corresponding sparse representation is ùë•={ùë•0=0.2,ùë•3=
0.4,ùë•9998=0.6,ùë•9999=0.8}, where‚à•ùë•‚à•=4.
Since the similarity measure in this work is based on the inner
product, we formally define its computation on sparse vectors as
follows.
Definition 2(Inner Product on Sparse Vectors).Let ‚Éóùë•1,‚Éóùë•2‚ààD,
and letùë•1andùë•2denote their sparse representations. Define the set
of common non-zero dimensions as
Œ©(‚Éóùë• 1,‚Éóùë•2)={ùëó|ùë•ùëó
1‚ààùë•1‚àßùë•ùëó
2‚ààùë•2}.
The inner product between‚Éóùë• 1and‚Éóùë• 2is then given by
ùõø(‚Éóùë• 1,‚Éóùë•2)=‚àëÔ∏Ç
ùëó‚ààŒ©(‚Éóùë• 1,‚Éóùë•2)ùë•ùëó
1¬∑ùë•ùëó
2.
Given the formal definition of the inner product for sparse vec-
tors, we now define the Sparse Maximum Inner Product Search
(Sparse-MIPS) task, which aims to find the vector in the dataset that
maximizes this similarity measure with the query.
Definition 3(Sparse Maximum Inner Product Search).Given a
sparse datasetD ‚äÜRùëëand a query point ‚Éóùëû‚ààRùëë, the Sparse
Maximum Inner Product Search (Sparse-MIPS) returns a vector
‚Éóùë•‚àó‚ààDthat has the maximum inner product with‚Éóùëû, i.e.,
‚Éóùë•‚àó=arg max
‚Éóùë•‚ààDùõø(‚Éóùë•,‚Éóùëû).(1)
For small datasets, exact Sparse-MIPS can be obtained by scan-
ning all vectors. For large-scale high-dimensional collections, this is
prohibitively expensive, and Approximate MIPS mitigates the cost
by trading a small loss in accuracy for much higher efficiency.
Definition 4(Approximate Sparse Maximum Inner Product Search).
Given a sparse dataset D‚äÜRùëë, a query point‚Éóùëû, and an approxima-
tion ratioùëê‚àà(0,1] , let‚Éóùë•‚àó‚ààD be the vector that has the maximum
1234x!x"x#x$x%x&x"‚Éóùë•!InvertedListGraph
Dataaccesstomemory(Maycausecachemiss).DistanceComputation(Highcomplexity).‚Éóùë•"‚Éóùë•#‚Éóùë•$‚Éóùë•%ÃÖùë•&‚Éóùë•#‚Éóùë•!‚Éóùë•&‚Éóùë•&‚Éóùë•"‚Éóùë•$‚Éóùë•"‚Éóùë•#‚Éóùë•#‚Éóùë•%‚Éóùëûùëû!ùëû"Figure 2: The bottleneck of the graph index and inverted index during
searching process.
inner product with ‚Éóùëû. Aùëê-maximum inner product search ( ùëê-MIPS)
returns a point‚Éóùë•‚ààDsatisfyingùõø(‚Éóùëû,‚Éóùë•) ‚â•ùëê¬∑ùõø(‚Éóùëû,‚Éóùë•‚àó).
In practice, ùëê-Sparse-MIPS methods can reduce query latency
by orders of magnitude compared with exact search, making them
preferable for large-scale, real-time applications such as web search,
recommender systems, and computational advertising.
For ease of reference, the main notations and their meanings are
summarized in Table 2, which will be referred to throughout the rest
of the paper.
2.2 Existing Solutions
Representative algorithms for the AMIPS problem on sparse vec-
tors include the inverted-index based SEISMIC[ 2], the graph based
PYANNs. SEISMICconstructs an inverted list based on vector di-
mensions. PYANNs creates a proximity graph where similar vectors
are connected as neighbors.
Example 3.Figure 2 illustrates a proximity graph and an in-
verted index constructed for ‚Éóùë•1to‚Éóùë•6. Consider a query vector ‚Éóùëûwith
two non-zero entries ùëû1andùëû2. In the proximity graph, when the
search reaches‚Éóùë•4, the algorithm computes distances between ‚Éóùëûand
all its neighbors, sequentially accessing ùë•1,ùë•2,ùë•4, andùë•6from mem-
ory. In the inverted index, the algorithm traverses the posting lists
for dimensions 1and2, accessingùë•1,ùë•6,ùë•2, andùë•4. Since vector
access during search is essentially random, this incurs substantial
random memory access overhead. Moreover, because ‚à•ùë•‚à•varies
across vectors, the distance computation between ‚Éóùëûand‚Éóùë•has a time
complexity ofO(‚à•ùëû‚à•+‚à•ùë•‚à•)
Redundant Distance Computations .Sparse vectors incur high dis-
tance computation cost due to (i) the explicit lookup needed to
identify the common dimensions Œ©(‚Éóùë•,‚Éóùëû) between a document ‚Éóùë•and
a query‚Éóùëû, resulting in complexity O(‚à•ùëû‚à•+‚à•ùë•‚à•) , and (ii) the inability
of existing algorithms to exploit SIMD acceleration for inner product
computation. Profiling 6980 queries on the MSMARCOdataset (1M
vectors) usingPERFandVTUNEshows that PYANNs spent 83.3%
of CPU cycles on distance calculation.
Random Memory Accesses. The inefficiency of memory access in
existing algorithms can be attributed to two main factors. First,
they organize similar data points into the same partition and. To
improve accuracy, vectors are replicated across multiple partitions.
3

This replication breaks the alignment between storage layout and
query traversal order, preventing cache-friendly sequential access.
During retrieval, the index returns candidate vector IDs, which incur
random memory accesses to fetch their corresponding data, leading
to frequent cache misses. Moreover, ‚à•ùë•‚à•varies across sparse vectors,
requiring offset table lookups to locate each vector‚Äôs data. In our
measurements, SEISMICaveraged 5168 random vector accesses per
query (5.1 MB), with an L3 cache miss rate of 67.68%.
3 FULL PRECISION INVERTED INDEX
This section introduces full-precision SINDI, an inverted index de-
signed for sparse vector retrieval. Its advantages are organized along
three aspects: index structure, distance computation, and cache opti-
mization.
‚Ä¢In ¬ß 3.1 SINDIconstructs a value-based inverted index by storing
both vector identifiers and their corresponding dimension values
in posting lists. This design eliminates the redundant dimension-
matching overhead present in traditional inverted indexes.
‚Ä¢In ¬ß 3.2, SINDIemploys a two-phase search process involv-
ingproduct computationandaccumulation. By using SIMD in-
structions in multiplication, it reduces query complexity from
ùëÇ(‚à•ùëû‚à•+‚à•ùë•‚à•) toùëÇ(Ô∏Ç
‚à•ùëû‚à•
ùë†)Ô∏Ç
. This maximizes CPU utilization and
improves query throughput.
‚Ä¢In ¬ß 3.3, to mitigate cache misses caused by random access to the
distance array, SINDIintroducesWindow Switchstrategy, which
partitions each posting list into fixed-size segments of length ùúÜ
while using a shared distance array. This reduces memory over-
head without increasing computation, and both theoretical analy-
sis and experimental evaluation (Figure 5) demonstrate the exis-
tence of an optimalùúÜthat minimizes memory access overhead.
3.1 Value-storing Inverted Index
Redundant inner product computations arise because identifying
the common non-zero dimensions Œ©(‚Éóùëû,‚Éóùë•) between a query vector ‚Éóùëû
and a document vector ‚Éóùë•requires scanning many irrelevant entries
outside their intersection. We observe that the document identifiers
retrieved from traversing an inverted list correspond precisely to
the dimensions in Œ©(‚Éóùë•,‚Éóùëû) . Therefore, when accessing a document
‚Éóùë•from the list of dimension ùëó, we can simultaneously retrieve its
valueùë•ùëó, thereby enabling direct computation of the inner product
without incurring the overhead of findingŒ©(‚Éóùëû,‚Éóùë•).
Example 4.Figure 3 shows the inverted lists constructed for
vectorsùë•1toùë•5, comprising five term lists. When a query ùëûarrives, it
sequentially probes the inverted lists for dimensions 1, 3, and 5. The
right side illustrates the common non-zero dimensions found when
computing the inner product between the query and the documents.
For example, in dimension 1, the inverted list retrieves ùë•2andùë•4,
and the inner-product computation also multiplies ùëûwithùë•1
2andùë•1
4.
Therefore, the document IDs retrieved from the inverted lists overlap
exactly with those used in finding common non-zero dimensions for
the inner product. This indicates that we can compute the products
of these non-zero entries during document retrieval itself.
Inspired by this observation, we extend the inverted list to store
not only the ID ùëñof each vector‚Éóùë•ùëñ, but also its value ùë•ùëófor the corre-
sponding dimension ùëó. This design eliminates the cost of searching
123450.30.80.60.30.40.50.90.30.50.40.20.10.7‚ÉóùëûInvertedListInnerProductDimensionDocIDSparseVectorSearchedDocorCommonEntry‚Éóùë•!‚Éóùë•!‚Éóùë•"‚Éóùë•"‚Éóùë•#‚Éóùë•"‚Éóùë•$‚Éóùë•$‚Éóùë•$‚Éóùë•$‚Éóùë•#‚Éóùë•!‚Éóùë•"‚Éóùë•$Figure 3: Overlap of Inverted List Entries and Common Non-Zero
Dimensions in Inner-Product Computation.
Œ©(‚Éóùë•ùëñ,‚Éóùëû)during the inner product computation, as well as the random
memory access overhead for retrievingùë• ùëñ.
3.2 Efficient Distance Computation
The entire query process of SINDIcan be summarized into two
stages: (1)Product Computation.Given an incoming query vector ‚Éóùëû,
for each non-zero ùëûùëówe fetch the ùëó-th inverted list ùêºùëó, compute the
productsùëûùëó√óùë•ùëó
ùëñfor allùë•ùëó
ùëñ‚ààùêºùëó, and temporarily store the results into
the arrayùëáùëó. (2)Accumulation.The values in ùëáùëóare accumulated
into the distance array ùê¥. The length of ùê¥is set to‚à•D‚à• , so that each
entryùê¥[ùë°] corresponds uniquely to a vector ‚Éóùë•ùë°‚ààD, allowing the
accumulation from ùëáùëótoùê¥to be completed inO(1) time per element.
After completing the accumulation for all ùëáùëówithùëûùëó‚ààùëû, eachùê¥[ùë°]
containsùê¥[ùë°]=ùõø(‚Éóùë• ùë°,‚Éóùëû). Ifùê¥[ùë°]=0, it means thatŒ©(‚Éóùë• ùë°,‚Éóùëû)=‚àÖ.
Example 5.Figure 4 illustrates a query example. Since ‚à•D‚à•=9 ,
the distance array ùê¥is initialized with size(ùê¥)=9 and all elements
set to 0. The query‚Éóùëûcontains three non-zero components ùëû1,ùëû5, and
ùëû8, thus only the inverted lists ùêº1,ùêº5, andùêº8need to be traversed. Take
‚Éóùë•4as an example: in ùêº1, the value is ùë•1
4=6.8 , and the product with
ùëû1is17.0, which is temporarily stored in ùëá[0] . Accumulating to ùê¥[4]
givesùê¥[4]=0+17.0=17.0 . Similarly, in ùêº5we haveùë•5
4√óùëû5=14.0 ,
which is stored in ùëá[2] ; addingùëá[2] toùê¥[4] yieldsùê¥[4]=31.0 .
The computation for ùêº8is analogous. Finally, we obtain ùê¥[4]=
36.1, which equals ùõø(‚Éóùë• 4,‚Éóùëû). Although‚Éóùë•4has another non-zero entry
ùë•2
4, it does not contribute to the inner product and thus can be
ignored. The same accumulation procedure applies to ‚Éóùë•2,‚Éóùë•3,‚Éóùë•6, and
‚Éóùë•7. Eventually, ùê¥[4] has the highest value, so the nearest neighbor
of‚Éóùëûis‚Éóùë• 4.
Using SIMD instructions, SINDIbatch-processes the ùëó-th inverted
listùêºùëó, multiplying ùëûùëówith eachùë•ùëó
ùëñit contains and writing the re-
sults sequentially into ùëáùëó. This not only utilizes CPU resources
efficiently, but also reduces the time complexity of the inner product
computation fromO(‚à•ùë£‚à•+‚à•ùëû‚à•) toO(Ô∏Ç
‚à•ùëû‚à•
ùë†)Ô∏Ç
, whereùë†is the number
of elements processed per SIMD operation. The derivation of the
distance computation complexity for SINDIis as follows:
Theorem 3.1 (Amortized Time Complexity of SINDIDistance
Computation).Let Dbe the dataset, and let ùêºdenote an inverted
4

‚Ä¶‚Ä¶InvertedIndexDistanceArray
01234567816.8ùë•!"6.4ùë•#"4.0ùë•$"2.4ùë•%"55.8ùë•%&4.2ùë•$&4.0ùë•!&3.8ùë•'&86.7ùë•'(5.1ùë•!(2.6ùë•#(0.8ùë•$(2.5ùëû"3.5ùëû&1.0ùëû(0010.0017.0016.06.00000000000
0024.7031.0016.026.30ùë®ùüíisthemaximumTop1isùíôùüí0025.520.036.1018.626.30‚Ä¶‚Ä¶SIMDùë®‚Ä¶‚Ä¶ùëá"ùë®ùë®ùë®ùëá#ùëá$17.016.010.06.020.314.714.013.36.75.12.60.8Figure 4: An example of SINDIindex and query process.
index, andùêºùëóis theùëó-th inverted list, let the set Iùëó={ùë•ùëó
ùëñ|ùë•ùëó
ùëñ‚ààùë•ùëñ}
contains the non-zero entries inùêº ùëó.
Given a query vector ‚Éóùëû, letJ={ùëó|ùëûùëó‚ààùëû} denote the set of
dimensions containing non-zero entries in ùëû. LetX={‚Éóùë•ùëñ|ùë•ùëó
ùëñ‚àà
ùë•ùëñ,ùëó‚ààJ}be the set of candidate vectors retrieved by‚Éóùëû.
Letùë†be the number of dimensions that can be processed si-
multaneously using SIMD instructions. Then, the per-vector time
complexity of computing the inner product between ‚Éóùëûand all‚Éóùë•ùëñ‚ààX
is
ùõ©(Ô∏É‚à•ùëû‚à•
ùë†)Ô∏É
.
Proof.The total number of non-zero entries accessed across all
inverted lists corresponding toJis:
‚àëÔ∏Ç
ùëó‚ààJ‚à•Iùëó‚à•.
Sinceùë†dimensions can be processed in parallel using SIMD, the
total time complexity for computing inner products between ‚Éóùëûand
all‚Éóùë•ùëñ‚ààXis:
ùëátotal=‚àëÔ∏Ç
ùëó‚ààJ‚à•Iùëó‚à•
ùë†.
The amortized complexity per vector is obtained by dividing ùëátotal
by the number of candidates‚à•X‚à•:
ùëá=ùëátotal
‚à•X‚à•=‚àëÔ∏Å
ùëó‚ààJ‚à•Iùëó‚à•
ùë†¬∑‚à•X‚à•.
In general, for any =‚Éóùë•ùëñ‚ààX, we haveùëû‚à©ùë•ùëñ‚äÜùëû, implying that
Œ©(=‚Éóùë•ùëñ,=‚Éóùëû)is at most‚à•ùëû‚à•. Therefore:
‚àëÔ∏Ç
ùëó‚ààJ‚à•Iùëó‚à•‚â§‚àëÔ∏Ç
‚Éóùë•ùëñ‚ààX‚à•ùëû‚à•.
Substituting this relation into the expression forùëáyields:
ùëá‚â§‚àëÔ∏Å
‚Éóùë•ùëñ‚ààX‚à•ùëû‚à•
ùë†¬∑‚à•X‚à•=‚à•ùëû‚à•¬∑‚à•X‚à•
ùë†¬∑‚à•X‚à•=‚à•ùëû‚à•
ùë†.
Hence, the per-vector time complexity is:
ùõ©(Ô∏É‚à•ùëû‚à•
ùë†)Ô∏É
.
‚ñ°Algorithm 1:PreciseSINDIConstruction
Input:A sparse datasetDand dimensionùëë, window sizeùúÜ
Output:Inverted listùêº
1forùëó‚àà{0,...,ùëë‚àí1}do
2X‚Üê{‚Éóùë• ùëñ|ùë•ùëó
ùëñ‚ààùë•ùëñ,‚Éóùë•ùëñ‚ààD};
3foreach‚Éóùë• ùëñ‚ààXdo
4ùë§‚Üê‚åäùëñ
ùúÜ‚åã
5ùêº ùëó,ùë§.ùëéùëùùëùùëíùëõùëë(ùë•ùëó
ùëñ)
6returnùêº
In summary, the SINDIindex eliminates the overhead of redun-
dant term searches and enables SIMD to compute inner products in
batches, fully leveraging the CPU‚Äôs computational power. Moreover,
it avoids the memory access costs associated with traversing the orig-
inal vectors. Instead, the required values can be directly retrieved
from the lists, allowing in-place product computation.
3.3 Cache Optimization
When the dataset size Dreaches the million scale, the distance array
becomes very large. Random access to such a long array leads to
frequent cache misses, causing the query performance to be highly
memory-bound. Moreover, maintaining a distance array of size D
for every query consumes significant memory resources. To limit
the length of the distance array, SINDIemploys theWindow Switch
strategy that restricts the range of vector IDs accessed in a single
window. Within each window, the reduced size of the distance array
makes the access pattern nearly sequential.
3.3.1 Window Switch.During index construction, SINDIpar-
titions the datasetDinto contiguous ID segments, referred to as
windows. The window size is denoted by ùúÜ(0<ùúÜ‚â§‚à•D‚à• ), and
the total number of windows ùúéis‚åàÔ∏Ç
‚à•D‚à•
ùúÜ‚åâÔ∏Ç
. Theùë§-th window contains
vectors from‚Éóùë•ùë§ùúÜto‚Éóùë•(ùë§+1)ùúÜ‚àí1 , and the window index to which vec-
tor‚Éóùë•ùëñbelongs is‚åäÔ∏Åùëñ
ùúÜ‚åãÔ∏Å
. Each inverted list is partitioned in the same
way, so every list has ùúéwindows. We denote the ùë§-th window of the
ùëó-th inverted list byùêº ùëó,ùë§, with0‚â§ùë§<ùúé.
At query time, the length of the distance array ùê¥is set to the
window size ùúÜ, and all windows share the same ùê¥. During a window
search, each vector ‚Éóùë•ùëñis mapped to a unique entry ùê¥[ùëñmodùúÜ] . The
search procedure in the ùë§-th window consists of two steps: (1)
Inner product computation. For each scanned list, compute inner
products for vectors ‚Éóùë•ùë§ùúÜto‚Éóùë•(ùë§+1)ùúÜ‚àí1 . The computation follows the
same two-stage process as described in Section 3.2, namely, product
computation followed by accumulation. (2)Heap update.After
computing the inner product for the current window, the distance
arrayùê¥contains the final distance. Scan ùê¥to insert the top candidates
(with the largest inner products) into a minimum heap ùêª, which
maintains the vector IDs and distances of the results to be returned.
Note that we need to recover the vector ID from ùê¥‚Äôs index,ùê¥[ùë°]
corresponds to‚Éóùë• ùë°+ùúÜ√óùë§ .
Clearly,Window Switchchanges only the order of list entries
scanned and does not affect the total number of computations. There-
fore, the time complexity of distance computation remains O(Ô∏Ç
‚à•ùëû‚à•
ùë†)Ô∏Ç
.
5

3.3.2 Construction and Search.The construction process of the
full-precision SINDIindex withWindow Switchis detailed in Algo-
rithm 1. Given a sparse vector dataset Dwith maximum dimension
ùëë. For each dimension ùëó(Line 1), all vectors ‚Éóùë•ùëñinDwithùë•ùëó
ùëñinùë•ùëñ
are collected into a temporary set X(Line 2). These vectors are then
appended into the corresponding windows of the inverted list ùëóbased
on their IDs (Lines 3-5). Finally, the constructed index ùêºis returned
after processing all dimensions (Line 6). The time complexity of
the construction process is ùëÇ(‚à•D‚à•‚à• ¬Øùë•‚à•), where‚à•¬Øùë•‚à•represents the
average number of non-zero entries, that is‚àëÔ∏Å
‚Éóùë•ùëñ‚ààD‚à•ùë•ùëñ‚à•
‚à•D‚à•.
The search process for the full-precision SINDIindex is sum-
marized in Algorithm 2, consisting of three main stages: product
computation, accumulation, and heap update. Given a query ‚Éóùëû, an
inverted index ùêº, and the recall number of nearest neighbors ùëò, the
algorithm initializes a distance array ùê¥with length equal to ùúÜ, setting
all elements to zero (Lines 1‚Äì2), and creates an empty min-heap ùêª
(Line 3). The outer loop iterates over all windows ùë§‚àà{0,...,ùúé‚àí1}
(Line 4). For each non-zero query component ùëûùëó‚àà‚Éóùëû(Line 5), the
algorithm performs SIMD-based batched multiplication between
ùëûùëóand all components ùë•ùëó
ùëñcontained in ùêºùëó,ùë§, storing the results se-
quentially into the temporary product array ùëáùëó(Line 6). Next, it
retrieves each ùë•ùëó
ùëñinùêºùëó,ùë§(Lines 7-8), computes the mapped index
ùëöofùê¥(Line 9), and accumulates ùëáùëó[ùë°]intoùê¥[ùëö] (Line 10). After
processing all ùëûùëó‚ààùëûfor the current window, the algorithm proceeds
to the heap update stage (Line 12). For each entry in ùê¥, ifùê¥[ùëö]
is greater than the current minimum in ùêªor if the heap contains
fewer thanùëòelements, the algorithm inserts the corresponding global
vector ID(ùëö+ùúÜ√óùë§) along with its distance ùê¥[ùëö] into the heap
(Lines 13‚Äì14). If the heap exceeds size ùëò, the smallest element is
removed (Lines 15‚Äì16). Finally, ùê¥[ùëö] is reset to zero to prepare
for the next window (Line 17). After all windows have been pro-
cessed, the heap ùêªcontains at most ùëòvector IDs paired with their
full-precision distances to the query, which is returned as the final
result (Line 20).
Complexity.Assuming ùëôis the average of non-zero entries in the
traversed list, that is ùëô=‚àëÔ∏Å
ùëûùëó‚ààùëûùêºùëó.ùë†ùëñùëßùëí()
‚à•ùëû‚à•. Even withWindow Switch,
the total number of non-zero entries traversed remains constant,
which is‚à•ùëû‚à•ùëô . Therefore, the time complexity of a full-precision
query isO(‚à•ùëû‚à•ùëô
ùë†). In conclusion, the computational cost of querying
is independent ofùúÜ.
3.4 Analysis of Window Size‚Äôs Impact on
Performance
While theWindow Switchstrategy does not change the overall com-
putational complexity, it has a significant impact on memory access
costs. When ùúÜdecreases, the distance array becomes shorter, leading
to a lower cache-miss rate during random writes. However, the num-
ber of windows ùúé=‚à•D‚à•
ùúÜincreases, causing more frequent random
accesses when switching between inverted sub-lists. Therefore, an
appropriate choice of ùúÜis required to balance these two effects. The
following example can illustrate this:
Example 6.Figure 5 reports experimental results for the full-
precisionSINDIon the SPLADE-1M and SPLADE-FULL datasets.
For each dataset, we executed 6,980 queries under different window
sizesùúÜand measured the QPS. In addition, we used the Intel VTuneAlgorithm 2:PreciseSINDISearch
Input:Query‚Éóùëû, an inverted listùêº, andùëò
Output:At mostùëòpoints inD
1forùëö‚àà{0,...,ùúÜ‚àí1}do
2ùê¥[ùëö]‚Üê0
3Initializeùêªis an emptyùëöùëñùëõùëñùëöùë¢ùëö‚Ñéùëíùëéùëù;
4forùë§‚àà{0,...,ùúé‚àí1}do
5foreachùëûùëó‚ààùëûdo
6ùëáùëó‚ÜêSIMDProduct(ùëûùëó, ùêºùëó,ùë§);
7forùë°‚àà{0,...,ùêº ùëó,ùë§.ùë†ùëñùëßùëí()‚àí1}do
8ùë•ùëó
ùëñ‚Üêùêºùëó,ùë§[ùë°];
9ùëö‚ÜêùëñmodùúÜ;
10ùê¥[ùëö]‚Üêùê¥[ùëö]+ùëáùëó[ùë°];
11forùëö‚àà{0,...,ùúÜ‚àí1}do
12ifùê¥[ùëö]>ùêª.ùëöùëñùëõ()orùêª.ùëôùëíùëõ()<ùëòthen
13ùêª.ùëñùëõùë†ùëíùëüùë°(ùëö+ùúÜ√óùë§,ùê¥[ùëö])
14ifùêª.ùëôùëíùëõ()=ùëò+1then
15ùêª.ùëùùëúùëù()
16ùê¥[ùëö]‚Üê0
17returnùêª
Profiler to record memory bound metrics for two types of memory
accesses: distance array updates and sub-list switches. Here, mem-
ory bound refers to the percentage of execution time stalled due to
memory accesses. For the SPLADE-1M dataset, as ùúÜincreases from
1K to 1M, the distance array miss rate decreases monotonically,
while the sub-list switching miss rate increases monotonically. The
total memory-bound latency reaches its minimum near ùúÜ‚âà100 K,
corresponding to the highest query throughput. The SPLADE-FULL
dataset exhibits the same trend, confirming the existence of an opti-
mal window size.
Based on this, the memory access latency for queries can be
expressed in a double power-law form [1, 8] as follows:
ùëámem(ùúÜ)=ùê¥ùúÜ+ùõº+ùêµùúÜ‚àíùõΩ+ùê∂,(2)
where:
‚Ä¢The independent variable is the window sizeùúÜ;
‚Ä¢The termùê¥ùúÜ+ùõºcaptures the increasing cost of distance array cache
misses asùúÜgrows;
‚Ä¢The termùêµùúÜ‚àíùõΩreflects the decreasing cost of sub-list switching
misses with largerùúÜ,
‚Ä¢The constant ùê∂represents baseline memory access costs unrelated
toùúÜ.
According to the properties of the double power-law function,
ùëáùëöùëíùëö(ùúÜ)reaches its minimum at ùúÜ‚àó=(Ô∏Ç
ùêµùõΩ
ùê¥ùõº)Ô∏Ç1
ùõº+ùõΩ. For smallùúÜ‚â™ùúÜ‚àó,
ùëámemis dominated by the sub-list switching term and decreases as
ùúÜgrows. For large ùúÜ‚â´ùúÜ‚àó, the distance-array term dominates and
ùëámemincreases with ùúÜ. The optimum ùúÜ=ùúÜ‚àóoccurs when the two
terms balance.
Example 7.Figure 5 shows that the dashed line represents the
theoretical QPS curve. It‚Äôs derived by estimating (ùõº,ùõΩ) via log‚Äìlog
6

List Access Memory Bound
Distance Array Memory BoundMeasured QPS
Theoretical QPS
1K 3K 10K 20K 100K 200K 500K 1M
Window Size (Œª)0204060Memory Bound (%)98%2%
97%3%
95%5%
90%10%
88%12%
46%54%
41%59%
33%67%
050100150200250
QPS
(a)SPLADE-1M
1K 5K10K 100K 300K 1M 4M 8M
Window Size (Œª)0204060Memory Bound (%)92%8%
91%9%
87%13%
85%15%
78%22%
57%43%
46%54%
16%84%
01020
QPS
 (b)SPLADE-FULL
Figure 5: Impact of Window Size on Query Throughput and Memory
Accesses.
regression and(ùê¥,ùêµ,ùê∂) via least-squares fitting of the double power-
law model. For the SPLADE-1M dataset, the model predicts an
optimalùúÜ‚àó‚âà7.35√ó104, while for SPLADE-FULL it predicts
ùúÜ‚àó‚âà1.25√ó105. These predictions are of the same order as the
measured optimal ùúÜ‚âà105, with small deviations attributable to
model abstraction and measurement variability. This agreement
in both magnitude and trend supports the validity of our scaling
analysis.
4 APPROXIMATE INVERTED INDEX
We focus on optimizing the query process through pruning and
re-ranking.Pruningreduces the size of vectors or lists to improve
search efficiency, whilere-rankingcompensates for pruning-induced
precision loss by computing full inner products for a select set of can-
didates. Together, these techniques achieve a significant performance
boost with only a minimal sacrifice in accuracy.
4.1 Pruning Strategies
A notable advantage of sparse vectors is that a small number of
high-valued non-zero entries can represent most of the information
in the entire vector [ 6]. This property arises from the training mecha-
nisms of sparse models. For example, SPLADEtends to concentrate
important information in a few non-zero dimensions. From a se-
mantic perspective, many low-valued non-zero entries correspond to
stopwords (e.g., ‚Äúis‚Äù, ‚Äúthe‚Äù), which can be pruned.
Let‚Éóùë•‚Ä≤
ùëñdenote the pruned version of document ‚Éóùë•ùëñ, and‚Éóùëû‚Ä≤the
pruned version of query ‚Éóùëû,ùëôis the average length of list before
pruning,ùëô‚Ä≤is the average length of list after pruning. The reduction
in computational cost achieved by pruning is
‚à•ùëû‚à•ùëô‚àí‚à•ùëû‚Ä≤‚à•ùëô‚Ä≤
while theinner product errorùúÄintroduced is
ùúÄ=‚àëÔ∏Ç
‚Éóùë•ùëñ‚ààD(Ô∏Ç
ùõø(‚Éóùë•ùëñ,‚Éóùëû)‚àíùõø(‚Éóùë•‚Ä≤
ùëñ,‚Éóùëû‚Ä≤))Ô∏Ç
.
Smaller‚à•ùë•‚Ä≤
ùëñ‚à•leads to higher throughput gains, but also larger
inner product error. Therefore, pruning must be designed with a
trade-off between efficiency and accuracy. The following example
shows that it is possible to retain only part of the non-zero entries
while incurring only a small loss in inner product accuracy.
Example 8.We prune each document vector ‚Éóùë•ùëñand the query
vector‚Éóùëûby retaining only the non-zero entries with the largest abso-
lute values, producing ‚Éóùë•‚Ä≤
ùëñand‚Éóùëû‚Ä≤. We vary the pruning ratio, defined
0.2 0.4 0.6 0.8 1.0
Doc Cut Ratio0.20.40.60.81.0Query Cut Ratio
0123456
Error(a)Inner Product Error
0.1 0.15 0.2
Query Cut Ratio0.50.60.70.80.91.0RecallDCR(0.2)-10@500
DCR(0.2)-10@10DCR(0.4)-10@500
DCR(0.4)-10@10 (b)Recall Comparision
Figure 6: Intuition of Pruning and Reorder
as‚à•ùë•‚Ä≤
ùëñ‚à•
‚à•ùë•ùëñ‚à•, to control the proportion of entries preserved. Figure 6(a)
shows the corresponding inner product error under different pruning
ratios. The results indicate that the error decreases sharply as the
ratio increases from 0.1to0.3, and becomes nearly zero when the
ratio is between 0.5and1, revealing a saturation effect of pruning
ratio on inner product error.
As discussed in Section 3, for full-precision SINDIthe upper
bound for computing the inner product between ‚Éóùë•ùëñand‚Éóùëûisùõ©(Ô∏Ç
‚à•ùëû‚à•
ùë†)Ô∏Ç
.
For a given‚Éóùëû, the overall time complexity of the query process is
O(Ô∏Ç
‚à•ùëû‚à•ùëô
ùë†)Ô∏Ç
, whereùëôis the average number of inverted lists traversed.
Reducing query latency therefore amounts to reducing ùëô,‚à•ùë•‚à•, and
‚à•ùëû‚à•. This can be approached from three directions: pruning lists,
pruning documents, and pruning queries. List pruning and docu-
ment pruning are applied during the index-construction stage, while
query pruning is applied at query time. In this work, we mainly
focus on the construction stage, as query pruning and document
pruning are essentially both forms of vector pruning. We compare
three pruning strategies‚Äîlist pruning, document-count pruning, and
quality-ratio pruning‚Äîand analyze their respective advantages and
disadvantages:
List Pruning (LP).LP operates at the inverted-list level: for each
dimensionùëó, only the non-zero entries with the largest absolute
values are retained in its inverted list ùêºùëó, restricting the list length to
ùëô‚Ä≤. Since the size of ùêºùëóvaries across dimensions, highly valued |ùë•ùëó
ùëñ|
entries in longer lists may be pruned, while lower-valued non-zero
entries in shorter lists are retained. After this list-wise pruning, each
document vector‚Éóùë•ùëñis transformed into a pruned version ùúôLP(‚Éóùë•ùëñ)
containing only those coordinates that survive the list truncation.
Vector Number Pruning (VNP).VNP reduces dimensionality at the
vector level. Its core idea is to retain, for each vector ‚Éóùë•ùëñ, theùë£ùëõnon-
zero entries with the largest magnitudes, so that ‚à•ùë•‚Ä≤
ùëñ‚à•=ùë£ùëõ . However,
since‚à•ùë•ùëñ‚à•varies across vectors, this scheme cannot consistently
preserve the non-zero entries that contribute the most to the inner
product computation.
Vector Number Pruning (VNP).VNP applies the pruning operator
ùúôVNPat the vector level. For each document vector ‚Éóùë•ùëñ,ùúôVNP(‚Éóùë•ùëñ)re-
tains only the ùë£ùëõnon-zero entries with the largest absolute values, so
that‚à•ùúôVNP(‚Éóùë•ùëñ)‚à•=ùë£ùëõ . Since‚à•‚Éóùë•ùëñ‚à•varies across vectors, this scheme
cannot consistently preserve the non-zero entries that contribute the
most to the inner product computation.
Mass Ratio Pruning (MRP).MRP applies the pruning operator
ùúôMRPbased on the cumulative sum of absolute values of a vector‚Äôs
7

LP(ùíç‚Ä≤=ùüê)VNP(ùíóùíè=ùüê)MRP(ùú∂=ùüé.ùüï)0.85ùë•!!0.65ùë•"!0.59ùë•"#0.36ùë•##0.580.050.39ùë•#!0.06ùë•!#0.04ùë•!"ùë•""ùë•#"term1term2term3ReservedentryPrunedentry0.85ùë•!!ùíôùüèùíôùüêùíôùüë0.06ùë•!#0.04ùë•!"0.39ùë•#!0.36ùë•##0.05ùë•#"0.65ùë•"!0.59ùë•"#0.58ùë•""0.85ùë•!!ùíôùüèùíôùüêùíôùüë0.06ùë•!#0.04ùë•!"0.39ùë•#!0.36ùë•##0.05ùë•#"0.65ùë•"!0.59ùë•"#0.58ùë•""ùëû=[1,0.5,2,0.3,(3,0.7)]
ùú∫(ùë≥ùë∑)=ùüé.ùüêùüíùüèReducethesamecomputation,MassRatioPruningachievesthesmallestinnerproducterror.ùú∫(ùëΩùëµùë∑)=ùüé.ùüíùüîùüóùú∫(ùë¥ùëπùë∑)=ùüé.ùüéùüñùüèFigure 7: An example ofList Pruning,Vector Number PruningandMass
Ratio Pruning.
non-zero entries. For each document vector ‚Éóùë•ùëñ,ùúôMRP(‚Éóùë•ùëñ)ranks all
non-zero entries in descending order of absolute value and retains
the smallest prefix whose cumulative sum reaches a target fraction ùõº
of the vector‚Äôs total mass. This adaptive scheme discards low-value
components that contribute little to the inner product while allowing
vectors with different value-distribution to keep variable numbers
of entries, reducing inverted list size without imposing a uniform
length limit.
To formally introduce MRP, we first define the mass of a vector
as the sum of the absolute values of its non-zero entries.
Definition 5(Mass of a Vector).Let ‚Éóùë•‚ààRùëëbe a vector. Themass
of‚Éóùë•is defined as the sum of the absolute values of ùë•‚Äôs non-zero
entries:
ùúâ(‚Éóùë•)=‚àëÔ∏Ç
ùë•ùëó‚ààùë•|ùë•ùëó|.
Then we define the ùõº-mass subvector as the shortest prefix of
sorted non-zero entries whose cumulative absolute value reaches a
fractionùõºof the total mass.
Definition 6( ùõº-Mass Subvector).Consider a vector ‚Éóùë•‚ààRùëëand a
permutation ùúãthat orders the non-zero entries of ‚Éóùë•by non-increasing
absolute value, i.e. |ùë•ùúãùëó|‚â•|ùë•ùúãùëó+1|. For a constant ùõº‚àà(0,1] , let
1‚â§ùëü‚â§‚à•ùë•‚à•be the smallest integer satisfying
ùëü‚àëÔ∏Ç
ùëó=1|ùë•ùúãùëó| ‚â§ùõºùúâ(‚Éóùë•).
The collection ùë•‚Ä≤_ùõº={Ô∏Å
ùë•ùúãùëó}Ô∏Åùëü
ùëó=1is the sparse representation of a
vector‚Éóùë•_ùõº, where‚Éóùë•_ùõº]is theùõº-mass subvector of‚Éóùë•.
Example 9.Figure 7 illustrates three pruning methods applied
to sparse vectors‚Éóùë•1,‚Éóùë•2, and‚Éóùë•3. List Pruning prunes each list to
sizeùëô‚Ä≤=2, Vector Number Pruning retains ùë£ùëõ=2 top entries of
each vector, and Mass Ratio Pruning prunes each vector ¬Øùë•ùëñto¬Øùë•ùëñ,ùõº
withùõº=0.7 . The pruning result is shown in the figure: (i) the three
strategies all reduce the same computation, which is ‚à•ùëû‚à•ùëô‚àí‚à•ùëû‚Ä≤‚à•ùëô‚Ä≤=
9‚àí6=3 ; (ii) Mass Ratio Pruning‚Äôs inner product error is the
smallest. The reason is that List Pruning fails to retain the larger
valueùë•1
2, as each list is limited to a maximum of 2 vectors. Similarly,
Vector Number Pruning does not retain ùë•3
3. In contrast, Mass Ratio
Pruning minimizes error by prioritizing influential entries.
Algorithm 3 outlines the process for constructing the approximate
version of the SINDIindex. Given a sparse vector dataset D, maxi-
mum dimension ùëë, window size ùúÜ, and pruning ratio ùõº, the algorithm
begins by initializing an empty set D‚Ä≤to store pruned vectors (LineAlgorithm 3:APPROXIMATESINDICONSTRUCTION
Input:Sparse datasetDof dimensionùëë; window sizeùúÜ;
pruning ratioùõº
Output:Inverted indexùêº
1D‚Ä≤‚Üê‚àÖ
2foreach‚Éóùë• ùëñ‚ààDdo
3‚Éóùë•ùëñ_ùõº‚Üêùõº-mass vector of‚Éóùë• ùëñ
4D‚Ä≤‚ÜêD‚Ä≤‚à™‚Éóùë•ùëñ_ùõº
5ùêº=PRECISESINDICONSTRUCTION(D‚Ä≤,ùëë,ùúÜ)
6returnùêºandD
Algorithm 4:APPROXIMATESINDISEARCH
Input:Query‚Éóùëû, an inverted indexùêº, query prune ratioùõΩ,
reorder numberùõæ, andùëò
Output:At mostùëòpoints inD
1ùêªis an emptyùëöùëñùëõùëñùëöùë¢ùëö‚Ñéùëíùëéùëù;
2ùëÖis an emptyùëöùëñùëõùëñùëöùë¢ùëö‚Ñéùëíùëéùëù;
3‚Éóùëû_ùõº=ùõº-mass vector of‚Éóùëû
4ùêª=PRECISESINDISEARCH(‚Éóùëû_ùõº,ùêº,ùõæ)
5while!ùêª.ùëíùëöùëùùë°ùë¶()do
6ùëñ,ùëëùëñùë†‚Üêùêª.ùëùùëúùëù();
7ùëëùëñùë†‚Ä≤‚Üêùõø(‚Éóùë•ùëñ,‚Éóùëû);
8ifùëëùëñùë†‚Ä≤>ùëÖ.ùëöùëñùëõ()orùëÖ.ùëôùëíùëõ()<ùëòthen
9ùëÖ.ùëñùëõùë†ùëíùëüùë°(ùëñ,ùëëùëñùë†‚Ä≤)
10ifùëÖ.ùëôùëíùëõ()=ùëò+1then
11ùëÖ.ùëùùëúùëù()
12returnùëÖ
1). For each vector ùë•ùëñinD(Line 2), its ùõº-mass subvector is derived
and assigned to‚Éóùë•‚Ä≤
ùëñ_ùõº(Line 3). The remaining steps are identical to
the full-precision index construction process, with the exception that
the approximate index stores the original dataset to enable reordering
during retrieval.
Figure 6(a) shows that retaining less than half of the non-zero
entries in a query can reduce the inner product error to nearly zero.
This significantly reduces the search space, shrinking it by more
than half. It indicates that the term lists corresponding to a few high-
value non-zero entries in the query already cover most recall points.
Therefore, SINDIappliesMass Vector Pruningto queries as well.
4.2 Reordering
Retaining a small portion of non-zero entries can preserve most
of the inner product but fails to maintain the partial order of the
full inner product. Using pruned results directly for recall reduces
accuracy. However, experiments show that with enough candidates,
the nearest neighbors are likely to be included. Figure 6(b) shows
Recall 10@500 and Recall 10@10 under different pruning ratios for
documents and queries. Retaining 16% of document entries and 30%
of query entries achieves Recall 10@500=0.98, but Recall 10@10
is only 0.63. This suggests a two-step strategy: first, perform coarse
recall with the pruned index to retrieve many candidates, then refine
them using full inner product reordering for efficient AMIPS.
8

Table 3: Dataset statistics and characteristics
Dataset‚à•D‚à•ùëéùë£ùëî‚à•ùë• ùëñ‚à•ùëõùëû ùëéùë£ùëî‚à•ùëû‚à•ùëëSparsitySize (GB)ùëéùë£ùëîùëôModel Language
SPLADE-1M 1,000,000 126.3 6980 49.1 30108 0.9958 0.94 4569.2 splade English
SPLADE-FULL 8,841,823 126.8 6980 49.1 30108 0.9958 8.42 40447.3 splade English
AntSparse-1M 1,000,000 40.1 1000 5.8 250000 0.9998 0.31 902.6 bge-m3 Chinese
AntSparse-10M 10,000,000 40.1 1000 5.8 250000 0.9998 3.06 6560.7 bge-m3 Chinese
NQ 2,681,468 149.4 3452 47.0 30510 0.9951 3.01 13914.7 splade English
RANDOM-5M 5,000,000 150.0 5000 50.4 30000 0.9950 5.62 25000.0 - -
Detail of Algoruthm 4Algorithm 4 details the query procedure
for the approximate SINDIindex. Given a query ùëû, an inverted index
ùêº, query pruning ratio ùõΩ, re-ranking threshold ùõæ, and the number of
nearest neighbors ùëò, the algorithm starts by initializing two empty
min-heaps,ùêªandùëÖ, for storing coarse recall candidates and final
top-ùëòneighbors, respectively (Lines 1-2). The ùõΩratio subvector
ùëû‚Ä≤of the query ùëûis then generated (Line 3), and Algorithm 2 is
invoked to retrieve ùõæcoarse recall results into ùêª(Line 4). While ùêª
is not empty (Line 5), the algorithm processes its top element by
extracting the stored vector ID ùëñand partial inner product score ùëëùëñùë†
(Line 6). The full inner product between ùë•ùëñand the original query ùëû
is calculated as ùëëùëñùë†‚Ä≤(Line 7). The tuple(ùëñ,ùëëùëñùë†‚Ä≤)is inserted into heap
ùëÖ, maintaining its size at most ùëòby removing the smallest element if
necessary (Lines 8-11). This process continues until all coarse recall
candidates inùêªare processed.
5 EXPERIMENTAL STUDY
5.1 Experimental Settings
DatasetsTable 3 presents the datasets used in this experimental
evaluation. The experiments cover real-world datasets with varying
languages and training models, ranging in size from 1M to 10M
vectors. The English datasets were trained using the SPLADEmodel,
while the Chinese datasets were trained using the BGE-M3 model,
which was developed internally by Ant Group. Notably, the Chinese
datasets have significantly higher dimensionality compared to the
English datasets, primarily due to the larger vocabulary size in Chi-
nese. Table 3 also includes the average number of non-zero entries
per vector (ùëéùë£ùëî‚à•ùë•ùëñ‚à•) and the average number of vectors per inverted
list (ùëéùë£ùëîùëô ), which can serve as references for selecting specific prun-
ing parameters. Additionally, the experiments incorporate randomly
generated datasets, in which both the number of non-zero entries
and the corresponding values follow a random distribution. Table
3 further provides thesparsityof each dataset, which quantifies
how sparse the data is. Thesparsityof a dataset Dis calculated as:
sparsity= 1‚àí‚àëÔ∏Å
ùë•‚ààD‚à•ùë•‚à•
‚à•D‚à•¬∑ùëë, where‚à•ùë•‚à•denotes the number of non-zero
entries in vector‚Éóùë•(as defined in Definition 1), dd is the maximum
dimension, and‚à•D‚à•is the total number of vectors inD.
We compare SINDIwith five SOTA algorithms: SEISMIC, PYANNs,
SOSIA, BMP, and HNSW. Below is a description of each algorithm:
‚Ä¢SEISMIC[2]: A sparse vector index based on inverted lists.
‚Ä¢BMP[ 3,14]: A dynamic pruning strategy for learning sparse
vector retrieval. It divides the original dataset into fine-grained
blocks and generates a maximum value vector for each block to
evaluate whether the block should be queried.‚Ä¢HNSW[ 13]: A graph-based index designed for dense vector
search. We adjusted the data format and distance computation to
adapt it for sparse vectors.
‚Ä¢PYANNs: The open-source champion of the BigANN Bench-
mark 2023 Sparse Track. It is built on HNSW and incorporates
quantization, query pruning, and rerank strategies.
‚Ä¢SOSIA[18]: A sparse vector index based on min-hash.
Parameter SettingsWe used the optimal parameters for each
algorithm to ensure a fair evaluation. The parameter selection ei-
ther follows the recommendations from the original authors or is
determined through grid search.
Performance MetricsWe evaluate the index construction time,
index size, recall, and QPS for all baselines. Since approximate
methods require a trade-off between query efficiency and accuracy,
we usethroughputto measure query efficiency, which is defined
as the number of queries processed within a specific time interval.
For a given query ‚Éóùëû, the result of Approximate Maximum Inner
Product Search (AMPIS) is denoted as ùëÖ={‚Éóùë• 1,‚Éóùë•2,...,‚Éóùë•ùëò}. Let
ùëÖ‚àó={‚Éóùë•‚àó
1,‚Éóùë•‚àó
2,...,‚Éóùë•‚àó
ùëò}denote the exact top- ùëòresults obtained via
Maximum Inner Product Search (MIPS). The recall is computed as
follows:ùëÖùëíùëêùëéùëôùëô=|ùëÖ‚à©ùëÖ‚àó|
|ùëÖ‚àó|. We specifically evaluate the recall metrics
Recall@50 and Recall@100.
The experiments are conducted on a server with an Intel(R)
Xeon(R) Platinum 8269CY CPU @ 2.50GHz and 512GB mem-
ory. We implement SINDIin C++, and compile it with g++ 10.2.1,
-Ofast flag, andAVX-512instructions enabled.
5.2 Overall Performance
5.2.1 Recall and QPS.Figure 8 analyzes the relationship be-
tween recall (Recall@50 and Recall@100) and the QPS performance
of various algorithms on a single-threaded setup. For each algorithm,
we report its best performance across all tested parameter configura-
tions.
On both English and Chinese datasets, SINDIachieves the highest
QPS under the same recall levels. When Recall@50 is 99%, on the
SPLADE-1M dataset, the QPS of SINDIis 2.0 √óthat of SEISMIC
and 26.4√óthat of PYANNs; on the SPLADE-FULL dataset, the QPS
of SINDIis 4.16√óthat of SEISMICand 5.6 √óthat of PYANNs. When
Recall@100 is 98%, on the SPLADE-1M dataset, the QPS of SINDI
is 1.9√óthat of SEISMICand 3.2√óthat of PYANNs.
On the Chinese dataset encoded by the BGE-M3 model, SINDI
also achieves the best performance. When Recall@50 is fixed at
97%, on the AntSparse-10M dataset, the QPS of SINDIis 2.5 √ó
that of SEISMIC. The RANDOM-5M dataset is generated uniformly
at random, resulting in extremely sparse intersections of term IDs
between data points. This sparsity causes the graph structures of
9

Sindi Seismic Pyanns Bmp Sosia Hnsw
1
0.800.850.900.951.00
Recall102103QPS
(a)SPLADE-1M Recall@50
0.800.850.900.951.00
Recall102103QPS
 (b)SPLADE-1M Recall@100
0.800.850.900.951.00
Recall101102103QPS
 (c)SPLADE-FULL Recall@50
0.800.850.900.951.00
Recall101102QPS
 (d)SPLADE-FULL Recall@100
0.800.850.900.951.00
Recall102QPS
(e)NQ Recall@50
0.800.850.900.951.00
Recall102QPS
 (f)NQ Recall@100
0.50.60.70.80.9
Recall101102103QPS
 (g)RANDOM-5M Recall@50
0.50.60.70.80.9
Recall101102103QPS
 (h)RANDOM-5M Recall@100
0.800.850.900.95
Recall102103QPS
(i)AntSparse-1M Recall@50
0.80 0.85 0.90 0.95
Recall102103QPS
 (j)AntSparse-1M Recall@100
0.80 0.85 0.90 0.95
Recall102103QPS
 (k)AntSparse-10M Recall@50
0.80 0.85 0.90 0.95
Recall102103QPS
 (l)AntSparse-10M Recall@100
Figure 8: Overall Performance.
PYANNs and HNSW to become disconnected, leading to poor re-
call performance. The effectiveness of SEISMIC‚Äôs clustering is also
sensitive to data distribution, causing noticeable performance degra-
dation. In contrast, SINDIremains unaffected by data distribution
and continues to achieve the best overall performance.
These results demonstrate that SINDI consistently achieves SOTA
performance across datasets of various languages, models, and dis-
tributions.
5.2.2 Index Size and Construction Time.Figure 9 summa-
rizes the index size and construction time for SINDI, SEISMIC, and
PYANNs across three datasets. SINDIdemonstrates the lowest con-
struction cost across all datasets.
SEISMIC, which requires storing summary vectors for each block,
results in the largest index size. On the NQ dataset, its size is 3 √óthat
of SINDI. On the other hand, PYANNs‚Äô graph index construction
involves a large number of distance computations to find neighbors,
leading to extremely high construction time. For example, on the
SPLADE-FULL dataset, PYANNs‚Äô construction time is 71 √óthat of
SINDI. In contrast, SINDI‚Äôs index construction primarily involves
sorting non-zero entries for pruning, which keeps the overall cost
low and enables rapid index building.5.3 Parameters
5.3.1 The Impact of ùõº.This section explores how the document
pruning parameter ùõºaffects SINDI‚Äôs performance. The parameter
ùõºdetermines the proportion of high-mass non-zero entries retained.
For the MsMarco and NQ datasets, ùõºis tested from 0.4 to 0.8, with
a step size of 0.1. For the AntSparse dataset, ùõºranges from 0.7 to 1,
with a step size of 0.05. Figure 10 presents the changes in recall and
QPS asùõºincreases under fixed ùõΩandùõæ. On the MsMarco dataset, re-
call improves and QPS decreases as ùõºgrows, but both changes slow
down when ùõºbecomes larger. On the AntSparse dataset, recall also
improves slowly, but QPS drops more rapidly. When ùõºis small, in-
creasing it retains more high-scoring non-zero entries, which boosts
recall significantly. As ùõºbecomes larger, the additional retained
entries have less impact, making recall improvements slower. For
the AntSparse dataset, the scores of non-zero entries have smaller
variance and are closer to zero compared to English datasets. This
causes more non-zero entries to be retained as ùõºincreases, leading
to a faster decrease in QPS compared to the MsMarco dataset.
5.3.2 The Impact of sparsity.We analyze the performance of
SINDIunder varying datasetsparsitylevels.sparsitymeasures the
proportion of non-zero entries in a dataset. For a fixed number of
non-zero entries, larger dimensions result in highersparsity. To
investigate this, we generated five random datasets, each with 1M
10

Sindi Seismic Pyanns
1
IT (s) IS (GB)01000200030004000Construction Time (s)
58.2220.54163.0
024681012
Index Size (GB)9.9512.88
9.54(a)SPLADE-FULL
IT (s) IS (GB)0500100015002000Construction Time (s)
33.0551.41915.8
05101520
Index Size (GB)4.9919.18
4.33 (b)AntSparse-10M
Figure 9: Index Size and Construction Time for Different Datasets and
Algorithms.
recall qps
0.4 0.6 0.8
Œ±0.9000.9250.9500.975Recall@50
200300400500
QPS
(a)SPLADE-FULL Recall@50
0.70.80.91.0
Œ±0.930.940.95Recall@50
6008001000
QPS
 (b)AntSparse-10M Recall@50
Figure 10: The Impact ofùõº.
vectors and an average of 120 non-zero entries per vector. The
dataset dimensions were set to [10,000; 30,000; 50,000; 70,000;
100,000], increasingsparsityprogressively. Figure 11 compares
the performance of SINDIand SEISMICacross these datasets on
Recall@50 = 90% and Recall@50 = 99%.
The results show that assparsityincreases, both SINDIand SEIS-
MICachieve higher QPS at the same recall levels. This is because
highersparsityallows the IVF structure to partition data more ef-
fectively. Assparsityincreases, each term list cotains fewer vectors,
which reduces the number of vector candidates that need to be
searched during a query. Consequently, the number of inner prod-
uct computations decreases, leading to improved QPS. Crucially,
since all nearest neighbors are still captured within the lists, recall
is not affected. In contrast, graph-based indexes face challenges
with highersparsity. Sparse high-dimensional data leads to weaker
connectivity between nodes in the proximity graph, resulting in de-
graded search performance.Thus, IVF provide a natural advantage
for sparse datasets compared with graph structure.
SINDIconsistently outperforms SEISMICby maintaining approxi-
mately 10√óhigher QPS across allsparsitylevels. This demonstrates
SINDI‚Äôs efficiency and scalability on sparse datasets. Additionally,
SINDIis particularly suitable for languages like Chinese, where
larger vocabularies result in high-dimensional and sparse datasets.
Its ability to adapt to differentsparsitylevels and data distributions
Sindi Seismic
1
10K 30K 50K 70K 100K
Dimension0246QPS (√ó103)
6.8√ó11.0√ó7.8√ó6.8√ó5.6√ó(a)Recall@50=90%
10K 30K 50K 70K 100K
Dimension012345QPS (√ó103)
23.4√ó10.2√ó7.6√ó9.1√ó9.8√ó (b)Recall@50=99%
Figure 11: QPS of SINDIand SEISMICon RANDOM-1M Dataset with
different sparsity
MRP LP VNP LP+MRP
0.800.850.900.95
Recall@10103
2√ó1023√ó1024√ó1026√ó102QPS
(a)SPLADE-FULL Recall@10
0.80 0.85 0.90 0.95
Recall@103√ó1024√ó1026√ó102QPS
 (b)AntSparse Recall@10
Figure 12: Recall@10 vs QPS on MsMarco and AntSparse ofMass Ratio
Pruning,List PruningandVector Number Pruning.
highlights its robustness and wide applicability in real-world scenar-
ios.
5.4 Ablation
5.4.1 The Impact of Pruning Method.Figure 12 illustrates the
performance of different pruning strategies on the SPLADE-FULL
and AntSparse datasets. The experiments evaluate all pruning strate-
gies under the same ùõΩandùõæsettings, while varying ùõºto measure
Recall and QPS. The results demonstrate thatMass Ratio Pruning
achieves the best performance, followed byVector Number Prun-
ingandList Pruning, with the lowest performance observed when
combiningList PruningandMass Ratio Pruning.
This is becauseMass Ratio Pruningeffectively preserves the non-
zero entries that contribute the most to inner product computation,
resulting in more true nearest neighbors being retained during the
partial inner product stage. In contrast,List Pruningrestricts the
size of posting lists for each dimension, which creates two problems:
some lists contain too few documents, retaining small-value entries,
while others have too many documents, removing large-value entries.
As a result,List Pruningis unsuitable for SINDI. In comparison,
SEISMICemploysList Pruningsince it computes the full inner
product for all vectors within the lists, thereby avoiding significant
losses in accuracy. However, when bothList PruningandMass
Ratio Pruningare applied simultaneously, more non-zero entries are
discarded, leading to further decreases in recall.
11

Accumulation Time Reorder Time Accumulation Recall Reorder Recall
0.3 0.4 0.5 0.6
Œ±051015202530Time (ms) (√ó103)
0.00.20.40.60.81.0
Recall
(a)SPLADE-FULL
0.7 0.8 0.9 1
Œ±05101520Time (ms) (√ó102)
0.00.20.40.60.81.0
Recall
 (b)AntSparse-10M
Figure 13: Reorder vs. Non-Reorder on SPLADE-FULL and AntSparse-
10M Datasets: Time Cost and Recall@50 with Varyingùõº.
5.4.2 The Impact of Reorder.To investigate the impact of the
reordering strategy on the performance of SINDI, we compared the
differences between using and not using reordering. Experiments
were conducted on the SPLADE-FULL and AntSparse datasets. We
set the query cut ratio ùõΩto 0.2 and reordering number ùõætop 500. The
non-reordering strategy did not require storing dataset.
The results are shown in Figure 13. We evaluated the query time
and Recall@50 for both strategies under variousùõº. For the reorder-
ing strategy, the query time includes accumulation time and reorder-
ing time, while the non-reordering strategy only includes accumula-
tion time. Since the parameter ùõæis fixed, the reorder time remains
relatively constant. However, as ùõºincreases, the accumulation time
grows accordingly. Although reorder time accounts for only a small
portion of the overall query time, it significantly improves recall.
For example, on the SPLADE-FULL dataset, when ùõº=0.6 , the
accumulation time is 17099 ms, and the reorder time is 3553 ms.
Despite this, the recall improves substantially from 0.71 to 0.97.
This demonstrates the substantial efficiency improvement achieved
by the reordering strategy.
The reordering strategy demonstrates its effectiveness for two
reasons. First, it focuses on computing only a small subset of non-
zero entries that contribute the most to the inner product, significantly
reducing computations. Second, the partial inner product derived
from high-mass entries can largely preserve the true ranking order
of the inner product. This ensures that a small subset of candidate
vectors is sufficient to include the true nearest neighbors, improving
both efficiency and accuracy.
5.5 Scalability
To further evaluate the scalability of the SINDIalgorithm, we con-
ducted a multi-threaded performance test on two large-scale datasets:
SPLADE-FULL and AntSparse-10M. We measured QPS at differ-
ent recall targets ( Recall@50‚àà{0.91,0.95,0.999} ) while varying
the number of CPU cores from 2 to 10, as shown in Figure 14. On
AntSparse-10M at Recall@50=0.90 , using 2 CPU cores yields
1979.49 QPS (approximately 989.75 QPS per core), while using
10 cores achieves 8374.01 QPS (approximately 837.40 QPS per
core). The per-core efficiency remains high, dropping by less than
16% when scaling from 2 to 10 cores. Similar scaling behavior is
observed for SPLADE-FULL, confirming that SINDIeffectively
utilizes available CPU cores with minimal parallelization overhead.
Recall@50=90% Recall@50=95% Recall@50=99%
246810
Threads103QPS
(a)SPLADE-FULL
246810
Threads1032√ó1033√ó1034√ó1036√ó103QPS
 (b)AntSparse-10M
Figure 14: Multi-threaded scalability of SINDI(QPS) at different Re-
call@50 targets on SPLADE-FULL and AntSparse-10M datasets.
These results demonstrate that SINDImaintains high multi-core
efficiency across datasets and accuracy levels, making it suitable
for deployment in scenarios requiring both high recall and high
throughput.
6 RELATED WORK
Existing methods for MIPS on sparse vectors adopt diverse index
structures, including inverted index-, graph-, and hash-based de-
signs. Inverted index-based methods, such as SEISMIC[ 2], build
postings for each dimension and prune low-value entries to reduce
the candidate set. They cluster postings into blocks to skip irrelevant
candidates. However, as each vector appears in multiple postings,
value retrieval still requires random access to scattered data.
Graph-based methods, such as HNSW-based PYANNs [ 13], orga-
nize vectors as nodes in a proximity graph. At query time, greedy
graph traversal identifies neighbors, but vector sparsity leads to weak
connectivity and frequent random memory accesses.
Hash-based methods, like SOSIA[ 18], transform sparse vectors
into sets (via SOS) and use min-hash to estimate Jaccard similarity.
Multiple hash functions improve accuracy but scatter vector storage
across buckets, limiting locality and making direct computation in
postings impractical.
BMP[ 3,14] also stores non-zero values in postings and prunes
with block-level maximum value vectors. While this avoids ID
lookups, its fine-grained partitioning leads to excessive block evalua-
tions, degrading efficiency. Our method retains value-stored postings
without requiring costly block-level filtering.
7 CONCLUSION
In this work, we propose SINDI, an inverted index for sparse vectors
that eliminates redundant distance computations. By storing term
weights directly in the postings, SINDIremoves both the ID lookup
and random memory access overhead in distance computation, and
leverages SIMD to fully exploit CPU parallelism. It further intro-
ducesMass Ratio Pruningthat preserves maximum inner-product ac-
curacy while enabling scalable approximate search on large datasets.
Experiments on multilingual, multi-scale real-world datasets demon-
strate that SINDIachieves state-of-the-art performance.
12

REFERENCES
[1] Dimitri Bertsekas and Robert Gallager.Data networks. Athena Scientific, 2021.
[2]Sebastian Bruch, Franco Maria Nardini, Cosimo Rulli, and Rossano Venturini.
Efficient inverted indexes for approximate retrieval over learned sparse repre-
sentations. InProceedings of the 47th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 152‚Äì162, 2024.
[3]Parker Carlson, Wentai Xie, Shanxiu He, and Tao Yang. Dynamic superblock
pruning for fast learned sparse retrieval. InProceedings of the 48th Interna-
tional ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR ‚Äô25, page 3004‚Äì3009, New York, NY , USA, 2025. Association
for Computing Machinery.
[4] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and St√©phane Clinchant.
Splade v2: Sparse lexical and expansion model for information retrieval.arXiv
preprint arXiv:2109.10086, 2021.
[5] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and St√©phane Clinchant.
From distillation to hard negative sampling: Making sparse neural ir models more
effective. InProceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval, SIGIR ‚Äô22, page 2353‚Äì2359,
New York, NY , USA, 2022. Association for Computing Machinery.
[6] Thibault Formal, Benjamin Piwowarski, and St√©phane Clinchant. Splade: Sparse
lexical and expansion model for first stage ranking. InProceedings of the 44th In-
ternational ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR ‚Äô21, page 2288‚Äì2292, New York, NY , USA, 2021. Association
for Computing Machinery.
[7] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai,
Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for
large language models: A survey.arXiv preprint arXiv:2312.10997, 2(1), 2023.
[8] John L Hennessy and David A Patterson.Computer architecture: a quantitative
approach. Elsevier, 2011.
[9]Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards re-
moving the curse of dimensionality. InProceedings of the thirtieth annual ACM
symposium on Theory of computing, pages 604‚Äì613, 1998.
[10] Omid Keivani, Kaushik Sinha, and Parikshit Ram. Improved maximum inner
product search with better theoretical guarantee using randomized partition trees.
Mach. Learn., 107(6):1069‚Äì1094, June 2018.[11] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim
Rockt√§schel, et al. Retrieval-augmented generation for knowledge-intensive nlp
tasks.Advances in neural information processing systems, 33:9459‚Äì9474, 2020.
[12] Guangyuan Ma, Yongliang Ma, Xuanrui Gou, Zhenpeng Su, Ming Zhou, and
Songlin Hu. Lightretriever: A llm-based hybrid retrieval architecture with 1000x
faster query inference.arXiv preprint arXiv:2505.12260, 2025.
[13] Yu A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest
neighbor search using hierarchical navigable small world graphs.IEEE Trans.
Pattern Anal. Mach. Intell., 42(4):824‚Äì836, April 2020.
[14] Antonio Mallia, Torsten Suel, and Nicola Tonellotto. Faster learned sparse re-
trieval with block-max pruning. InProceedings of the 47th International ACM
SIGIR Conference on Research and Development in Information Retrieval, SIGIR
‚Äô24, page 2411‚Äì2415, New York, NY , USA, 2024. Association for Computing
Machinery.
[15] Ninh Pham. Simple yet efficient algorithms for maximum inner product search
via extreme order statistics. InProceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining, KDD ‚Äô21, page 1339‚Äì1347, New York,
NY , USA, 2021. Association for Computing Machinery.
[16] Yang Song, Yu Gu, Rui Zhang, and Ge Yu. Promips: Efficient high-dimensional
c-approximate maximum inner product search with a lightweight index. In2021
IEEE 37th International Conference on Data Engineering (ICDE), pages 1619‚Äì
1630. IEEE, 2021.
[17] Xiao Yan, Jinfeng Li, Xinyan Dai, Hongzhi Chen, and James Cheng. Norm-
ranging lsh for maximum inner product search.Advances in Neural Information
Processing Systems, 31, 2018.
[18] Xi Zhao, Zhonghan Chen, Kai Huang, Ruiyuan Zhang, Bolong Zheng, and Xi-
aofang Zhou. Efficient approximate maximum inner product search over sparse
vectors. In2024 IEEE 40th International Conference on Data Engineering
(ICDE), pages 3961‚Äì3974. IEEE, 2024.
[19] Xi Zhao, Bolong Zheng, Xiaomeng Yi, Xiaofan Luan, Charles Xie, Xiaofang
Zhou, and Christian S Jensen. Fargo: Fast maximum inner product search via
global multi-probing.Proceedings of the VLDB Endowment, 16(5):1100‚Äì1112,
2023.
13