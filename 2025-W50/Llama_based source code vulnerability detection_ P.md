# Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning

**Authors**: Dyna Soumhane Ouchebara, Stéphane Dupont

**Published**: 2025-12-09 12:08:24

**PDF URL**: [https://arxiv.org/pdf/2512.09006v1](https://arxiv.org/pdf/2512.09006v1)

## Abstract
The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.

## Full Text


<!-- PDF content starts -->

Llama-based source code vulnerability detection:
Prompt engineering vs Fine tuning
Dyna Soumhane Ouchebara1[0009−0002−7136−3178]and Stéphane
Dupont1[0000−0003−3674−6747]
University of Mons, Mons, Belgium
DynaSoumhane.OUCHEBARA@umons.ac.be
Stephane.DUPONT@umons.ac.be
Abstract.The significant increase in software production, driven by
the acceleration of development cycles over the past two decades, has led
to a steady rise in software vulnerabilities, as shown by statistics pub-
lished yearly by the CVE program. The automation of the source code
vulnerability detection (CVD) process has thus become essential, and
several methods have been proposed ranging from the well established
program analysis techniques to the more recent AI-based methods. Our
research investigates Large Language Models (LLMs), which are consid-
ered among the most performant AI models to date, for the CVD task.
The objective is to study their performance and apply different state-of-
the-art techniques to enhance their effectiveness for this task. We explore
variousfine-tuningandpromptengineeringsettings.Weparticularlysug-
gest one novel approach for fine-tuning LLMs which we call Double Fine-
tuning, and also test the understudied Test-Time fine-tuning approach.
We leverage the recent open-source Llama-3.1 8B, with source code sam-
ples extracted from BigVul and PrimeVul datasets. Our conclusions high-
light the importance of fine-tuning to resolve the task, the performance
of Double tuning, as well as the potential of Llama models for CVD.
Though prompting proved ineffective, Retrieval augmented generation
(RAG)performedrelativelywellasanexampleselectiontechnique.Over-
all,someofourresearchquestionshavebeenanswered,andmanyarestill
on hold, which leaves us many future work perspectives. Code repository
is available here: https://github.com/DynaSoumhaneOuchebara/Llama-
based-vulnerability-detection.
Keywords:Software vulnerability detection·Source code analysis·
Deep learning·Large language models·Cybersecurity.
1 Introduction
While building functional software is already complex, ensuring its security is
even more challenging. The push for automation and rapid development pro-
cesses, enabled by the wide adoption of open-source libraries, has significantly
increased software production. However, these open-source components often
contain flaws, which can propagate to thousands of dependent projects. AmongarXiv:2512.09006v1  [cs.SE]  9 Dec 2025

2 D. Ouchebara and S. Dupont
the most critical defects areSoftware Security Vulnerabilities, which refer to
faults caused by mistakes in design, development or configuration of a software
system, which can be exploited by attackers to breach system security [24].
Thenumberofsuchvulnerabilitiesisrisingrapidly,asshownbytheCommon
Vulnerabilities and Exposures (CVE) [31] reports in Figure 1. This highlights
the urgent need for robust vulnerability management, and vulnerability detec-
tion (CVD) is the first crucial step in this process. Approaches have evolved from
manual expert analysis to automated program analysis techniques, and more re-
cently, to AI-based methods. Machine and Deep Learning models are indeed
increasingly favored due to their ability to extract meaningful patterns from raw
data, which makes them particularly interesting for vulnerability detection. The
latest advances in this field involveLarge Language Models (LLMs), which have
shown exceptional performance in both natural language and software engineer-
ing tasks. Their strong reasoning and code comprehension abilities have led to
promising results in recent studies applying LLMs to automated CVD.
Fig. 1.Evolution of the number of CVEs (Common vulnerabilities and exposures)
recorded from 1999 to 2024 [7].
Our research is part of this broader effort, and focuses on investigating the
capabilities of LLMs for the task of vulnerability detection in source code and
proposing improvements, adaptations, and the use of various learning techniques
to enhance their effectiveness for this particular task. Our main contributions
are:
–Conducting an experimental protocol where we evaluate the recent open-
source Llama-3.1 8B on the vulnerability detection task on two real-world
source code datasets : BigVul and PrimeVul.
–Investigating various Prompt engineering and Fine-tuning approaches, in-
cluding Zero-shot prompting, Few-shot prompting with three approaches for
example selection (random, same vulnerabilty type and RAG) and Efficient
fine-tuning with QLoRA (Quantized Low Rank Adapaters).

Llama-based source code vulnerability detection 3
–Testing one understudied technique known as Test-Time fine-tuning, and
suggesting a novel fine-tuning approach which we call Double Fine-tuning.
–Comparing two fine-tuning fashions for the binary classification task of vul-
nerability detection, namely the generative fashion and the classification
fashion (explained later in the approach section).
The remainder of this paper is organized as follows: section 2 covers the
state-of-the-art in LLMs and vulnerability detection in source code, section 3
presents the ideas proposed and experiments conducted, section 4 presents the
results obtained along with a discussion, and section 5 concludes the paper with
the answers to our research questions as well as future work perspectives.
2 Related work
In his section, we present a brief literature review on LLMs and vulnerability
detection in source code.
2.1 Large language models
Large Language Models (LLMs) are the result of decades of research in lan-
guage modeling, evolving through three main waves: Statistical, Neural, and
Pre-trained language models [4]. Statistical Language Models (SLMs) see text
as a sequence of words and estimate its probability by computing the product
of individual word probabilities, but they struggle to fully capture the richness
and variability of natural language due to data sparsity [30]. Neural Language
Models (NLMs) address data sparsity by mapping words to low-dimensional
continuous vectors (embedding vectors) and neural networks. Early NLMs, such
as RNNs, LSTMs, and GRUs improved NLP applications but were task-specific
and could only deal with short sequences. Pre-trained Language Models (PLMs)
came to adress these shortcomes and introduced the Transformer architecture
which allows for parallelized processing of the sequence, consequently enabling
large-scale training on vast datasets for general tasks, which we call Pre-training
[30]. Researchers discovered that the bigger PLMs get, the more powerful they
become on general-purpose tasks, and this gets us to the powerful LLMs that
became the new AI standard since 2022, and which mainly refer to transformer-
based PLMs that contain tens to hundreds of billions of parameters [30,4]. In
order to allow general-purpose LLMs to adapt to some specific task in hand,
two categories of techniques are available: Prompt engineering and Fine-tuning.
Prompt engineering is a rapidly evolving discipline which consists of crafting the
optimal input (prompt) to achieve a specific goal with a generative model [30].
Sometimes, when the task is too complex or very specific to particular data,
prompting becomes insufficient and we need to fine-tune the model on the spe-
cific task and data in hand. But since LLMs are particularly large-sized, we must
use resource efficient strategies.

4 D. Ouchebara and S. Dupont
2.2 Software vulnerability detection
Early efforts in source code vulnerability detection (CVD) were manual, relying
on expert review, which, despite its accuracy, was unscalable [18]. This led to
the development of automated tools using static and dynamic program analy-
sis. Static techniques [45,6,38] inspect code without execution, while dynamic
methods [44,37,47] analyze runtime behavior [18]. However, these approaches
struggled with false positives and scalability.
To address these issues, AI-based CVD emerged around 2007 [33], start-
ing with Machine Learning (ML) methods which use manually engineered fea-
tures such as lexical statistics and code metrics to learn patterns from past
vulnerabilities [18]. While promising, these approaches required tedious feature
engineering. Deep Learning (DL) alleviated this by learning representations di-
rectly from code. Models like VulDeePecker [23] for instance treated code as
token sequences and applied RNNs. Given code’s structured nature, researchers
later shifted toward graph-based representations such as Abstract Syntax Trees
(ASTs) and Code Property Graphs (CPGs), processed using Graph Neural Net-
works (GNNs). Introduced around 2019 [21] for CVD, GNN-based models like
Devign [49] and Reveal [3] achieved state-of-the-art results. More recently, the
rise of Transformers and large-scale pre-trained models brought renewed inter-
est in sequence-based modeling. Medium-sized models like CodeBERT [12] have
been used in top-performing systems such as Linevul [13] and VulBERTa [17],
and UniXcoder [16] tested in works like [8,46].
Current research focuses on leveraging Large Language Models (LLMs). One
category of studies investigates Prompt engineering techniques. We cite a few
representative works: [48] propose to design different prompting templates to
query the close-sourced GPT-3.5 and GPT-4; [36] propose to study different
techniques (zero-shot, few-shot, CoT) to query two open source LLMs includ-
ing Llama-2 and Falcon and closed source ChatGPT using SARD and CVE
datasets; [14] evaluate 16 LLMs using few-shot prompting for both binary and
multi-class vulnerability detection on a dataset constructed from "Capture-the-
flag" (CTF) challenges; [40] evaluate 14 LLMs (from which Llama, Bigcode, Mis-
tral, DeepSeek, GPT and Gemini) on SVEN dataset using different techniques
(zero-shot, few-shot, CoT, contrastive in-context); [11] propose Graph-enhanced
Soft prompt tuning on CodeLlama and CodeGemma using Diversevul dataset;
[25] propose a RAG framework for GPT-3.5 using efficient retrieval techniques
such as BM-25 and TF-IDF. Another category of studies investigates the perfor-
manceofFine-tuningLLMsfortheCVDtask.Somerepresentativeworksare:[9]
propose VulLLM, in which they fine-tune the open source LLMs codellama and
starcoderonSVENdataset;[41]evaluatefine-tunedLlama,CodeLlama,Gemma
and CodeGemma on DiverseVul dataset; [21] investigate fine-tuning Llama2,
Llama3, Llama3.1 and CodeLlama on multiple CVD datasets; [48] experiment
fine-tuning CodeLlama and Mistral on their own proposed dataset; [40] evaluate
fine-tuning different LLMs from Llama family, Bigcode family, and DeepSeek.
Unlike most previous research, we conduct our experiments on one LLM
(Llama-3.1) using two datasets and focus on investigating the difference between

Llama-based source code vulnerability detection 5
various prompt engineering and fine-tuning approaches, and between the two
fine-tuningfashionsavailablewithmostLLMs.Wealsosuggestoneunderstudied
technique (Test-Time tuning) and one novel approach (Double tuning) for fine-
tuning LLMs. Moreover, we underline the importance of understanding each
evaluation metric rather than just observing the global F1-score, and highlight
the necessity of further studying the explainability of the model predictions (as
part of our most urgent future work).
3 Proposed approach
3.1 Problem formulation
Code vulnerability detection is typically framed as a binary classification prob-
lem:X i→y i. Specifically, given an input source code functionX i, a model
(neural network) predicts whether the input function is vulnerable (y i= 1) or
non-vulnerable (y i= 0). Of course, presenting the vulnerability detection prob-
lem as a binary classification task is one first step to solving the actual "real-life"
problem, where we do not only want to know if the code is vulnerable, but also
know the exact type of vulnerability we are facing. The problem will thus be
later extended into a multi-class classification task, where the classes represent
the different possible vulnerability types, generally noted by CWE (common
weakness enumeration) [32] types in literature and available datasets.
3.2 Datasets
To conduct our experiments, we choseBigVul[10] andPrimeVul[8] datasets.
We justify the choice of BigVul by the fact that it is constructed from real
source code projects from Github, as well as being a very well-known dataset
used by most prior research, particularly state-of-art solutions. This facilitates
the process of comparing our experiments with those conducted by other re-
searchers. BigVul was created in 2020 by crawling the entries from the CVE [31]
database, and linking vulnerability descriptions to publicly available GitHub
repositories [10]. It contains 3,754 code vulnerabilities (distinct CVEs) spanning
91 different vulnerability types (CWE types) which were extracted from 348
projects mainly written in C/C++ [10]. Overall, the dataset, contains a total
of of 188,636 C/C++ functions with a ratio 5.7% vulnerable and 94.3% non-
vulnerable [13]. As for our experiments, we did not use BigVul dataset as-is, but
applied some pre-processing as follows:
1. We first extracted the columns needed by our models to function, which are
only two columns:func-beforewhich contains the source code as text, and
vulwhich contains the label (0 for non-vulnerable or 1 for vulnerable).
2. Then we split the dataset into 90% training, 5% validation and 5% testing
sets. Our data split is available here1.
1https://huggingface.co/datasets/DynaOuchebara/BigVul_2columns

6 D. Ouchebara and S. Dupont
3. Finally, we proceeded to balancing the dataset. Though we acknowledge the
limitations of using artificially balanced datasets, which do not reflect the
real-worldimbalancebetweenbenignandvulnerablecode,thisstepisimpor-
tant to ensure our models do not trivially default to predicting the majority
class. For this, we used the random under-sampling method, consisting of
randomly removing instances from the majority class to match the size of
the minority class. This is of course one method among others to deal with
class imbalance (data augmentation, k-fold training, focal loss, etc.).
Despite being a well-known and very utilized dataset, BigVul is 5 years old
and some recent studies [5,8] have questioned the accuracy of its labels. So to
further enrich our study and confirm the confidence in our results, we recon-
ducted all experiments on PrimeVul, which is a more recent dataset created
in 2024 by merging security-related commits from many prior datasets (BigVul
[10], CrossVul [35], CVEfixes [2], and DiverseVul [5]) while ensuring better label
accuracy with new labeling techniques, as well as reducing the possibility of data
duplication. PrimeVul contains 6,968 vulnerable and 228,800 benign functions
covering 140 CWEs. For our experiments, we applied to PrimeVul the same pro-
cess previously described for BigVul, except for the data splitting where we used
the original data split2published by the authors.
3.3 Baselines
We chose CodeBERT [12] and UniXcoder [16] models as baselines, which are
considered as state-of-the-art models, as shown by multiple comparative studies
[21], [39] as well as the papers which first introduced these models for CVD
(LineVul [13] and SvulD [34]). These models are medium-size language models
with 125 million parameters. They are based on the transformer architecture,
and were pre-trained on big source code corpuses.
To prepare our baselines, we decided not to take results from existing papers
who have tested these models before us, due to the lack of unification observed
in these papers (almost every paper presents different results due to the differ-
ent experimental setup and parameters). We finetuned them on BigVul (resp.
PrimeVul) training set, and then tested the fine-tuned models on the test set.
3.4 Approach
As described earlier in the introduction section, the motivation behind study-
ing the potential of LLMs for vulnerability detection lies in the fact that these
models are first of all pre-trained on vast amounts of textual data, among which
natural language and programming code corpuses. Consequently, there is a high
probability that these corpuses contain data related to security issues such as
source code vulnerabilities. This prior knowledge makes LLMs an interesting
starting point for building a CVD solution. Llama [43] models, in particular,
2https://huggingface.co/datasets/colin/PrimeVul

Llama-based source code vulnerability detection 7
are a good choice for they are open-source and efficient. In fact, unlike propri-
etary models, they can be fine-tuned on security-specific datasets, allowing for
an improved accuracy in tasks like vulnerability detection. Llama models are
also optimized for inference, making them cheaper to deploy compared to larger
models like GPT-4 or Claude. They provide a good balance between model size,
latency and accuracy. From the Llama series, we chose to conduct our experi-
ments on the Llama-3.1 8B version from Llama 3 series [15]. The 3.1 version is
the latest version of Llama available in Europe which proposes a "medium" sized
LLM like the 8B one, the 3.2 version being only available in USA and Canada
at the moment and the 3.3 version proposing only bigger models (over 70B).
Moreover, 8 billion parameters is convenient because it is large enough to under-
stand complex code patterns, yet small enough for cost-effective deployment and
inference. The idea is to test different techniques to make Llama-3.1 8B more
suitable for our CVD task. Two main categories of approaches for adapating
LLMs are studied: Prompting and Fine-tuning.
Prompting:AdaptingLLMswithoutchangingtheirweightsPrompting
is the process of guiding an LLM’s behavior by crafting well-structured inputs
(prompts).Insteadofmodifyingthemodel’sparameters,weusecleverlydesigned
prompts to get the model to generate useful outputs. We explore two main
prompting techniques in our study : Zero-shot and Few-shot prompting.
In Zero-Shot Prompting, we only give the instruction to the model. No exam-
ples are given and the model relies only on its pre-trained knowledge. Generally,
this technique works well for general knowledge questions, but performance may
be poor for specialized tasks. Our zero-shot prompt is the following:
Listing 1.1.Zero-shot prompt
"""␣Classify␣the␣source␣code␣into␣Vulnerable␣or␣Safe ,␣and␣
return␣the␣answer␣as␣the␣corresponding␣label .
Code:␣#code␣snippet␣we␣want␣to␣predict
Label :␣"""
In Few-Shot Prompting, we give the instruction to the model in addition to
a few labeled examples to guide the model. The model learns the pattern from
examples and this helps it better understand the expected format of the answers
it should return. Our few-shot prompt is the following:
Listing 1.2.Few-shot prompt
"""␣Classify␣the␣source␣code␣into␣Vulnerable␣or␣Safe ,␣and␣
return␣the␣answer␣as␣the␣corresponding␣label .␣Here␣are␣
some␣examples :
Code:␣#example␣code␣1
Label :␣#example␣label␣1
Code:␣#example␣code␣2
Label :␣#example␣label␣2
...
Code:␣#code␣snippet␣we␣want␣to␣predict
Label :␣"""

8 D. Ouchebara and S. Dupont
To constitute the prompt, for each test code, we choose 6 examples from
the training set. We initially conducted our few-shot experiments with 4, 6 and
10 examples, but we will only report the results obtained with 6-shot prompts
because it proved most performant and efficient. We followed 3 strategies to
select these examples. In the first one, we randomly choose 3 vulnerable code
examples and 3 safe code examples from the training set. In the second one, we
choose 3 vulnerable examples that correspond to the same type of vulnerability
(CWEtype)ofthetestcode,andwerandomlychoosethe3safeexamples.Inthe
third one, we use Retrieval Augmented Generation (RAG) [22]. We first generate
embeddings for all code snippets in the training set using an embedding model
for code; we chose CodeBERT for its excellent code understanding capability.
We save these embeddings in an index (using FAISS3library), then for each test
code, we search for the 6 most similar code snippets in the training set leveraging
that index, using L2 (Euclidean) distance as a similarity measure.
Fine-tuning: Adapting LLMs by tuning their weightsFine-tuning in-
volves training an LLM on a custom dataset so that it adapts to a specific task,
in our case CVD. Two main approaches exist: Full Fine-tuning and Efficient
Fine-tuning. For our experiments, we tested Efficient Fine-tuning with LoRA as
well as Quantization, because Full Fine-tuning requires too much GPU memory
requirements (since it updates all the weights of a billion parameter model).
LoRA [19] is a method that adds small, trainable adapter layers instead of
modifying all model weights. This approach requires less resources than Full
Fine-tuning while maintaining performance. Quantized LoRA (QLoRA) is an
even more memory-efficient version of LoRA. Instead of working with the full-
precisionmodel(32-bit),weapply4-bitquantizationtothemodel,whichreduces
itsmemoryfootprint.Itisimportanttonotethatsinceweareusingacompressed
model in addition to LoRA, the performance of the resulting fine-tuned model
doesnotmatchafullyfine-tunedmodel,butstill,theperformancedropisusually
not too penalizing if the right hyperparameters are chosen.
We study 3 different approaches for fine-tuning.
The first approach is the classic training then testing, where we first train the
model on our whole training data (using QLoRA), and then test the fine-tuned
model on the test data. To do the training phase for our binary classification task
of vulnerability detection, we have two options. The first one is to fine-tune the
LLM as a generative model and then analyze the textual response generated and
see if it is "Vulnerable" or "Safe". The second one is to add a classification head
to the model, which consists of a feed-forward neural network (FFNN) with one
output neuron which returns the probability of vulnerability. We consequently
tested both fine-tuning fashions. As for the approaches that follow, we applied
the second fine-tuning fashion (classification head).
The second approach is Test-Time fine-tuning, where for each test sample,
we retrieve 6 similar examples from the training data (using RAG just like
explained for the 3rd few-shot learning strategy), then we do a quick fine-tuning
3https://faiss.ai/

Llama-based source code vulnerability detection 9
of the model using only these examples. The idea is that instead of just adding
the examples to the prompt and relying on the model to effectively leverage the
information present in the input to generate the most suitable output, we can
use the examples to actually change the model weights which have a more direct
effect on the answer generated. Another benefit of this technique is that we can
use as many examples as we want for the fine-tuning, whereas we are limited
by the maximum context length when adding the examples to the prompt. The
idea of Test-time training was studied by a few researches [1,42,20].
Finally, the third approach merges the two previous ones, where we first train
the model on the whole train data, then we further tune the model at test-time
using the closest training samples. We call this Double fine-tuning.
Models usedWe specifically experimented on two models.
Llama-3.1 8B Base [27], is a pre-trained only version of the model. The model
has been pre-trained on a massive corpus of text in an unsupervised manner on
next-wordprediction(auto-regressivelanguagemodeling).Itactsasafoundation
model for further specialization (tuning) on custom datasets.
LLama-3.1 8B Instruct [28] is a fine-tuned version of the previously described
base model using supervised instruction datasets. It is thus optimized for zero-
shot and few-shot prompting settings. Instruct models can also be fine-tuned
further to be more performant on a specific task, but with some challenges (it
is important to carefully adapt it to our task without loosing its instruction-
following ability, ie. catastrophic forgetting).
So in our experiments, for prompting, we used Llama-3.1 8B Instruct, and
for Fine-tuning, we used both Llama-3.1 8B Base and Llama-3.1 8B Instruct.
4 Results and discussion
4.1 Evaluation metrics
When evaluating a binary classification model for vulnerability detection, we
need to carefully interpret the following key evaluation metrics.
Accuracymeasures the overall correctness of predictions, ie. the overall
proportion of correctly classified instances (both Safe and Vulnerable) out of all
instances. While accuracy gives a general sense of model performance, it can
be misleading if the dataset is imbalanced. Since we are working on a balanced
dataset, this is not our case, however, it still does not tell us whether the model
is better at detecting vulnerabilities or avoiding false alarms. In order to answer
these questions, we must calculate other metrics, which follow.
Precisionmeasures how many of the instances predicted as "Vulnerable"
are actually vulnerable. A high precision means that when the model says "this
code is vulnerable", it is usually correct, ie. the model makes fewer false alarms.
Recallmeasures how many of the actual "Vulnerable" instances were de-
tected. A high recall means that the model catches most vulnerabilities.

10 D. Ouchebara and S. Dupont
F1-scoreis the harmonic mean of "Precision" and "Recall", balancing both
metrics. If both avoiding false alarms and detecting as much vulnerabilities as
possible are important, F1-score is the best single metric to consider.
TheROC curve(Receiver Operating Characteristic Curve) is a graphical
representation of a classifier’s performance across different decision thresholds. It
plots the True Positive Rate (TPR) on the y-axis against the False Positive Rate
(FPR) on the x-axis. A classifier with a perfect separation of classes will have a
ROC curve that reaches the top-left corner (with recall close to 1 and precision
close to 1), while a random classifier produces a diagonal line. TheAUC(Area
Under the Curve) is a numerical metric ranging from 0 to 1 calculated from the
ROC curve, which quantifies how well a classifier separates positive and negative
classes. A higher AUC indicates better model performance.
For CVD in a general context, high recall (for vulnerable class) is often
desirable to catch as many vulnerabilities as possible, while keeping precision
(for vulnerable class) high to reduce false alarms. Consequently, we should watch
both metrics to make a good interpretation of how good each model works. F1
score being helpful to find a good trade-off between Recall and Precision, and
AUC being the metric that best summarizes the classification performance of a
model, we will consider these as metrics to globally compare our models.
4.2 Experimental results and discussion
Now that we thoroughly explained our experiments and the way we are evalu-
ating them, we can review Table 1 which presents the results.
As for the baselines, we observe that bothCodeBERTandUniXcoder
models, when fullyfine-tunedon the BigVul training data, have an excellent
ability to detect vulnerabilities reaching a performance of 0.92 F1 score for Code-
BERT and 0.94 F1 score for UniXcoder for "vulnerable" class on the test data.
The detection ability is reduced on PrimeVul dataset to 0.74 and 0.77 F1 score
for CodeBERT and UniXcoder respectively. This is expected since the authors
of the dataset have observed the same behavior over different CodeLMs. This
suggests that the models cannot effectively learn from the more complex and
realistic distribution of vulnerabilities in PrimeVul, which is a more challenging
evaluation environment than most previous benchmarks [8]. We note, however,
that we reach a very good performance compared to the original paper (which
tested other models than those we test in our research).
As for our proposed approaches,Llama-3.1 8B instruct with zero-shot
promptingachieves a medium F1 score of 0.514 for "vulnerable" class with
BigVul data. However, the low precision and recall of "safe" class reveal that this
result is not due to a medium detection capability, but rather to a bias toward
predicting "vulnerable" for most input codes. The same behavior is observed on
PrimeVul data. This could indicate that the model has some knowledge about
whatvulnerabilitiesare(probablygainedfromtheirlargepre-trainingonvarious
typesoftext,amongwhichtextrelatedtocybersecurityaswellascodecorpuses),
but this knowledge is not enough to make precise predictions.

Llama-based source code vulnerability detection 11
Table 1.Performance comparison between baselines and our proposed approaches.
Model Technique Data Accuracy Precision Recall F1 Score
0 1 0 1 0 1 Avg
Baselines
CodeBERT Full Fine-tuningBigVul 0.920 0.91 0.93 0.93 0.91 0.920 0.920 0.920
PrimeVul 0.761 0.73 0.79 0.81 0.70 0.770 0.740 0.760
UniXcoder Full Fine-tuningBigVul 0.943 0.94 0.94 0.94 0.94 0.940 0.940 0.940
PrimeVul 0.770 0.770.77 0.77 0.78 0.7700.7700.770
Our proposed approaches
Llama-3.1 8B instruct Zero shotBigVul 0.399 0.31 0.43 0.16 0.64 0.211 0.514 0.363
PrimeVul 0.510 0.55 0.51 0.11 0.91 0.180 0.650 0.410
Llama-3.1 8B instruct Few shot randomBigVul 0.588 0.56 0.74 0.90 0.28 0.690 0.400 0.590
PrimeVul 0.640 0.60 0.74 0.84 0.44 0.700 0.550 0.640
Llama-3.1 8B instruct Few shot same CWE typeBigVul 0.582 0.55 0.72 0.90 0.27 0.680 0.390 0.540
PrimeVul 0.648 0.60 0.75 0.860.44 0.710 0.560 0.630
Llama-3.1 8B instruct Few shot RAGBigVul 0.700 0.69 0.71 0.73 0.67 0.710 0.692 0.700
PrimeVul 0.670 0.66 0.68 0.70 0.64 0.680 0.660 0.670
Llama-3.1 8B instruct RAG + Test-Time fine-tuningBigVul 0.780 0.82 0.75 0.72 0.84 0.770 0.792 0.780
PrimeVul 0.690 0.74 0.67 0.61 0.78 0.670 0.720 0.690
Llama-3.1 8B instruct Efficient Fine-tuning with QLoRABigVul 0.900 0.88 0.94 0.94 0.86 0.910 0.900 0.900
PrimeVul 0.573 0.57 0.58 0.62 0.53 0.590 0.550 0.570
Llama-3.1 8B base + classification head Efficient Fine-tuning with QLoRABigVul 0.949 0.96 0.94 0.94 0.96 0.950 0.950 0.950
PrimeVul 0.740 0.72 0.77 0.79 0.69 0.750 0.730 0.740
Llama-3.1 8B base + classification head Double fine-tuningBigVul 0.970 0.96 0.98 0.98 0.96 0.970 0.970 0.970
PrimeVul 0.768 0.820.73 0.680.85 0.7500.790 0.770
Double fine-tuning of Llama-base with a classification head yields the best performance of 0.97 F1-score on BigVul, exceeding the
baselines. Zero-shot and Few-shot prompting are overall unsatisfactory, though RAG is relatively performant, whether it is used
for Few-shot prompting or for Test-time fine-tuning, and the latter is the most performant option. Fine-tuning the base model
with a classification head ("classifier fashion") gives better results than fine-tuning the instruct model ("generative fashion").
Overall results on PrimeVul data mostly show the same behavior as on BigVul, except that the best approach (double fine-tuning)
only matches UniXcoder baseline with an 0.770 average F1-score without exceeding it.
Zero-shotpromptingresultsbeingunsatisfactory,wetestedFew-shotprompt-
ing, suggesting that giving examples of vulnerable and safe code to the model
would help make more accurate predictions. With the first strategy where we
randomly sample examples, we indeed observe some improvement, however it
only concerns the previous bias of the model towards predicting "vulnerable"
which is no longer present. The model does not necessarily recognize vulnerable
code better, but it does recognize safe code better, with an F1 score for "safe"
class improved from 0.211 to 0.690 on with BigVul data. The results on Prime-
Vul show the same behavior. Few-shot prompting thus proved helpful. We then
tested the second strategy, suggesting that having examples of the same vulner-
ability type could better help the model recognize the vulnerability in our test
code. This hypothesis was proven wrong, as the performance did not improve
when compared to random sampling on both datasets. The last strategy using
RAG was the most effective one, with an enhanced average F1-score of 0.700
against 0.590 and 0.540 for the first and second strategy respectively on BigVul.
We particularly note a better recall of 0.67 for "vulnerable" predictions and an

12 D. Ouchebara and S. Dupont
overall improved recognition of "safe" code (0.710 F1-score). The same obser-
vation is made on PrimeVul. This suggests that choosing the most similar code
snippets to our test code based on embeddings is a relatively good approach.
We then followed withTest-Time (TT) Fine-tuningusing the examples
retrievedbytheRAGsystem,suggestingitwouldfurtherimprovethecapacityof
the model to benefit from the examples. The detection capacity indeed improved
withanF1-scoreof0.792for"vulnerable"classagainst0.692withFew-shotRAG
on BigVul, and the same observation is made on PrimeVul. This suggests that
using examples to quickly train the model before inference is more effective than
feeding them through the prompt.
To see if performance can be further improved, we studied more "complete"
fine-tuning approaches. We first tested the first fine-tuning approach (genera-
tive fashion) onLlama-3.1 8B instructusingEfficient Fine-tuning using
QLoRA. The expectation was to get better accuracy since the model gets a sup-
plementarytrainingonawholevulnerabilitydetectiondataset.Theperformance
indeed improved significantly, from 0.780 (TT fine-tuning) to 0.900 average F1-
score. This observation is however not made with PrimeVul. We thus tested
the second fine-tuning approach which consists of usingLlama-3.1 8B base
and adding a classification headwhich classifies the code into 0 (safe) or 1
(vulnerable). The performance improved even further, yielding the best results
among all previous experiments, with an F1-score of 0.95, precision of 0.94 and
recall of 0.96 for "vulnerable" class on BigVul. This performance is comparable
to the fine-tuned UniXcoder, rather slightly better (0.95 versus 0.94). What we
can deduce from this is that, for our vulnerability classification task, feeding the
embeddings generated by the fine-tuned LLM into a binary classifier (FFNN)
is more effective than using these embeddings for text generation and observing
the generated text. This conclusion also applies even more to Primevul, where
we observe an importantly improved average F1-score of 0.740 against 0.570.
Further fine-tuning this model at Test-time with examples retrieved by the
RAG system improved the detection capacity to reach an F1-score of 0.970 on
BigVuldata,makingthisfinalDoubleFine-tuningapproachthemosteffective
one. This approach also gives best results on PrimeVul data with an average F1-
score of 0.770, however it only matches the best baseline UniXcoder. Conducting
a root-cause analysis and assessing more datasets would help us justify this gap
and make a more general conclusion as to the effectiveness of the approach. We
still note that it slightly exceeds UniXcoder in terms of F1-score for "vulnerable"
class which is class which interests us most, with 0.79 against 0.77.
It is important to note that all metrics reported above are based on one
dataset split. To further improve the statistic rigor of our experimental evalua-
tion, we will include confidence intervals in our future contributions.
To further analyze our best performing approaches we represented the ROC
curve for our different fine-tuning approaches as well as the fine-tuned Code-
BERT and UniXcoder baselines. Figure 2 shows the comparative ROC curves.
With BigVul data, we observe that our double fine-tuning and one-step fine-
tuning approaches with Llama have near-perfect performance, as they are close

Llama-based source code vulnerability detection 13
Fig. 2.Comparative ROC curves between our best performing models and baselines.
Our double fine-tuned Llama-base with a classification head yields best performance as it is closest to the
perfect classifier in the upper left corner, with an AUC value of 0.997 on BigVul data. This approach also
performs best on PrimeVul data, however it only matches UniXcoder baseline with an AUC of 0.842.
totheupperleftcorner,exceedingthebaselines,wheredoublefine-tuningachieves
an AUC of 0.997 against 0.981 and 0.984 for CodeBERT and UniXcoder, respec-
tively. This means that the model has an excellent capacity to discriminate safe
and vulnerable classes. As for test-time fine-tuning, it is far behind the other
models, which is expected since the model is only fine-tuned for the 6 closest
examples instead of the whole dataset, but it is still a relatively effective model
compared to prompting. On PrimeVul, results show the same behaviour, except
our best model only matches the best baseline UniXcoder (0.842 AUC).
To confirm the impact of fine-tuning on the discrimination capacity of the
model, we can represent t-SNE plots using the embeddings generated by the
model before fine-tuning and after fine-tuning. These embeddings are retrieved
at the last layer before the classifier layers, where we have an embedding vector
of size 4096 for each token in the input sequence. We chose to use the mean of
these embeddings as an embedding for the whole sequence. Figure 3 presents the
t-SNE plots for our best performing models on BigVul before and after tuning, as
well as our other proposed approaches using Llama-instruct. We do not include
PrimeVul plots for a lack of space, but the conclusions are fairly similar.
T-SNE is a dimensionality reduction technique which allows us to represent
high dimensional embeddings in a 2D or 3D space. For a performant classifier,
test samples within the same class should be represented close to each other,
and samples from different classes should be far from each other. As shown by
Figure 3, the only models capable of generating embeddings that are sufficiently
discriminative are the double and the one-step fine-tuned Llama-base with a

14 D. Ouchebara and S. Dupont
Fig. 3.Comparison between t-SNE plots for our proposed models on BigVul dataset.
The only models capable of generating embeddings that are sufficiently discriminative are the double fine-tuned and one-step fine-tuned
Llama-base with a classification head, where we clearly see two groups in the plot (the blue group corresponding to vulnerable samples
and red group corresponding to safe samples).
classification head, where we clearly see two groups in the plot (the blue group
corresponding to vulnerable samples and red group corresponding to safe sam-
ples). There are still a few samples for which the embeddings do not capture
the corresponding class, mainly vulnerable samples which seem to "look like"
safe ones (blue dots in the red group). Further investigation of the actual source
code corresponding to these samples (failure cases) may help us understand the
reason behind this behavior. Comparing the plot before fine-tuning Llama-base,
where we do not see any clear separation between the classes, and after fine-
tuning, implies that fine-tuning on a dataset specific to the CVD task is more
essential to solve the task than possessing general code-related knowledge.
4.3 Takeaways
From these results, we can keep the following takeaways.
Fine-tuning is crucial: Llama performs poorly in untuned, zero-shot or
few-shot settings for vulnerability detection task. This is probably due to the
fact that techniques like zero-shot and few-shot prompting only allow the model
to retrieve information from its past knowledge, such as this information is most
relevant to the task in hand. If this prior knowledge is insufficient to solve the
problem, giving examples will not help in any way more than simply guiding the
modeloutput-formatwise.Fine-tuning,ontheotherhand,allowsforgainingnew

Llama-based source code vulnerability detection 15
knowledge and this is what significantly boosts performance. So, our hypothesis
is that the CVD task is complex enough to require the model to be trained
specifically for this task and on data relevant to it. Nevertheless, a useful result
to retain from our different Few-shot prompting experiments is that RAG seems
to be the best approach for choosing examples and yields relatively good results,
in addition to the fact that Test-Time fine-tuning is an even more effective way
to benefit from the examples rather than simply adding them to the prompt. Of
course, it is important to note that our conclusions are only made regarding the
model and datasets we tested, we still need to experiment on other models and
datasets to state this as a general conclusion about LLMs in CVD.
Llama-3.1 models show strong potential: Our results suggest that a
properlyfine-tunedLlama3.18Bcanmatch,evenexceedcurrentstate-of-the-art
CVD models. Now, one may ask again why such a big sized model is interesting
if its performance only slightly exceed the lighter baselines. The answer is that:
–An LLM like Llama-3.1 is potentially better at reasoning over complex pat-
terns. Unlike UniXcoder, which is strictly trained on code, Llama-3.1 has
been exposed to a wide range of knowledge, which might help in detecting
subtle vulnerability patterns that require contextual understanding. But this
is only a hypothesis which we will later verify by testing the two models on
datasets containing more subtle vulnerabilities than those present in BigVul
and PrimeVul (though PrimeVul has already proven to be quite complex).
–An LLM like Llama-3.1 is a more flexible and versatile model which might
allow us to generalize the solution to a broader security workflow in the
future (e.g., vulnerability reasoning, security report generation, etc.). We
can thus imagine a solution that encompasses the complete vulnerability
management cycle in one tool based on one unique model, which would
be more convenient for users who will not have to juggle between different
softwares. But the real-life applicability of the solution will still need to be
verified first (through manual auditing and CVE rediscovery experiments).
–An LLM-based solution leaves the door open for exploring many recent LLM
techniques,whichareconstantlyevolvingatafasterpacethanthetechniques
proposed in the range of medium sized pre-trained models.
–Finally, if the only point in disfavor of Llama 3.1 8B is its big size compared
to UniXcoder-like models, it is important to note that there is currently
a constant effort in making smaller LLMs as effective as bigger ones. For
instance, Llama3.2 3B is not too far in performance from to Llama3.1 8B
in benchmarks [29], despite being almost three times smaller. There are also
multiple ways to "compress" a model into a smaller one while keeping most
of its capabilities. One such technique is knowledge distillation, which can
for instance be applied to compress Llama-3.1 8B into a Llama-3.1 1B, while
keeping an important percentage of its capabilities [26]. We, however, state
that the hypothesis we make as to the potential efficiency and performance
of these smaller variants of Llama needs to be concretely verified.

16 D. Ouchebara and S. Dupont
5 Conclusion
Through experiments conducted with the Llama-3.1 8B model on BigVul and
PrimeVul dataset, we have demonstrated that LLMs hold significant potential
for vulnerability detection, matching (on PrimeVul) rather even surpassing (on
BigVul) current state-of-the-art models like CodeBERT and UniXcoder. Our
findings highlight that simple prompting techniques such as zero-shot and few-
shot learning are insufficient to extract meaningful CVD capabilities from LLMs
like LLama-3.1 8B, reinforcing the importance of specialized training to achieve
competitive performance. Efficient fine-tuning methods like QLoRA have proven
to be key to optimizing performance while maintaining computational feasibility.
We proposed the novel Double fine-tuning technique, which proved to be the
most performant approach among all those we tested. We also observe that for
our CVD task, the classifier fashion for fine-tuning (binary classifier on top of
the LLM) is more effective than the generative fashion. Finally, while prompting
techniques were ineffective, a useful result to retain is that RAG seems to be
the best approach for example selection. We also suggested a rather unexplored
technique to benefit from the selected examples, which is Test-Time fine-tuning,
and which gave better results than few-shot prompting.
While we have answered a few of our initial research questions, many re-
main to be explored. Future work will focus on improving model interpretabil-
ity through explainability techniques and failure case investigation (per-CWE
analysis), analyzing the cost/scalability concerns for practical deployment of
our solution (LLMs being resource-intensive) in terms of training/inference la-
tency, GPU, carbon footprint and possible optimizations to reduce compute,
experimenting with more advanced prompting techniques, and testing different
datasets (other C/C++ datasets, other programming languages, more interest-
ingly memory-safe languages) to assess the model’s generalization capacity. In
addition to that, more LLMs and alternative architectures, such as CodeLlama,
DeepSeekCoder and emerging Graph LLMs, will be evaluated to determine the
most suitable LLMs for CVD, but also to better point out the value of our
proposed double fine-tuning strategy by making sure to distinguish the gains
stemming from the model from those due to the strategy itself. Beyond these
short-term objectives, we plan to expand our research towards multi-class vul-
nerability classification, and with a bigger vision, to integrating LLMs into a
broader vulnerability management workflow encompassing vulnerability assess-
ment and/or remediation phases. We also plan to broaden the security relevance
dimension of our research by studying the real-world exploitability and SDLC
integration of our solution. Investigating dynamic vulnerability detection meth-
ods using AI, an understudied area in literature, is also another path that we
could explore.
Acknowledgments.This study was funded by CyberExcellence project of CyberWal
program by Digital Wallonia.
Disclosure of Interests.The authors have no competing interests to declare that
are relevant to the content of this article.

Llama-based source code vulnerability detection 17
References
1. Akyürek,E.,Damani,M.,Zweiger,A.,Qiu,L.,Guo,H.,Pari,J.,Kim,Y.,Andreas,
J.: The surprising effectiveness of test-time training for few-shot learning (2025),
https://arxiv.org/abs/2411.07279
2. Bhandari, G., Naseer, A., Moonen, L.: Cvefixes: automated collection of vulner-
abilities and their fixes from open-source software. In: Proceedings of the 17th
International Conference on Predictive Models and Data Analytics in Software
Engineering. pp. 30–39 (2021)
3. Chakraborty, S., Krishna, R., Ding, Y., Ray, B.: Deep learning based vulnerability
detection: Are we there yet? IEEE Transactions on Software Engineering48(9),
3280–3296 (2022). https://doi.org/10.1109/TSE.2021.3087402
4. Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang,
C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P.S., Yang, Q., Xie, X.: A survey
on evaluation of large language models. ACM Trans. Intell. Syst. Technol.15(3)
(Mar 2024). https://doi.org/10.1145/3641289, https://doi.org/10.1145/3641289
5. Chen, Y., Ding, Z., Alowain, L., Chen, X., Wagner, D.: Diversevul: A new vul-
nerable source code dataset for deep learning based vulnerability detection. In:
Proceedings of the 26th International Symposium on Research in Attacks, In-
trusions and Defenses. p. 654–668. RAID ’23, Association for Computing Ma-
chinery, New York, NY, USA (2023). https://doi.org/10.1145/3607199.3607242,
https://doi.org/10.1145/3607199.3607242
6. Cppcheck team: Cppcheck (Oct 2008), https://cppcheck.sourceforge.io/
7. CVE Program team: Published cve records (Jan 2025),
https://www.cve.org/About/Metrics#PublishedCVERecords
8. Ding, Y., Fu, Y., Ibrahim, O., Sitawarin, C., Chen, X., Alomair, B., Wagner, D.,
Ray, B., Chen, Y.: Vulnerability detection with code language models: How far are
we? (2024), https://arxiv.org/abs/2403.18624
9. Du, X., Wen, M., Zhu, J., Xie, Z., Ji, B., Liu, H., Shi, X., Jin, H.: Generalization-
enhancedcodevulnerabilitydetectionviamulti-taskinstructionfine-tuning(2024),
https://arxiv.org/abs/2406.03718
10. Fan, J., Li, Y., Wang, S., Nguyen, T.N.: A c/c++ code vulnera-
bility dataset with code changes and cve summaries. In: Proceed-
ings of the 17th International Conference on Mining Software Repos-
itories. p. 508–512. MSR ’20, Association for Computing Machinery,
New York, NY, USA (2020). https://doi.org/10.1145/3379597.3387501,
https://doi.org/10.1145/3379597.3387501
11. Feng, R., Pearce, H., Liguori, P., Sui, Y.: Cgp-tuning: Structure-aware soft prompt
tuning for code vulnerability detection (2025), https://arxiv.org/abs/2501.04510
12. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B.,
Liu, T., Jiang, D., Zhou, M.: Codebert: A pre-trained model for programming and
natural languages (2020), https://arxiv.org/abs/2002.08155
13. Fu, M., Tantithamthavorn, C.: Linevul: a transformer-based line-level vulnera-
bility prediction. In: Proceedings of the 19th International Conference on Min-
ing Software Repositories. p. 608–620. MSR ’22, Association for Computing Ma-
chinery, New York, NY, USA (2022). https://doi.org/10.1145/3524842.3528452,
https://doi.org/10.1145/3524842.3528452
14. Gao, Z., Wang, H., Zhou, Y., Zhu, W., Zhang, C.: How far have
we gone in vulnerability detection using large language models (2023),
https://arxiv.org/abs/2311.12420

18 D. Ouchebara and S. Dupont
15. Grattafiori, A., Dubey, A., Jauhri, A., Al.: The llama 3 herd of models (2024),
https://arxiv.org/abs/2407.21783
16. Guo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., Yin, J.: Unix-
coder: Unified cross-modal pre-training for code representation (2022),
https://arxiv.org/abs/2203.03850
17. Hanif, H., Maffeis, S.: Vulberta: Simplified source code pre-training for vulner-
ability detection. In: 2022 International Joint Conference on Neural Networks
(IJCNN). pp. 1–8 (2022). https://doi.org/10.1109/IJCNN55064.2022.9892280
18. Harzevili, N.S., Belle, A.B., Wang, J., Wang, S., Ming, Z., Jiang, Nagappan, N.: A
survey on automated software vulnerability detection using machine learning and
deep learning (2023), https://arxiv.org/abs/2306.11673
19. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
L., Chen, W.: Lora: Low-rank adaptation of large language models (2021),
https://arxiv.org/abs/2106.09685
20. Hübotter, J., Bongni, S., Hakimi, I., Krause, A.: Efficiently learning at test-time:
Active fine-tuning of llms (2025), https://arxiv.org/abs/2410.08020
21. Jiang, X., Wu, L., Sun, S., Li, J., Xue, J., Wang, Y., Wu, T., Liu, M.: Investigating
large language models for code vulnerability detection: An experimental study
(2025), https://arxiv.org/abs/2412.18260
22. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler,
H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-
augmented generation for knowledge-intensive nlp tasks. In: Larochelle, H.,
Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Infor-
mation Processing Systems. vol. 33, pp. 9459–9474. Curran Associates, Inc. (2020),
https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-
Paper.pdf
23. Li, Z., Zou, D., Xu, S., Ou, X., Jin, H., Wang, S., Deng, Z., Zhong,
Y.: Vuldeepecker: A deep learning-based system for vulnerability detection.
In: Proceedings 2018 Network and Distributed System Security Symposium.
NDSS 2018, Internet Society (2018). https://doi.org/10.14722/ndss.2018.23158,
http://dx.doi.org/10.14722/ndss.2018.23158
24. Liang, C., Wei, Q., Du, J., Wang, Y., Jiang, Z.: Survey of source
code vulnerability analysis based on deep learning. Computers & Security
148, 104098 (2025). https://doi.org/https://doi.org/10.1016/j.cose.2024.104098,
https://www.sciencedirect.com/science/article/pii/S0167404824004036
25. Liu, Z., Liao, Q., Gu, W., Gao, C.: Software vulnerability detec-
tion with gpt and in-context learning. In: 2023 8th International Con-
ference on Data Science in Cyberspace (DSC). pp. 229–236 (2023).
https://doi.org/10.1109/DSC59305.2023.00041
26. Meta: Distilling llama3.1 8b into 1b in torchtune (Nov 2024),
https://pytorch.org/blog/llama-into-torchtune/
27. Meta: Llama 3.1 8b (Jul 2024), https://huggingface.co/meta-llama/Llama-3.1-8B
28. Meta: Llama 3.1 8b instruct (Jul 2024), https://huggingface.co/meta-
llama/Llama-3.1-8B-Instruct
29. Meta: Llama 3.2 3b (Sep 2024), https://huggingface.co/meta-llama/Llama-3.2-3B
30. Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R.,
Amatriain, X., Gao, J.: Large language models: A survey (2024),
https://arxiv.org/abs/2402.06196
31. Mitre Corporation: Common vulnerabilities and exposures (Sep 1999),
https://cve.mitre.org/

Llama-based source code vulnerability detection 19
32. Mitre Corporation: Common weakness enumeration (Apr 2007),
https://cwe.mitre.org/
33. Neuhaus, S., Zimmermann, T., Holler, C., Zeller, A.: Predicting vulnerable soft-
ware components. In: Proceedings of the 14th ACM Conference on Computer and
Communications Security. p. 529–540. CCS ’07, Association for Computing Ma-
chinery, New York, NY, USA (2007). https://doi.org/10.1145/1315245.1315311,
https://doi.org/10.1145/1315245.1315311
34. Ni, C., Yin, X., Yang, K., Zhao, D., Xing, Z., Xia, X.: Distinguishing look-
alike innocent and vulnerable code by subtle semantic representation learn-
ing and explanation. In: Proceedings of the 31st ACM Joint European Soft-
ware Engineering Conference and Symposium on the Foundations of Software
Engineering. p. 1611–1622. ESEC/FSE 2023, Association for Computing Ma-
chinery, New York, NY, USA (2023). https://doi.org/10.1145/3611643.3616358,
https://doi.org/10.1145/3611643.3616358
35. Nikitopoulos, G., Dritsa, K., Louridas, P., Mitropoulos, D.: Crossvul: a cross-
language vulnerability dataset with commit data. In: Proceedings of the 29th ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering. pp. 1565–1569 (2021)
36. Nong, Y., Aldeen, M., Cheng, L., Hu, H., Chen, F., Cai, H.: Chain-of-thought
prompting of large language models for discovering and fixing software vulnerabil-
ities (2024), https://arxiv.org/abs/2402.17230
37. Serebryany, K., Bruening, D., Potapenko, A., Vyukov, D.: Addresssan-
itizer: A fast address sanity checker. In: USENIX ATC 2012 (2012),
https://www.usenix.org/conference/usenixfederatedconferencesweek/addresssanitizer-
fast-address-sanity-checker
38. Sonarteam:Sonarqube(Dec2008),https://www.sonarsource.com/products/sonarqube/
39. Steenhoek, B., Gao, H., Le, W.: Dataflow analysis-inspired deep learning for effi-
cient vulnerability detection. In: Proceedings of the IEEE/ACM 46th International
Conference on Software Engineering. ICSE ’24, Association for Computing Ma-
chinery, New York, NY, USA (2024). https://doi.org/10.1145/3597503.3623345,
https://doi.org/10.1145/3597503.3623345
40. Steenhoek, B., Rahman, M.M., Roy, M.K., Alam, M.S., Tong, H., Das, S., Barr,
E.T., Le, W.: To err is machine: Vulnerability detection challenges llm reasoning
(2025), https://arxiv.org/abs/2403.17218
41. Sultana, S., Afreen, S., Eisty, N.U.: Code vulnerability detection:
A comparative analysis of emerging large language models (2024),
https://arxiv.org/abs/2409.10490
42. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A.A., Hardt, M.: Test-time
training with self-supervision for generalization under distribution shifts (2020),
https://arxiv.org/abs/1909.13231
43. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave,
E., Lample, G.: Llama: Open and efficient foundation language models (2023),
https://arxiv.org/abs/2302.13971
44. Valgrind team: Valgrind (Jul 2002), https://valgrind.org/
45. Wheeler, D.A.: Flawfinder (Jan 2007), https://dwheeler.com/flawfinder/
46. Xia, Y., Shao, H., Deng, X.: Vulcobert: A codebert-based system for
source code vulnerability detection. In: Proceedings of the 2024 In-
ternational Conference on Generative Artificial Intelligence and Informa-
tion Security. p. 249–252. GAIIS ’24, Association for Computing Machin-

20 D. Ouchebara and S. Dupont
ery, New York, NY, USA (2024). https://doi.org/10.1145/3665348.3665391,
https://doi.org/10.1145/3665348.3665391
47. Zalewski, M.: American fuzzy lop (Nov 2013), https://lcamtuf.coredump.cx/afl/
48. Zhou,X.,Cao,S.,Sun,X.,Lo,D.:Largelanguagemodelforvulnerabilitydetection
and repair: Literature review and the road ahead. ACM Transactions on Software
Engineering and Methodology (2024)
49. Zhou, Y., Liu, S., Siow, J., Du, X., Liu, Y.: Devign: Effective vulner-
ability identification by learning comprehensive program semantics via
graph neural networks. In: Wallach, H., Larochelle, H., Beygelzimer, A.,
d'Alché-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural In-
formation Processing Systems. vol. 32. Curran Associates, Inc. (2019),
https://proceedings.neurips.cc/paper_files/paper/2019/file/49265d2447bc3bbfe9e76306ce40a31f-
Paper.pdf
A Setup and parameters
Table 2 details the parameters used in the experiments. As for the setup, we
used a system with the following resources: RTX 6000 Ada GPU with 40GB of
VRAM, 100GB of RAM, 64GB of disk space.
Table 2.Parameters used for training.
Parameter Value
QLoRA parameters
Quantization 4-bit
Rank (r) 16
Scaling (alpha) 8
Target modules ’q_proj’, ’k_proj’, ’v_proj’, ’o_proj’
Training parameters
N°epochs 4
Batch size 16
Optimizer paged_adamw_32bit
Learning rate 2e-4
Other parameters
Rag retrieval depth 6 examples