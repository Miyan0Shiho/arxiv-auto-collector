# An Agentic AI System for Multi-Framework Communication Coding

**Authors**: Bohao Yang, Rui Yang, Joshua M. Biro, Haoyuan Wang, Jessica L. Handley, Brianna Richardson, Sophia Bessias, Nicoleta Economou-Zavlanos, Armando D. Bedoya, Monica Agrawal, Michael M. Zavlanos, Anand Chowdhury, Raj M. Ratwani, Kai Sun, Kathryn I. Pollak, Michael J. Pencina, Chuan Hong

**Published**: 2025-12-09 14:46:16

**PDF URL**: [https://arxiv.org/pdf/2512.08659v1](https://arxiv.org/pdf/2512.08659v1)

## Abstract
Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.

## Full Text


<!-- PDF content starts -->

 1 An Agentic AI System for Multi-Framework Communication Coding  Bohao Yang1, BS · Rui Yang2, MS · Joshua M. Biro3, PhD · Haoyuan Wang4, MS · Jessica L. Handley3, MA · Brianna Richardson5, BS · Sophia Bessias1, MPH · Nicoleta Economou-Zavlanos1, PhD · Armando D. Bedoya1,6, MD · Monica Agrawal1, PhD · Michael M. Zavlanos6, PhD · Anand Chowdhury7, MD· Raj M. Ratwani3, PhD · Kai Sun7, MD · Kathryn I. Pollak 5,8,#, PhD · Michael J. Pencina1,#, PhD · Chuan Hong1,#*  , PhD 1  Department of Biostatistics and Bioinformatics, Duke School of Medicine, Durham, NC, USA 2 Centre for Quantitative Medicine, Duke-NUS Medical School, Singapore, Singapore 3 Medstar Health National Center for Human Factors in Healthcare, Washington, DC, USA 4  Applied Mathematics and Computational Science, University of Pennsylvania, PA, USA 5 Cancer Prevention and Control Research Program, Duke Cancer Institute, Durham, NC, USA 6  Department of Medicine, Duke School of Medicine, Durham, NC, USA, Durham, NC, USA 7 Department of Mechanical Engineering & Materials Science, Duke University, Durham, NC, USA 8  Department of Population Health Sciences, Duke School of Medicine, Durham, NC, USA #Kathryn I. Pollak, Michael J. Pencina and Chuan Hong are equally contributed 

 2 Correspondence to: Chuan Hong. Email: chuan.hong@duke.edu 

 3 SUMMARY Background  Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. Methods  In this study, we developed an Multi-framewOrk Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC’s outputs against gold-standard annotations created by trained human coders. Findings  

 4 We developed and evaluated MOSAIC using 26 gold-standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 92·8%. Performance was highest in the Rheumatology subset (F1 = 96·2%) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking. Interpretation  MOSAIC demonstrates that agentic, multi-framework AI systems can enable high-fidelity and generalizable annotation of clinical communication at scale. By converting raw transcripts into structured behavioral signals, the system offers infrastructure for quality improvement, shared decision-making evaluation, and equity auditing. As ambient scribe technologies expand across health systems, tools like MOSAIC provide a foundation for transparent, reproducible, and human-guided analysis of clinical conversations.   

 5 INTRODUCTION Clinician-patient communication quality is a modifiable, measurable driver of clinical outcomes, with specific behaviors (e.g., open-question rate, reflection-to-question ratio, interruptions, response latency, and empathic uptake) shaping trust, adherence, and fairness-sensitive endpoints.1,2 Evidence-based communication techniques (e.g., 5 A’s for smoking cessation, Motivational Interviewing) have been found to help patients change behaviors,3 and communication-centered workflows that surface adherence barriers enable targeted problem-solving in chronic care.4 These practices yield downstream benefits, such as improved blood pressure control, modest hemoglobin A1c gains in communication-mediated programs, and lower readmissions with stronger discharge communication.5,6  Clinicians and patients often do not spontaneously improve their communication. They need feedback and a chance to practice new skills to improve. To provide feedback, we need to code their communication. Communication coding assigns predefined labels to specific verbal and nonverbal behaviors at the level of utterances, turns, exchanges, or whole visits, so that rich, real-world interactions become analyzable data.7,8 Using explicit codebooks and decision rules, human coders mark the presence, frequency, quality, and timing of behaviors and then derive metrics, such as counts, rates, durations, and state transitions. Example event-level labels include empathic opportunities and empathic responses,9,10 open-ended questions and reflective statements,1,11 interruptions,12 and shared decision-making moves.13 Global scales summarize relational climate (e.g., respect, warmth, attentiveness, flow).7,14 The resulting time-stamped sequences support research, training, fidelity audits, and quality improvement.15,16 Recently, as the volume of recorded patient–provider interactions grows, the need for scalable, reliable coding 

 6 has become increasingly urgent. For example, a 10-week pilot study reported AI scribe use in over 300,000 patient encounters involving thousands of physicians,17 illustrating how AI scribing fuels both the demand and the opportunity for automated approaches to communication coding, However, scaling communication coding introduces significant operational challenges. Manual multi-framework annotation requires unitizing interactions (e.g., utterance boundaries, speaker roles) and juggling overlapping codebooks with precedence rules, driving cognitive load, coder drift, and workflow inefficiencies. Fine-grained labeling requires double-coding, adjudication, and calibration, making it time- and cost-intensive. Consistency is difficult, especially for rare or ambiguous labels: inter-coder disagreement rises with more frameworks, and codebook updates trigger costly versioning and back-annotation. Boundary decisions and label conflicts reduce reproducibility. These constraints highlight the need for tools that can calibrate uncertainty, defer low-confidence spans to humans, and learn continuously from adjudication feedback. Large language models (LLMs) can generate structured annotations from text, but most existing approaches rely on single-pass prompting without feedback or correction. Embedding LLMs in agentic architectures mitigates this by introducing tools, memory, and goal-directed loops.18 Extending further, multi-agent systems can coordinate specialized LLM-agents across subtasks, an approach well-suited to communication coding, where overlapping frameworks and rare behaviors must be managed in parallel.19,20 However, no multi-agent system has yet been developed for communication coding, and developing such a system remains challenging due to the need for dynamic framework selection, conflict resolution, uncertainty calibration, and adjudication-informed learning. In this study, we developed MOSAIC, a Multi-framewOrk Structured Agentic AI system for Clinical Communication. Designed to meet the growing demand for scalable and reliable 

 7 communication coding, MOSAIC coordinates multiple specialized AI agents to annotate clinical conversations across overlapping frameworks, such as empathy, behavior change, and bias. Unlike existing single-task approaches, MOSAIC dynamically selects the appropriate framework for each segment, reconciles conflicting labels, and flags low-confidence cases for human review. It also learns from adjudicated feedback over time, enabling more consistent and transparent annotation. By combining automation with human oversight, MOSAIC offers a practical and extensible solution for turning large volumes of patient–provider dialogue into structured, actionable insights. METHODS We present the development and evaluation of MOSAIC in three parts: (1) data collection, including encounter recordings, gold-standard annotations, and codebooks (WISER, Global, Intervention, Patient Behavior, and Bias); (2) system architecture, featuring a LangGraph-based multi-agent framework, RAG backbone, dynamic prompting, and human-in-the-loop workflow; and (3) evaluation design, including benchmarks, metrics, sensitivity analyses, and clinician feedback. This study was approved by the Duke University Health System IRB (Pro00115358, Pro00108633). Data Collection  We collected 76 patient–provider audio recordings across two clinical domains (rheumatology and OB/GYN), including 26 for training and 50 for evaluation. Transcripts were generated using an internal Ambient Digital Scribing (ADS) system and reviewed by clinicians for accuracy.21 All transcripts were then manually annotated by trained coders to create gold-standard labels for MOSAIC development and evaluation. 

 8  Codebook Configuration Clinician-provided communication codebooks were reformatted for multi-agent use (Supplementary eTable 1).22 The WISER Codebook captures clinician behaviors and empathy; the Global Codebook assesses relational quality (e.g., flow, warmth, concern expression); and the Intervention Codebook supports fine-grained event-level coding of behavior change strategies (Ask, Advise, Assess, Assist, Arrange). Additional codebooks include Patient Behaviors (e.g., question-asking [AQ], assertiveness [AR]) and Bias, which flags both explicit and subtle cues, stereotyping, guardedness, rapport mismatch, and trust signals, reflecting clinical power dynamics.  Proposed Multi-Agent Framework The MOSAIC framework (Figure 1) employs a LangGraph-based23 multi-agent architecture to automate multi-framework clinical communication coding. The workflow begins with Preprocessing to segment and batch transcripts, which are then processed by the Agentic Core (Plan, Update, Annotation, and Verification Agents) supported by a retrieval-augmented generation (RAG) backbone.24 The system securely interfaces with Duke’s Azure OpenAI GPT‑4o deployment to preserve patient privacy.25 

 9 Preprocessing. Transcripts from our internal ADS system21 are automatically preprocessed to ensure compatibility and preserve context (Supplementary eMethod 1). Utterances are segmented with assigned speaker roles and timestamps. Transcripts exceeding length thresholds are split into fixed-length, timestamp-aligned batches to accommodate LLM context limits (Supplementary eMethod 2 and Supplementary eFigure 1).  Plan Agent. The Plan Agent coordinates the workflow by processing user inputs (transcripts, codebooks, and prompts) and routing them to the appropriate agents. It sends updated codebooks to the Update Agent, transcripts to the Annotation Agent, assigns subtasks, validates inputs, and triggers alerts for errors (e.g., missing data, malformed codebooks; Supplementary eMethod 2).  Update Agent. The Update Agent is responsible for handling new/revised codebooks. When a new codebook is provided, the agent parses and chunks its content, generates embeddings, and rebuilds the corresponding FAISS26 vector index. The retriever is then refreshed to ensure that the Annotation Agent accesses the most up-to-date coding rules and examples. Once the update is complete, the system routes the transcript back to the annotation process for re-coding under the updated framework. If no new codebook is detected, the agent bypasses the update and proceeds with the existing configuration (Supplementary eMethod 2).  Annotation Agent. The Annotation Agent assigns communication codes using codebook-specific agents (e.g., global, clinician, patient, bias). The Plan Agent selects the appropriate agent per task. Each agent initializes a retriever, fetches relevant definitions and examples, and constructs prompts for LLM inference. RAG aligns transcripts with codebooks segmented into semantically coherent chunks (via sliding window), embedded using MiniLM (all-MiniLM-L6-v2)27 and 

 10 stored in a FAISS vector DB. Chunks are retrieved using MMR,28 reranked with MedCPT,29 and filtered to retain only those with valid annotation tags, reducing hallucinations and enforcing label alignment (Supplementary eMethod 2 and eFigure 2).  Verification Agent. The Verification Agent ensures output quality during both training and inference. In training mode, it compares predictions to gold-standard labels and computes metrics to guide prompt and example refinement. In inference mode, it flags low-confidence or inconsistent annotations for human review. In both settings, it generates structured feedback that supports ongoing system optimization (Supplementary eMethod 2).  Feedback Optimization Loop. The Feedback Optimization Loop converts verification metrics into prompt and annotation refinements. For each task, the Plan Agent builds a tailored prompt comprising: (1) system instructions specifying the active codebook, (2) validated annotation labels, (3) RAG-retrieved rule definitions and canonical examples, and (4) few-shot examples from the Example Library RAG, continuously updated via Verification Agent feedback (Supplementary eMethod 2 and eFigure 3). The sentence paired with its surrounding context, human coder label, and agent-generated label were stored in the Example Library as one candidate few-shot examples (collected only from the 26 training transcripts). The examples include both correct matches and contrastive errors, with a precision-weighted policy to reduce overannotation. During training, Verification Agent metrics serve as reinforcement signals to: (1) update prompts, (2) promote high-value examples, (3) prune/reorder those less effective ones, and (4) adapt few-shot retrieval strategies. Evaluation and Performance Metrics 

 11 The MOSAIC framework was evaluated on a combined dataset of 50 transcripts spanning rheumatology and obstetrics/gynecology contexts, with annotations derived from five codebooks (WISER, Global, Intervention, Patient Behavior, and Bias).  To assess the decision-making of the Plan Agent, we conducted a qualitative analysis of its routing performance. We examined its ability to activate appropriate annotation sub-agents based on user inputs and to generate warnings for malformed or ambiguous instructions. Transcripts and prompts representing a range of routing scenarios, including synonymous expressions, vague language, and nonsensical inputs, were used to evaluate robustness and failure models. To assess the annotation performance of MOSAIC, we computed accuracy, precision, recall, and F1-score at both the transcript and codebook levels. Accuracy was defined as the proportion of correctly classified annotation instances out of the total number of annotated instances. All metrics were calculated as weighted averages, where each label’s contribution was proportional to its frequency in the gold-standard annotations. This weighting ensures that more commonly occurring codes have greater influence on the overall metric, while still retaining the ability to capture performance on less frequent codes. For subgroup comparisons (rheumatology vs. obstetrics/gynecology), we first computed performance metrics for each individual transcript and then averaged them within each subgroup. For codebook-specific performance, per-label metrics were aggregated across all transcripts associated with that codebook to provide a behavioral dimension–specific evaluation. To assess the contribution of system components, we evaluated four approaches on the test dataset with 50 transcripts. (1) The Single-Agent Baseline used a single LLM with codebook RAG, static prompts, and canonical examples, without multi-agent orchestration or verification. 

 12 (2) Multi-Agent Without Optimization added Plan, Update, and Verification Agents but retained fixed prompts and codebook-only guidance. (3) Dynamic Prompting further included adaptive prompt assembly and tailored templates, without verification signals. (4) The Full MOSAIC system integrated all components (multi-agent coordination, codebook RAG, dynamic few-shot prompting, task-specific templates, example-library retrieval, and verification-informed feedback). Sensitivity Analysis To assess robustness and design trade-offs, we varied key parameters in a series of experiments. We tested LLM temperatures (0·0, 0·1, 0·3, 0·5, 0·7), balancing stability at lower settings against potentially richer outputs at higher ones. We also compared a single generic prompt template against multiple task-specific templates across sub-agents, evaluating their impact on annotation reliability and contextual accuracy. Cost and Efficiency Analysis To assess potential gains in scalability and efficiency, we compared the time required for manual human annotation versus automated MOSAIC annotation. For human coding, we recorded the average time required to annotate transcripts across varying codebooks and domains, based on input from trained coders involved in the gold-standard label generation. For MOSAIC, we measured end-to-end runtime per transcript, including preprocessing, retrieval, prompt construction, LLM inference, and verification.  

 13 RESULTS We begin by summarizing the dataset, followed by overall MOSAIC performance, transcript- and label-level variability, and a sentence-level comparison to gold annotations. We then benchmark against baselines to assess contributions of RAG, verification, and adaptive prompting. Sensitivity analyses test robustness across parameters. Finally, we present the MOSAIC UI to highlight usability and transparency. Dataset Characteristics The training dataset comprised a total of 26 patient-provider transcripts (Table 1a). Clinical domains included rheumatology (n = 14) and obstetrics/gynecology (n = 12). On average, each transcript contained approximately 18·5 minutes of clean speech and 151·2 speaker turns (median = 141, IQR 97 - 195), with an average of 285·6 sentences (2558·5 words) per transcript. All the 26 transcripts were gold-standard annotated through clinician review and trained coder verification. Across these transcripts, annotations spanned five codebooks (WISER, Global, Intervention, Patient Behavior, and Bias), yielding a total of 271 labeled segments and an average of 10.5 codes per transcript. The testing dataset included 50 gold-standard–annotated transcripts (rheumatology = 33; OB/GYN = 17) (Table 1b). On average, each transcript contained 20 minutes of clean speech, 177·5 speaker turns (median = 145, IQR 98·3–237), 322·4 sentences, and 2871·9 words, yielding a total of 504 labeled segments. The distribution of annotations varied across codebooks in both training and testing datasets. In the training set (Supplementary eTable 2a), WISER was applied to 6 transcripts (103 labeled turns), followed by Patient Behavior (6 transcripts, 57 turns) and Bias (7 transcripts, 48 turns); Intervention and Global codes were less common, appearing in 2 and 5 transcripts, respectively. 

 14 The testing set (Supplementary eTable 2b) showed similar patterns, with WISER applied to 19 transcripts (226 turns), Patient Behavior to 11 transcripts (144 turns), and Bias to 12 transcripts (67 turns), while Intervention and Global codes appeared in 5 (529 turns) and 3 transcripts (358 turns), respectively. Qualitative Evaluation of Plan Agent We qualitatively evaluated the Plan Agent’s routing behavior (Supplementary eResult 1 and Supplementary eTable 3). It reliably mapped clear prompts to correct sub-agents, recognized synonyms (e.g., “empathy” → WISER), and handled malformed inputs. Annotation Performance Overall and Codebook-Specific Performance. MOSAIC achieved an overall accuracy of 93·1% precision of 93·4%, recall of 93·1%, and F1 of 93·0% and the annotation performance varied by coding framework (Figure 2a and Supplementary eTable 4). Patient Behavior coding achieved the highest F1 (95·1%), followed by Bias (94·8%) and WISER for clinician behaviors (94·0%). Moderate performance was observed for Intervention (86·3%), while Global relational quality showed the lowest F1 (83·1%).  Category-Specific Performance. Performance remained strong across clinical domains, with rheumatology transcripts reaching an F1 of 96·2%, and obstetrics/gynecology transcripts showing moderately lower performance with an F1 of 86·8% (Figure 2b and Supplementary eTable 4).  Transcript-Level Variability. Transcript-level performance variability revealed that MOSAIC consistently maintained high annotation quality across various clinical encounters, with a median 

 15 F1 of 0·964 and tight interquartile ranges across all metrics (Figure 2b and Supplementary eTable 4). Rheumatology transcripts exhibited slightly higher median performance than obstetrics/gynecology, likely reflecting more structured dialogue patterns. A small number of transcripts showed performance dips, particularly in recall, corresponding to longer encounters or skewed label distributions. Comparison With Baselines and Ablation Variants. We compared MOSAIC against a baseline configuration and three ablation variants (Figure 3). The single-agent baseline achieved a weighted F1 of 85·9%, while the automated multi-agent setup where the Plan Agent automatically selected the appropriate codebook, performed comparably at 85·7%. Adding dynamic few-shot prompting improved performance further (F1 = 89·5%). The full MOSAIC system achieved the best performance (F1= 93·0%), representing a relative improvement of 7·9% over the single-agent baseline.  Sensitivity Analyses We selected 0·3 as the default temperature and evaluated system performance across parameter configurations. At lower temperatures (≤ 0·1), the model generated highly consistent outputs, showing limited flexibility (e.g., temperature = 0·0, F1 = 90·2%). Higher temperatures (0·3 to 0·7) occasionally captured additional nuanced cases (temperature = 0·3, F1 = 92·8%; temperature = 0·5, F1 = 91·9%; temperature = 0·7, F1 = 91·7%) but tended to miss certain original content. These results show robust performance, with 0·3 yielding the best balance. Cost and Efficiency Analysis 

 16 We empirically compared annotation time between human coders and the MOSAIC system. Human annotation required 60–90 minutes per 30-minute recording (approximately 20 minutes of clear speech), reflecting 2–3 times audio length and including double-coding of 10% of transcripts. In contrast, MOSAIC completed end-to-end annotation, including preprocessing, retrieval, prompting, and verification, in 2–5 minutes per transcript. Unlike human workflows, MOSAIC supports automated, reproducible re-annotation: the Update Agent detects codebook changes, refreshes the retrieval index, and re-runs the pipeline, enabling scalable and iterative deployment. User Interface To support end-to-end use, we developed a secure Gradio-based web interface (Supplementary eFigure 4) for uploading transcripts, configuring coding parameters, and reviewing annotations. Users can select codebooks, adjust advanced settings (e.g., retrieval depth, chunk size), and trigger the MOSAIC pipeline with one click. Outputs are viewable in editable formats (text, .docx, preview), with speaker turns, codes, and references. The interface enables manual edits, logs corrections, and updates the Example Library.  DISCUSSION In this study, we developed MOSAIC, a LangGraph-based multi-agent framework for annotating clinical communication. It includes a Plan Agent to coordinate workflows, an Update Agent to refresh retrieval databases with codebook revisions, a set of Annotation Agents that apply codebook-guided RAG with rule-based definitions, and a Verification Agent ensures label consistency with humans. Reliability is enhanced by dynamic few-shot prompting from an evolving example library.  

 17 MOSAIC achieved strong overall performance (F1 = 93·0%), exceeding typical human inter-rater agreement reported in communication coding studies (75–90%) and matching or surpassing trained coder reliability for many annotation tasks.1,30 For example, in more structured domains like rheumatology, MOSAIC reached an F1 score of 96·2%. Although variability persisted, particularly in OB/GYN transcripts and Global codes, these patterns mirrored known challenges in human annotation, underscoring the system's clinical relevance. Performance gains were especially notable in challenging categories like bias, where multi-agent coordination, codebook-guided retrieval (RAG), and dynamic few-shot prompting improved recall and label balance. Sensitivity analyses identified temperature as a key factor: while lower settings (≤ 0·1) yielded consistent but rigid outputs (e.g., temperature = 0·0, F1 = 90·2%), moderate values (0·3 to 0·7) improved nuance capture (e.g., temperature = 0·3, F1 = 92·8%; 0·5, F1 = 91·9%; 0·7, F1 = 91·7%) at the cost of some omissions. Additionally, retrieval caching and prompt reuse reduced latency by approximately 25%. Together, these findings demonstrate that MOSAIC delivers human-level annotation quality with structured orchestration and retrieval-informed design, offering a scalable and robust alternative to single-pass prompting. To improve interpretability, we evaluated MOSAIC at the sentence level using eight-turn context. Errors, particularly in Bias and Intervention labels, often arose from limited context, revealing a trade-off between granularity and completeness. While performance declined relative to transcript-level runs, sentence-level analysis revealed precision gains and underscored the potential of audio cues and optimized chunking strategies for future improvement. Building on these observations, several patterns emerged with representative examples shown in Table 2. In successful cases, MOSAIC not only matched human annotations but also recovered missed labels, especially in the WISER and Intervention frameworks. It correctly identified 

 18 subtle empathic or reflective statements that human coders overlooked, demonstrating sensitivity to relational and affective cues. However, challenging cases, particularly in the Global and Bias frameworks, exposed limitations of text-only input. Discrepancies often involve categories like Attentive vs. Concerned or Guarded–Open, where distinctions rely on paralinguistic features such as tone, emphasis, and speech tempo. These findings highlight the need for multimodal extensions that incorporate acoustic and temporal features to capture interpersonal nuance. MOSAIC thus serves a dual role: as a scalable annotator and a diagnostic tool that flags ambiguous or voice-dependent cases. This capability may improve transparency and guide targeted human review. Future iterations will integrate audio features and human-in-the-loop verification to better capture empathy, communication flow, and bias-related behaviors. Several broader limitations emerged. First, the current chunking strategy may not optimally preserve semantic coherence across conversational boundaries, potentially affecting recall. Second, our retrieval pipeline using MiniLM embeddings and MedCPT reranking could likely be improved with newer, domain-specific models. Third, the study’s focus on rheumatology and obstetrics/gynecology limits generalizability to other settings. Fourth, some codebooks (e.g., SDOH, Weight) were included in RAG retrieval but lacked sufficient gold-standard labels for evaluation. The dataset was also highly imbalanced, with most utterances labeled “None,” which may skew metrics and reduce sensitivity to rare yet clinically important behaviors. Global coding underperformed, reflecting its dependence on prosodic cues unavailable in text-only data. Model performance remains sensitive to codebook quality and LLM variability, including hallucinations and sampling effects. Practical constraints include latency under multi-agent parallelism and the need for cache invalidation to prevent stale retrieval. Finally, race and ethnicity data were unavailable, precluding subgroup analyses. Although MOSAIC does not use race-based inputs, 

 19 sociocultural and structural factors may influence communication behaviors. Future work should evaluate generalizability across diverse populations and incorporate fairness-aware methods into system design. Future work will target four areas: methodology, evaluation, generalization, and optimization. We will refine chunking and embedding strategies to preserve conversational coherence, explore reinforcement learning (e.g., joint training, reward shaping) to enhance agent coordination and prompt adaptation, and incorporate multimodal inputs (e.g., tone, rhythm) for codes dependent on relational or affective signals such as Global and Bias. Evaluation efforts will assess whether clearer prompts improve categories like Intervention, test the Verification Agent’s effectiveness, and examine the use of LLMs as judges for converting annotations into decision-support insights. We aim to generalize MOSAIC to additional specialties, languages, and underrepresented codes (e.g., SDOH, weight).  CONCLUSION MOSAIC demonstrates that an agentic, multi-agent framework can transform clinical communication coding into a scalable, accurate, and transparent process. By combining retrieval-augmented generation, structured verification, and adaptive prompting strategies, it enhances consistency across diverse coding frameworks while retaining human-in-the-loop oversight. As AI scribes and ambient documentation tools increasingly generate vast volumes of clinical dialogue, systems like MOSAIC provide critical infrastructure to extract structured, trustworthy insights. This capability not only supports quality improvement, training, and research but also enables real-time feedback loops that promote shared decision making and foster high-quality care across clinical settings.  

 20 REFERENCES 1 Moyers TB, Rowell LN, Manuel JK, Ernst D, Houck JM. The motivational interviewing treatment integrity code (MITI 4): rationale, preliminary reliability and validity. Journal of substance abuse treatment. 2016 Jun 1;65:36–42. 2 Pollak KI, Arnold RM, Jeffreys AS, et al. Oncologist communication about emotion during visits with patients with advanced cancer. Journal of Clinical Oncology. 2007 Dec 20;25(36):5748–52. 3 Lawson PJ, Flocke SA, Casucci B. Development of an instrument to document the 5A's for smoking cessation. American journal of preventive medicine. 2009 Sep 1;37(3):248–54. 4 Cornelius T, Voils CI, Umland RC, Kronish IM. Validity of the self-reported domains of subjective extent of nonadherence (DOSE-nonadherence) scale in comparison with electronically monitored adherence to cardiovascular medications. Patient preference and adherence. 2019 Oct 3:1677–84. 5 Huang X, Xu N, Wang Y, Sun Y, Guo A. The effects of motivational interviewing on hypertension management: a systematic review and meta-analysis. Patient education and counseling. 2023 Jul 1;112:107760. 6 Becker C, Zumbrunn S, Beck K, Vincent A, et al. Interventions to Improve Communication at Hospital Discharge and Rates of Readmission: A Systematic Review and Meta-analysis. JAMA Network Open. 2021 Aug 27;4(8):e2119346–e2119346. 7 Roter D, Larson S. The Roter interaction analysis system (RIAS): utility and flexibility for analysis of medical interactions. Patient education and counseling. 2002 Apr 1;46(4):243-51. 8 Barton Laws M, Magill M, Mastroleo NR, et al. A sequential analysis of motivational interviewing technical skills and client responses. Journal of Substance Abuse Treatment. 2018 Sep 1;92:27–34. 9 Malhotra C, Kanesvaran R, Krishna L, et al. Oncologists’ responses to patient and caregiver negative emotions and patient perception of quality of communication: results from a multi-ethnic Asian setting. Supportive Care in Cancer. 2018 Mar;26(3):957–65. 10 Shen MJ, Ostroff JS, Hamann HA, et al. Structured analysis of empathic opportunities and physician responses during lung cancer patient-physician consultations. Journal of health communication. 2019 Sep 2;24(9):711–8. 11 Miller WR, Rollnick S. Motivational Interviewing. Guilford Publications; 2023. 354 p. 12 Marvel MK, Epstein RM, Flowers K, Beckman HB. Soliciting the patient's agenda: have we improved?. Jama. 1999 Jan 20;281(3):283–7. 13 Krupat E, Frankel R, Stein T, Irish J. The Four Habits Coding Scheme: validation of an 

 21 instrument to assess clinicians’ communication behavior. Patient education and counseling. 2006 Jul 1;62(1):38–45. 14 Tanana M, Hallgren KA, Imel ZE, Atkins DC, Srikumar V. A comparison of natural language processing methods for automated coding of motivational interviewing. Journal of substance abuse treatment. 2016 Jun 1;65:43–50. 15 Chen Z, Gibson J, Chiu MC, et al. Automated empathy detection for oncology encounters. In2020 IEEE International Conference on Healthcare Informatics (ICHI) 2020 Nov 30 (pp. 1-8). IEEE. 16 Butow P, Hoque E. Using artificial intelligence to analyse and teach communication in healthcare. The breast. 2020 Apr 1;50:49–55. 17 Tierney AA, Gayre G, Hoberman B, et al. Ambient artificial intelligence scribes to alleviate the burden of clinical documentation. NEJM Catalyst Innovations in Care Delivery. 2024 Feb 21;5(3):CAT–23. 18 Luo J, Zhang W, Yuan Y, et al Large language model agent: A survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460. 2025 Mar 27. 19 Li J, Zhang Q, Yu Y, Fu Q, Ye D. More agents is all you need. arXiv preprint arXiv:2402.05120. 2024 Feb 3. 20 Topsakal O, Akinci TC. Creating large language model applications utilizing langchain: A primer on developing llm apps fast. InInternational conference on applied engineering and natural sciences 2023 Jul 10 (Vol. 1, No. 1, pp. 1050–1056). 21 Wang H, Yang R, Alwakeel M, et al. An evaluation framework for ambient digital scribing tools in clinical applications. npj Digital Medicine. 2025 Jun 13;8(1):358. 22 Stortenbeker I, Salm L, Olde Hartman T, Stommel W, Das E, van Dulmen S Coding linguistic elements in clinical interactions: a step-by-step guide for analyzing communication form. BMC medical research methodology. 2022 Jul 11;22(1):191. 23 GitHub [Internet]. [cited 2025 Aug 31]. GitHub - langchain-ai/langgraph: Build resilient language agents as graphs. Available from: https://github.com/langchain-ai/langgraph 24 Lewis P, Perez E, Piktus A, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems. 2020;33:9459–74. 25 Azure OpenAI in Azure AI Foundry Models - Azure OpenAI [Internet]. [cited 2025 Aug 31]. Available from: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models 26 Douze M, Guzhva A, Deng C, et al. The faiss library. IEEE Transactions on Big Data. 2025 Oct 13. 

 22 27 Wang W, Wei F, Dong L, Bao H, Yang N, Zhou M. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems. 2020;33:5776–88. 28 Carbonell J, Goldstein J. The use of MMR, diversity-based reranking for reordering documents and producing summaries. InProceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval 1998 Aug 1 (pp. 335–336). 29 Jin Q, Kim W, Chen Q, et al. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. Bioinformatics. 2023 Nov 1;39(11):btad651. 30. Imel ZE, Pace BT, Soma CS, et al. Design feasibility of an automated, machine-learning based feedback system for motivational interviewing. Psychotherapy. 2019 Jun;56(2):318.            

 23 TABLES Table 1. Summary characteristics patient-provider transcripts  a. Training dataset Characteristic Value Total transcripts 26  Clinical domains Rheumatology (n = 14); Obstetrics/Gynecology (n = 12) Total speech time ~ 8 hours Average transcript length Approximately 18·5 minutes (SD ~ 4min) Sentences per transcript Mean = 285·62 (SD = 116·76; range 129 - 517); Median = 272 (IQR 194·25 - 337·5) Words per transcript Mean = 2558·5 (SD = 1045·00; range 943 - 4783); Median = 2381 (IQR 1726·75 - 3180·25) Speaker turns per transcript Mean = 151·15 (SD = 70·77; range 76 - 330);  Median = 141 (IQR 97 - 194·75) Gold-standard annotated transcripts 26 Codebooks applied WISER, Global, Intervention (5As), Patient Behavior, Bias Total annotated turns with labels (from human annotation) 271 speaker turns Annotated turns with labels per transcript (from human annotation) Mean = 10·54 (SD = 7·89; range 1 - 31);  Median = 8·50 (IQR 6 - 13·25)    Total turns annotated as None (from human annotation) 5819 speaker turns 

 24 Turns annotated as None per transcript (from human annotation) Mean = 223·81 (SD = 101·23; range 70 - 442);  Median = 218·5 (IQR 152·75 - 288·5)    b. Testing dataset Characteristic Value Total transcripts 50 Clinical domains Rheumatology (n = 33); Obstetrics/Gynecology (n = 17) Total speech time ~16·5 hours Average transcript length Approximately 20 minutes (SD ~4min) Sentences per transcript Mean = 322·36 (SD = 167·51; range 67 - 809); Median = 313 (IQR 183 - 412·25) Words per transcript Mean = 2871·88 (SD = 1445·57; range 602 - 5525); Median = 2913 (IQR 1518·25 - 3635·75) Speaker turns per transcript Mean = 177·48 (range 39 - 376);  Median = 145 (IQR 98·25 - 237) Gold-standard annotated transcripts 50 Codebooks applied WISER, Global, Intervention (5As), Patient Behavior, Bias Total annotated turns with labels (from human annotation) 504 speaker turns Annotated turns with labels per transcript (from human annotation) Mean = 10·08 (SD = 6·84; range 1 - 31);  Median = 8 (IQR 5·25 - 12·75)   

 25 Total turns annotated as None (from human annotation) 14683 speaker turns Turns annotated as None per transcript (from human annotation) Mean = 293·66 (SD = 167·96; range 38 - 588);  Median = 314·5 (IQR 124·75 - 443·75)      

 26 Table 2. Representative examples of single-label analysis Case Type Codebook Excerpt (Context) Target Sentence Human Annotation Agent Annotation (MOSAIC) Notes Successful Case 1 WISER Clinician: “Do you have a boy or girl at home?” Patient: “Girl.” Clinician: “A girl.” “A girl.” — (missed) Reflective Statement MOSAIC correctly detected a Reflective  Statement missed by the human coder. 
Successful Case 2 WISER Patient: “Somebody I wanted to tell I was pregnant died before I could tell them.” Clinician: “Oh, I’m sure they know… they’re watching down on you.” “Oh, I’m sure they know… they’re watching down on you.” — (missed) Reflective Statement Correctly identified Empathic Statement missed by human. 
Successful Case 3 Intervention (5As) Clinician: “Are you currently smoking any tobacco products?” Patient: “I’ve been doing patches off and on and still smoke, but much less now.” “Are you currently smoking any tobacco products?” ASK-START ASK-START Perfect alignment; correct identification of Ask phase initiation. Successful Case 4 Patient Behavior Patient: “I struggle with headaches… I think some of it is from the plaquenil.” “I struggle with headaches… I think some of it is from the plaquenil.” Affective Response Affective Response Perfect agreement; correctly captured Affective Response. Challenging Case 1 (Needs Voice) Global Clinician: “OK, sometimes you can try some vitamins… magnesium, melatonin, riboflavin.” “OK, sometimes you can try some vitamins…” Concerns: 4 Attentive: 4 Both labels plausible; distinction depends on tone and emphasis, highlighting the need for audio input. 

 27 Challenging Case 2 (Needs Voice) Bias Clinician: “So, a diagnostic test is where you go into where the baby is and you’re getting chromosomes… there’s a small chance of miscarriage.” “So, a diagnostic test is where you go into where the baby is…” Rushed: 4 — (missed) Speech tempo and intonation affect judgment; requires audio for accurate detection. Challenging Case 3 (Needs Voice) Bias Clinician: “Do you think he would be willing to do that?” Patient: “Not really. No, I just have my fiancé…” “Not really. No, I just have my fiancé…” Guarded–Open: 4 — (missed) Requires prosodic cues (hesitation, tone) to differentiate guardedness vs openness.    

 28 FIGURE LEGENDS  Figure 1. Overview of the MOSAIC workflow. Input data include user-provided dialogue transcripts and versioned clinician codebooks. Transcripts are preprocessed into structured, timestamp-aligned segments, then passed to the Agentic Core. The Plan Agent coordinates task routing and triggers the Update Agent when codebooks change. The Annotation Agent uses retrieval-augmented generation (RAG) to assign communication codes based on the codebooks. The Verification Agent compares outputs to gold-standard labels and provides feedback, which drives a feedback optimization loop. A dynamic Example Library RAG connects the Verification and Annotation Agents by incorporating adjudicated examples to iteratively refine few-shot prompts and improve annotation accuracy.
 


 29 Figure 2. Annotation performance across codebooks and clinical domains. (a) Overall and Codebook-Specific Annotation Performance. Bar chart showing overall and codebook-specific accuracy, precision, recall and F1 scores for five codebooks (WISER, Global, Intervention, Patient Behaviors, and Bias). (b) Category-Specific Annotation Performance. Distribution of MOSAIC’s annotation performance across rheumatology and obstetrics/gynecology. 
       


 30 Figure 3. Performance Comparison of MOSAIC With Baselines and Ablation Variants.  Grouped bar chart showing F1 scores for different system configurations, including the single-agent baseline (codebook RAG only, static instructions, no example-library retrieval, no verification, no dynamic prompting), multi-agent with codebook RAG alone, few-shot examples, dynamic few-shot prompting and complete MOSAIC system. 
       


 31 Supplementary Materials  Supplement to: An Agentic AI System for Multi-Framework Communication Coding   Table of contents Content                                                                                                                                                                      Page Supplementary eMethods                                                                                                                33                                                                                                              eMethod 1. Audio Data Transforming and Internal ADS System                                                                                   33 eMethod 2. Details of The Proposed Multi-Agent Framework (MOSAIC)                                                               34 eMethod 3. User Interface                                                                                                                  38 eMethod 4. The Annotation Performance Calculation of MOSAIC                                                                             39 Additional Results                                                                                                         40                                                                                                                                                        eResult 1. Qualitative Evaluation of The Plan Agent                                                                                                   40 eResult 2. Qualitative Analysis on Labelling Details                                                                                                     41  eResult 3. Qualitative Analysis on Number of Sentences, Words, and Speaker Turns                                                    42 Supplementary eTables                                                                                                                                                43 eTable 1. Codebook and representative examples                                                                                                                  43 eTable 2. Codebook-specific characteristics of patient-provider transcripts.                                                                                      44 eTable 3. Representative prompt examples and results of qualitative evaluation of the Plan Agent                                         45   eTable 4. Overall, category-specific and codebook-specific performance of MOSAIC                                                                47 eTable 5. Qualitative analysis on labelling details                                                                                                                     48 eTable 6. Results of OLS regression results for F1 score by department and total number of sentences                    50 eTable 7. Results of OLS regression results for F1 score by department and total number of words                              52 eTable 8. Results of OLS regression results for F1 score by department and total number of speaker turns                         54 eTable 9. Summary across models                                                                                                                              56 Supplementary eFigures                                                                                                         57 

 32 eFigure 1. Date preprocessing through internal ADS tool and chunking                                                                                            57 eFigure 2. RAG techniques in the Annotation Agent                                                                                                  58 eFigure 3. Example Library RAG techniques in Dynamic Prompting Strategy                                                   59 eFigure 4. User interface                                                                                                                               60 eFigure 5. The comparison of different regression analysis on F1 vs. text length                                                                       62 References                                                                                                                                                                                      63                

 33 Supplementary Methods eMethod 1. Audio Data Transforming and Internal ADS System The original version of our internally developed ADS system has been described previously.1 In this study, we used the system as the foundation and further refined its components to enhance robustness. The pipeline integrates voice activity detection (VAD), audio-to-text transcription, speaker diarization, and LLM-based structured note generation. During VAD, clinician–patient speech intervals were identified,2  using a combination of energy-based detection,3 Silero VAD,4 and SpeechBrain,5 with majority voting to extract accurate segments. Clinical audio was transcribed with OpenAI Whisper-turbo.6 Speaker diarization relied on a voice-embedding and clustering approach via Pyannote,7 with detected segments aligned to Whisper transcriptions at sub-second resolution. Speaker role assignment (clinician vs. patient) was further processed based on the reasoning ability of GPT-4o8 with the self-consistency prompting,9 where the model was independently prompted 10 times using representative utterances for each speaker. The final speaker roles would then be determined via majority voting across the 10 predictions. This process enabled the generation of structured transcripts that include timestamps (1 min per block), speaker roles, utterances, and silence notations. All processing of our internal system was conducted within either a secure institutional virtual machine environment or local environment to ensure clinical data security and patient privacy. Personally identifiable information was removed or masked, ensuring compliance with standard clinical data governance practices.   

 34 eMethod 2. Details of The Proposed Multi-Agent Framework (MOSAIC) This system was implemented based on LangGraph.10 Each “Agent” corresponded to one or more Nodes and routing was handled by typed Edges based on the evolving State.  LangGraph Primitives. The LangGraph orchestration proceeds through a structured sequence of modular nodes that dynamically adapt to task state and error conditions. The process begins at the Plan Node, which interprets the input transcript, identifies required annotation frameworks, and determines whether codebook updates or downstream verifications are necessary. If the “codebook update” flag is True, control is passed to the Update Node, which refreshes or augments rule sets and few-shot examples before routing execution to the Annotation Node. Otherwise, the pipeline proceeds directly from planning to annotation. The Annotation Node performs the core workload, dispatching annotation subtasks across multiple agents or frameworks with bounded parallelism, aggregating results, and updating the shared state. If the “run verification” flag is True, outputs are then passed to the Verify Node, where automatic consistency checks, metric evaluations, or rule-based audits are conducted. When verification is not required, the process continues directly to completion. If any node raises an error or produces an “error message”, control is redirected to the Feedback Node, which logs diagnostic information, surfaces user-facing feedback, and optionally triggers corrective suggestions or re-runs. Finally, all execution paths converge at the End Node, ensuring deterministic closure of the workflow and safe serialization of the final state. This design enables modular retries, conditional branching, and human-in-the-loop feedback while maintaining idempotent and fault-tolerant execution across all nodes. Data Preprocessing. After transforming the raw audio data into structured transcripts using our internal ADS tool, the processed transcript text can be used for downstream analysis. Once the user uploads the transcript into the MOSAIC system, it will then perform chunking and segmenting transcripts into manageable units (Supplementary eFigure 1). This step ensures that subsequent annotation tasks operate on semantically coherent and temporally aligned segments.  Plan Agent. The Plan Agent serves as the central controller within the MOSAIC, responsible for ingesting diverse input data sources, including audio transcripts, optionally updated or newly 

 35 uploaded codebooks, and gold-standard transcripts containing reference labels for direct verification. Acting as the orchestration layer, the agent manages data preprocessing, routing, and task assignment for all downstream annotation nodes. During routing, the Plan Agent maps each transcript segment to one or more relevant coding standards using clinician-predefined coding definitions and logic.  In the task assignment phase, the agent determines whether the codebook has been updated; if an update is detected, control is routed to the Update Agent, whereas unchanged inputs proceed directly to the Annotation Agent. It then populates the "requested annotations" state with the selected codebooks and delegates each subtask to a corresponding Annotation Sub-Agent specialized in that domain. Finally, the Plan Agent implements robust error-handling mechanisms to ensure workflow stability, validating input completeness, detecting malformed or inconsistent codebooks, and logging descriptive diagnostic messages in the "error" state. Overall, the Plan Agent functions as an intelligent dispatcher, dynamically coordinating the flow of annotation tasks, optimizing data routing, and ensuring transparent, fault-tolerant execution across the multi-agent pipeline. Update Agent. The Update Agent is responsible for detecting and processing newly uploaded or modified codebooks within the MOSAIC workflow. Upon identifying an update, the agent automatically refreshes the underlying codebook vector databases and retrieval modules to incorporate the revised coding definitions, examples, or rules, without requiring manual system reconfiguration. This dynamic update mechanism ensures that all subsequent annotations leverage the most current and contextually accurate codebook representations. Once the retrieval context has been successfully refreshed, the Update Agent triggers the Annotation Agent to initiate annotation or continue downstream processing using the updated framework, thereby maintaining consistency and adaptability across the multi-agent pipeline. Annotation Agent (with Specialized Sub-Agents). For each segmented transcript chunk and each requested coding framework, the Annotation Agent executes a RAG-prompted large language model (LLM) call that integrates retrieved and reranked contextual rules and few shot examples derived from our training results from the Verification Agent feedback (Supplementary eFigure 2).  All the Specialized Sub-Agents are executed via Azure OpenAI GPT- 4o. The final prompt concatenates retrieved rules, chunked transcript segments, few-shot examples, and the corresponding label schema. Each sub-agent specializes in a distinct annotation framework based 

 36 on a domain specific clinician-provided codebook (Supplementary eTable 1). The WISER Sub-Agent identifies empathic opportunities expressed by patients and classifies clinician responses such as Empathic Responses, Empathic Statements, Sorry Statement, Open-ended Questions, Reflective Statements, and Elicit Questions. The Patient Behavior Sub-Agent captures patient-initiated behaviors, including question-asking and assertive responses that indicate agency or preference expression. The Global Sub-Agent automatically rates relational qualities, such as Flow, Respect, Warmth, Attentiveness, and Concern, on a 1–5 scale for each encounter or dialogue segment. The Intervention (5As) Sub-Agent detects discrete behavioral intervention steps (Ask, Advise, Assess, Assist, Arrange). The Bias Sub-Agent identifies subtle and explicit bias cues, including Judgement, Stereotyping, Tailoring, Interrupting, Establishing Rapport, and Mismatched Rapport, along with broader relational dynamics, and trust or distrust expressions. Finally, the SDOH and Weight Sub-Agent, which detects discussions of social determinants of health, such as financial stress, housing, food insecurity, and safety, as well as weight-related topics and potential judgement language regarding their initiation or framing. This sub-agent was set up in our user interface but was not used during current model testing due to the small amount of relevant data. In our future work we will include more of these data and continue to contribute to this work.  Codebook Retrieval Augmented Generation (RAG) Module. The Codebook RAG module enables contextual retrieval of domain-specific coding rules and definitions from clinician-provided codebooks. Each codebook (e.g., WISER and Bias) is semantically chunked into rule-level segments and embedded using all-MiniLM-L6-v211 to create a FAISS-based vector store.12 During annotation, relevant text segments from the transcript are used as queries to retrieve the most semantically similar rules through a hybrid retrieval strategy combining Maximal Marginal Relevance (MMR) and MedCPT re-ranking.13 This ensures both diversity and clinical relevance in the retrieved content. The resulting set of retrieved rules and few-shot examples are then injected into the final prompt to guide the Sub-Agent, allowing it to align its coding behavior with clinician-authored definitions and ensure consistency across frameworks.  Example Library RAG Module for Dynamic Prompting Strategy. Example Library RAG Module for Dynamic Prompting Strategy. The Example Library RAG provides a mechanism for dynamic prompt optimization based on past annotation outcomes (Supplementary eFigure 3). Verified 

 37 examples, derived from comparisons between gold-standard annotations and model-generated outputs, are aggregated and semantically chunked to form an evolving example library. These examples are refreshed through the Verification Agent, which identifies mismatched sentences and corresponding contexts and filters representative examples for retraining or prompt refinement. The refreshed examples are re-embedded and stored in a FAISS vector database using hybrid sparse–dense retrieval to support continual improvement of few-shot prompting. This iterative loop, linking Annotation, Verification, and Example Library RAG, enables the system to adaptively refine its behavior through human-in-the-loop feedback, enhancing both precision and robustness in dynamic clinical coding tasks. Verification Agent. The Verification Agent serves as the system’s quality gatekeeper and Example Library updating manager, operating in two complementary modes. In training mode, it compares Annotation Agent outputs with gold-standard labels to compute performance metrics, such as accuracy, precision, recall, and F1, and uses these as qualitative references to optimize few shot examples in the Example Library. In new transcript mode , the agent performs an independent verification pass, and flags low-confidence or potential inconsistent annotations for clinician review, ensuring continuous quality improvement across the entire pipeline.    

 38 eMethod 3. User Interface MOSAIC provides a web-based graphical interface designed to streamline annotation and verification workflows for clinical communication analysis. The Annotation Panel allows users to upload transcripts, select one or more Sub-Agents (e.g., WISER, Bias, Intervention, Global, Patient Behavior), and optionally provide custom codebooks to override default configurations. Uploaded transcripts are automatically processed through the selected frameworks, and users can view, edit, or download the resulting annotated outputs (Supplementary eFigure 4a).  The Verification Panel supports model evaluation against gold-standard references by allowing upload of gold-labeled transcripts. Upon execution, the interface displays computed performance metrics, including accuracy, precision, recall, and F1, as well as detailed mismatch reports at both sentence and label levels. Users can preview the first 50 rows of comparison results directly in the browser or download comprehensive reports (Supplementary eFigure 4b). Together, these modules provide an intuitive, transparent interface for managing annotation, evaluation, and iterative model refinement across the MOSAIC system.   

 39 eMethod 4. The Annotation Performance Calculation of MOSAIC At transcript level, Accuracy, precision, recall, and F1 were computed as weighted averages, where each label’s contribution was proportional to its frequency in the gold-standard annotations relative to the total number of annotated instances. Specifically, for a set of labels , with  denoting the number of gold-standard instances of label l and 𝑁=∑!∈#𝑛!  , the weighted metric is defined as: 𝑊𝑒𝑖𝑔ℎ𝑡𝑒𝑑	𝑀𝑒𝑡𝑟𝑖𝑐	=	∑!∈#	$!%	×	𝑀!, where 𝑀! denotes the per-label precision, recall, or F1. Accuracy was defined as the proportion of correctly classified instances over the total number of instances. At the category level (e.g., rheumatology vs. obstetrics/gynecology), transcript-level metrics were averaged within each category, and then compared across categories. At the codebook level, per-label metrics were averaged across all transcripts belonging to the same codebook, providing a view of performance on specific behavioral constructs. It is also worth noting that accuracy and weighted recall are often numerically very similar, this occurs because weighted recall essentially measures the proportion of correctly identified instances across all labels, which is mathematically close to overall accuracy, especially when one label (e.g., “None”) dominates the dataset.                  


 40 Additional Results eResult 1. Qualitative Evaluation of The Plan Agent To evaluate the behavior of the Plan Agent, we conducted a qualitative analysis of its ability to activate the appropriate annotation sub-agent and to raise warnings when misrouting occurred. We examined transcripts where the Plan Agent flagged text segments and compared them against unflagged portions to understand the conditions under which its routing decisions were accurate or problematic. Representative prompts were used to illustrate performance across a range of conditions. In positive cases, the Plan Agent reliably mapped specialized categories (e.g., Wiser, Patient Behavior, Intervention, Bias, Global) to the correct sub-agents, even when prompts differed in wording, case, or synonyms. This shows robustness to expression variants and consistency with clear instructions. In negative cases, however, the agent struggled with ambiguous or underspecified prompts, such as “tone,” “sentiment,” “communication style,” or “conversation flow”, which were misrouted to inappropriate agents (e.g., confused among Global, Wiser and Patient Behavior) or flagged as invalid with no routing. Additionally, nonsense inputs (“asdfghjkl,” “Test123”) or negations (“run none”) triggered useful warnings. These patterns highlight the Plan Agent’s strengths in handling explicit, well-structured prompts, noisy inputs, synonyms, and requests involving multiple agents, where it generally produced consistent and accurate routing. However, its performance declined when the target (e.g. clinician and patient) of the instruction was vague or underspecified, leading to fewer correct matches or, in some cases, over-assignment of extra sub-agents compared to the intended standard. This underscores the need for improved disambiguation, synonym mapping, and fallback strategies in future iterations (Supplementary eTable 3).   

 41 eResult 2. Qualitative Analysis on Labelling Details The aggregated results indicate that the system performs strongly on high-frequency categories, particularly the “None” class, which constitutes most instances and achieves high accuracy (0·95), precision (0·98), recall (0·98), and F1-score (0·98). Major behavioral categories such as “S” and “ASK START” also maintain excellent accuracy (≈0·99), though their F1-scores remain moderate (0·5–0·6) due to imbalances between precision and recall. In contrast, medium-frequency categories such as “OE” and “AQ” exhibit lower precision (≤0·45) and F1-scores (<0·4), reflecting more frequent misclassifications. For rare categories with only a few positive instances—particularly those requiring acoustic or paralinguistic cues—both precision and recall show substantial variability, often leading to unstable or near-zero F1-scores. Overall, the system demonstrates robust performance on dominant classes but limited generalizability to minority or context-dependent labels, underscoring the need for additional annotated data and the integration of audio-based features to improve model reliability and coverage (Supplementary eTable 5). The comparison between human and agent annotations further illustrates both alignment and divergence. In simple statements such as “Clinician: A girl,” the agent correctly identified the segment as “RS” while the human annotator missed it; similarly, in “Clinician: Oh, I’m sure they know,” the agent marked “ES” but the human again overlooked the label. Such straightforward cases suggest that humans are more prone to missing or inconsistently applying repetitive labels, whereas the agent maintains higher consistency. However, in more complex utterances like “OK, I’m glad you got the flu shot though…”, the human annotator assigned both “RS” (implicit thought) and “ES” (explicit emotion), while the agent captured only “RS”, reflecting reduced sensitivity to overlapping affective cues. In other examples, the human flagged “Warmth” or “Concerns” (e.g., “So she’s doing it for everybody else too, right?” [Warmth, 4]) that the agent either omitted or misaligned. Similarly, stylistic or conversational flow markers such as “Good. I assume no big gushes of water” were annotated by humans as “Flow: 2” but not consistently captured by the agent. This type of annotation depends heavily on contextual, prosodic, and tonal features. Collectively, these findings highlight that while the agent performs reliably on explicit, structured, or repetitive communication behaviors, it continues to face challenges in detecting nuanced emotional tones and relational subtleties that rely on multimodal contextual cues.  

 42 eResult 3. Qualitative Analysis on Number of Sentences, Words, and Speaker Turns  Across transcripts, F1 performance showed a moderate positive association with transcript length metrics, including the number of sentences, words, and speaker turns per transcript (Supplementary eFigure 5). Pearson correlation coefficients ranged from r = 0·41 to 0·50, suggesting that longer or more interactive transcripts tended to yield slightly higher F1 scores. Consistent with these trends, the OLS models demonstrated that category (rheumatology vs obstetrics/gynecology) was a significant predictor of F1 (p < 0·01), while transcript length variables—whether measured by words, sentences, or speaker turns—did not reach statistical significance (p > 0·3). Despite the small effect sizes, all three metrics exhibited weak but positive slopes, indicating that model performance may improve marginally with richer dialogue context or longer utterance content (Supplementary eTables 6-8). Taken together, these results suggest that domain differences primarily drive performance variation, whereas transcript length exerts only a minor influence once domain is controlled (Supplementary eTable 9). Since our current transcripts predominantly fall within the range of approximately 150-500 sentences, 1 000-5 000 words, and 50-300 speaker turns, future work will incorporate additional real-world data with broader coverage to further validate and enhance the robustness of these findings.                 

 43 Supplementary Tables eTable 1. Codebook and representative examples  Codebook Main Focus Level of Detail Unique Twist Example Global  Overall relational quality Summary scores (1–5) Good for simple benchmarking Flow: [Flow: 4] — “Conversation flowed well, few interruptions.” Respect: [Respect: 5] — “Clinician consistently asked permission before exam.” Intervention (5A’s) Specific behavior changes steps Detailed event coding Tracks guideline fidelity (e.g., smoking) Asked: [ASK] — “Do you currently smoke?” Assessed: [ASSESS] — “Would you like to quit in the next month?” Assisted: [ASSIST w/ Sol]— “What barriers might get in the way of quitting?” WISER  Empathy,  Captures empathy  Open-ended Question: [OE] — “How’s your weekend?” Empathic Opportunity: [EO] — “I’m really worried about paying for this medication.” Reflective Statement: [RS] — “So you’re feeling unsure about the plan.” Patient Behaviors Patient agency Captures patient behaviors Tracks how patients push back or engage Patient Asking: [AQ] — Patient: “Is this safe for the baby?” Patient Assertive: [AR] — “I don’t want to take that medication right now.” Bias Trust, distrust, explicit bias Combines behavior counts & global impression Captures subtle power dynamics & explicit stereotyping Judgement: [J] — “Oh no. You’re not still eating all that fast food, are you?” Stereotyping: [S] — “Is the baby daddy still not helping out?” Guarded-Open: [GO: 2] — “Patient sounded reserved, hesitant.” Trust: [TP] — “So, you think I should keep taking this?” Distrust: [D] — “But I really think we should do another test.”     

 44 eTable 2.  Codebook-specific characteristics of patient-provider transcripts  a.  Training dataset  Codebook Number of transcripts Sentences pre transcript Words per transcript Speaker turns per transcript Total annotated turns with labels Total turns annotated as None  Wiser 6 Mean = 347·8 (SD = 92·5)  Mean = 3088·2 (SD = 1067·6)  Mean = 215·2 (SD = 88)  103 1619 Patient Behavior 6 Mean = 291·5 (SD = 126·8)  Mean = 2748·8 (SD = 1212·5)  Mean = 131·8 (SD = 37·1)  57 1632 Bias 7 Mean = 276·3 (SD = 102·9)  Mean = 2384·4 (SD = 654·8)  Mean = 134·7 (SD = 55·6)  48 1665 Intervention (5As)  2 Mean = 149 (SD = 28·3)  Mean = 1161 (SD = 304·1) Mean = 87·5 (SD = 3·5) 37 761 Global 5 Mean = 271·6 (SD = 150·8)  Mean = 2497·4 (SD = 1177·8)  Mean = 156·4 (SD = 77·8)  51 737  b. Testing dataset Codebook Number of transcripts Sentences pre transcript Words per transcript Speaker turns per transcript Total annotated turns with labels Total turns annotated as None  Wiser 19 Mean = 346·4 (SD = 182·4)  Mean = 3169·3 (SD = 1583·3)  Mean = 188·8 (SD = 103·8)  226 6177 Patient Behavior 11 Mean = 425 (SD = 164·2)  Mean = 3838·6 (SD = 1172·5)  Mean = 240·3 (SD = 94·5)  144 4514 Bias 12 Mean = 270·4 (SD = 120·1)  Mean = 2310·9 (SD = 1072·3)  Mean = 146·6 (SD = 60·7)  67 3105 Intervention (5As) 5 Mean = 196·4 (SD = 106·7)  Mean = 1553·2 (SD = 790·4)  Mean = 101·6 (SD = 54·6)  44 529 Global 3 Mean = 211·7 (SD = 115·6)  Mean = 1885·3 (SD = 714·9)  Mean = 125·7 (SD = 77·5)  23 358     

 45 eTable 3. Representative prompt examples and results of qualitative evaluation of the Plan Agent Prompt Case Expected Sub-Agent Routing Sub-Agent Routing Decided by Update Agent Verdict Error Message Run Bias and WISER Basic positive Bias, Wiser Bias, Wiser PASS – Please run Global Basic positive Global Global PASS – Run Patient Behavior and Intervention Basic positive Intervention, Patient Behavior Intervention, Patient Behavior PASS – Run all Basic positive Bias, Global, Intervention, Patient Behavior, SDOH & weight, Wiser Bias, Global, Intervention, Patient Behavior, SDOH & weight, Wiser PASS – run bias & wiser Variant expression Bias, Wiser Bias, Wiser PASS – RUN GLOBAL AND INTERVENTION Variant expression Global, Intervention Global, Intervention PASS – Annotate empathy and advice Variant expression Wiser, Intervention Wiser, Intervention PASS – Evaluate stigma and prejudice Variant expression Bias Bias PASS – Check SDOH factors and weight discussion Variant expression SDOH & weight SDOH & weight PASS – Only run Wiser Single agent Wiser Wiser PASS – Annotate just Bias Single agent Bias Bias PASS – Focus on patient behaviors Single agent Patient Behavior Patient Behavior PASS – Check obesity coding Single agent SDOH & weight SDOH & weight PASS – Run Wiser, Global, and Bias Complex prompt Bias, Global, Wiser Bias, Global, Wiser PASS – Please run intervention, bias, and sdoh Complex prompt Bias, Intervention, SDOH & weight Bias, Intervention, SDOH & weight PASS – Analyze communication style of the doctor Ambiguous Wiser Global, Wiser FAIL – Check tone and sentiment of patient response Ambiguous Patient Behavior Wiser FAIL – Do empathy coding for this transcript Ambiguous Wiser Wiser PASS – Look at overall dialogue quality Ambiguous Global Global PASS – Just check conversation length Negative – – PASS No valid annotation agents found Evaluate conversation flow Ambiguous Global Global PASS – Check doctor dominance in dialogue Ambiguous Bias Bias PASS – Identify unclear patient responses Ambiguous Patient Behavior – FAIL No valid annotation agents found 

 46 Review if intervention was successful Ambiguous Intervention Intervention PASS – asdfghjkl Extreme (nonsense) – – PASS No valid annotation agents found Please annotate with all modules available Extreme Global, Wiser, Intervention, Bias, SDOH & weight, Patient Behavior Global, Wiser, Intervention, Bias, SDOH & weight, Patient Behavior PASS – Can you check social determinants, empathy, and overall coding? Extreme Global, Wiser, SDOH & weight Global, Wiser, SDOH & weight PASS – RUN EVERYTHING Extreme Global, Wiser, Intervention, Bias, SDOH & weight, Patient Behavior Global, Wiser, Intervention, Bias, SDOH & weight, Patient Behavior PASS – Please run none of the agents Negative – – PASS No valid annotation agents found Evaluate ALL aspects of patient-doctor communication Extreme Global, Wiser, Intervention, Bias, SDOH & weight, Patient Behavior Global, Wiser, Intervention, Bias, SDOH & weight, Patient Behavior PASS – Test123 !!! ??? Extreme (nonsense) – – PASS No valid annotation agents found    

 47 eTable 4. Overall, category-specific and codebook-specific performance of MOSAIC, regarding accuracy, precision, recall and F1 Category / Codebook Accuracy Precision Recall F1  Overall (all transcripts) 93·1% 93·4% 93·1% 93·0% Rheumatology (subset)  96·1% 96·4% 96·1% 96·2% Obstetrics/Gynecology (subset) 87·3% 87·6% 87·3% 86·8% By Codebook     WISER (clinician empathy/behaviors) 94·4% 93·8% 94·4% 94·0% Global (relational quality) 79·0% 91·4% 79·0% 83·1% Intervention (5As) 85·6% 87·7% 85·6% 86·3% Patient Behaviors 95·0% 95·5% 95·0% 95·1% Bias 95·9% 93·8% 95·9% 94·8%         

 48 eTable 5.  Qualitative analysis on labelling details a. Detailed performance of good cases  Label TP FP FN TN Total Accuracy Precision Recall F1 Support (Number of gold labelling) None 15363 364 394 165 16286 0·953 0·977 0·975 0·976 15757 Rushed, 3 1 0 1 152 154 0·994 1 0·5 0·667 2 ASSIST w/ Explore 1 1 0 143 145 0·993 0·5 1 0·667 1 S 16 6 12 6262 6296 0·997 0·727 0·571 0·64 28 ASK START 2 1 3 567 573 0·993 0·667 0·4 0·5 5 TP 2 5 0 1039 1046 0·995 0·286 1 0·444 2 OE 28 34 60 6770 6892 0·986 0·452 0·318 0·373 88 ASSIST w/ Solution 2 8 0 326 336 0·976 0·2 1 0·333 2 ASK END 1 1 3 514 519 0·992 0·5 0·25 0·333 4 AQ 23 97 22 6677 6819 0·983 0·192 0·511 0·279 45 EO 8 46 14 5903 5971 0·99 0·148 0·364 0·211 22 ASSIST END 1 0 8 564 573 0·986 1 0·111 0·2 9 EQ 1 8 0 2316 2325 0·997 0·111 1 0·2 1 AR 16 50 110 6143 6319 0·975 0·242 0·127 0·167 126 ER 6 54 14 7121 7195 0·991 0·1 0·3 0·15 20 RS 6 22 51 7794 7873 0·991 0·214 0·105 0·141 57 ES 1 2 12 3700 3715 0·996 0·333 0·077 0·125 13   b. Detailed performance of bad cases.  Most of these poor performances were from Global and Bias coding, indicating the need for audio inclusion. Label TP FP FN TN Total Accuracy Precision Recall F1 Support (Number of gold labelling) Attentive: 4 0 19 0 275 294 0·935 0 0 0 0 Concerns: 3 0 2 0 303 305 0·993 0 0 0 0 

 49 Concerns: 4 0 14 0 62 76 0·816 0 0 0 0 Establishing Rapport 0 4 0 910 914 0·996 0 0 0 0 Respect: 4 0 2 0 74 76 0·974 0 0 0 0 Warmth: 1 0 2 0 216 218 0·991 0 0 0 0 Warmth: 3 0 2 0 216 218 0·991 0 0 0 0 Warmth: 4 0 5 0 158 163 0·969 0 0 0 0     

 50 eTable 6. Results of OLS regression results for F1 score by department and total number of sentences. F1~Category + Sentences per transcript a. Model summary Statistic Value Dependent Variable F1 Model Type OLS (Least Squares) R squared 0·488 Adjusted R squared 0·466 F statistic 22·40 Prob (F statistic) 1·47e−07 No.of Observations 50 Df Model 2 Df Residuals 47 Log Likelihood 83·135 AIC −160·3 BIC −154·5 Covariance Type Nonrobust b. Coefficient estimates  

 51 Variable Coef.   Std. Err.  t  P>|t| 95% CI (Lower) 95% CI (Upper) Intercept 0·591 0·015 56·935 0·000 0·829 0·889 Department (Rheumatology)  0·0866  0·016  5·422 0·000 0·054 0·119 Length (sentences per transcript) 4·317e−05  4·56e−05   0·946 0·349 −4·86e−05 0·000 c. Diagnostic statistics Test/Metric Statistic p-value  Omnibus 44·456 p = 0·000 Jarque–Bera (JB) 182·050 p = 2·94e−40 Skew −2·314 — Kurtosis 11·122 — Durbin–Watson 1·742 — Condition No. 941 — Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.    

 52 eTable 7. Results of OLS regression results for F1 score by department and total number of words.  a. Model summary Statistic Value Dependent Variable F1 Model Type OLS (Least Squares) R-squared 0·486 Adjusted R-squared 0·464 F-statistic 22·23 Prob (F-statistic) 1·61e-07 No. of Observations 50 Df Model 2 Df Residuals 47 Log-Likelihood 83·042 AIC -166·1 BIC -154·3 Covariance Type Nonrobust b. Coefficient estimates Variable Coef.   Std. Err.  t  P>|t| 95% CI (Lower) 95% CI (Upper) 

 53 Intercept 0·8610 0·015 57·000 0·000 0·830 0·975 Department (Rheumatology)  0·0841  0·018  4·665 0·000 0·048 0·120 Length (sentences per transcript) 5·56e-06 5·97e-06  0·847 0·401 -6·96e-06 1·71e-05 c. Diagnostic statistics Test / Metric Statistic p-value Omnibus 45·095 0·000 Jarque–Bera (JB) 194·803 5·00e-43 Skew -2·322 — Kurtosis 11·482 — Durbin–Watson 1·726 — Condition Number 8·70e+03 — Notes: [1] Standard errors assume the covariance matrix of the errors is correctly specified. [2] The condition number is large (8.70e+03), which may indicate strong multicollinearity or other numerical problems    

 54 eTable 8. Results of OLS regression results for F1 score by department and total number of speaker turns. F1~Category + Speaker turns per transcript a. Model summary  Statistic Value Dependent Variable F1 Model Type OLS (Least Squares) R-squared 0·488 Adjusted R-squared 0·466 F-statistic 22·43 Prob (F-statistic) 1·60e-07 No. of Observations 50 Df Model 2 Df Residuals 47 Log-Likelihood 83·043 AIC -166.1 BIC -154·5 Covariance Type Nonrobust  b. Coefficient estimates  Variable Coef.   Std. Err.  t  P>|t| 95% CI (Lower) 95% CI (Upper) Intercept 0·8594 0·015 57·989 0·000 0·830 0·889 Department (Rheumatology)  0·0865  0·016  5·423 0·000 0·054 0·119 

 55 Length (sentences per transcript) 7·67e-05 8·04e-05 0·954 0·345 -8·5e-05 0·000  c. Diagnostic statistics   Test / Metric Statistic p-value Omnibus 43·902 0·000 Jarque–Bera (JB) 180·461 6·51e-40 Skew -2·271 — Kurtosis 11·124 — Durbin–Watson 1·765 — Condition Number 521·0 —  Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.        

 56 eTable 9. Summary across models  Length Metric Coef (Length) T (Length) 95%CI Low 95%CI High R² Adj.R² N Department Sentences per transcript 0·0000 0·3488 -0·0001 0·0001 0·4880 0·4662 50 obstetrics/gynecology Words per transcript 0·0000 0·4014 -0·0000 0·0000 0·4861 0·4642 50 obstetrics/gynecology Speaker turns per transcript 0·0001 0·3447 -0·0001 0·0002 0·4882 0·4664 50 obstetrics/gynecology                                

 57  Supplementary Figures  eFigure 1. Date preprocessing through internal ADS tool and chunking  
  


 58 eFigure 2. RAG techniques in the Annotation Agent 
    


 59 eFigure 3. Example Library RAG techniques in Dynamic Prompting Strategy   
            


 60 eFigure 4. MOSAIC User Interface a. The Annotation Panel 
       


 61 b. The Verification Panel 
        


 62  eFigure 5. The comparison of different regression analysis on F1 vs. text length  
      


 63 References  1 Wang H, Yang R, Alwakeel M, et al. An evaluation framework for ambient digital scribing tools in clinical applications. npj Digital Medicine. 2025 Jun 13;8(1):358. 2 Graf S. Design of Scenario-specific Features for Voice Activity Detection and Evaluation for Different Speech Enhancement Applications. 3 GitHub [Internet]. [cited 2025 Aug 31]. GitHub - jiaaro/pydub: Manipulate audio with a simple and easy high level interface. Available from: https://github.com/jiaaro/pydub 4 GitHub [Internet]. [cited 2025 Aug 31]. GitHub - snakers4/silero-vad: Silero VAD: pre-trained enterprise-grade Voice Activity Detector. Available from: https://github.com/snakers4/silero-vad 5 GitHub [Internet]. [cited 2025 Aug 31]. GitHub - speechbrain/speechbrain: A PyTorch-based Speech Toolkit. Available from: https://github.com/speechbrain/speechbrain 6 Radford A, Kim JW, Xu T, Brockman G, McLeavey C, Sutskever I. Robust speech recognition via large-scale weak supervision. InInternational conference on machine learning 2023 Jul 3 (pp. 28492–28518). PMLR. 7 Bredin H, Yin R, Coria JM, et al. Pyannote. audio: neural building blocks for speaker diarization. InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020 May 4 (pp. 7124–7128). IEEE. 8 Achiam J, Adler S, Agarwal S, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. 2023 Mar 15. 9 Wang X, Wei J, Schuurmans D, et al. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. 2022 Mar 21. 10 langgraph: Build resilient language agents as graphs [Internet]. Github; [cited 2025 Oct 21]. Available from: https://github.com/langchain-ai/langgraph 11 sentence-transformers/all-MiniLM-L6-v2 · Hugging Face [Internet]. [cited 2025 Oct 21]. Available from: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 12 faiss: A library for efficient similarity search and clustering of dense vectors [Internet]. Github; [cited 2025 Oct 21]. Available from: https://github.com/facebookresearch/faiss 13 Jin Q, Kim W, Chen Q, et al. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. Bioinformatics. 2023 Nov 1;39(11):btad651. 