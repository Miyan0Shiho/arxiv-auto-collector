# Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights

**Authors**: Hyunjae Kim, Jiwoong Sohn, Aidan Gilson, Nicholas Cochran-Caggiano, Serina Applebaum, Heeju Jin, Seihee Park, Yujin Park, Jiyeong Park, Seoyoung Choi, Brittany Alexandra Herrera Contreras, Thomas Huang, Jaehoon Yun, Ethan F. Wei, Roy Jiang, Leah Colucci, Eric Lai, Amisha Dave, Tuo Guo, Maxwell B. Singer, Yonghoe Koo, Ron A. Adelman, James Zou, Andrew Taylor, Arman Cohan, Hua Xu, Qingyu Chen

**Published**: 2025-11-10 06:00:12

**PDF URL**: [https://arxiv.org/pdf/2511.06738v1](https://arxiv.org/pdf/2511.06738v1)

## Abstract
Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.

## Full Text


<!-- PDF content starts -->

Rethinking Retrieval-Augmented Generation for Medicine:
A Large-Scale, Systematic Expert Evaluation and
Practical Insights
Hyunjae Kim1, Jiwoong Sohn2, Aidan Gilson3, Nicholas Cochran-Caggiano4, Serina Applebaum1, Heeju Jin5,
Seihee Park5, Yujin Park5, Jiyeong Park5, Seoyoung Choi5, Brittany Alexandra Herrera Contreras1,6,
Thomas Huang1, Jaehoon Yun7, Ethan F. Wei1,8, Roy Jiang1, Leah Colucci1, Eric Lai1, Amisha Dave1,
Tuo Guo1, Maxwell B. Singer1, Yonghoe Koo9, Ron A. Adelman1, James Zou10, Andrew Taylor11,
Arman Cohan12, Hua Xu1, and Qingyu Chen1,*
1Yale School of Medicine, Yale University, New Haven, CT, USA
2Department of Biosystems Science and Engineering, ETH Zurich, Zurich, Switzerland
3Massachusetts Eye and Ear, Harvard Medical School, Boston, MA, USA
4Geisel School of Medicine at Dartmouth, Hanover, NH, USA
5Seoul National University College of Medicine, Seoul, Republic of Korea
6San Juan Bautista School of Medicine, Caguas, PR, USA
7Hanyang University College of Medicine, Seoul, Republic of Korea
8PA Leadership Charter School, West Chester, PA, USA
9Asan Medical Center, University of Ulsan College of Medicine, Seoul, Republic of Korea
10Stanford University School of Medicine, Stanford, CA, USA
11University of Virginia School of Medicine, Charlottesville, VA, USA
12Yale School of Engineering & Applied Science, Yale University, New Haven, CT, USA
*qingyu.chen@yale.edu
ABSTRACT
Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with
rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has
been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably
achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen
medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across
200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components—(i) evidence
retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and
completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant,
evidence selection remained weak (precision 41–43%, recall 27–49%), and factuality and completeness dropped by up to 6% and 5%,
respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to
the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation,
substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These
findings call for re-examining RAG’s role in medicine and highlight the importance of stage-aware evaluation and deliberate system design
for reliable medical LLM applications.
1 Introduction
Large language models (LLMs) have gained significant traction in medicine1–3, with growing efforts focused on their deploy-
ment across a range of tasks, including medical question answering4,5, disease diagnosis6,7, and treatment planning8,9. Yet,
despite these advances, critical challenges persist in ensuring their safe and effective use in clinical settings. First, medical
knowledge such as clinical guidelines and drug information is frequently revised as new evidence emerges10,11. To ensure
clinical reliability, LLMs must continuously incorporate such updates to produce accurate and clinically relevant responses.
A systematic evaluation of six leading LLMs—spanning the GPT12, Gemini13, and Llama families14—revealed marked lim-
itations in addressing questions about newly approved drugs, with performance remaining consistently low10. Second, inarXiv:2511.06738v1  [cs.CL]  10 Nov 2025

medicine, providing a decision or recommendation alone is rarely sufficient, and what often matters more is the ability to sup-
port that output with credible evidence15,16. Healthcare professionals routinely consult authoritative sources, particularly in
uncertain or high-stakes scenarios where accountability and transparency are paramount17,18. Current LLMs, however, lack
explicit mechanisms for verification and typically generate responses without grounding in reliable references19–22. Earlier
generations of LLMs frequently hallucinated in medical applications19,20, and despite some progress, even the latest models
still struggle to produce accurate citations or verifiable evidence21,22.
Retrieval-augmented generation (RAG) has emerged as a promising paradigm to address these challenges23–25. RAG is
designed to help models access up-to-date information and improve the factuality and trustworthiness of their responses, by
incorporating external evidence at inference time24,26. A standard RAG pipeline comprises three main stages25. First, domain-
specific documents (e.g., clinical guidelines, biomedical publications, institutional protocols) are selected by practitioners, split
into passages, and encoded into vector representations to build a searchable database. Second, given a user query, the system
retrieves the top- kmost relevant passages (typically using cosine similarity between vector embeddings) and appends them
to the input prompt. Third, the LLM integrates the retrieved passages with the query to generate the final response. This
architecture offers notable advantages: LLMs can access recent information without retraining, and users can provide domain-
specific and authoritative knowledge sources from which the model retrieves evidence to support its outputs. Owing to these
strengths, RAG has attracted growing attention27,28and now is widely adopted in medical applications, such as clinical ques-
tion answering, trial screening, triage and diagnosis, and disease management29–34.
However, despite its growing adoption, few studies have systematically examined how RAG performs in practice and
whether it meaningfully addresses challenges related to factuality and verifiability. Most existing studies in medicine treat the
RAG framework as a black box, applying it directly and evaluating only end-task performance, without analyzing intermediate
steps such as retrieval quality or evidence usage—a gap also highlighted as a pressing need in recent reviews35,36. Previous
efforts have laid important groundwork but often remain constrained to pilot-scale studies (e.g., 30 queries in a specific medical
specialty37) or examine only specific aspects such as citation relevance38. Furthermore, recent studies have reported mixed
results, with some suggesting that RAG may even reduce accuracy in downstream medical tasks39,40. These observations
motivate a systematic investigation into how RAG interacts with LLMs in medical contexts. Several key aspects warrant in-
depth examination: (1) whether the retriever retrieves evidence that is relevant to the query, since irrelevant documents may
degrade response quality; (2) whether the LLM effectively identifies and uses the retrieved information; and (3) how retrieval-
augmented responses compare to those generated without retrieval, particularly in medical decision-support scenarios.
We conducted a large-scale, expert-driven evaluation to systematically assess the effectiveness of RAG in medicine. 18 med-
ical professionals manually evaluated and contributed a total of 80,502 expert annotations across four model configurations—
GPT-4o12and Llama-3.1-8B14, each with and without RAG. The evaluation covered 200 medical queries, comprising 100
real free-text patient queries and 100 complex, context-rich scenarios modeled after the United States Medical Licensing Ex-
amination (USMLE). As shown in Fig. 1, annotators evaluated RAG-LLMs across three stages: (I) Evidence retrieval: We
assessed the relevance and coverage of the retrieved passages by determining whether they provided sufficient information to
logically infer the key elements necessary for a correct answer. (II) Evidence selection: We evaluated whether LLMs effectively
incorporated retrieved passages into their responses. Specifically, we measured the proportion of retrieved documents cited in
the responses and how many of these were judged relevant in the evidence retrieval stage. (III) Response generation: We com-
pared the final outputs of LLMs with and without RAG in head-to-head evaluations, focusing on factuality and completeness.
Evaluations were performed at both the response level (i.e., the overall factuality and completeness of the full answer) and the
statement level (i.e., the factuality and completeness of each individual statement).
Our findings reveal substantial limitations in the use of RAG for medical applications. Although RAG is widely assumed
to improve response quality, it does not consistently enhance factuality or evidence usage, and in some cases, it may even
diminish the overall quality of the generated responses. (I) Evidence retrieval: The top-retrieved passages were often irrelevant
and provided limited coverage of medical queries. On average, only 22% of the top-16 passages were judged relevant, and
the proportion was even lower for USMLE queries (15%). In terms of coverage, the top-16 passages together contained
information for only 33% of must-have statements, dropping to 26% for USMLE queries. (II) Evidence selection: Even
when relevant passages were retrieved, LLMs often failed to incorporate them effectively. Both precision and recall for selecting
relevant evidence remained low. For GPT-4o, precision was 41% and recall 49%; for Llama-3.1, precision was 43% and recall
only 28%. As a result, the already limited coverage of must-have statements from retrieval (33%) dropped by nearly 10%
after evidence selection. At the same time, irrelevant passages were frequently included, with their frequency nearly twice that
of relevant passages. (III) Response generation: RAG may not improve LLM outputs. At both the response and statement
levels, factuality and completeness were consistently lower for GPT-4o and Llama-3.1 with RAG than without. Response-
level factuality dropped by up to 6% for GPT-4o, while statement-level completeness declined by more than 5% for Llama 3.1.
2/34

DatabasePassage 1Query (n=200): “Benadryl makes me super sleepy. Can I take Claritin during the day instead?”
LLM(GPT-4o and Llama-3.1-8B)Model Statements(n = 15,970)
GoldResponseModelResponseEvidence SelectionWhether each cited reference is:
✅ Relevant
❌ IrrelevantI. Evidence Retrieval(n = 30,800)
II. Evidence Selection(n = 26,032)
RetrievalModel Response and Supporting References(n = 1,627)Diphenhydramine,theactiveingredientinBenadryl,isafirst-generationH1histaminereceptorantagonistthatiscommonlyassociatedwithdrowsinessorsleepinessasasideeffect[1][2].Duetothesesedativeproperties,Benadrylisoftenusedasasleepaid,butitcancausedaytimesleepiness,grogginess,andanincreasedriskoffalls[2].Claritin(loratadine)isasecond-generation…###References1. Mohan S, et al. A Case of Massive Diphenhydramine and Naproxen Overdose .…2. Benadryl. Wikipedia. Accessed October 14, 2023…
III. Response Generation(n = 23,670)
Diphenhydramine is the active ingredient in Benadryl.
Diphenhydramine is a first-generation H1 histamine receptor antagonist.
Benadryl has sedative properties.
Diphenhydramine (Benadryl) is a first-generation antihistamine.
A first-generation antihistamine frequently causes sedation.
Loratadine (Claritin) is a second-generation antihistamine…
…Must-have Statements (n = 1,925)FactualityWhether each model statement is:
✅ Factually Accurate
❌ InaccurateCompletenessWhether essential information is:
✅ Supported
✔ Partially Supported
❌ Unsupported…
Passage 16RetrievedContent +QueryRelevanceWhether each retrieved passage is:
✅Relevant
❌Irrelevant
80,502 Annotations
Must-have statements(n=1,925)Top-16 Passages
Responses from four diﬀerent systems
…
Cross-referenceFigure 1. Study design and evaluation framework. Our fine-grained framework decomposes the RAG pipeline into three
components, evidence retrieval, evidence selection, and response generation, enabling systematic evaluation of each stage.
First, retrieved passages are annotated for relevance (evidence retrieval; n=30,800). Next, model responses are evaluated to
determine whether they are grounded in relevant evidence (evidence selection; n=26,032). Finally, responses are broken
down into individual statements and assessed for correctness (factuality; n=15,970) and coverage of required information
(completeness; n=7,700). In total, our framework comprises 80,502 expert annotations, enabling rigorous error
attribution across the full RAG pipeline.
These degradations were more pronounced when the RAG system incorporated irrelevant passages or failed to retrieve relevant
ones. For instance, Llama-3.1’s factuality dropped by over 8% when irrelevant passages were cited, compared to outputs
grounded in relevant content. Similarly, completeness declined by over 6% when relevant information was not retrieved at all.
We proposed two simple and practical strategies designed to directly mitigate the observed issues. First, evidence filtering
removes irrelevant passages from the retrieved set, which is a necessary step given the high proportion of irrelevant content
and the models’ tendency to mistakenly incorporate it. Second, query reformulation rewrites the initial query to guide retrieval
toward more relevant evidence, thereby improving the precision and coverage of the retrieval step. We evaluated each strategy
3/34

independently and in combination across five medical question-answering benchmarks. In line with earlier findings, the RAG
models did not consistently improve performance and in some cases reduced accuracy. In contrast, combining evidence filtering
and query reformulation yielded substantial gains on more challenging datasets: Llama-3.1 improved by +12% on MedMCQA
and +8.2% on MedXpertQA, while GPT-4o achieved gains of +3.4% and +6.6% on the same datasets.
Our study motivates a re-examination of RAG in the medical domain. Despite its promise, our study shows that RAG
pipelines can introduce new sources of failure, including the retrieval of irrelevant information, failure to integrate relevant
evidence, and reductions in both the factual accuracy and completeness of model outputs. By systematically evaluating each
stage of the RAG process with a large number of expert annotations, we identify critical bottlenecks that have been largely
overlooked in previous studies. Importantly, we demonstrate that targeted interventions, such as evidence filtering and query
reformulation, substantially improve performance on challenging medical tasks. These findings suggest that the path forward
lies not in applying RAG as a default solution but in rethinking both its system design and its evaluation.
2 Results
2.1 Annotation Summary
We briefly summarize the annotation process (Fig. 1) and associated statistics below; full details, including stage-specific
guidelines, annotator instructions, and interface design, are provided in the Methods section.
2.1.1 Model Selection
We conducted a comprehensive survey of current RAG practices in medicine, summarized in Supplementary Table 1, covering
retrievers, LLMs, and knowledge sources. Our review includes 29 papers published between June 2023 and August 2025.
Among the retrievers, MedCPT41, an open-source, domain-specialized retriever for biomedicine, was the most commonly
used (28%). Given its scalability and strong domain relevance, we selected MedCPT as the retriever for our pipeline. Further
discussion on alternative retrievers is provided in the Discussion section. For LLMs, the majority of studies (over 65%) relied
on proprietary models from the GPT family (GPT-3.5, GPT-4, and GPT-4o), while a smaller subset employed open-source
models, most notably variants of Llama. In our study, we adopted both proprietary and open-source LLMs, GPT-4o and
Llama-3.1-8B, respectively, to ensure a balanced evaluation across model types. For the retrieval corpora, we included a broad
range of widely used medical sources, including PubMed, Wikipedia, clinical guidelines, StatPearls, and medical textbooks.
2.1.2 Data Curation
We randomly sampled 100 patient queries from the K-QA dataset42and 100 USMLE-style questions from the MedBullets
dataset43. Both datasets provide gold-standard responses annotated by human experts. In K-QA, each gold response is divided
into “must-have” and “nice-to-have” statements, both verified by human annotators. Note that these annotations were created
independently of our study and originate from the original datasets. The must-have statements capture the essential factual
information required for a correct answer, whereas the nice-to-have statements provide supportive but non-essential details.
In contrast, the MedBullets dataset does not provide annotated statements. To address this, we prompted GPT-4o to extract
statements from each gold response and classify them as must-have or nice-to-have, following prior work demonstrating the
strong performance of LLMs in this task38. We used only the must-have statements for evaluating evidence retrieval and
completeness scores, yielding a total of 1,925 statements across the two datasets.
For a total of 200 questions, we generated 800 responses using four model configurations: GPT-4o, GPT-4o with RAG,
Llama-3.1, and Llama-3.1 with RAG. For the RAG-based models, the top- kretrieved passages were provided as input context.
We set k=16to limit the annotation workload to a manageable level while still ensuring sufficient context for model reasoning.
Each response was accompanied by a list of supporting references, that is, source materials or citations the model cited as
evidence for its answer. For annotation, we exclusively used the 1,627 references produced by the RAG models. Each response
was then segmented into individual statements using GPT-4o to enable statement-level factuality assessment. During this
process, any inline citations (e.g., [1][2]) provided by the model were also separated along with each statement to enable later
tracing of which reference supported which statement. Non-factual claims that could not be objectively verified were excluded,
resulting in a final total of 15,970 model-generated statements.
4/34

2.1.3 Annotation Procedure and Statistics
We conducted a multi-stage annotation process using a curated dataset comprising 200 queries, their top-16 retrieved passages,
1,925 must-have statements, 15,970 model-generated statements from 800 responses, and 1,627 references. The paragraphs
below describe how the full set of 80,502 annotations was derived by aggregating results from the three evaluation stages.
Evidence retrieval For each of the 200 queries, the retriever returned the top-16 passages. Annotators then evaluated each
passage by comparing it to the corresponding must-have statements, labeling it as relevant or irrelevant. A passage was labeled
as relevant if it fully (or partially) supported at least one of the must-have statements associated with the query; otherwise,
it was labeled irrelevant. This process yielded a total of 30,800 passage–statement pairs, computed as 1,925 must-have
statements (aggregated across 200 queries) each evaluated against the top-16 retrieved passages for its corresponding query
(1,925 x 16 = 30,800).
Evidence selection Annotators compared the 1,627 references generated by the two RAG models against each of the top-16
retrieved passages per query, identifying which passages corresponded to the sources actually cited by the model. A single
reference could be matched to multiple retrieved passages when appropriate, as multiple passages might originate from the
same document or from different documents containing overlapping content. This involved checking metadata (e.g., title,
source name, URL) and reviewing the citation context in the model’s response to confirm alignment between the cited reference
and the retrieved passage. For example, as shown in Fig. 1, if the model cited a PubMed article titled “ A Case of Massive
Diphenhydramine and Naproxen Overdose , ” annotators assessed whether any retrieved passage matched that article based on both
metadata and textual content. Once the alignments were established, this provided a basis for evaluating whether each reference
was grounded in a relevant or irrelevant source, using relevance labels from the previous stage. This process produced a total
of 26,032 passage–reference alignment annotations, calculated as 1,627 references evaluated against 16 retrieved passages
per query (1,627 x 16 = 26,032).
Response generation This stage was evaluated along two axes: factuality and completeness. The factuality task assessed the
correctness of individual statements generated by all four models, yielding a total of 15,970 statement-level annotations cor-
responding to all model-generated statements across queries. The completeness task evaluates whether each model response
adequately captures the essential content of the gold-standard response. For each of the 1,925 must-have statements, an-
notators judged whether the model’s response fully supported, partially supported, or failed to support it, producing 7,700
response-statement annotations (1,925 statements x 4 models = 7,700).
2.1.4 Inter-annotator Agreement
Annotation reliability was assessed using Krippendorff ’s α(nominal), with 10,000 bootstrap samples for confidence intervals.
Each sample in this analysis was independently labeled by two annotators. For evidence retrieval, support andpartial support la-
bels were merged into a single category to address class imbalance during agreement estimation. This yielded 1,280 annotated
passage-statement pairs, with a resulting Krippendorff ’s αof 0.757 (95% CI: 0.638–0.852), indicating substantial agree-
ment. For evidence selection, a total of 416 reference-passage pairs were evaluated, resulting in a Krippendorff ’s αof 0.926
(95% CI: 0.906–0.944), indicating substantial inter-annotator consistency. For the factuality criterion, 972 statements gen-
erated across the four model conditions were evaluated, yielding moderate inter-annotator agreement with a Krippendorff ’s α
of 0.512 (95% CI: 0.372–0.636). Although factuality judgments are grounded in objective evidence, they may still vary
depending on each clinician’s background knowledge and experience. Importantly, each annotator assessed all model out-
puts for a given query, ensuring internal consistency within each case. Thus, while the absolute agreement level was moderate,
relative comparisons between models (e.g., RAG vs. non-RAG) remain reliable. For the completeness criterion, 328 response–
statement pairs were evaluated. As with evidence retrieval, partial support labels were merged with support labels. This yielded
a Krippendorff ’s αof 0.742 (95% CI: 0.666–0.812), again indicating substantial inter-annotator agreement.
2.2 Evidence Retrieval
We employed three complementary metrics, all computed based on expert annotations of the top-16 retrieved passages per
query. (1) Precision@k measures the proportion of relevant passages among the top-k retrieved results. Here, a passage is con-
sidered relevant if it partially or fully supports at least one of the must-have statements associated with the query. (2) Miss@k
5/34

00.10.20.30.40.5
k=1481600.10.20.30.4
k=14816Precision@kCoverage@k
AverageUSMLE-style QueriesPatient Queriesac0.2170.2810.1530.3280.2560.40000.20.40.60.81
k=14816Miss@kb0.3100.2500.370Figure 2. Evidence retrieval performance across different evaluation metrics and query types. a , Precision@k: proportion
of relevant passages among the top-k; higher is better. b, Miss@k: proportion of queries with no relevant passage in the
top-k; lower is better. c, Coverage@k: proportion of must-have statements supported by the top-k; higher is better.
reflects the proportion of queries for which no relevant passage was retrieved within the top-k. (3) Coverage@k quantifies
the proportion of must-have statements that were supported by at least one of the top-k retrieved passages.
Retrieval performance was markedly limited, with most retrieved passages failing to provide relevant support. (1) Preci-
sion@k remained low across all top-k values (Fig. 2a). At k=16, precision was 0.217, indicating that only ~22% of pas-
sages were identified as relevant. Performance was even lower for USMLE-style queries (0.153) compared to patient queries
(0.281). This discrepancy reflects the dense contextual details in USMLE-style queries, such as medical history and symptom
descriptions, which can hinder retrieval accuracy. (2) Miss@k revealed that a substantial fraction of queries failed to retrieve
any relevant evidence, even at high top-k (Fig. 2b). Specifically, 31% of queries had zero relevant passages among the top-16
retrieved results, highlighting the challenge of surfacing useful external information in complex clinical scenarios. The failure
rate was even higher for USMLE-style queries (37%) than for patient queries (25%). Notably, the issue was more pronounced
at lower top-k, for instance, at top-1, 82% of USMLE-style queries and 63% of patient queries returned no relevant evidence.
(3) Coverage@k of essential content was also limited (Fig. 2c). Here, essential information refers to must-have statements,
which are manually annotated parts of the gold-standard answer required for a correct response. The score was 0.328, indi-
cating that only ~33% of must-have statements were supported by the top-16 retrieved passages. While coverage gradually
improved with larger top-k, a substantial portion of essential content remained unretrieved. Once again, performance diverged
by query type: USMLE-style queries achieved only 0.256 coverage, compared to 0.400 for patient queries.
2.3 Evidence Selection
We evaluated whether models effectively cited relevant information from the top-16 retrieved passages, which included a mix
of relevant and irrelevant content. On average, GPT-4o generated 4.9 references per query, while Llama-3.1 generated 4.5
(Fig. 3a). Each reference was either retrieval-based—drawn directly from the top-16 passages—or self-generated using the
model’s internal knowledge. The majority of GPT-4o’s references (89.8%, 4.4 per query) were retrieval-based, compared to
Llama-3.1 (62.2%, 2.8 per query), indicating a higher proportion of self-generated references in the latter. We computed
micro-averaged precision and recall by comparing all retrieval-based references against expert relevance annotations. Self-
generated references were excluded from this analysis, as annotations were available only for the retrieved passages.
Models struggled to selectively incorporate relevant evidence from the retrieved passages, as reflected in the low precision
and recall scores across both models (Fig. 3b). GPT-4o achieved a precision of 0.412 and a recall of 0.486, while Llama-3.1
achieved a similar precision of 0.430 but a substantially lower recall of 0.275. The low precision indicates that models fre-
quently incorporated irrelevant content. On average, GPT-4o cited 2.6 irrelevant passages per query out of 4.9 total references,
while Llama-3.1 cited 1.6 out of 4.5 (Fig. 3a). This indicates that even high-performing models like GPT-4o often treat irrele-
vant content as valid evidence, reflecting limitations in their ability to distinguish useful information from misleading content.
In contrast, the low recall highlights the models’ failure to effectively incorporate relevant information, even when it was readily
available. Based on the earlier precision@16 of 0.217, each query included approximately 3.5 relevant passages among the
top-16 retrieved. GPT-4o cited 1.8 of these on average, while Llama-3.1 cited just 1.2, indicating that both models failed to
make full use of available evidence, with Llama-3.1 omitting nearly two-thirds of the relevant passages. In both models, the
6/34

00.10.20.30.40.5
PrecisionRecallGPT-4oLlama-3.1-8B0.4120.4300.4860.275Evidence Selection PerformancebRetrieval-basedSelf-generated012345
GPT-4oLlama-3.1-8BAverage Number of References per Query by Type
1.84.94.52.60.5(10.2%)1.7(37.8%)1.61.24.4(89.8%)2.8(62.2%)+Retrieval-based (Relevant)Retrieval-based (Irrelevant)aFigure 3. Analysis of citation types and evidence selection performance. a , Average number of references per query,
categorized by evidence source (retrieval-based vs. self-generated); retrieval-based references are further broken down by
relevance. Self-generated references refer to citations produced by the RAG model itself that do not appear among the
retrieved passages. b, Precision and recall for identifying relevant evidence among retrieved passages.
number of relevant passages cited per query was lower than the number of irrelevant ones: 1.8 vs. 2.6 for GPT-4o and 1.2 vs.
1.6 for Llama-3.1.
Verifiability of self-generated references Beyond retrieved content, both models also produced references not found among
the top-16 retrieved passages. These self-generated references made up 10.2% of all citations in GPT-4o and 37.8% in Llama-
3.1 (Fig. 3a). To verify their authenticity, we manually checked self-generated references from GPT-4o and Llama-3.1 to
determine whether they could be located online using the generated citation details. A small fraction (13.3%) of GPT-4o’s
self-generated references were unverifiable, whereas a much larger proportion (77.2%) of those from Llama-3.1 could not be
confirmed, often containing fabricated metadata such as non-existent titles, authors, or publication sources. This discrepancy
underscores a key limitation of small models: while they more frequently produce self-generated citations, they are also more
prone to hallucinating plausible-sounding but non-existent references, raising concerns about the verifiability of their outputs.
2.4 Response Generation
We evaluated model outputs using two key criteria: factuality, which measures the correctness of the presented informa-
tion, and completeness, which assesses whether essential information is fully conveyed. All scores were manually determined
through expert annotation. For the RAG models, responses were generated using the top-16 retrieved passages.
2.4.1 Factuality
We evaluated factuality at two levels: response and statement. At the response level, each answer was assigned a binary score—
1 if the entire response was factually correct, and 0 if any part contained a factual error. At the statement level, each individual
statement within the response was evaluated independently as true or false, and we computed the proportion of factual state-
ments for each query. The response-level and statement-level scores were both averaged across all queries.
Factuality performance declined under the RAG setting for both models (Fig. 4a). At the response level, GPT-4o exhibited
a 6.0% drop in factuality, while Llama-3.1 showed a smaller decrease of 1.0%. At the statement level, both models experienced
comparable declines: 1.6% for GPT-4o and 1.9% for Llama-3.1. These results suggest that RAG negatively affects models
across different scales and capabilities. Notably, GPT-4o showed a sharper decline at the response level, indicating that factual
errors emerged more broadly across queries. However, since GPT-4o started from a much higher baseline (68.0%) compared
to Llama-3.1 (39.5%), the smaller drop observed in Llama-3.1 does not necessarily indicate greater robustness.
To better understand how evidence retrieval and selection affect model performance, we analyzed factuality at the statement
level by grouping each statement based on the type of evidence cited by the model. Specifically, for every statement produced
by a RAG model, we identified whether it was supported by (1) a retrieved relevant passage (True Positive), (2) a retrieved
but irrelevant passage (False Positive), (3) a self-generated source without retrieval grounding (Self-generated), or (4) no
reference at all (No Reference). As shown in Fig. 1(see II. Evidence Selection), references were explicitly marked within the
7/34

96.290.9
020406080100
GPT-4oLlama-3.1-8BBase LLMRAG
60.852.2020406080100
GPT-4oLlama-3.1-8BBase LLMRAG94.6(-1.6)89.0 (-1.9)
60.4 (-0.4)46.8 (-5.4)020406080100
GPT-4oLlama-3.1-8BBase LLMRAG
30.523.0020406080100
GPT-4oLlama-3.1-8BBase LLMRAG38.5 (-1.0)
28.0 (-2.5)20.5 (-2.5)39.568.0Statement-levelResponse-level62.0 (-6.0)Factuality Score (%)a
Statement-levelResponse-levelCompleteness Score (%)c7580859095100
GPT-4o + RAGLlama-3.1 + RAGTrue PositiveFalse PositiveSelf-generatedNo Reference97.1Statement-level Analysis (Factuality)b96.493.795.293.890.885.585.0
4050607080
GPT-4oGPT-4o + RAGLlama-3.1Llama-3.1 + RAGSupported & ReferencedSupported but MissedUnsupportedd71.762.160.668.265.364.250.557.251.356.944.856.0Statement-level Analysis (Completeness)Figure 4. Factuality and completeness of model responses. a , Average factuality scores at the response and statement levels
across queries. b, Statement-level factuality broken down by the type of evidence cited: relevant retrieved (True Positive),
irrelevant retrieved (False Positive), self-generated, or none. c, Average completeness scores at the response and statement
levels, based on coverage of must-have statements. d, Completeness for must-have statements, categorized by whether the
supporting evidence was retrieved and cited (Supported & Referenced), retrieved but not cited (Supported but Missed), or
not retrieved at all (Unsupported).
model’s response using numbered citation-style indicators (e.g., [1], [2]), each corresponding to a retrieved passage. By link-
ing each statement to its cited references, and using our manual relevance annotations for the retrieved passages, we assigned
every statement to one of the four evidence categories.
Factuality was highest when statements were grounded in relevant evidence, and declined when models relied on irrelevant
or self-generated sources (Fig. 4b). When citing true positive passages, GPT-4o achieved a factuality of 97.1%, while Llama-
3.1 reached 93.8%. When citing false positives, GPT-4o maintained relatively high performance with 95.2%, whereas Llama-
3.1 dropped more substantially to 85.5%. This suggests that GPT-4o is more robust to noisy or off-topic evidence, often
generating factually accurate content even when the cited material is not directly relevant. In contrast, Llama-3.1 appears more
sensitive to the quality of retrieved content. When models relied on self-generated evidence without any retrieved support, the
lowest performance was observed. GPT-4o again maintained a high factuality score of 96.4%, while Llama-3.1 fell to 85.0%,
indicating Llama-3.1’s greater vulnerability to hallucination in the absence of external grounding.
2.4.2 Completeness
At the response level, each response was assigned a score of 1 if it successfully addressed all must-have statements defined in the
gold-standard reference; otherwise, it received a score of 0. At the statement level, each response was assessed for how many
of the must-have statements it explicitly covered, and the proportion of addressed statements was calculated. Both scores were
averaged across all queries.
Completeness performance declined under the RAG setting for both models (Fig. 4c). At the response level, GPT-4o and
8/34

Llama-3.1 each showed a 2.5% decrease. At the statement level, GPT-4o exhibited a minimal decline of 0.4%, while Llama-3.1
experienced a more pronounced drop of 5.4%. These results suggest that incorporating retrieved evidence does not guarantee
improved coverage of essential content and may even hinder completeness, particularly in smaller or less capable models.
We further analyzed model performance at the statement level by categorizing must-have statements according to the
quality of the retrieved evidence and whether it was cited in the response. Specifically, each must-have statement was grouped
into one of three categories: (1) Supported and Referenced: The statement was supported by one of the top-16 retrieved
passages and explicitly referenced in the model’s response. (2) Supported but Missed: The statement was supported by a
retrieved passage but was not cited by the model. (3) Unsupported: The statement was not supported by any of the retrieved
passages.
Completeness was highest when models correctly referenced relevant evidence, and declined when such evidence was ei-
ther missed or not retrieved. (Fig. 4d). In the Supported and Referenced category, both models improved under the RAG
setting: GPT-4o increased from 68.2% to 71.7%, and Llama-3.1 from 50.5% to 56.9%. However, when models failed to
cite available supporting evidence (Supported but Missed), GPT-4o’s score decreased from 65.3% to 60.6%, and Llama-3.1
also experienced a slight decrease from 57.2% to 56.0%. These results suggest that access to high-quality evidence alone is
insufficient, and effective evidence selection remains critical for achieving high completeness. In the Unsupported category,
completeness declined across both models with RAG: GPT-4o fell from 64.2% to 62.1%, and Llama-3.1 from 51.3% to
44.8%. This indicates that even with retrieval augmentation, models may struggle to compensate when key content is absent
from the retrieved documents, especially in the case of Llama-3.1, which showed the steepest drop.
Answer accuracy We evaluated answer accuracy on the full dataset from which the USMLE-style queries were originally sam-
pled. In line with our earlier findings on factuality and completeness, both GPT-4o and Llama-3.1 showed clear performance
degradation after applying standard RAG at top- k=16, with accuracy dropping from 71.1% to 70.5% and from 49.4% to
45.8%, respectively. These results suggest that the declines in factuality and completeness are not isolated errors, but reflect
broader impairments in overall response quality.
2.5 Enhanced RAG Pipeline
We evaluated the impact of retrieval augmentation using five held-out QA datasets on GPT-4o and Llama-3.1. Four configura-
tions were tested: (i) standard RAG, which retrieves and appends passages to the input; (ii) RAG with evidence filtering, which
removes irrelevant passages to suppress retrieval noise; (iii) RAG with query reformulation, which addresses low coverage in
the initial retrieval by using the model’s first response as a rewritten query; and (iv) a combined pipeline integrating both fil-
tering and reformulation. The filtering module was fine-tuned on our expert-annotated relevance dataset using Llama-3.1-8B
as the backbone, while the reformulation component adopted a rationale-driven querying strategy to generate more targeted
follow-up queries (see Methods for details). Accuracy was automatically computed, and confidence intervals were estimated
via 10,000 bootstrap replicates sampled with replacement at the same size as each test set. Detailed results are provided in the
Supplementary Table 2.
Standard RAG produced inconsistent effects across datasets and retrieval depths (Fig. 5a, i). On Llama-3.1, standard RAG
led to accuracy gains in some cases (e.g., MedMCQA) but performance drops in others (e.g., MedQA). Even within the same
dataset, outcomes varied by top-k: for example, MMLU-Pro showed degradation from k=1tok=16and improvement only
atk=32. This instability highlights the sensitivity of standard RAG to retrieval noise and passage overload. Additionally,
standard RAG on GPT-4o also exhibited inconsistent effects, though performance degradation was even more pronounced
(Fig. 5b, i). On datasets where GPT-4o already achieved strong baseline accuracy, such as MedQA (0.922), MMLU (0.930),
and MMLU-Pro (0.864), standard RAG led to a decline in performance across all top-k settings. This suggests that for high-
performing LLMs, the additional context can introduce more distraction than utility. As with Llama-3.1, MedXPertQA was a
notable exception, where standard RAG yielded some gains at higher top-k values.
Combining evidence filtering and query reformulation yielded consistent and significant improvements across all bench-
marks (Fig. 5a, ii-iv). Individually, both filtering and reformulation led to performance improvements in some settings, but
failed to produce consistent gains across datasets. When integrated, however, the two components complemented each other—
filtering reduced noise from irrelevant passages, while reformulation compensated for low coverage in retrieved evidence—
resulting in stable and monotonic gains from k=4tok=32. The combined pipeline outperformed the non-RAG baseline by
up to +9.0% (MedQA), +6.2% (MMLU), +1.1% (MMLU-Pro), +12.0% (MedMCQA), and +8.2% (MedXpertQA), with all
improvements statistically significant (McNemar exact test, p<0.001for most and p=0.0022 for MMLU) except MMLU-
Pro. For GPT-4o, the combined pipeline led to notable gains on the more challenging datasets, MedMCQA (+3.4%) and
9/34

0.7200.6780.6520.650k=10.7080.6820.6500.65820.7100.7060.6700.65240.7200.7080.6900.65080.7620.7340.7200.660160.7400.7300.7020.636320.7660.7480.7480.7300.7800.7760.7360.7220.7820.7560.7500.7460.7960.7820.7520.7360.7860.7920.7580.7640.8000.7720.7600.7380.6040.5700.5910.5560.5700.5560.5530.5450.6280.5990.5990.5880.6260.5640.6150.5880.6280.6280.5830.5830.6310.6180.6020.6420.5860.5660.5640.5520.6060.5660.6040.5600.6320.6120.6460.5920.6340.5840.6180.5600.6220.6040.6360.5780.6480.6300.6340.6200.1360.1360.1360.1360.1780.1500.1440.1600.1680.1480.1820.1280.1700.1640.1840.1600.2060.1760.1840.1620.2180.1860.1720.180a
Base LLM: 0.672Base LLM: 0.738Base LLM: 0.620Base LLM: 0.528Base LLM: 0.136MedQAMMLUMMLU-ProMedMCQAMedXPertQA
0.8940.8960.8860.902k=10.9120.9020.8880.89820.9080.9040.8860.90840.9180.9140.8960.90480.9160.9140.9020.900160.9060.9200.9020.922320.7900.7780.7420.7820.7600.7740.7540.7700.7780.7800.7580.7680.7680.7780.7720.7520.7960.7800.7700.7920.8140.7860.7900.8040.9000.9120.8860.9040.9020.9040.8840.9020.9000.8980.9020.9000.9120.8960.9000.9140.9080.8900.9100.9100.9100.9080.9120.9120.8340.8340.8240.8260.8370.8400.8290.8500.8400.8160.8290.8450.8580.8560.8420.8400.8500.8500.8400.8400.8400.8400.8420.8320.2880.3180.2940.3100.3000.2900.2940.2880.3240.3240.3080.3000.3400.3200.3100.3140.3540.3320.3100.3140.3400.3420.3180.322Base LLM: 0.922Base LLM: 0.930Base LLM: 0.864Base LLM: 0.780Base LLM: 0.288b
Relative Gain(%)+-
Relative Gain(%)+-(i)(ii) (iii)(iv)
MedQAMMLUMMLU-ProMedMCQAMedXPertQA(i)(ii) (iii)(iv)(i)(ii) (iii)(iv)(i)(ii) (iii)(iv)(i)(ii) (iii)(iv)
(i)(ii) (iii)(iv)(i)(ii) (iii)(iv)(i)(ii) (iii)(iv)(i)(ii) (iii)(iv)(i)(ii) (iii)(iv)Figure 5. Performance of RAG variants and non-RAG baselines across five QA datasets . MedQA44, MMLU45,
MMLU-Pro46, MedMCQA47, and MedXPertQA48were used. a, Results using Llama-3.1-8B as the base model. b, Results
using GPT-4o as the base model. Cell colors indicate the magnitude and direction of accuracy changes relative to the base
LLM: sky-blue denotes performance gains, while red indicates performance drops. Four RAG configurations are compared:
(i) standard RAG (retrieval-only), (ii) retrieval + evidence filtering, (iii) retrieval + query reformulation, and (iv) retrieval +
both evidence filtering and query reformulation, evaluated at different top-k settings (k =1,2,4,8,16,32). Each cell displays
the accuracy of the model, while the color represents the relative gain compared to the base LLM. Positive gains (blue)
indicate improved performance over the base LLM, while negative values (red) indicate performance drops. Base LLM
accuracies are shown beneath each block for reference. The best scores are highlighted in bold.
MedXpertQA (+6.6%) ( p=0.013and0.027), whereas no improvement was observed on MedQA, MMLU, and MMLU-Pro,
where where baseline accuracy was already high and left limited room for further gains (Fig. 5b, ii–iv).
3 Discussion
This study presents the first large-scale, fine-grained evaluation of widely used medical RAG frameworks to elucidate their
behavior throughout the end-to-end pipeline. We surveyed the literature to understand which RAG configurations are most
commonly used in medical applications. Based on these representative frameworks, we evaluated two LLMs (GPT-4o and
Llama-3.1-8B) on 200 queries (real patient inquiries and USMLE-style questions) across evidence retrieval, evidence selection,
and response generation. Eighteen medical experts contributed over 80,000 expert annotations. Such an extensive, expert-
annotated analysis has not been previously achieved in the medical RAG literature. Whereas prior efforts, such as the MIRAGE
benchmark39, focused primarily on answer accuracy across large question sets, these efforts offered limited insight into how
and where failures emerge within the RAG pipeline. In contrast, our fully manual, component-level evaluation dissects the
end-to-end process and highlights system weaknesses, directly addressing recent calls for more nuanced and clinically relevant
RAG assessment25.
First and importantly, our evaluation shows that standard RAG may not improve and can even degrade LLM performance
on medical tasks. In head-to-head comparisons with their non-RAG counterparts, GPT-4o and Llama-3.1-8B both exhibited
modest drops in factuality and more pronounced declines in completeness, with the latter decreasing by over five percent-
age points at the statement level. This pattern held across both query types: for patient queries, GPT-4o’s factuality and
completeness dropped by approximately 2% and 8%, respectively; for USMLE-style queries, the decreases were 1% and 3%.
While many prior studies have highlighted RAG’s ability to improve factuality and mitigate hallucinations in medical applica-
tions29,42,49,50, others–particularly in the general domain–have reported that irrelevant or noisy retrieved content can distract
models or sustain hallucinations51–53. This aligns with our findings: when the model incorporated irrelevant rather than rel-
evant passages (i.e., the “False Positive” category in Fig. 4), statement-level factuality scores declined, with the effect being
especially pronounced for the smaller model, Llama-3.1, which exhibited a drop of over 8%. Completeness is another critical
10/34

dimension where retrieval is generally expected to fill knowledge gaps and broaden the scope of model responses. However,
prior evidence remains mixed. In long-form clinical QA tasks, one study reported improved completeness over non-RAG
LLMs based on manual evaluation29, whereas others observed declines–for example, a drop from 3.47 to 3.27 on a five-point
scale in one study37, and a roughly 5% decrease reported in another42. Our results indicate a consistent shortfall: when no
supporting passage was retrieved for a must-have statement (i.e., Unsupported), models often overlooked key information
they had previously included without retrieval, leading to substantial performance degradation, with over 6% for Llama-3.1.
Interestingly, when a relevant passage was retrieved but not cited by the model (i.e., Supported but Missed), completeness
still dropped by 1-5%, suggesting that the model may have been distracted by other retrieved passages and failed to integrate
critical evidence.
Through our stage-wise analyses, we highlight two primary bottlenecks underlying these declines. First, evidence retrieval
is often inadequate: only 22% of the top-16 passages were judged relevant, and 31% of queries lacked any relevant passage at
all. Prior studies have largely examined how irrelevant or conflicting retrieved content influences end-task performance51–57;
however, the retrieval accuracy for medical queries itself has rarely been evaluated in a standardized, expert-grounded manner.
Our manual evaluation addressed this gap, showing that current retrievers frequently mishandled clinical questions. Moreover,
the retrieved content covered only 33% of must-have statements overall and just 26% for USMLE-style queries. This indicates
that even with RAG, LLMs often receive limited knowledge support and must rely primarily on their internal knowledge to
address medical queries, highlighting the persistent challenge of providing sufficient context58,59. Importantly, our notion of
relevance differs from conventional definitions in information retrieval systems60–65. Rather than requiring an explicit answer
span, lexical overlap, or topical relatedness, we adopt a coverage-based criterion that evaluates whether a passage supplies the
necessary medical information to support key statements. This yields a more clinically grounded and functionally meaningful
view of retrieval quality.
The second bottleneck is evidence selection, where LLMs struggle to identify and integrate relevant information even when
it is retrieved. Both GPT-4o and Llama-3.1 struggled in selecting relevant evidence, achieving only 41-43% precision and 27-
49% recall. GPT-4o tended to cite a larger number of retrieved passages (90% of citations vs. 62% for Llama-3.1), indicating
a stronger inclination to incorporate retrieved content into its responses. This behavior presents a potential advantage: when
provided with high-quality, relevant evidence, GPT-4o appears better equipped to integrate it effectively. However, this respon-
siveness comes at a cost—its limited evidence selection precision leads to a greater number of irrelevant references, averaging
2.6 per query compared to 1.6 for Llama-3.1. In contrast, Llama-3.1 cited fewer retrieved passages and more frequently missed
relevant content, selecting only 34% of the relevant evidence available. It also relied more heavily on self-generated references
(37.8% of all citations), the majority of which (77.2%) could not be verified and often included fabricated titles, authors, or
publication sources. These findings highlight complementary failure modes between model scales: while larger models like
GPT-4o risk over-incorporating retrieved information without sufficient discrimination, smaller models like Llama-3.1 may
under-utilize retrieved evidence and compensate by hallucinating references. Our evidence selection analysis is closely related
to the actively growing body of research on evidence attribution and citation generation, both of which aim to evaluate and
improve the verifiability of LLM responses38,66–74. Consistent with these studies, our findings confirm that current LLMs
still exhibit notable limitations in reliably using available evidence and grounding their responses appropriately. Additionally,
we positioned evidence selection as a connecting layer within the medical RAG pipeline, enabling us to trace how retrieved
content contributes to model responses at the statement level. This represents a novel analytical perspective in the context of
medical RAG, bridging retrieval and generation more explicitly than in previous studies.
To better understand why RAG underperformed compared to its non-RAG counterparts, we conducted a qualitative error
analysis focusing on cases where RAG responses demonstrated lower statement-level factuality. We observed that RAG often
introduced errors by anchoring responses to misleading numerical references in the retrieved passages. For instance, in one
example, a prolactin level of 28 ng/mL was initially judged normal by GPT-4o, but the RAG model flagged it as abnormal.
This change appears to stem from retrieved information that defined the upper limit as 25 ng/mL; although this information
was contextually misaligned with the original question, the model incorrectly adopted the reference range and judged 28
ng/mL as abnormal. A similar issue emerged in response to the question, “What is the safest amount of Advil to take at one
time?” The GPT-4o-based RAG model responded that 400 mg is the safest dose–an overstatement likely driven by the fact
that multiple retrieved studies consistently referenced 400 mg as the standard single adult dose in bioequivalence and safety
evaluations, though these studies were situated in different clinical or experimental contexts. Another recurrent failure mode
involved lexical ambiguity or synonym mismatches in retrieval. When asked “Is poison ivy contagious between people?” , the
retriever surfaced content about the character Poison Ivy from DC Comics and the 1992 film of the same name rather than
about the allergenic plant. Consequently, Llama-3.1 generated a response describing the fictional character, entirely missing
the medical intent of the question. This error illustrates both the retriever’s failure to disambiguate terms and the model’s
11/34

inability to recover from such retrieval mistakes. We also found that RAG models were more prone to misinterpreting user
intent, especially when retrieved content introduced a competing frame. In response to the question “How long does this year’s
flu usually last?” , both non-RAG models correctly addressed symptom duration at the individual level. However, once RAG was
introduced, the models instead focused on the epidemiological length of flu seasons, which is an error likely caused by retrieved
passages discussing historical influenza trends and population-level data. These examples collectively demonstrate that errors
originating in the retrieval stage act as noise, impairing the accuracy and reliability of the model’s generated responses.
Beyond evaluation, we proposed augmenting the standard RAG pipeline with two lightweight and model-agnostic com-
ponents: evidence filtering and query reformulation. Unlike approaches that require retraining either the retriever or the
LLM75–77, our methods are readily applicable to any retrieval systems and LLMs. For evidence filtering, we first evaluated
zero-shot performance using our 3,200 expert-annotated samples spanning 200 queries and their corresponding top-16 re-
trieved passages. Zero-shot filtering proved insufficient, yielding limited performance for both Llama-3.1 (precision = 0.483,
recall = 0.566, F1 = 0.521) and GPT-4o (precision = 0.697, recall = 0.324, F1 = 0.442). After fine-tuning Llama-3.1 with
five-fold cross-validation, performance improved substantially (precision = 0.592, recall = 0.657, F1 = 0.623). These results
highlight that filtering is effective but cannot be reliably achieved in a zero-shot setting. Supervised signal remains important,
underscoring the value of our large-scale expert annotations as a reusable training resource for future modules. For query
reformulation, we drew inspiration from prior work showing that models capable of articulating explicit rationales tend to
generate more focused and contextually appropriate queries40,78,79. Building on this rationale-guided formulation approach,
we prompted the model to first produce an intermediate response to the initial query and then used that response itself as a
reformulated query. To assess the effect of reformulation, we compared retrieval outcomes between initial and reformulated
queries using the MedQA dataset. For each query, we retrieved 256 passages and applied our fine-tuned evidence filtering
model to assess relevance. On average, reformulated queries yielded substantially more passages deemed relevant than the
original queries (32% vs. 13%), indicating that reformulation enhanced both the precision and contextual alignment of the
retrieval stage.
Building on the same evaluation framework, we also explored whether stronger retrievers could further improve perfor-
mance. We evaluated two representative approaches: BM2561, a classical lexical baseline widely used for its simplicity, and
Qwen3-Embedding-0.6B80, an open-source LLM-based embedding model that outperforms commercial API-based models
(text-embedding-ada-002 ,text-embedding-3-large ) on the MTEB leaderboard81(https://huggingface.co/spaces/mteb/
leaderboard ). As shown in Supplementary Table 2, no single retriever consistently outperformed others across the five medi-
cal QA datasets. Performance varied by dataset, and even the most advanced model, Qwen3-Embedding-0.6B, did not achieve
the highest accuracy overall. These results indicate that retriever replacement alone cannot overcome the intrinsic limitations
of the RAG architecture. In contrast, our proposed modules (evidence filtering and query reformulation) yielded more robust
and generalizable improvements across both BM25 and Qwen3 retrievers, consistent with our MedCPT-based findings.
Looking forward, we recommend that future RAG development in medicine move beyond end-task performance metrics
and adopt a stage-wise perspective that explicitly examines retrieval, selection, and generation behaviors. Since RAG pipelines
do not consistently lead to performance gains across tasks, a safer and more reliable approach is to incorporate lightweight mod-
ules such as evidence filtering and query reformulation, which can offer practical improvements without requiring retriever or
model retraining. Further research should explore how these interventions interact with different retrievers, search databases,
medical subdomains, and LLM architectures, as well as how they influence factual grounding and reasoning consistency. We
also observed that the benefits of RAG augmentation tend to diminish when the underlying model already demonstrates strong
performance. In such cases, engineers may need to determine whether RAG should be applied using a small validation set. In
the longer term, an adaptive architecture that selectively invokes RAG only when the model’s internal knowledge appears
insufficient could offer a promising and efficient solution. Finally, establishing and sharing standardized expert-annotated
datasets, such as those introduced in this study, will be essential for advancing medical RAG. While expert-driven evaluation
remains critical, it should be complemented by more scalable strategies, including community-based review82and automated
assessment83. These efforts will support more reliable and sustainable evaluation across diverse medical settings.
12/34

4 Methods
4.1 Annotation
4.1.1 Data Curation
Patient queries We used a sample of 100 queries from the K-QA dataset42This dataset originally comprises 201 free-form
clinical questions sourced from real-world patient–physician conversations on an online consultation platform. The queries,
submitted by patients, span a broad range of topics, including ear, nose, and throat (ENT), dermatology, mental and emotional
health, and vision and eye care. Each query is paired with a long-form, natural language response written by a physician. In
addition, the dataset includes manual annotations identifying must-have and nice-to-have statements within the physician
responses.
USMLE-stytle queries We used a sample of 100 multiple-choice questions from the MedBullets dataset43, which is curated
for USMLE Step 2 and Step 3 preparation. Unlike other datasets that provide only questions and answers44,45, MedBullets
includes detailed, expert-authored explanations, offering richer context and rationale.
Statement extraction As the MedBullets dataset does not include statement-level annotations, we curated must-have state-
ments by using GPT-4o to segment each explanation into individual statements. We employed a few-shot prompting strategy,
using a mixture of general- and medical-domain examples adapted from prior work42,84. The full prompt with examples is
shown in Supplementary Table 3. The same procedure was also applied to segment model responses into individual statements
for downstream evaluation.
Must-have statement identification The full set of extracted statements, along with the question and gold answer, was then
passed to GPT-4o using a few-shot prompt to identify which statements were essential for answering the question, guiding
the model to select only the most critical information. The model produced a list of binary labels indicating whether each
statement was considered must-have. The full prompt with examples is shown in Supplementary Table 4.
Response generation We generated model responses using both RAG and non-RAG variants. We used identical prompts
to generate responses from both GPT-4o and Llama-3.1-8B. However, different sampling parameters were applied to ac-
commodate each model’s behavior. Specifically, GPT-4o was run with temperature = 0.8 , while Llama-3.1-8B used
temperature = 1.0 and top_p = 0.9 . We intentionally avoided greedy decoding, as deterministic outputs often failed
to comply with formatting requirements, particularly for citation generation. Instead, we introduced controlled randomness
and, if necessary, performed multiple inference attempts to obtain outputs that conformed to the expected structure. The full
prompt used is shown in Supplementary Table 5.
Model statement filtering Some model statements could not be reliably assessed for factuality, as they lacked verifiable factual
content. For instance, they simply rephrased the question, provided generic commentary, or included procedural language. To
exclude such statements, we applied a GPT-4o classifier prompt to identify and filter out non-distinctive content. A statement
was considered distinctive only if it introduced new clinical reasoning, factual knowledge, or a definitive judgment not already
stated or implied in the question. The prompt used for this classification is shown in Supplementary Table 6.
Statement-citation alignment We aligned inline citations with the individual statements extracted from each response. Any
inline citations were preserved alongside their corresponding statements, ensuring that each statement remained linked to
its supporting reference (or none, if no citation was originally provided). This process enabled us to later trace which refer-
ence was intended to support each claim. The alignment was performed using GPT-4o, with the prompt details provided in
Supplementary Table 7.
4.1.2 Annotation Procedure
Overview All annotations were conducted using a custom interface built on Label Studio ( https://labelstud.io/ ), de-
signed to provide annotators with a streamlined and user-friendly labeling workflow. Annotators were given written anno-
tation guidelines describing each task, along with example cases, and we held synchronous meetings prior to the annotation
phase to clarify task definitions and resolve ambiguities. A total of 18 annotators participated in this study. Of these, nine
13/34

were assigned to the evidence retrieval and evidence selection tasks, and the remaining nine were assigned to the response
generation tasks (factuality and completeness). All annotators involved in the response generation tasks were either medical
residents or clinical fellows, ensuring sufficient clinical expertise to assess the accuracy and completeness of model-generated
responses. Details on each annotator’s clinical training stage, medical specialty, and institutional affiliation at the time of an-
notation are provided in Supplementary Table 8. We ultimately collected over 80,502 annotations across all stages of the
pipeline, representing the largest human-labeled dataset for fine-grained evaluation of medical RAG systems.
Evidence Retrieval This subtask evaluates the relevance of retrieved passages with respect to the given query, defined by
whether a passage contains information essential to support the answer. Annotators are presented with a query, the top-
16 passages retrieved by the system, and a set of must-have statements identified from the gold response. Each passage is
paired with each must-have statement to form individual annotation units. For every passage-statement pair, annotators as-
sess whether the passage provides sufficient information to support the given statement. Support is defined as follows. First,
a statement is considered fully supported if the passage contains all the necessary information to infer the statement directly
and unambiguously. Second, partial support is assigned when the passage includes content that is thematically or contextually
related to the statement, but additional assumptions or reliance on external knowledge are required to complete the inference.
Finally, a passage is labeled as providing no support if it fails to offer any relevant information for the statement, or if it con-
tains content that contradicts it. Illustrative examples for each support category are provided in Supplementary Table 9. These
examples were also included in the official annotation guidelines and used during annotator training to ensure consistency and
shared understanding of the labeling criteria.
Evidence Selection This subtask evaluates the source attribution of model-generated references. Annotators are given (1) a
query, (2) the top-16 passages retrieved by the system, and (3) the references produced by the model. For each reference, they
determine which passage(s) served as its source of information. A passage is considered a source if its metadata and content
together provide sufficient information to reconstruct or justify the reference. When multiple passages jointly contribute to
a single reference, annotators are instructed to select all relevant passages. If none of the retrieved passages offer adequate
support, the reference is labeled as self-generated, indicating that the content originated from the model rather than from
retrieved evidence.
Response Generation This subtask evaluates the quality of model-generated responses along two axes: factuality and com-
pleteness. The factuality task assesses the accuracy of individual statements produced by the model. Annotators are provided
with the patient query, the model-generated response, and the corresponding reference response for context. Each model state-
ment is labeled as true if it aligns with the reference response or can reasonably be considered accurate based on the annotator’s
clinical expertise or authoritative medical sources. Conversely, a statement is labeled as false if it contradicts the reference,
conflicts with established medical knowledge, or introduces implausible or unverifiable claims. The completeness task exam-
ines whether the model response adequately captures the essential content of the reference response. Each reference response
is decomposed into a set of must-have statements, i.e., statements deemed clinically important and expected to appear in a
complete answer. For each must-have statement, annotators evaluate whether the model response fully supports it, partially
supports it, or fails to support it. This task adopts the same three-way support classification scheme as the evidence retrieval
task, enabling consistent, stage-aligned evaluation across the pipeline (see Supplementary Table 9for examples).
4.2 RAG Implementation
To design our RAG configuration, we conducted an extensive review of prior studies, particularly focusing on the choice of
retrieval corpora and retriever (i.e., search engine) architectures. Rather than adopting a custom corpus or architecture, we
opted for a widely adopted pipeline structure commonly reported in the literature. A summary of this review is provided in
Supplementary Table 1. Building on this baseline RAG system, we incorporated two key components–query formulation and
evidence filtering–to enhance performance. All implementation was based on the MedRAG toolkit ( https://github.com/
Teddy-XiongGZ/MedRAG ), with additional functionalities newly developed for this study. The resulting pipeline is illustrated
in Fig. 6.
Retrieval Corpora We adopted the multi-source retrieval setup proposed by MedRAG39, an early framework for medical-
domain RAG, and further augmented it with clinical guidelines. Our final retrieval corpus comprises five complementary
sources: (1) PubMed: A widely used biomedical article repository containing 23.9 million articles, covering diverse topics
14/34

RetrievalReformulatedQueryQuery ReformulationEvidence FilteringResponse GenerationInitial Query: “Benadryl makes me super sleepy. Can I take Claritin during the day instead?”Reformulated Query: “Diphenhydramine, the active ingredient in Benadryl, is a first-generation H1 histamine receptor antagonist … ”
…
LLM
Filtering Model
RetrieverVectorDatabaseCandidate Passages…
…
Refined Evidence Set
InitialQueryCandidate Passages
…
…
LLMRefined Evidence Set
Response: “Yes, you can take Claritin (loratadine) during the day as it is a non-drowsy antihistamine. Benadryl (diphenhydramine) is  … ”
InitialQuery
Figure 6. Enhanced RAG pipeline. This introduces two lightweight components–query reformulation and evidence
filtering–to improve the quality of retrieved evidence before generation. Given an input question, the LLM first reformulates
it to better suit retrieval (Query Reformulation). The retriever then searches external knowledge sources such as PubMed or
Wikipedia to gather candidate passages (Retrieval). A filtering model removes irrelevant evidence to retain informative
evidence (Evidence Filtering). Finally, the LLM generates a response conditioned on the refined evidence set (Response
Generation).
across the biomedical and life sciences literature. (2) StatPearls: A structured educational corpus comprising 301.2k passages
from 9.3k peer-reviewed clinical articles, designed primarily for medical board exam preparation and continuing medical edu-
cation. Each document follows a consistent format with sections such as epidemiology, pathophysiology, clinical presentation,
and management, making it a valuable resource for evidence-grounded summarization and retrieval. (3) Wikipedia: A broad-
coverage knowledge source with 29.9 million passages from 6.5 million documents, covering both general-domain and medi-
cal topics. (4) Medical textbooks: A set of 18 licensed medical textbooks spanning foundational medical disciplines, including
anatomy, histology, neurology, pathology, physiology, biochemistry, immunology, obstetrics and gynecology, pediatrics, psy-
chiatry, cell biology, internal medicine, pharmacology, and surgery. This corpus yielded 125.8k expert-authored snippets.
(5) Clinical guidelines: we additionally curated 723K passages from guidelines issued by nine leading health organizations
and online clinical sources, including: Cancer Care Ontario (CCO), Centers for Disease Control and Prevention (CDC), Cana-
dian Medical Association (CMA), International Committee of the Red Cross (ICRC), National Institute for Health and Care
Excellence (NICE), PubMed Clinical Queries, Strategy for Patient-Oriented Research (SPOR), World Health Organization
(WHO), and WikiDoc.
Retriever We used MedCPT41, a dual-encoder retriever pretrained on PubMed search logs. It comprises two components: a
query encoder and an article encoder. The query encoder transforms an input query into a dense vector at inference time, while
the article encoder pre-encodes all passages from the retrieval corpora into dense vector representations, which are stored in a
vector database. During inference, the query vector is matched against the pre-encoded passage vectors using vector similarity
metrics, and the top-k most similar passages are retrieved.
Evidence Filtering For the filtering model, we used Llama-3.1-8B to maintain architectural consistency with the base LLM.
The model is designed to estimate p(ˆy|q,d), where it takes a query qand a candidate passage das input and predicts a binary
label ˆy∈ {0,1}, indicating whether the passage is relevant to the query. We constructed a dataset of 3,200 query–passage
pairs with expert-provided annotations serving as ground truth labels. Given a query and a candidate passage, the model was
prompted to generate one of two tokens: “Yes” if the passage contained supporting evidence for the query, or “No” otherwise
(see Supplementary Table 10for the detailed prompt). The model was trained using standard causal language modeling
loss (cross-entropy loss) over the next token. Model hyperparameters were tuned through five-fold cross-validation. The
final configuration used a learning rate of 2e-6, a batch size of 8, and 3 training epochs. Training was performed using the
LLaMA-Factory repository85on a single 80GB A100 GPU.
15/34

Query Reformulation In medical QA, the formulation of user queries plays a critical role in retrieval performance. Overly
detailed queries, such as those containing lengthy patient narratives or exhaustive symptom descriptions, may overwhelm
the retriever with peripheral information, obscuring the core diagnostic clues. Conversely, overly concise queries often lack
sufficient clinical context, making it difficult to identify relevant evidence. To address these challenges, we generate rationale
queries that reflect the model’s internal reasoning process for solving the clinical task. This rationale serves as an optimized
query representation: for verbose inputs, it helps filter out irrelevant details and highlights diagnostically salient information;
for underspecified inputs, it fills in missing but medically necessary context that would be expected in a clinician’s reasoning.
Given an input question q, an LLM Mis prompted to produce a rationale ras follows:
Respond to the following clinical decision-making task using the provided patient information, in a step-by-step fash-
ion. Output your explanation and single option from the given options as the final answer.
During retrieval, only the rationale ris provided to the retriever. Preliminary analysis showed that including both the
original question and the rationale often exceeds the retriever’s input length limit and may introduce redundant or conflicting
signals. We retrieve the top-k passages using r, and the same model Mis then used to generate the final response.
4.3 Held-out Test Sets
We used five medical-domain QA benchmarks that consist of exam-style multiple-choice questions and are widely used as
standard testbeds in this field. These datasets primarily provide answer keys without long-form explanations, and our evalu-
ation focused solely on answer matching (i.e., accuracy). (1) MedQA44: A dataset of four-option multiple-choice questions
collected from medical question banks and exam preparation sites. (2) MedMCQA47: A large-scale dataset of Indian post-
graduate medical entrance exam questions (AIIMS, NEET-PG), covering a wide range of medical subjects. (3) MMLU45: A
popular, comprehensive multi-domain multiple-choice benchmark comprising 57 subjects. We selected a subset of MMLU
focused on clinical knowledge, college medicine, and professional medicine. (4) MMLU-Pro46: An enhanced and more chal-
lenging version of MMLU that increases the number of answer choices per question (from 4 to 10), filters out trivial or noisy
items, and emphasizes reasoning complexity. We likewise selected subsets of MMLU-Pro concentrated on clinical knowledge,
college medicine, and professional medicine. (5) MedXpertQA48: A highly challenging medical QA benchmark comprising
multiple-choice questions across 17 specialties and 11 body systems. It incorporates specialty board–style questions with ex-
tensive filtering, augmentation, and expert review. Even experts achieve only around 42% accuracy, making it one of the most
difficult medical QA benchmarks to date. Due to inference cost constraints, we randomly sampled up to 500 examples per
dataset when the original size exceeded this threshold.
References
1.Tian, S. et al. Opportunities and challenges for chatgpt and large language models in biomedicine and health. Briefings
Bioinforma. 25(2023).
2.Thirunavukarasu, A. J. et al. Large language models in medicine. Nat. medicine 29, 1930–1940 (2023).
3.Liu, F. et al. Application of large language models in medicine. Nat. Rev. Bioeng. 1–20 (2025).
4.Lucas, M. M., Yang, J., Pomeroy, J. K. & Yang, C. C. Reasoning with large language models for medical question
answering. J. Am. Med. Informatics Assoc. 31, 1964–1975 (2024).
5.Yang, R. et al. Integrating umls knowledge into large language models for medical question answering. arXiv preprint
arXiv:2310.02778 (2023).
6.Zhou, S. et al. Large language models for disease diagnosis: A scoping review. npj Artif. Intell. 1, 9 (2025).
7.Liu, X. et al. A generalist medical language model for disease diagnosis assistance. Nat. medicine 31, 932–942 (2025).
8.Kim, K. et al. Llm-guided multi-modal multiple instance learning for 5-year overall survival prediction of lung cancer.
InInternational Conference on Medical Image Computing and Computer-Assisted Intervention , 239–249 (Springer, 2024).
9.Kim, H. et al. Leme: Open large language models for ophthalmology with advanced reasoning and clinical validation.
arXiv e-prints arXiv–2410 (2024).
16/34

10. Wu, E., Wu, K. & Zou, J. Limitations of learning new and updated medical knowledge with commercial fine-tuning
large language models. NEJM AI 2, AIcs2401155 (2025).
11. Li, Q. et al. Reviewing clinical knowledge in medical large language models: Training and beyond. arXiv preprint
arXiv:2502.20988 (2025).
12. Hurst, A. et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024).
13. Team, G. et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint
arXiv:2403.05530 (2024).
14. Grattafiori, A. et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).
15. Hager, P. et al. Evaluation and mitigation of the limitations of large language models in clinical decision-making. Nat.
medicine 30, 2613–2622 (2024).
16. Li, J. et al. Benchmarking large language models in evidence-based medicine. IEEE J. Biomed. Heal. Informatics (2024).
17. Kell, G. et al. Question answering systems for health professionals at the point of care—a systematic review. J. american
medical informatics association 31, 1009–1024 (2024).
18. Gallifant, J. et al. The tripod-llm reporting guideline for studies using large language models. Nat. medicine 31, 60–69
(2025).
19. Kim, Y. et al. Medical hallucinations in foundation models and their impact on healthcare. arXiv preprint
arXiv:2503.05777 (2025).
20. Chen, Q. et al. Benchmarking large language models for biomedical natural language processing applications and rec-
ommendations. Nat. communications 16, 3280 (2025).
21. Press, O. et al. Citeme: Can language models accurately cite scientific claims? Adv. Neural Inf. Process. Syst. 37, 7847–
7877 (2024).
22. Wang, X. et al. Medcite: Can language models generate verifiable text for medicine? arXiv preprint arXiv:2506.06605
(2025).
23. Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv. neural information processing
systems 33, 9459–9474 (2020).
24. Gupta, S., Ranjan, R. & Singh, S. N. A comprehensive survey of retrieval-augmented generation (rag): Evolution,
current landscape and future directions. arXiv preprint arXiv:2410.12837 (2024).
25. Amugongo, L. M., Mascheroni, P., Brooks, S., Doering, S. & Seidel, J. Retrieval augmented generation for large language
models in healthcare: A systematic review. PLOS Digit. Heal. 4, e0000877 (2025).
26. Jin, Q., Yang, Y., Chen, Q. & Lu, Z. Genegpt: Augmenting large language models with domain tools for improved access
to biomedical information. Bioinformatics 40, btae075 (2024).
27. Fan, W. et al. A survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the
30th ACM SIGKDD conference on knowledge discovery and data mining , 6491–6501 (2024).
28. Gargari, O. K. & Habibi, G. Enhancing medical ai with retrieval-augmented generation: A mini narrative review. Digit.
health 11, 20552076251337177 (2025).
29. Zakka, C. et al. Almanac—retrieval-augmented language models for clinical medicine. Nejm ai 1, AIoa2300068 (2024).
30. Unlu, O. et al. Retrieval-augmented generation–enabled gpt-4 for clinical trial screening. NEJM AI 1, AIoa2400181
(2024).
31. Kresevic, S. et al. Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval
augmented generation-based framework. NPJ digital medicine 7, 102 (2024).
32. Wada, A. et al. Retrieval-augmented generation elevates local llm quality in radiology contrast media consultation. npj
Digit. Medicine 8, 395 (2025).
33. Ke, Y. H. et al. Retrieval augmented generation for 10 large language models and its generalizability in assessing medical
fitness. npj Digit. Medicine 8, 187 (2025).
34. Gaber, F. et al. Evaluating large language model workflows in clinical decision support for triage and referral and diag-
nosis. npj Digit. Medicine 8, 263 (2025).
17/34

35. Yu, H. et al. Evaluation of retrieval-augmented generation: A survey. In CCF Conference on Big Data , 102–120 (Springer,
2024).
36. Singh, A., Ehtesham, A., Kumar, S. & Khoei, T. T. Agentic retrieval-augmented generation: A survey on agentic rag.
arXiv preprint arXiv:2501.09136 (2025).
37. Gilson, A. et al. Enhancing large language models with domain-specific retrieval augment generation: A case study on
long-form consumer health question answering in ophthalmology. arXiv preprint arXiv:2409.13902 (2024).
38. Wu, K. et al. An automated framework for assessing how well llms cite relevant medical references. Nat. Commun. 16,
3615 (2025).
39. Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmarking retrieval-augmented generation for medicine. In Findings of the
Association for Computational Linguistics ACL 2024 , 6233–6251 (2024).
40. Sohn, J. et al. Rationale-guided retrieval augmented generation for medical question answering. In Proceedings of the
2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language
Technologies (Volume 1: Long Papers) , 12739–12753 (2025).
41. Jin, Q. et al. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical
information retrieval. Bioinformatics 39, btad651 (2023).
42. Manes, I. et al. K-qa: A real-world medical q&a benchmark. In Proceedings of the 23rd Workshop on Biomedical Natural
Language Processing , 277–294 (2024).
43. Chen, H., Fang, Z., Singla, Y. & Dredze, M. Benchmarking large language models on answering and explaining chal-
lenging medical questions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for
Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , 3563–3599 (2025).
44. Jin, D. et al. What disease does this patient have? a large-scale open domain question answering dataset from medical
exams. Appl. Sci. 11, 6421 (2021).
45. Hendrycks, D. et al. Measuring massive multitask language understanding. In International Conference on Learning
Representations (2021).
46. Wang, Y. et al. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Adv. Neural
Inf. Process. Syst. 37, 95266–95290 (2024).
47. Pal, A., Umapathi, L. K. & Sankarasubbu, M. Medmcqa: A large-scale multi-subject multi-choice dataset for medical
domain question answering. In Conference on health, inference, and learning , 248–260 (PMLR, 2022).
48. Zuo, Y. et al. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. In Forty-second International
Conference on Machine Learning (2025).
49. Miao, J., Thongprayoon, C., Suppadungsuk, S., Garcia Valencia, O. A. & Cheungpasitporn, W. Integrating retrieval-
augmented generation with large language models in nephrology: advancing practical applications. Medicina 60, 445
(2024).
50. Luo, M.-J. et al. Development and evaluation of a retrieval-augmented large language model framework for ophthalmol-
ogy. JAMA ophthalmology 142, 798–805 (2024).
51. Ding, H., Pang, L., Wei, Z., Shen, H. & Cheng, X. Retrieve only when it needs: Adaptive retrieval augmentation for
hallucination mitigation in large language models. arXiv preprint arXiv:2402.10612 (2024).
52. Niu, C. et al. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. In
Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 10862–
10878 (2024).
53. Sun, Z. et al. Redeep: Detecting hallucination in retrieval-augmented generation via mechanistic interpretability. In The
Thirteenth International Conference on Learning Representations (2025).
54. Cuconasu, F. et al. The power of noise: Redefining retrieval for rag systems. In Proceedings of the 47th International ACM
SIGIR Conference on Research and Development in Information Retrieval , 719–729 (2024).
55. Yoran, O., Wolfson, T., Ram, O. & Berant, J. Making retrieval-augmented language models robust to irrelevant context.
InThe Twelfth International Conference on Learning Representations (2024).
18/34

56. Wu, S. et al. How easily do irrelevant inputs skew the responses of large language models? In First Conference on Language
Modeling (2024).
57. Amiraz, C., Cuconasu, F., Filice, S. & Karnin, Z. The distracting effect: Understanding irrelevant passages in RAG. In
Che, W., Nabende, J., Shutova, E. & Pilehvar, M. T. (eds.) Proceedings of the 63rd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , 18228–18258 (Association for Computational Linguistics, Vienna,
Austria, 2025).
58. Joren, H. et al. Sufficient context: A new lens on retrieval augmented generation systems. In The Thirteenth International
Conference on Learning Representations (2025).
59. Xie, K., Laban, P., Choubey, P. K., Xiong, C. & Wu, C.-S. Do rag systems cover what matters? evaluating and optimizing
responses with sub-question coverage. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the
Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , 5836–5849 (2025).
60. Wei, X. & Croft, W. B. Lda-based document models for ad-hoc retrieval. In Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in information retrieval , 178–185 (2006).
61. Robertson, S., Zaragoza, H. et al. The probabilistic relevance framework: Bm25 and beyond. Foundations Trends Inf.
Retr. 3, 333–389 (2009).
62. Lavrenko, V. & Croft, W. B. Relevance-based language models. In ACM SIGIR Forum , vol. 51, 260–267 (ACM New
York, NY, USA, 2017).
63. Karpukhin, V. et al. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP) , 6769–6781 (2020).
64. Khattab, O. & Zaharia, M. Colbert: Efficient and effective passage search via contextualized late interaction over bert.
InProceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , 39–48
(2020).
65. Zhao, W. X., Liu, J., Ren, R. & Wen, J.-R. Dense text retrieval based on pretrained language models: A survey. ACM
Transactions on Inf. Syst. 42, 1–60 (2024).
66. Rashkin, H. et al. Measuring attribution in natural language generation models. Comput. Linguist. 49, 777–840 (2023).
67. Gao, T., Yen, H., Yu, J. & Chen, D. Enabling large language models to generate text with citations. In Proceedings of the
2023 Conference on Empirical Methods in Natural Language Processing (Association for Computational Linguistics, 2023).
68. Liu, N. F., Zhang, T. & Liang, P. Evaluating verifiability in generative search engines. In Findings of the Association for
Computational Linguistics: EMNLP 2023 , 7001–7025 (2023).
69. Yue, X. et al. Automatic evaluation of attribution by large language models. In Findings of the Association for Computational
Linguistics: EMNLP 2023 , 4615–4635 (2023).
70. Huang, C., Wu, Z., Hu, Y. & Wang, W. Training language models to generate text with citations via fine-grained
rewards. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
2926–2949 (2024).
71. Malaviya, C. et al. Expertqa: Expert-curated questions and attributed answers. In Proceedings of the 2024 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long
Papers) , 3025–3045 (2024).
72. Asai, A. et al. Openscholar: Synthesizing scientific literature with retrieval-augmented lms. arXiv preprint
arXiv:2411.14199 (2024).
73. Zhang, J. et al. LongCite: Enabling LLMs to generate fine-grained citations in long-context QA. In Che, W., Nabende,
J., Shutova, E. & Pilehvar, M. T. (eds.) Findings of the Association for Computational Linguistics: ACL 2025 , 5098–5122
(Association for Computational Linguistics, Vienna, Austria, 2025).
74. Wang, X. et al. MedCite: Can language models generate verifiable text for medicine? In Che, W., Nabende, J., Shutova, E.
& Pilehvar, M. T. (eds.) Findings of the Association for Computational Linguistics: ACL 2025 , 18891–18913 (Association
for Computational Linguistics, Vienna, Austria, 2025).
75. Asai, A., Wu, Z., Wang, Y., Sil, A. & Hajishirzi, H. Self-rag: Learning to retrieve, generate, and critique through self-
reflection. In International Conference on Learning Representations (2024).
19/34

76. Jeong, M., Sohn, J., Sung, M. & Kang, J. Improving medical reasoning through retrieval and self-reflection with retrieval-
augmented large language models. Bioinformatics 40, i119–i129 (2024).
77. Jin, B. et al. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint
arXiv:2503.09516 (2025).
78. Wang, L., Yang, N. & Wei, F. Query2doc: Query expansion with large language models. In Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing , 9414–9423 (2023).
79. Jagerman, R., Zhuang, H., Qin, Z., Wang, X. & Bendersky, M. Query expansion by prompting large language models.
arXiv preprint arXiv:2305.03653 (2023).
80. Zhang, Y. et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint
arXiv:2506.05176 (2025).
81. Muennighoff, N., Tazi, N., Magne, L. & Reimers, N. Mteb: Massive text embedding benchmark. In Proceedings of the
17th Conference of the European Chapter of the Association for Computational Linguistics , 2014–2037 (2023).
82. Chiang, W.-L. et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International
Conference on Machine Learning (2024).
83. Ru, D. et al. Ragchecker: A fine-grained framework for diagnosing retrieval-augmented generation. Adv. Neural Inf.
Process. Syst. 37, 21999–22027 (2024).
84. Min, S. et al. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing , 12076–12100 (2023).
85. Zheng, Y., Zhang, R., Zhang, J., YeYanhan, Y. & Luo, Z. Llamafactory: Unified efficient fine-tuning of 100+ language
models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System
Demonstrations) , 400–410 (2024).
86. Li, Y. et al. Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical
domain knowledge. Cureus 15(2023).
87. Rau, A. et al. A context-based chatbot surpasses radiologists and generic chatgpt in following the acr appropriateness
guidelines. Radiology 308, e230970 (2023).
88. Vector embeddings - OpenAI API.
89. Russe, M. F. et al. Performance of chatgpt, human radiologists, and context-aware chatgpt in identifying ao codes from
radiology reports. Sci. Reports 13, 14215 (2023).
90. Lozano, A., Fleming, S. L., Chiang, C.-C. & Shah, N. Clinfo. ai: An open-source retrieval-augmented large language
model system for answering medical questions using scientific literature. In Pacific Symposium on Biocomputing 2024 ,
8–23 (World Scientific, 2023).
91. National Center for Biotechnology Information. Entrez Programming Utilities Help . National Library of Medicine (US),
Bethesda, MD (2023).
92. Guo, Y., Qiu, W., Leroy, G., Wang, S. & Cohen, T. Retrieval augmentation of large language models for lay language
generation. J. Biomed. Informatics 149, 104580 (2024).
93. Ferber, D. et al. Gpt-4 for information retrieval and comparison of medical oncology guidelines. NEJM AI 1,
AIcs2300235 (2024).
94. Chen, X. et al. Evaluating and enhancing large language models’ performance in domain-specific medicine: development
and usability study with docoa. J. medical Internet research 26, e58158 (2024).
95. Izacard, G. et al. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118
(2021).
96. Cohan, A., Feldman, S., Beltagy, I., Downey, D. & Weld, D. SPECTER: Document-level representation learning using
citation-informed transformers. In Jurafsky, D., Chai, J., Schluter, N. & Tetreault, J. (eds.) Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics , 2270–2282 (Association for Computational Linguistics, Online,
2020).
97. Alkhalaf, M., Yu, P., Yin, M. & Deng, C. Applying generative ai with retrieval augmented generation to summarize and
extract key clinical information from electronic health records. J. Biomed. Informatics 156, 104662 (2024).
20/34

98. Benfenati, D., De Filippis, G. M., Rinaldi, A. M., Russo, C. & Tommasino, C. A retrieval-augmented generation applica-
tion for question-answering in nutrigenetics domain. Procedia Comput. Sci. 246, 586–595 (2024). 28th International
Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024).
99. Li, Z. et al. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281
(2023).
100. Bora, A. & Cuayáhuitl, H. Systematic analysis of retrieval-augmented generation-based llms for medical chatbot appli-
cations. Mach. Learn. Knowl. Extr. 6, 2355–2374 (2024).
101. Chen, X. et al. Eyegpt for patient inquiries and medical education: Development and validation of an ophthalmology
large language model. J. Med. Internet Res. 26, e60063 (2024).
102. Reimers, N. & Gurevych, I. Sentence-bert: Sentence embeddings using Siamese BERT-networks. In Inui, K., Jiang,
J., Ng, V. & Wan, X. (eds.) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , 3982–3992 (Association for
Computational Linguistics, Hong Kong, China, 2019).
103. Xiong, G. et al. Improving retrieval-augmented generation in medicine with iterative follow-up questions. In Biocom-
puting 2025: Proceedings of the Pacific Symposium , 199–214 (World Scientific, 2024).
104. Services, A. W. Titan text embeddings v2 (amazon.titan-embed-text-v2:0). Embedding model via Amazon Bedrock
(2024). Supports up to 8192 tokens, output dimensions 256/512/1024, multilingual 100+ languages.
105. Cui, L. et al. Bailicai: A domain-optimized retrieval-augmented generation framework for medical applications. Big
Data Min. Anal. (2025).
106. Bai, Y. et al. Qwen: Open foundation and instruction models by alibaba cloud. arXiv preprint arXiv:2309.16609 (2023).
107. Woo, J. J. et al. Custom large language models improve accuracy: comparing retrieval augmented generation and artificial
intelligence agents to noncustom models for evidence-based medicine. Arthrosc. The J. Arthrosc. & Relat. Surg. 41, 565–
573 (2025).
108. Zhang, G. et al. Leveraging long context in retrieval augmented language models for medical question answering. npj
Digit. Medicine 8, 239 (2025).
109. Shi, Y. et al. Mkrag: Medical knowledge retrieval augmented generation for medical question answering. In AMIA...
Annual Symposium proceedings. AMIA Symposium , vol. 2024, 1011–1020 (2025).
110. Chiang, W.-L. et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.
lmsys. org (accessed 14 April 2023) 2, 6 (2023).
111. Yang, Q. et al. Dual retrieving and ranking medical large language model with retrieval augmented generation. Sci.
Reports 15, 18062 (2025).
112. Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C. & Zaharia, M. ColBERTv2: Effective and efficient retrieval
via lightweight late interaction. In Carpuat, M., de Marneffe, M.-C. & Meza Ruiz, I. V. (eds.) Proceedings of the 2022
Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies ,
3715–3734 (Association for Computational Linguistics, Seattle, United States, 2022).
113. Wu, J. et al. Medical graph RAG: Evidence-based medical large language model via graph retrieval-augmented genera-
tion. In Che, W., Nabende, J., Shutova, E. & Pilehvar, M. T. (eds.) Proceedings of the 63rd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers) , 28443–28467 (Association for Computational Linguistics, Vi-
enna, Austria, 2025).
114. Tayebi Arasteh, S. et al. Radiorag: Online retrieval–augmented generation for radiology question answering. Radiol.
Artif. Intell. 7(2025).
115. Wada, A. et al. Retrieval-augmented generation elevates local llm quality in radiology contrast media consultation. npj
Digit. Medicine 8, 395 (2025).
116. Chen, Z. et al. Towards omni-RAG: Comprehensive retrieval-augmented generation for large language models in med-
ical applications. In Che, W., Nabende, J., Shutova, E. & Pilehvar, M. T. (eds.) Proceedings of the 63rd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers) , 15285–15309 (Association for Computational
Linguistics, Vienna, Austria, 2025).
21/34

117. Wang, Z., Khatibi, E. & Rahmani, A. M. Medcot-RAG: Causal chain-of-thought RAG for medical question answering.
InIEEE-EMBS International Conference on Body Sensor Networks 2025 (2025).
Acknowledgements
This study is supported by the National Institutes of Health National Library of Medicine under Award Number R01LM014604.
Author Contributions Statement
H.K. and Q.C. contributed to study design. H.K., H.J., S.P., Y.P., J.P., and S.C. contributed to drafting and refining the
annotation guidelines. H.K. and J.S. configured the annotation interface. S.A., H.J., S.P., Y.P., J.P., S.C., B.A.H.C., T.H., and
J.Y. contributed to data annotation (evidence retrieval and evidence selection). A.G., N.C., R.J., L.C., E.L., A.D., T.G., M.B.S.,
and Y.K. contributed to data annotation (factuality and completeness). A.G., N.C., S.A., and R.J. contributed to qualitative
analysis and case study. H.K., J.S., and E.F.W. contributed to model development. H.K. and Q.C. drafted the manuscript.
R.A.A., J.Z., A.T., A.C., and H.X. provided critical feedback on the manuscript and study design. All authors read and approved
the final version of the manuscript.
Data/Code Availability
Models and code are available at https://github.com/Yale-BIDS-Chen-Lab/medical-rag , and the full set of expert
annotations will be released upon publication acceptance.
Competing Interests
The authors declare no competing interests.
22/34

Supplementary Information
Supplementary Table 1. Summary of existing medical RAG frameworks. Frameworks are organized chronologically by
publication date, summarizing their retriever and LLM configurations, underlying knowledge sources, and downstream
medical applications. Only peer-reviewed studies published up to August 2025 were included, focusing specifically on
medical and healthcare domains while excluding general biomedical NLP applications. Studies lacking sufficient
implementation details were omitted. Each framework utilizes an LLM released after November 2022 (i.e., post-GPT-3.5).
Descriptions are kept at a high level; although some frameworks draw from similar knowledge sources (e.g., clinical
guidelines), their specific implementations and scopes vary across studies.
Publication
DateFramework Retriever(s) LLM(s) Knowledge Sources Application(s)
Jun 2023 ChatDoctor86Lexical matching Llama Wikipedia, MedlinePlus QA
Jul 2023 accGPT87Text-embedding-ada-
00288GPT-3.5 Clinical guidelines Imaging recommendation
Aug 2023 FraCChat89Text-embedding-ada-002 GPT-3.5, GPT-4 AO/OTA Fracture and Dis-
location Classification Com-
pendium (2018)AO code identification
Dec 2023 Clinfo.ai90BM25, Entrez API91GPT-3.5, GPT-4 PubMed QA, Summarization
Jan 2024 Almanac29Text-embedding-ada-002 GPT-4 PubMed, Clinical guidelines,
etc.QA
Jan 2024 RALL92DPR63, etc. GPT-4, Llama-2 Wikipedia, UMLS Lay language generation
May 2024 Ferber et al.93Text-embedding-ada-002 GPT-4 Clinical guidelines QA
Jun 2024 RECTIFIER30Text-embedding-ada-002 GPT-3.5, GPT-4 Clinical guidelines Clinical trial screening
Jul 2024 Self-BioRAG76MedCPT Llama-2 PubMed, PMC, Clinical guide-
lines, TextbooksQA
Jul 2024 DocOA94Not specified GPT-3.5 PubMed, Clinical guidelines Osteoarthritis management
Aug 2024 MedRAG39BM25, MedCPT, Con-
triever95, SPECTER96GPT-3.5, GPT-4,
Llama-2, Mixtral-
8x7B, etc.PubMed, StatPearls, Textbooks,
WikipediaQA
Aug 2024 Alkhalaf et al.97BM25 Llama-2 EHR Summarization
Sep 2024 ChatZOC50BM25 Baichuan Clinical guidelines, FAQs Opthalmology QA
Sep 2024 Benfenati et al.98BM25, GTE99GPT-3.5, Mistral Nutrigenetic polymorphism
dataset, PubMed-derived
nutrition–gene studiesNutrigenetics QA
Oct 2024 Bora and
Cuayáhuitl100GTR-T5-Large Llama-2, Mistral Textbooks, Journals QA
Dec 2024 EyeGPT101all-MiniLM-L6-v2102Llama-2 Textbooks, Custom Database Ophthalmology QA
Jan 2025 i-MedRAG103MedCPT GPT-4 StatPearls, Textbooks QA
Jan 2025 Azimi et al.100Titan Text Embeddings
V2104GPT-4o, Claude
3.5 SonnetRegistered Dietitian (RD) exam
questions across four nutrition
domains (Academy of Nutrition
and Dietetics guidelines and ref-
erences)Dietetic QA
Feb 2025 Bailicai105MedCPT Qwen106, Llama-2 PubMed, StatPearls, Textbooks,
WikipediaQA
Mar 2025 Woo et al.107Internal retriever GPT-3.5, GPT-4,
Claude 3, Llama-3,
Mistral-8×7BClinical guidelines Clinical decision support
Apr 2025 RAG squared40MedCPT GPT-4o, Llama-3,
etc.PubMed, PMC, Clinical guide-
lines, TextbooksQA
May 2025 BriefContext108BM25, MedCPT GPT-4o, Llama-
3.1PubMed, Textbooks Summarization
May 2025 MKRAG109Contriever, etc. Vicuna-7B110Disease Database Knowledge graph QA
May 2025 Yang et al.111Elasticsearch, Col-
BERTv2112GPT-4, Claude 3 Internal, expert-reviewed hospi-
tal documentsClinical decision support
Jul 2025 MedGraphRAG113Graph-based retrieval GPT-4, Gemini
1.0 Pro, Llama-2,
Llama-3MIMIC-IV, FakeHealth, Pub-
Health, UMLS GraphQA
23/34

Table 1 (continued)
Publication
DateFramework Retriever(s) LLM(s) Knowledge Sources Application(s)
Jul 2025 RadioRAG114Text-embedding-ada-002 GPT-4o mini Radiopaedia Case retrieval, Report genera-
tion
Jul 2025 Wada et al.115Text-embedding-3-
large88, Lexical matchingGPT-4o Clinical guidelines (ACR Man-
ual, ESUR Guidelines, etc.)Safety consultation
Jul 2025 MedOmniKB116MedCPT, Graph-based re-
trievalGPT-4, Gemini 1.5 PubMed, Clinical guidelines,
Textbooks, Wikipedia, UMLS,
DrugBankQA
Aug 2025 MedCoT-RAG117MedCPT Llama-3.1 PubMed, StatPearls, Textbooks,
WikipediaQA
24/34

Supplementary Table 2. Accuracy of RAG configurations using different retrievers with Llama-3.1-8B, with or without
evidence filtering and query reformulation. Each cell reports accuracy with 95% confidence intervals. The non-RAG
baseline is shown at the top. Blue and red shading denote gains and drops from the non-RAG baseline, respectively.
RAG Components Benchmarks
Retrieval Filtering Query Reform. MedQA MedMCQA MMLU MMLU-Pro MedXpertQA
7(Non-RAG) 7 7 0.672 0.528 0.738 0.620 0.136
(0.630, 0.712) (0.484, 0.572) (0.700, 0.776) (0.572, 0.668) (0.106, 0.166)
MedCPT 7 7 0.660 0.578 0.764 0.583 0.179
(k=16) (0.618, 0.702) (0.534, 0.620) (0.726, 0.800) (0.535, 0.634) (0.146, 0.214)
7 3 0.720 0.636 0.758 0.583 0.172
(0.680, 0.758) (0.594, 0.678) (0.718, 0.796) (0.532, 0.634) (0.140, 0.204)
3 7 0.734 0.604 0.792 0.628 0.186
(0.696, 0.772) (0.562, 0.648) (0.756, 0.828) (0.580, 0.679) (0.152, 0.220)
3 3 0.762 0.622 0.786 0.628 0.218
(0.724, 0.800) (0.580, 0.664) (0.750, 0.822) (0.578, 0.676) (0.182, 0.254)
MedCPT 7 7 0.636 0.620 0.738 0.642 0.162
(k=32) (0.594, 0.678) (0.578, 0.662) (0.698, 0.776) (0.594, 0.690) (0.130, 0.194)
3 7 0.702 0.634 0.760 0.602 0.191
(0.660, 0.742) (0.592, 0.676) (0.722, 0.798) (0.551, 0.652) (0.158, 0.226)
7 3 0.730 0.630 0.772 0.618 0.186
(0.690, 0.768) (0.586, 0.674) (0.734, 0.808) (0.570, 0.666) (0.152, 0.220)
3 3 0.740 0.648 0.800 0.631 0.206
(0.702, 0.778) (0.606, 0.690) (0.764, 0.834) (0.583, 0.679) (0.170, 0.242)
BM25 7 7 0.610 0.552 0.704 0.564 0.150
(k=16) (0.568, 0.654) (0.508, 0.596) (0.662, 0.744) (0.513, 0.612) (0.120, 0.182)
3 7 0.642 0.574 0.748 0.626 0.188
(0.598, 0.684) (0.530, 0.618) (0.710, 0.786) (0.578, 0.674) (0.154, 0.222)
7 3 0.692 0.560 0.765 0.687 0.158
(0.652, 0.732) (0.516, 0.604) (0.728, 0.802) (0.639, 0.733) (0.126, 0.190)
3 3 0.716 0.610 0.744 0.671 0.166
(0.676, 0.754) (0.568, 0.652) (0.704, 0.782) (0.623, 0.719) (0.134, 0.200)
BM25 7 7 0.648 0.574 0.749 0.593 0.158
(k=32) (0.606, 0.690) (0.532, 0.618) (0.712, 0.788) (0.543, 0.642) (0.126, 0.190)
3 7 0.676 0.584 0.752 0.591 0.200
(0.634, 0.716) (0.540, 0.628) (0.712, 0.790) (0.543, 0.639) (0.166, 0.236)
7 3 0.718 0.588 0.790 0.653 0.154
(0.678, 0.756) (0.546, 0.630) (0.752, 0.824) (0.604, 0.703) (0.122, 0.186)
3 3 0.744 0.616 0.780 0.663 0.174
(0.706, 0.782) (0.574, 0.658) (0.742, 0.814) (0.615, 0.711) (0.142, 0.208)
Qwen3-Emb-0.6B 7 7 0.654 0.596 0.738 0.612 0.170
(k=16) (0.612, 0.694) (0.554, 0.638) (0.700, 0.776) (0.561, 0.660) (0.138, 0.202)
3 7 0.698 0.620 0.764 0.639 0.182
(0.658, 0.736) (0.576, 0.662) (0.726, 0.802) (0.588, 0.687) (0.148, 0.216)
7 3 0.748 0.604 0.760 0.668 0.174
(0.710, 0.786) (0.562, 0.646) (0.722, 0.798) (0.620, 0.717) (0.142, 0.206)
3 3 0.734 0.630 0.768 0.663 0.184
(0.694, 0.772) (0.588, 0.672) (0.730, 0.806) (0.615, 0.711) (0.150, 0.218)
Qwen3-Emb-0.6B 7 7 0.696 0.588 0.746 0.602 0.184
(k=32) (0.656, 0.736) (0.544, 0.630) (0.708, 0.784) (0.553, 0.652) (0.150, 0.218)
3 7 0.702 0.614 0.758 0.660 0.178
(0.662, 0.742) (0.572, 0.656) (0.720, 0.796) (0.612, 0.709) (0.144, 0.212)
7 3 0.726 0.600 0.764 0.676 0.196
(0.686, 0.764) (0.556, 0.642) (0.726, 0.802) (0.628, 0.725) (0.162, 0.230)
3 3 0.760 0.630 0.750 0.655 0.192
(0.726, 0.796) (0.586, 0.672) (0.712, 0.788) (0.607, 0.703) (0.158, 0.226)
25/34

Supplementary Table 3. Prompt for statement extraction. The {input} placeholder is intended to be replaced with the
actual explanation or model response during prompting. We used GPT-4o with temperature set to 0 for deterministic output.
Prompt Template
Please breakdown the given text into independent facts. Review the examples provided below to gain a clearer understanding of the task requirements
and the expected output format.
Input: He made his acting debut in the film The Moon is the Sun’s Dream (1992), and continued to appear in small and supporting roles throughout
the 1990s. Output: [“He made his acting debut in the film. ” , “He made his acting debut in The Moon is the Sun’s Dream. ” , “The Moon is the Sun’s
Dream is a film. ” , “The Moon is the Sun’s Dream was released in 1992. ” , “After his acting debut, he appeared in small and supporting roles. ” , “After his
acting debut, he appeared in small and supporting roles throughout the 1990s. ”]
Input: He is also a successful producer and engineer, having worked with a wide variety of artists, including Willie Nelson, Tim McGraw, and Taylor
Swift. Output: [“He is successful. ” , “He is a producer. ” , “He is an engineer. ” , “He has worked with a wide variety of artists. ” , “illie Nelson is an artist. ” ,
“He has worked with Willie Nelson. ” , “Tim McGraw is an artist. ” , “He has worked with Tim McGraw. ” , “Taylor Swift is an artist. ” , “He has worked with
Taylor Swift. ”]
Input: Possible causes for right lower abdominal pain in a young female are Appendicitis, Inflammatory bowel disease, Diverticulitis, Kidney stone,
urinary tract infection, Ovarian cyst or torsion, Ectopic pregnancy, Pelvic inflammatory disease, endometriosis. Output: [“Possible cause for right
lower abdominal pain in a young female: Appendicitis. ” , “Possible cause for right lower abdominal pain in a young female: Inflammatory bowel
disease. ” , “Possible cause for right lower abdominal pain in a young female: Diverticulitis. ” , “Possible cause for right lower abdominal pain in a young
female: Kidney stone. ” , “Possible cause for right lower abdominal pain in a young female: urinary tract infection. ” , “Possible cause for right lower
abdominal pain in a young female: Ovarian cyst or torsion. ” , “Possible cause for right lower abdominal pain in a young female: Ectopic pregnancy. ” ,
“Possible cause for right lower abdominal pain in a young female: Pelvic inflammatory disease. ” ,“Possible cause for right lower abdominal pain in a
young female: endometriosis. ”]
Input: Hep A IgM refers to a specific type of antibody called Immunoglobulin M (IgM) against the virus hepatitis A. When infected with hepatitis
A, these antibodies are detectable at symptom onset and remain detectable for approximately three to six months. These antibodies might also be
detectable in the first month after hepatitis A vaccination. A negative or non-reactive result means no IgM antibodies against hepatitis A found in
your serum, meaning the absence of an acute or recent hepatitis A virus infection. Output: [“Hep A IgM refers to a specific type of antibody called
Immunoglobulin M (IgM) against the virus hepatitis A. ” , “When infected with hepatitis A, these antibodies are detectable at the time of symptom
onset. ” , “When infected with hepatitis A, these antibodies remain detectable for approximately three to six months after infection. ” , “These antibodies
might also be detectable in the first month after hepatitis A vaccination. ” , “The absence of IgM antibodies against hepatitis A in your serum indicates the
absence of an acute or recent hepatitis A virus infection. ” , “A negative or non-reactive result means that there were no IgM antibodies against hepatitis
A found in your serum. ”]
Input: methotrexate (Otrexup, Rasuvo, RediTrex) and thalidomide (Contergan, Thalomid) are both considered contraindicated for treatment of
UC in pregnancy. possible treatment for UC during pregnancy include low-risk drugs such as aminosalicylates (sulfasalazine and mesalamine),
immunomodulators (azathioprine, cyclosporine A ,6-mercaptopurine) and corticosteroids. Biological agents such as Infliximabl, Adalimumab,
Vedolizumab and Ustekinumab is generally avoided during pregnancy as their safety in pregnancy is not well established yet. Output: [“Methotrexate
(Otrexup, Rasuvo, RediTrex) is contraindicated for treatment of ulcerative colitis in pregnancy. ” , “Thalidomide (Contergan, Thalomid) is contraindi-
cated for treatment of ulcerative colitis in pregnancy. ” , “Aminosalicylates (sulfasalazine and mesalamine) are considered low-risk drugs for treatment
of ulcerative colitis during pregnancy. ” , “Immunomodulators (azathioprine, cyclosporine A, 6-mercaptopurine) are considered low-risk drugs for
treatment of ulcerative colitis during pregnancy. ” , “Corticosteroids are considered low-risk drugs for treatment of ulcerative colitis during pregnancy. ” ,
“Treatment for ulcerative colitis during pregnancy with biological agents such as Adalimumab is generally avoided during pregnancy as their safety
in pregnancy is not well established yet. ” , “Treatment for ulcerative colitis during pregnancy with biological agents such as Vedolizumab is generally
avoided during pregnancy as their safety in pregnancy is not well established yet. ” , “Treatment for ulcerative colitis during pregnancy with biological
agents such as Infliximab is generally avoided during pregnancy as their safety in pregnancy is not well established yet. ” , “Treatment for ulcerative
colitis during pregnancy with biological agents such as Ustekinumab is generally avoided during pregnancy as their safety in pregnancy is not well
established yet. ”]
# YOUR TASK
Input: {input}
Output:
26/34

Supplementary Table 4. Prompt for must-have statement identification. The {query} ,{answer} , and {statements}
placeholders are intended to be replaced with the actual question, gold answer, and corresponding list of statements during
prompting. We used GPT-4o with temperature set to 0 for deterministic output.
Prompt Template
Please categorize the provided statements as either “must-have” or “nice-to-have” .
*Must-have*: essential independent facts required to provide a complete answer to the given query.
*Nice-to-have*: supplementary or additional information that is useful but not essential.
Review the examples provided below to gain a clearer understanding of the task requirements and the expected output format.
Query: I am a 33 years old female with right lower abdominal pain , what could it be?
Answer: Possible causes for right lower abdominal pain in a young female are Appendicitis, Inflammatory bowel disease, Diverticulitis, Kidney stone,
urinary tract infection, Ovarian cyst or torsion, Ectopic pregnancy, Pelvic inflammatory disease, endometriosis.
Statement: [“Possible cause for right lower abdominal pain in a young female: Appendicitis. ” , “Possible cause for right lower abdominal pain in a young
female: Inflammatory bowel disease. ” , “Possible cause for right lower abdominal pain in a young female: Diverticulitis. ” , “Possible cause for right lower
abdominal pain in a young female: Kidney stone. ” , “Possible cause for right lower abdominal pain in a young female: urinary tract infection. ” , “Possible
cause for right lower abdominal pain in a young female: Ovarian cyst or torsion. ” , “Possible cause for right lower abdominal pain in a young female:
Ectopic pregnancy. ” , “Possible cause for right lower abdominal pain in a young female: Pelvic inflammatory disease. ” , “Possible cause for right lower
abdominal pain in a young female: endometriosis. ”]
Output: [“Must-have” , “Must-have” , “Must-have” , “Must-have” , “Must-have” , “Must-have” , “Must-have” , “Must-have” , “Must-have”]
Query: So what does the non reactive mean for the hep a igm
Answer: Hep A IgM refers to a specific type of antibody called Immunoglobulin M (IgM) against the virus hepatitis A. When infected with hepatitis
A, these antibodies are detectable at symptom onset and remain detectable for approximately three to six months. These antibodies might also be
detectable in the first month after hepatitis A vaccination. A negative or non-reactive result means no IgM antibodies against hepatitis A found in your
serum, meaning the absence of an acute or recent hepatitis A virus infection.
Statements: [“Hep A IgM refers to a specific type of antibody called Immunoglobulin M (IgM) against the virus hepatitis A. ” , “When infected with
hepatitis A, these antibodies are detectable at the time of symptom onset. ” , “When infected with hepatitis A, these antibodies remain detectable for
approximately three to six months after infection. ” , “These antibodies might also be detectable in the first month after hepatitis A vaccination. ” , “The
absence of IgM antibodies against hepatitis A in your serum indicates the absence of an acute or recent hepatitis A virus infection. ” , “A negative or
non-reactive result means that there were no IgM antibodies against hepatitis A found in your serum. ”]
Output: [“Must-have” , “Nice-to-have” , “Nice-to-have” , “Nice-to-have” , “Must-have” , “Must-have”]
Query: What medications are contraindicated for a pregnant woman with ulcerative colitis?
Answer: methotrexate (Otrexup, Rasuvo, RediTrex) and thalidomide (Contergan, Thalomid) are both considered contraindicated for treatment of
UC in pregnancy. possible treatment for UC during pregnancy include low-risk drugs such as aminosalicylates (sulfasalazine and mesalamine),
immunomodulators (azathioprine, cyclosporine A ,6-mercaptopurine) and corticosteroids. Biological agents such as Infliximabl, Adalimumab,
Vedolizumab and Ustekinumab is generally avoided during pregnancy as their safety in pregnancy is not well established yet. Treatment for ulcerative
colitis during pregnancy should be tailored by your OBGYN and gastroenterologist.
Statements: [“Methotrexate (Otrexup, Rasuvo, RediTrex) is contraindicated for treatment of ulcerative colitis in pregnancy. ” , “Thalidomide (Conter-
gan, Thalomid) is contraindicated for treatment of ulcerative colitis in pregnancy. ” , “Aminosalicylates (sulfasalazine and mesalamine) are considered
low-risk drugs for treatment of ulcerative colitis during pregnancy. ” , “Immunomodulators (azathioprine, cyclosporine A, 6-mercaptopurine) are con-
sidered low-risk drugs for treatment of ulcerative colitis during pregnancy. ” , “Corticosteroids are considered low-risk drugs for treatment of ulcerative
colitis during pregnancy. ” , “Treatment for ulcerative colitis during pregnancy with biological agents such as Adalimumab is generally avoided during
pregnancy as their safety in pregnancy is not well established yet. ” , “Treatment for ulcerative colitis during pregnancy with biological agents such as
Vedolizumab is generally avoided during pregnancy as their safety in pregnancy is not well established yet. ” , “Treatment for ulcerative colitis during
pregnancy with biological agents such as Infliximab is generally avoided during pregnancy as their safety in pregnancy is not well established yet. ” ,
“Treatment for ulcerative colitis during pregnancy with biological agents such as Ustekinumab is generally avoided during pregnancy as their safety in
pregnancy is not well established yet. ” , “Treatment for ulcerative colitis during pregnancy should be tailored by your OBGYN and gastroenterologist. ”]
Output: [“Must-have” , “Must-have” , “Nice-to-have” , “Nice-to-have” , “Nice-to-have” , “Nice-to-have” , “Nice-to-have” , “Nice-to-have” , “Nice-to-have” ,
“Must-have”]
# YOUR TASK
Respond only with a list containing either ‘Must-have’ or ‘Nice-to-have’ for each statement. No other responses are required.
Query: {query}
Answer: {answer}
Statements: {statements}
Output:
27/34

Supplementary Table 5. Prompt for response generation. Each prompt was constructed by combining a base query type
(either patient queries or USMLE-style questions) with one or more template components shown in the table. For instance,
the complete prompt for the RAG configuration with patient queries consists of (Patient Query) + (RAG) + (Input Query).
Prompt Type Template
Patient Queries Respond to the provided clinical inquiry. Your response must be accurate, clear, and include all essential information while omitting
extraneous details.
USMLE-style
QueriesAnswer to the provided multiple-choice question about medical knowledge in a step-by-step fashion. Output your explanation and
single option from the given options as the final answer.
Non-RAG Additionally, distinguish between statements within your response that require authoritative references and those that do not. Here,
a “statement” refers to an atomic or foundational piece of information, which may not necessarily form a complete sentence. For
statements requiring references, include citations immediately after the respective statements, with reference numbers enclosed in
square brackets (e.g., [1][2][3]). Citations may also be placed within the sentence, rather than just at the end, when appropriate.
At the end of your response, provide a consolidated list of references, formatted according to AMA guidelines. Label the section as
“### References, ” and number the references sequentially (1, 2, 3, etc.), with each reference on a separate line. Every reference
listed MUST be appropriately cited within the response using square brackets. Please double-check thoroughly to ensure this
requirement is met without exception.
RAG Additionally, distinguish between statements within your response that require authoritative references and those that do not. Here,
a ”statement” refers to an atomic or foundational piece of information, which may not necessarily form a complete sentence. For
statements requiring references, include citations immediately after the respective statements, with reference numbers enclosed in
square brackets (e.g., [1][2][3]). Citations may also be placed within the sentence, rather than just at the end, when appropriate.
The provided document snippets are retrieved through a search engine and may be used as references to support your response.
External references not included in the provided materials may also be incorporated. At the end of your response, provide a consoli-
dated list of references, formatted according to AMA guidelines. Label the section as “### References, ” and number the references
sequentially (1, 2, 3, etc.), with each reference on a separate line. Every reference listed MUST be appropriately cited within the
response using square brackets. Please double-check thoroughly to ensure this requirement is met without exception.
- Document -
{passage_1}
Metadata: {metadata_1}
- Document -
{passage_2}
Metadata: {metadata_2}
...
- Document -
{passage_16}
Metadata: {metadata_16}
Input Query ### Input Query
{query}
28/34

Supplementary Table 6. Prompt for model statement filtering. The {question} and {sentence} placeholders are
intended to be replaced with the actual question or model statement during prompting. We used GPT-4o with temperature
set to 0 for deterministic output.
Prompt Template
You are given a single sentence. Your task is to decide whether this sentence is distinctive or non-distinctive.
A sentence is Distinctive only if:
It does not simply restate or paraphrase the question stem, and
It introduces either:
(a) clinical reasoning or inference (e.g., ”this pattern suggests a mechanical cause” , ”these findings are consistent with TSC”), or
(b) definitions or factual information that go beyond what is stated in the question, or
(c) a final judgment or answer sentence (e.g., ”the most accurate test is ” , ”the answer is ”)
A sentence is Non-Distinctive if it falls into any of the following:
It restates or rewords content already found in the question
It presents a raw observation or finding from the question (e.g., age, vitals, exam result)
It provides a definition or fact that is already included or implied in the question
It is procedural, meta, or instructional (e.g., “Let’s analyze… ” or “We need to consider… ”)
You may refer to the list below when deciding. All of these are non-distinctive sentences:
- To determine the most strongly associated condition for the patient described, we need to analyze the clinical information provided step-by-step.
- To determine the correct answer, let’s analyze the given information step-by-step.
- To determine the most strongly associated condition with the patient’s symptoms, we need to analyze the information provided.
- To address the query, we will analyze the patient’s behavior and symptoms step-by-step to determine the most appropriate psychological defense
mechanism being demonstrated.
- The patient’s behavior will be analyzed to match the most appropriate psychological defense mechanism from the given options.
- To answer this question, we need to analyze the patient’s behavior and determine which coping mechanism he is exhibiting.
- To approach this question, let’s analyze the patient’s behavior and the options provided.
- To determine the most accurate test for the condition described in the question, we need to consider the patient’s symptoms.
- We need to consider physical examination findings.
- We need to consider laboratory test results in the context of common clinical scenarios.
- Step-by-Step Analysis: Patient Presentation.
- Step-by-Step Analysis: Laboratory Findings.
Question: A 1-year-old girl is brought to a neurologist due to increasing seizure frequency over the past 2 months. She recently underwent a neurology
evaluation which revealed hypsarrhythmia on electroencephalography (EEG) with a mix of slow waves, multifocal spikes, and asynchrony. Her parents
have noticed the patient occasionally stiffens and spreads her arms at home. She was born at 38-weeks gestational age without complications. She has
no other medical problems. Her medications consist of lamotrigine and valproic acid. Her temperature is 98.3°F (36.8°C), blood pressure is 90/75
mmHg, pulse is 94/min, and respirations are 22/min. Physical exam reveals innumerable hypopigmented macules on the skin and an irregularly
shaped, thickened, and elevated plaque on the lower back. Which of the following is most strongly associated with this patient’s condition?
For the given question, all of these are non-distinctive sentences (not limited to the followings):
- The patient is a 1-year-old girl.
- The patient has a history of increasing seizure frequency.
- The EEG reveals hypsarrhythmia.
- The patient has numerous hypopigmented macules.
- The patient exhibits occasional stiffening and spreading of arms.
- The patient has an irregularly shaped, thickened, and elevated plaque on the lower back.
- The patient is currently on lamotrigine.
- The patient is currently on valproic acid.
- The patient was born at term without complications.
- No other medical problems were reported for the patient.
- And more ...
# Question
{question}
# Target sentence (Do not output anything other than ”Distinctive” or ”Non-distinctive” .)
{sentence}
29/34

Supplementary Table 7. Prompt for statement-citation alignment. The {model_response} ,{model_statements} , and
{references} placeholders are intended to be replaced with the actual model response, corresponding list of statements,
and references during prompting. We used GPT-4o with temperature set to 0 for deterministic output.
Prompt Template
You are given a model-generated response that includes statements with inline citations (e.g., [1], [2], etc.), along with a list of references correspond-
ing to those citations.
Your task is to identify the alignment between each statement and the reference(s) it is associated with, based solely on the position of the citation
markers in the response.
Instructions:
1. For each statement in the response, determine whether it is linked to one or more references by locating citation markers (e.g., [1]) near or within
the statement.
2. For each linked reference, assign it to the corresponding statement.
3. Do not assess the factual accuracy or content of the reference. Only use the citation markers and their position in the response to make the alignment.
4. Consider a “statement” to be a single sentence or a semantically independent clause.
### Input:
- **Model Response**: {model_response}
- **Statements**: {model_statements}
- **References**: {references}
### Output format: Your output should be organized in JSON, without any other responses, using the following format:
{ “#1”: { “statement”: “<copy of the statement>” , “refs”: [1, 3] or [] if no references are associated }, “#2”: { “statement”: “ ... ” , “refs”: [...] } }
### Output:
30/34

Supplementary Table 8. Profiles of annotators. This table lists all clinicians and medical trainees who contributed to the
expert annotation of evidence retrieval, selection, and response evaluation tasks, along with their affiliations and specialties.
Affiliations are listed as of the project’s start and may have changed since then.
Name Affiliation Specialty (if applicable)
Residents / Fellows
Nicholas Cochran-Caggiano Yale School of Medicine (EMS Fellowship) Emergency Medicine
Leah Colucci Yale School of Medicine Emergency Medicine
Tuo Guo Yale School of Medicine Emergency Medicine
Roy Jiang Yale School of Medicine Dermatology
Eric Lai Yale School of Medicine Ophthalmology
Amisha Dave Yale School of Medicine Ophthalmology
Maxwell B. Singer Yale School of Medicine Ophthalmology
Aidan Gilson Massachusetts Eye and Ear, Harvard Medical School Ophthalmology
Yonghoe Koo Asan Medical Center, University of Ulsan College of Medicine Physical Medicine and Rehabilitation
MD Candidates and Graduates
Serina Applebaum Yale School of Medicine –
Thomas Huang Yale School of Medicine –
Brittany Alexandra Herrera Contreras Yale School of Medicine, San Juan Bautista School of Medicine –
Heeju Jin Seoul National University College of Medicine –
Seihee Park Seoul National University College of Medicine –
Yujin Park Seoul National University College of Medicine –
Seoyoung Choi Seoul National University College of Medicine –
Jiyeong Park Seoul National University College of Medicine –
Jaehoon Yun Hanyang University College of Medicine –
31/34

Supplementary Table 9. Examples used in annotator training for support label decisions. These training samples illustrate
how annotators were instructed to evaluate whether a given context supports the associated statement.
Field Content
Context Individuals with diabetes are recommended to limit their consumption of sweets to one or two times per week. It is also suggested
being selective with desserts and to focus on foods with a low glycemic index, such as high fiber foods like whole grains and legumes,
as well as certain lower sugar fruits like berries, melons, and apples.
Statement It is recommended that diabetics avoid sweets.
Explanation The context mentions to limit their consumption of sweets, which contrasts with the suggestion to avoid their consumption.
Label No Support
Context Right lower abdominal pain in a 25-year-old female could be caused by a variety of medical conditions. Some potential causes
include: Ovarian cyst: a fluid-filled sac on the ovary—Ectopic pregnancy: a pregnancy that occurs outside the uterus.
Statement Possible cause for right lower abdominal pain in a young female can be Appendicitis.
Explanation Insufficient information.
Label No Support
Context Mites and insects Ivermectin is also used to treat infection with parasitic arthropods. Scabies – infestation with the mite Sarcoptes
scabiei – is most commonly treated with topical permethrin or oral ivermectin. For most scabies cases, ivermectin is used in a two
dose regimen: a first dose kills the active mites, but not their eggs. Over the next week, the eggs hatch, and a second dose kills the
newly hatched mites. For severe “crusted scabies” , the U.S. Centers for Disease Control and Prevention (CDC) recommends up to
seven doses of ivermectin over the course of a month, along with a topical antiparasitic. Both head lice and pubic lice can be treated
with oral ivermectin, an ivermectin lotion applied directly to the affected area, or various other insecticides. Ivermectin is also used
to treat rosacea and blepharitis, both of which can be caused or exacerbated by Demodex folliculorum mites.
Statement Permethrin cream (Elimite) is a topical treatment for scabies.
Label Full Support
Context Overall, the heart rate is decreased while stroke volume is increased, resulting in a net increase in blood pressure, leading to increased
tissue perfusion. This causes the myocardium to work more efficiently, with optimized hemodynamics and an improved ventricular
function curve. Other electrical effects include a brief initial increase in action potential, followed by a decrease as the K+ conductance
increases due to increased intracellular amounts of Ca2+ ions. The refractory period of the atria and ventricles is decreased, while it
increases in the sinoatrial and AV nodes. A less negative resting membrane potential is made, leading to increased irritability. The
conduction velocity increases in the atria, but decreases in the AV node. The effect upon Purkinje fibers and ventricles is negligible.
Automaticity is also increased in the atria, AV node, Purkinje fibers, and ventricles.
Statement A calcium channel blocker would not change the relative velocities of conduction in Purkinje fibers, atria, and ventricles.
Explanation The context compares the pharmacological mechanism of action, action potential, and relative velocity after digoxin administration.
However, it does not provide an explanation of conduction velocity when a calcium channel blocker is administered. While it is
related to the statement in that it discusses conduction velocity after drug administration, it does not mention calcium channel
blockers. Please be careful not to annotate it as partial support incorrectly.
Label No Support
Context Junctional rhythm describes an abnormal heart rhythm resulting from impulses coming from a locus of tissue in the area of the
atrioventricular node, the “junction” between atria and ventricles. Under normal conditions, the heart’s sinoatrial node determines
the rate by which the organ beats – in other words, it is the heart’s “pacemaker” . The electrical activity of sinus rhythm originates
in the sinoatrial node and depolarizes the atria. Current then passes from the atria through the atrioventricular node and into the
bundle of His, from which it travels along Purkinje fibers to reach and depolarize the ventricles. This sinus rhythm is important
because it ensures that the heart’s atria reliably contract before the ventricles.
Statement Conduction through the AV node is slow to allow the ventricles enough time to fill with blood.
Explanation Insufficient information.
Label No Support
Context The Purkinje fibers (; Purkinje tissue or subendocardial branches) are located in the inner ventricular walls of the heart, just beneath
the endocardium in a space called the subendocardium. The Purkinje fibers are specialized conducting fibers composed of electrically
excitable cells. They are larger than cardiomyocytes with fewer myofibrils and many mitochondria. They conduct cardiac action
potentials more quickly and efficiently than any other cells in the heart. Purkinje fibers allow the heart’s conduction system to create
synchronized contractions of its ventricles, and are essential for maintaining a consistent heart rhythm. Histology Purkinje fibers
are a unique cardiac end-organ. Further histologic examination reveals that these fibers are split in ventricles walls. The electrical
origin of atrial Purkinje fibers arrives from the sinoatrial node. Given no aberrant channels, the Purkinje fibers are distinctly shielded
from each other by collagen or the cardiac skeleton.
Statement 1 Conduction through the Purkinje system is the fastest within the heart.
Label Full Support
32/34

Field Content
Statement 2 The conduction velocity through the heart in order of speed is Purkinje fibers > atria > ventricles > AV node.
Explanation From the statement, “ They conduct cardiac action potentials more quickly and efficiently than any other cells in the heart , ” it is partially
inferable that Purkinje fibers > others in terms of conduction speed and efficiency.
Label Partial Support
Context Function The AV node receives two inputs from the right atrium: posteriorly, via the crista terminalis, and anteriorly, via the inter-
atrial septum. Contraction of heart muscle cells requires depolarization and repolarization of their cell membranes. Movement of
ions across cell membranes causes these events. The cardiac conduction system (and AV node part of it) coordinates myocyte me-
chanical activity. A wave of excitation spreads out from the sinoatrial node through the atria along specialized conduction channels.
This activates the AV node. The atrioventricular node delays impulses by approximately 0.09s. This delay in the cardiac pulse is
extremely important: It ensures that the atria have ejected their blood into the ventricles first before the ventricles contract. This
also protects the ventricles from excessively fast rate response to atrial arrhythmias.
Statement The conduction velocity of the structures of the heart are in the following order: Purkinje fibers > atria > ventricles > AV node.
Explanation Insufficient information. Although there is some mention of “delay, ” the context does not provide information to compare the
conduction speeds of the AV node, Purkinje fibers, atria, and ventricles. Therefore, it should be annotated as no support.
Label No Support
Context Lexapro, also known by its generic name escitalopram, is a selective serotonin reuptake inhibitor (SSRI) commonly prescribed for
the treatment of major depressive disorder (MDD) and generalized anxiety disorder (GAD) [1]. It works by increasing the levels
of serotonin, a neurotransmitter associated with mood regulation, in the brain [2].
Dosage: The typical starting dose for adults with MDD or GAD is 10 mg once daily, which may be increased to a maximum of 20
mg per day depending on the patient’s response and tolerability [4].
Side Effects: Common side effects include nausea, insomnia, fatigue, dry mouth, increased sweating, and sexual dysfunction [5].
Serious side effects can include serotonin syndrome, which is characterized by symptoms such as agitation, hallucinations, and
rapid heart rate [6].
Contraindications: Lexapro should not be used in patients who are taking monoamine oxidase inhibitors (MAOIs) or have a known
hypersensitivity to escitalopram or citalopram [7].
Warnings and Precautions:
Suicidality: Antidepressants, including Lexapro, may increase the risk of suicidal thoughts and behaviors in children, adolescents,
and young adults [8].
Serotonin Syndrome: Risk increases when used in combination with other serotonergic drugs [6].
QT Prolongation: Lexapro can cause dose-dependent QT interval prolongation, which can lead to serious cardiac arrhythmias [9].
Statement 1 Escitalopram is sold under the brand names Lexapro and Cipralex.
Explanation From the statement “ Lexapro, also known by its generic name escitalopram , ” it is clear that Lexapro is the brand name for escitalopram.
However, since there is no mention of Cipralex, it should be annotated as partial support.
Label Partial Support
Statement 2 Side effects of Escitalopram include GI symptoms such as nausea, diarrhoea, constipation.
Explanation The context lists various side effects, but since there is no mention of diarrhea or constipation, it should be annotated as partial
support.
Label Partial Support
Statement 3 Side effects of Escitalopram include insomnia.
Label Full Support
Statement 4 The FDA had published a black box warning regarding Escitalopram.
Explanation Since “ The FDA had published a black box warning regarding Escitalopram ” is not mentioned, Partial Support would be a more appro-
priate annotation.
Label Partial Support
Statement 5 Lexapro is not approved for use in pediatric patients less than 12 years of age.
Explanation The context states, “ Antidepressants, including Lexapro, may increase the risk of suicidal thoughts and behaviors in children. ” However,
this does not indicate that the drug is not approved for use. Therefore, the appropriate annotation is no support.
Label No Support
Context Oral ivermectin is an effective option for the treatment of scabies. It is often used in a two-dose regimen, with the first dose killing
the active mites and the second dose, administered one week later, targeting the newly hatched mites [1]. Ivermectin is particularly
useful for treating crusted scabies and is sometimes prescribed in combination with a topical agent [2]. Although it is not FDA-
approved for scabies treatment in the United States, it is widely used off-label for this purpose [3]. Studies have shown that oral
ivermectin is effective and generally well-tolerated in children and infants, although its use in children weighing less than 15 kg is
off-label [4, 5].
33/34

Field Content
Statement Ivermectin (Stromectol) is not safe for children under 15kg.
Explanation Off-label use cannot always be considered unsafe. Therefore, this statement is not necessarily supported.
Label No Support
Supplementary Table 10. Prompt for evidence filtering.
Prompt Template
Given a query and a text passage, determine whether the passage contains supporting evidence for the query. Supporting evidence means that the
passage provides clear, relevant, and factual information that directly backs or justifies the answer to the query.
Respond with one of the following labels:
“Yes” if the passage contains supporting evidence for the query.
“No” if the passage does not contain supporting evidence.
You should respond with only the label (Yes or No) without any additional explanation.
Question: {question}
Passage: {passage}
34/34