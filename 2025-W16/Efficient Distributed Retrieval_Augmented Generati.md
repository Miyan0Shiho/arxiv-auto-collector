# Efficient Distributed Retrieval-Augmented Generation for Enhancing Language Model Performance

**Authors**: Shangyu Liu, Zhenzhe Zheng, Xiaoyao Huang, Fan Wu, Guihai Chen, Jie Wu

**Published**: 2025-04-15 13:53:08

**PDF URL**: [http://arxiv.org/pdf/2504.11197v2](http://arxiv.org/pdf/2504.11197v2)

## Abstract
Small language models (SLMs) support efficient deployments on
resource-constrained edge devices, but their limited capacity compromises
inference performance. Retrieval-augmented generation (RAG) is a promising
solution to enhance model performance by integrating external databases,
without requiring intensive on-device model retraining. However, large-scale
public databases and user-specific private contextual documents are typically
located on the cloud and the device separately, while existing RAG
implementations are primarily centralized. To bridge this gap, we propose
DRAGON, a distributed RAG framework to enhance on-device SLMs through both
general and personal knowledge without the risk of leaking document privacy.
Specifically, DRAGON decomposes multi-document RAG into multiple parallel token
generation processes performed independently and locally on the cloud and the
device, and employs a newly designed Speculative Aggregation, a dual-side
speculative algorithm to avoid frequent output synchronization between the
cloud and device. A new scheduling algorithm is further introduced to identify
the optimal aggregation side based on real-time network conditions. Evaluations
on real-world hardware testbed demonstrate a significant performance
improvement of DRAGON-up to 1.9x greater gains over standalone SLM compared to
the centralized RAG, substantial reduction in per-token latency, and negligible
Time to First Token (TTFT) overhead.

## Full Text


<!-- PDF content starts -->

Efficient Distributed Retrieval-Augmented Generation for
Enhancing Language Model Performance
Shangyu Liu1, Zhenzhe Zheng1, Xiaoyao Huang2, Fan Wu1, Guihai Chen1Jie Wu2
1Shanghai Jiao Tong University
2Cloud Computing Research Institute, China Telecom
{liushangyu,zhengzhenzhe}@sjtu.edu.cn, huangxy32@chinatelecom.cn,
{fwu,gchen}@cs.sjtu.edu.cn, wujie@chinatelecom.cn
Abstract
Small language models (SLMs) support efficient deployments on
resource-constrained edge devices, but their limited capacity com-
promises inference performance. Retrieval-augmented generation
(RAG) is a promising solution to enhance model performance by in-
tegrating external databases, without requiring intensive on-device
model retraining. However, large-scale public databases and user-
specific private contextual documents are typically located on the
cloud and the device separately, while existing RAG implemen-
tations are primarily centralized. To bridge this gap, we propose
DRAGON, a distributed RAG framework to enhance on-device
SLMs through both general and personal knowledge without the
risk of leaking document privacy. Specifically, DRAGON decom-
poses multi-document RAG into multiple parallel token generation
processes performed independently and locally on the cloud and
the device, and employs a newly designed Speculative Aggregation,
a dual-side speculative algorithm to avoid frequent output synchro-
nization between the cloud and device. A new scheduling algorithm
is further introduced to identify the optimal aggregation side based
on real-time network conditions. Evaluations on real-world hard-
ware testbed demonstrate a significant performance improvement
of DRAGON‚Äîup to 1.9√ógreater gains over standalone SLM com-
pared to the centralized RAG, substantial reduction in per-token
latency, and negligible Time to First Token (TTFT) overhead.
1 Introduction
Although large language models (LLMs) such as GPT-4 [ 42] and
DeepSeek-V3 [ 13] have demonstrated remarkable performance in
real-world applications, their substantial deployment costs have led
to predominant cloud-based hosting. As a result, users are required
to upload private context along with their queries, raising serious
privacy concerns [ 12]. Recently, small language models (SLMs)
such as Phi-4-mini [ 1] and Qwen2.5-1.5B [ 57], have emerged as
promising alternatives, offering efficient local deployment on edge
devices. However, although SLMs are notably smaller than cloud-
hosted LLMs‚Äîleading to reduced performance on both personal and
general tasks‚Äîthey still remain too large for resource-constrained
devices to support on-device fine-tuning or training [ 27] to adapt
to newly generated data and user feedback.
Retrieval-augmented generation (RAG) [ 36,46] has demonstrated
effectiveness in boosting the performance of SLMs by incorporating
contextually relevant documents from external databases, such as
Wikipedia [ 4]. The performance gain increases monotonically with
the scale of the database [ 49], showing an opportunity for SLMs to
achieve comparable or even better performance than standalone
LLMs [ 11]. More importantly, by expanding user-specific external
DistributedRetrievalCentralizedRAG
+
+
+
+
+
AggregateRusty Lake2simulation1puzzle1TheSims2simulation1StardewValley2
longwaitStardewValley2simulation1DistributedRAGCloudDevice
I like games, among which is a hot selleronSteam.‚ë†‚ë°
Languagemodels
Cloud-OnlyDevice-OnlyFigure 1: Comparison between different RAG architectures.
database (also known as the non-parametric memory [ 36]), model
customization and knowledge updates can be achieved efficiently
without model training. Typically, large-scale public databases con-
taining general knowledge are hosted in the cloud, whereas user-
specific private databases are maintained on-device. Since the query
context may involve both general and personal data, it is essential
for retrieval-augmented SLMs to support distributed databases lo-
cated in the cloud and device. Unfortunately, most existing RAG so-
lutions [ 7,36,46] adopt a centralized architecture. Figure 1 presents
an example of game recommendation. The cloud-only RAG returns
an incorrect game genre, although private documents indicate a
preference for simulation games. In contrast, the device-only RAG
fails to retrieve the best-selling game lists without accessing to the
general knowledge in the cloud.
An intuitive solution, similar to federated search [ 52], is to re-
trieve documents from the cloud-side database, merge them with
those retrieved locally on-device, and perform model inference in
a centralized manner. However, this approach may incur substan-
tial latency overhead considering key-value (KV) caching [ 64], a
fundamental mechanism in language model serving that stores
intermediate attention states to enable efficient reuse of past com-
putations. Given a context sequence length ùëõ, the KV cache trades
ùëÇ(ùëõ)storage for a reduction in decoding time complexity from
ùëÇ(ùëõ2)toùëÇ(ùëõ). Therefore, the KVs of documents are often pre-
computed, stored in the database, and retrieved. This leads to a
dilemma: when retrieving the raw text of cloud-side documents,
the device must compute their KVs from scratch, incurring signifi-
cant computation latency; Conversely, directly retrieving the KVs
from the cloud introduces substantial transmission latency, as the
size of KVs can be comparable to, or even larger than, the model
parameters, especially as the number of document grows [18].arXiv:2504.11197v2  [cs.LG]  16 Apr 2025

To address these issues, we propose DRAGON, a distributed
retrieval- augmented generati onframework designed to enhance
the performance of on-device language model inference. Follow-
ing the Law of Total Probability, DRAGON first decomposes the
multi-document RAG process into a dual-side workflow, and then
aggregates their output tokens for the final result. In this workflow,
the cloud and device sides independently execute their own LM
instances using documents retrieved from their databases. Docu-
ment KVs are stored and loaded locally without transmission or
re-computation, thereby reducing first-token latency and preserv-
ing document privacy. Nonetheless, the output aggregation requires
frequent exchange of data packets between the cloud and device at
every token generation step, due to the auto-regressive nature of
language models. This transmission pattern requires a persistent
low-latency connection between the cloud and device, which is
difficult to guarantee in real-world scenarios [39, 40].
To solve this challenge, we draw inspiration from the draft-then-
verify paradigm in Speculative Decoding [33] and propose a new
dual-side speculative algorithm, namely Speculative Aggregation . In
this algorithm, the decoding processes on both sides continuously
generates draft tokens, and an Aggregator on either side (depending
on certain scheduling criteria) asynchronously verifies and aggre-
gates them. Decoding is interrupted and the corresponding KV
states are rolled back for re-computation only when a draft is re-
jected. As our theoretical analysis proves the equivalence between
Speculative Aggregation and the vanilla synchronized version, the
end-to-end latency can be reduced by overlapping transmission
and decoding processes, especially when the output distributions
of the two sides are similar.
We implement a fully functional distributed RAG workflow and
construct a testbed using real-world hardware. Based on this, we
evaluate DRAGON against various RAG architectures using repre-
sentative SLMs on large-scale retrieval corpora and datasets. Exper-
imental results of language modeling on WikiText demonstrate that
DRAGON achieves 1.9√óand1.4√ógreater performance gains over
the standalone SLM than the centralized method, using Qwen2.5-
1.5B and OPT-1.3B, respectively. Moreover, DRAGON shows strong
robustness under various network conditions and achieves a 42.4%-
49.5% reduction in per-token latency compared to synchronized
methods under 300 ms network latency. While retrieving raw text
and KV states incurs up to 8.9√óand15.3√óoverhead in response
time, DRAGON introduces negligible overhead. Extensive simula-
tions further verify that the proposed scheduling algorithm achieves
increasing delay reduction as network latency grows.
We summarize the key contributions of this work as follows:
‚Ä¢We propose DRAGON, the first distributed RAG system that sup-
ports distributed documents retrieval and collaborative output
generation between cloud and device. It significantly enhances
the performance of on-device SLMs with the integration of both
personal and general knowledge.
‚Ä¢We introduce Speculative Aggregation , a dual-side speculative
algorithm that decouples synchronized aggregation from sequen-
tial decoding by asynchronously verifying the output alignment
between cloud and device, greatly reducing end-to-end latency.‚Ä¢We further design an adaptive scheduling algorithm to dynam-
ically identify the optimal aggregation side under varying net-
work conditions, effectively improving decoding efficiency.
‚Ä¢We implement DRAGON in a real-world hardware testbed and
perform comprehensive evaluations using representative SLMs
and large-scale retrieval corpora, demonstrating significant per-
formance improvements of on-device SLMs with negligible over-
head even under high-latency network conditions.
2 Preliminaries
2.1 Retrieval-Augmented Generation
Retrieval-augmented generation [ 36] integrates off-the-shelf lan-
guage models with documents retrieved from an external database
to capture long-tail knowledge and keep up-to-date with new infor-
mation. In traditional LM inference, given an input token sequence
ùë•<ùëÄ={ùë•0,...,ùë•ùëÄ‚àí1}(indices of tokens in vocabulary ùëâ) and the
maximum context length ùëÅ, the output generation process aims to
maximize the probability√éùëÅ‚àí1
ùë°=ùëÄùëù(ùë•ùë°|ùë•<ùë°). In order to incorporate
external documents, we process each document concatenated with
the query separately, and then interpolate the output distributions
(termed as output aggregation [3,36,51])1. Following the Law of
Total Probability, we can derive the interpolation as
ùíë(ùë•ùë°|ùë•<ùë°)=‚àëÔ∏Å
ùëë‚àºùëù(ùëë)ùëù(ùëë|ùë•<ùë°)¬∑ùíë(ùë•ùë°|ùëë,ùë•<ùë°), (1)
whereùëù(ùëë|ùë•<ùë°)denotes the weight of the document ùëëon the out-
put distribution ùíë(ùë•ùë°|ùëë,ùë•<ùë°). Sinceùëù(ùëë|ùë•<ùë°)cannot be directly
obtained in practice, we retrieve ùëëfrom a sufficiently large corpus
Dand only consider top- ùëòdocuments with the highest relevance
scoreRD(ùëë,ùë•<ùë°). Equation (1)offers the opportunity to decom-
pose the multi-document RAG workflow into parallel generation
processes, enabling device-cloud distributed RAG. This decomposi-
tion also significantly alleviates the limitation of maximum context
length on resource-constraint devices.
2.2 Device-Cloud Distributed RAG
To enhance the performance of on-device LLM inference, we pro-
pose a device-cloud distributed RAG framework based on the above
discussed output aggregation paradigm. Given an input ùë•<ùë°, we
retrieve personalized documents ùê∑devicefrom a device-side private
database and then compute the next-token distributions ùë∑device
ùë°=ùíë(ùë•ùë°|ùëë,ùë•<ùë°)‚ä§
ùëë‚ààùê∑device using an on-device LLM Mdevice. In paral-
lel, we employ a similar process in the cloud and obtain the cloud-
side next-token distributions ùë∑cloud
ùë°. After gathering all documents
ùê∑=ùê∑device‚à™ùê∑cloudand their corresponding output distributions
ùë∑ùë°=
ùë∑device
ùë°,ùë∑cloud
ùë°‚ä§, we sample the next token according to
ùë•ùë°‚àºùùé‚ä§
ùë°ùë∑ùë°=‚àëÔ∏Å
ùëë‚ààùê∑ùúîùë°(ùëë)¬∑ùíë(ùë•ùë°|ùëë,ùë•<ùë°), (2)
where ùùéùë°=ùúîùë°(ùëë)‚ä§
ùëë‚ààùê∑denotes the interpolation weights, which
are computed based on relevance scores Ras
ùúîùë°(ùëë)=expR(ùëë,ùë•<ùë°)/‚àëÔ∏Å
ùëë‚Ä≤‚ààùê∑expR(ùëë‚Ä≤,ùë•<ùë°).
We refer to this workflow as the vanilla distributed RAG.
1The output aggregation is different from context aggregation [46]), where external
documents are concatenated and prepended to the input query ùë•<ùë°all at once.
2

Despite its effectiveness, frequent synchronization between the
device and the cloud can introduce a substantial latency. On one
hand, the tight data coupling in distributed RAG leads to idle wait-
ing, especially when decoding latencies significantly differ due
to hardware heterogeneity in cloud and device. During the auto-
regressive LLM model inference, the output ùë•ùë°‚àí1is expected on
both sides as the input for generating ùë∑ùë°. At each token genera-
tion stepùë°, computing Equation (2)requires waiting for both the
device-side and cloud-side output distributions, ùë∑device
ùë°andùë∑cloud
ùë°.
On the other hand, frequent transmission of data packets makes
this device-cloud distributed RAG paradigm highly sensitive to
network stability. Transmitted data packets at each step includes
a 2-byte integer representing the token ùë•ùë°and a float matrix ùë∑ùë°
encoding the output distributions2. Due to the small data packet
size, transmission time is often dominated by data-independent fac-
tors [9, 20], such as the connection round-trip time (RTT). Finally,
idle waiting and transmission latency at each token generation step
accumulate over a long output sequence, significantly amplifying
the overall overhead.
2.3 Problem Formulation
We define the LLM inference as a distributed process where the
device-side and cloud-side token generation processes, Fdeviceand
Fcloud, executes alternatively. We assume the final output token
sequence is generated on-device by sampling ùë•‚àºùíëùë°. Letùê¥ùë°be an
auxiliary set for transferring information between the device and
the cloud at iteration ùë°, which is initially empty, and let ùíëùë°denote
the next-token distribution. The workflow can then be expressed as
ùê¥device
ùë°,ùíëùë°‚ÜêFdevice(ùê¥cloud
ùë°‚àí1,Mdevice,ùê∑device,ùë•<ùë°)on the device,
andùê¥cloud
ùë°‚Üê Fcloud(ùê¥device
ùë°,Mcloud,ùê∑cloud,ùë•<ùë°)on the cloud,
respectively. Finally, the optimization objective is given by
min
F1
ùëÅ‚àëÔ∏ÅùëÅ
ùë°=1 ‚àíùëù(ùë•‚àó
ùë°|ùë•<ùë°)logùíëùë°(ùë•‚àó
ùë°|ùë•<ùë°)+ùúÜùê∂(ùê¥ùë°,F),(3)
whereùë•‚àó
ùë°represents the optimal token at step ùë°andùê∂denotes the
end-to-end latency per token resulted from the transmission of ùê¥ùë°
and execution ofFbetween the cloud and device. The coefficient
ùúÜcontrols the trade-off between performance and efficiency.
3 Overview of DRAGON
To enhance on-device LLM inference while minimizing the latency
overhead, we propose DRAGON, a device-cloud distributed RAG
framework. In this framework, we sample tokens from distribu-
tions aggregated from the device-side and cloud-side RAG outputs,
enabling an integration of personalized information and generic
knowledge. To mitigate the inherent latency caused by frequent
device-cloud synchronizations in vanilla distributed RAG, we per-
form distribution aggregation and next-token sampling in a spec-
ulative manner, where draft tokens are generated on both sides
and then verified on either side. Accordingly, as shown in Figure 2,
DRAGON consists of four modules deployed on both sides, in-
cluding Decoders, Queues, Profilers, and Schedulers, along with a
device/cloud-switchable Aggregator on either side.
2The float matrix ùë∑ùë°has a size of|ùëâ|max(|ùê∑device|,|ùê∑cloud|), where the vocabulary
size|ùëâ|is typically less than 50,000.
‚Äúscience‚Äù‚Äúand‚Äù‚Äútechnology‚Äù‚Äúgames‚Äù‚Äú.‚ÄùDraftQueues
‚ÄúI‚Äù‚Äúlove‚Äù‚Äúcomputer‚Äù
TargetQueue
‚Äúgames‚Äù
‚Äúscience‚Äù
‚Äúgames‚Äù
decode()‚Äú.‚Äù
Aggregator+
SchedulerDecoder
+‚ù∂‚ù∑
‚ù∏‚ùπ
‚ùπDecoderDraft&TargetQueuesTransmissionBus
DeviceCloud
QueryTargettokenAcceptancestatusDrafttokensandoutputdistributionsFigure 2: Overview of the DRAGON framework.
We organize Decoders, Queues and Aggregator by a producer-
consumer paradigm, enabling asynchronous decoding of draft to-
kens. The Decoder serves as a token producer, and on each side
ùë†‚àà{device,cloud}it decodes draft tokens ùë•ùë†
ùë°independently based
on locally-aggregated output distributions ùíëùë†
ùë°=(Àúùùéùë†
ùë°)‚ä§ùë∑ùë†
ùë°where
Àúùùéùë°=ùúîùë°(ùëë)‚ä§
ùëë‚ààùê∑ùë†, similar to Equation (2)but using the retrieved
local documents ùê∑ùë†only ( 1). The draft tokens ùë•ùë†
ùë°and their cor-
responding distribution vectors ùíëùë†
ùë°are broadcast to the other side.
On each side, we enqueue ùë•ùë†
ùë°into Draft Queues ( 2). The Aggrega-
tor, as a consumer, continuously consumes draft tokens from the
front of local queues and performs aggregation process ( 3). Sub-
sequently, the aggregation results of the draft token are broadcast
to Draft Queues on both sides. For each queue, the first token is
dequeued if accepted, or the entire queue is cleared if rejected. The
final target token output by Aggregator is enqueued into Target
Queue on both sides ( 4). Although the dependencies between the
aggregator and decoder cannot be eliminated, the data transmission
latency can be overlapped with the decoding time, mitigating idle
waiting. To accommodate dynamic computing resources on both
sides and network bandwidth between them, we further design
Profilers and Schedulers to identify the optimal aggregation side.
4 Speculative Aggregation
Inspired by Speculative Decoding [33], we propose Speculative Ag-
gregation to reduce the device-cloud communication latency. Spec-
ulative Decoding adopts a draft-then-verify decoding paradigm to
reduce the number of calls to the resource-intensive LLM. Simi-
larly, Speculative Aggregation utilizes two independent decoding
processes, the device-side and cloud-side Decoders, to draft mul-
tiple candidate future tokens, which are then verifies through an
Aggregator. This is equivalent to directly sampling from the dis-
tributions aggregated from the device-side and cloud-side outputs.
As the aggregation involves collecting output distributions over
the network, we expect the speculative algorithm to reduce its
frequency and mitigate data transmission costs.
More specifically, the Aggregator stays in a blocked wait state
until both local Draft Queues are non-empty. Once this condition
is met, it retrieves one token ùë•device
ùë°/ùë•cloud
ùë°from the front of each
3

queue and fetches corresponding locally-aggregated output distri-
butions ùíëdevice
ùë°/ùíëcloud
ùë°from the cache. The tokens and the distribu-
tions are then provided as inputs to the aggregation. Subsection 4.1
presents the target distribution of the aggregation while Subsec-
tion 4.2 introduces an speculative strategy to sample from the target
distribution equivalently. Subsection 4.3 analyzes the acceptance
probability, providing guidance for further scheduling design. Since
the workflows of the device and cloud sides are designed to be
symmetric, we define {ùëô,ùëü}={device,cloud}to maintain general-
ity and avoid repetition. From the perspective of the Aggregator, ùëô
refers to the local side that performs aggregation, while ùëüdenotes
the remote side, which only generates draft tokens.
4.1 Target Token Distribution
The objective of speculative aggregation is to generate tokens
that are equivalent to those sampled from the target distribution
ùíëùë°=ùùé‚ä§
ùë°ùë∑ùë°as defined in Equation (2). We partition ùë∑ùë°block-
wise, grouping its distribution vectors by generation side, and have
ùíëùë°=(ùùéùëô
ùë°)‚ä§ùë∑ùëô
ùë°+(ùùéùëü
ùë°)‚ä§ùë∑ùëü
ùë°. For eachùë†‚àà{ùëô,ùëü}, we have ùùéùëô
ùë°=ùúÇùë†
ùë°Àúùùéùëô
ùë°
whereùúÇùë†
ùë°=‚Ñéùë†
ùë°/(‚Ñéùëô
ùë°+‚Ñéùëü
ùë°)and‚Ñéùë†
ùë°=√ç
ùëë‚ààùê∑ùë†expR(ùëë,ùë•<ùë°). As a result,
given the locally-aggregated output distributions ùíëùëô
ùë°andùíëùëü
ùë°(¬ß 3),
the target distribution ùíëùë°can be obtained by an interpolation:
ùíëùë°=ùúÇùëô
ùë°ùíëùëô
ùë°+ùúÇùëü
ùë°ùíëùëü
ùë°. (4)
To align with this computation process, on each side ùë†‚àà{ùëô,ùëü}, a
corrected value3of‚Ñéùë†
ùë°is computed and retained during decoding
ùë•ùë†
ùë°, and then broadcast and stored along with draft tokens and the
locally-aggregated distributions.
Dynamic weights. The interpolation weights ùúÇùëô
ùë°andùúÇùëü
ùë°in Equa-
tion (4)can be dynamic, as the relevance between documents and
the ongoing context may vary during generation. While current
studies [ 36,51] that employ output aggregation adopt static doc-
ument relevance scores, we explore dynamic weights by adopt-
ing a strategy inspired by those used in recommendation systems.
Upon receiving the input query ùë•<ùëÄ, each sideùë†‚àà{ùëô,ùëü}performs
a one-time retrieval of a relatively large document set ùê∑ùë†(as in
most existing works [ 23,35,51]) to avoid key-value recomputation
caused by changes in the document prefix. During the decoding of
draft token ùë•ùë°, we re-estimate the relevance scores R(ùëë,ùë•<ùë°)for
ùëë‚ààùê∑ùë†using a local re-ranking model (e.g., a Cross-Encoder [ 48])
and re-calculate the corrected ‚Ñéùë†
ùë°before transmission.
4.2 Design of Aggregation Strategy
To sampleùë•ùë°‚àºùíëùë°, we instead perform two independent speculative
sampling processes as follows:
‚Ä¢Keep the draft token ùë•ùëô
ùë°asÀúùë•ùëô
ùë°ifùíëùëô
ùë°(ùë•ùëô
ùë°)‚â§ùíëùëü
ùë°(ùë•ùëô
ùë°), and in case
ùíëùëô
ùë°(ùë•ùëô
ùë°)>ùíëùëü
ùë°(ùë•ùëô
ùë°)we reject the sample with probability ùúÇùëü
ùë°(1‚àí
ùíëùëü
ùë°(ùë•ùëô
ùë°)/ùíëùëô
ùë°(ùë•ùëô
ùë°))and sample Àúùë•ùëô
ùë°again from an adjusted distribu-
tion Àúùíëùëô
ùë°=norm(max(0,ùíëùëü
ùë°‚àíùíëùëô
ùë°)).
‚Ä¢Keep the draft token ùë•ùëü
ùë°asÀúùë•ùëü
ùë°ifùíëùëü
ùë°(ùë•ùëü
ùë°)‚â§ùíëùëô
ùë°(ùë•ùëü
ùë°), and in case
ùíëùëü
ùë°(ùë•ùëü
ùë°)>ùíëùëô
ùë°(ùë•ùëü
ùë°)we reject the sample with probability ùúÇùëô
ùë°(1‚àí
ùíëùëô
ùë°(ùë•ùëü
ùë°)/ùíëùëü
ùë°(ùë•ùëü
ùë°))and sample Àúùë•ùëü
ùë°again from an adjusted distribu-
tion Àúùíëùëü
ùë°=norm(max(0,ùíëùëô
ùë°‚àíùíëùëü
ùë°)).
3We adopt the log-sum-exp trick to maintain numerical stability (See Appendix A.1).Algorithm 1: SpeculativeAggregation
Input: Draft tokens ùë•ùë†
ùë°, locally-aggregated distributions ùíëùë†
ùë°,
and aggregation weights ‚Ñéùë†
ùë°, forùë†‚àà{ùëô,ùëü}
Output: Target token ùë•ùë°, acceptance status SùëôandSùëü
Function Sample(ùë•,ùíëùëé,ùíëùëè,ùúÇ):
Àúùë•‚Üêùë•,ùúéùëé‚àºùëà(0,1);
ifùíëùëé(ùë•)>ùíëùëè(ùë•),ùúéùëé<ùúÇ(1‚àíùíëùëè(ùë•)
ùíëùëé(ùë•))then
Àúùë•‚àºnorm(max(0,ùíëùëè‚àíùíëùëé));
return Àúùë•;
ùúÇùëô
ùë°‚Üê‚Ñéùëô
ùë°/(‚Ñéùëô
ùë°+‚Ñéùëü
ùë°),ùúÇùëü
ùë°‚Üê1‚àíùúÇùëô
ùë°;
Àúùë•ùëô
ùë°‚ÜêSample(ùë•ùëô
ùë°,ùíëùëô
ùë°,ùíëùëü
ùë°,ùúÇùëü
ùë°),Àúùë•ùëü
ùë°‚ÜêSample(ùë•ùëü
ùë°,ùíëùëü
ùë°,ùíëùëô
ùë°,ùúÇùëô
ùë°);
ùúé‚àºùëà(0,1),ùë•ùë°‚ÜêÀúùë•ùëô
ùë°¬∑1ùúé‚â§0.5+Àúùë•ùëü
ùë°¬∑1ùúé>0.5;
Sùëô‚Üêùë•ùëô
ùë°=ùë•ùë°,Sùëü‚Üêùë•ùëü
ùë°=ùë•ùë°;
returnùë•ùë°,Sùëô,Sùëü;
It is straightforward to show4that through these sampling pro-
cesses, both Àúùë•ùëô
ùë°and Àúùë•ùëü
ùë°are indeed drawn from the aggregated dis-
tributionùúÇùëô
ùë°ùíëùëô
ùë°+ùúÇùëü
ùë°ùíëùëü
ùë°. We select either Àúùë•ùëô
ùë°orÀúùë•ùëü
ùë°asùë•ùë°with uniform
probability, ensuring ùë•ùë°‚àºùíëùë°. Finally, each draft token ùë•ùëô
ùë°andùë•ùëü
ùë°
is accepted if it matches the target token ùë•ùë°; otherwise, it is re-
jected. The aggregation strategy at each step ùë°is summarized in
Algorithm 1. It is worth noting that we design a sampling-based
method rather than simply selecting between ùë•ùëô
ùë°andùë•ùëü
ùë°, in order to
ensure thatùë•ùë°‚àºùíëùë°holds. A counterexample for binary selection is
illustrated in cases where arg max ùíëùë°differs from both arg max ùíëùëô
ùë°
andarg max ùíëùëü
ùë°.
We now present a general procedure for sampling multiple con-
secutive tokens. At each step ùë°, the following workflow is executed:
1)The Aggregator waits until both Draft Queues are non-empty,
then fetches ùë•ùë†
ùë°from the front of the local Draft Queues and
retrieves the auxiliary variables ùíëùë†
ùë°and‚Ñéùë†
ùë°from the local cache,
for eachùë†‚àà{ùëô,ùëü}.
2)The Aggregator performs aggregation as defined in Algorithm 1.
The outputs, including the target token ùë•ùë°and the acceptance
status of each draft token, are broadcast to notify both sides.
3)Upon receiving the message, each side checks the acceptance
status of both ùë•ùëô
ùë°andùë•ùëü
ùë°. If a token is accepted, it is dequeued
from the corresponding Draft Queue and step 5) is executed;
otherwise, step 4) is executed.
4)Ifùë•ùë†
ùë°is rejected, its corresponding Draft Queues on both sides are
cleared and the side ùë†rolls back its KV cache and re-computes
the next draft token ùë•ùë†
ùë°+1using the target token ùë•ùë°as input.
5) Update step ùë°‚Üêùë°+1, and go back to step 1).
We adopt a pipeline approach rather than performing aggrega-
tion in parallel. In centralized Speculative Decoding , each execution
of the target LLM requires waiting for the draft model to generate
the current token and for the LLM to verify the previously gener-
ated one. By verifying consecutive tokens in parallel, multiple LLM
inferences can be merged into a single pass, shifting the primary
latency bottleneck from target LLM inference to the sequential
decoding of draft tokens. Conversely, in Speculative Aggregation for
4The proof is included in Appendix A.2.
4

distributed RAG, the time to the next token is dominated by data
transmission over the network. Consecutive transmission of small
data can be naturally overlapped since each transmission does not
significantly occupy the I/O for an extended period. Parallelizing
the aggregation process instead introduces waiting between draft
tokens until the batch is fully populated. We employ queues to
construct a pipeline, where each draft token is transmitted and
enqueued immediately upon generation, ensuring it is verified at
the earliest opportunity.
4.3 Analysis of Acceptance Rate
We now analyze the factors that influence the acceptance rate of
draft tokens on both the device and the cloud sides.
Definition 4.1. Forùë†‚àà{ùëô,ùëü}, the acceptance rate ùõΩùë†
ùë°, is the prob-
ability of accepting ùë•ùë†
ùë°‚àºùíëùë†
ùë°=√ç
ùëë‚ààùê∑ùë†ùúîùë°(ùëë)ùíë(ùë•ùë°|ùëë,ùë•<ùë°)by the
aggregation strategy, given a prefix ùë•<ùë°.
First, we consider the side ùëôas an example. The acceptance
of the draft token ùë•ùëô
ùë°, sampled from ùíëùëô
ùë°by the Decoder, can be
classified into two cases: i) it is accepted during the speculative
sampling of Àúùë•ùëô
ùë°and ii) it is output by the speculative sampling
ofÀúùë•ùëü
ùë°, where either ùë•ùëü
ùë°=ùë•ùëô
ùë°and is accepted, or ùë•ùëô
ùë°‚àºÀúùíëùëü
ùë°. Letùõæùëô
andùõæùëürepresent the weights assigned to Àúùë•ùëô
ùë°and Àúùë•ùëü
ùë°in the ran-
dom selection following these sampling processes ( ùõæùëô+ùõæùëü=1).
We adopt the definition of divergence from [ 33], given by ùõø=
ùê∑ùêøùêæ(ùíëùëô
ùë°,ùíëùëü
ùë°)=1‚àí√ç
ùë•min(ùíëùëô
ùë°(ùë•),ùíëùëü
ùë°(ùë•)). The expected acceptance
rateùõºùëô
ùë°=Eùë•‚àºùíëùëô
ùë°(ùë•)(ùõΩùëô
ùë°)is computed as
ùõºùëô
ùë°=ùõæùëô(1‚àíùúÇùëü
ùë°ùõø)+ùõæùëü‚àëÔ∏Å
ùë•ùíëùëô
ùë°(ùë•)ùíëùë°(ùë•), (5)
where the two terms represent the acceptance probability of the
two cases above, respectively. These terms are mutually-exclusive
and influenced by the mixture weights ùõæùëôandùõæùëü.
Theorem 4.2. Given any distributions ùíëùëô
ùë°andùíëùëü
ùë°, whenùúÇùëü
ùë°is fixed,
maximizing ùõºùëô
ùë°is equivalent to maximizing ùõæùëô.
Proof . Substituting ùíëùë°=ùúÇùëô
ùë°ùíëùëô
ùë°+ùúÇùëü
ùë°ùíëùëü
ùë°5and subtracting the two
terms in Equation (5)yields(1‚àíùúÇùëü
ùë°ùõø)‚àí√çùíëùëô
ùë°ùíëùë°=ùúÇùëô
ùë°(1‚àí√ç(ùëùùëô
ùë°)2)+
ùúÇùëü
ùë°√ç(min(ùíëùëô
ùë°,ùíëùëü
ùë°)‚àíùíëùëô
ùë°ùíëùëü
ùë°). Since 1>√ç
ùë•(ùíëùëô
ùë°)2,min(ùíëùëô
ùë°,ùíëùëü
ùë°) ‚â•
ùíëùëô
ùë°ùíëùëü
ùë°and0‚â§ùúÇùëô
ùë°,ùúÇùëü
ùë°‚â§1, it follows that 1‚àíùúÇùëü
ùë°ùõø‚â•√çùíëùëô
ùë°ùíëùë°al-
ways holds, with equality holding only when ùúÇùëô
ùë°=0,ùúÇùëü
ùë°=1and
ùõø=0. This condition implies that the two distributions ùíëùëô
ùë°andùíëùëü
ùë°
are completely disjoint. Consequently, maximizing ùõæùëôleads to the
maximization of the expected acceptance rate ùõºùëô
ùë°. ‚ñ°
For the side ùëü, Theorem 4.2 holds symmetrically, where maximiz-
ing the acceptance of ùë•ùëü
ùë°corresponds to maximizing ùõæùëü. Clearly, the
objectives on sides ùëôandùëüconflict with each other. For simplicity
in framework design, we adopt ùõæùëô=ùõæùëü=0.5to strike a balance (as
shown in Algorithm 1).
The expected acceptance rate is then influenced by the degree
of overlap between the draft distributions on the two sides. When
the distributions ùíëùëô
ùë°andùíëùëü
ùë°perfectly coincide, i.e., the divergence
5For brevity, the variable ùë•is omitted in the distribution notation throughout the
following proof.ùë•ùëô
ùë°‚àí1ùë•ùëü
ùë°‚àí1Waiting Time for ùë•ùëô
ùë°andùë•ùëü
ùë°
rejected accepted max(ùëêùëô
dec,ùúë(ùëêùëü
dec+ùëêùëü
trans))
accepted rejected max(ùúë(ùëêùëô
dec),ùëêùëô
trans+ùëêùëü
dec+ùëêùëü
trans)
accepted accepted max(ùúë(ùëêùëô
dec),ùúë(ùëêùëü
dec+ùëêùëü
trans))
rejected rejected max(ùëêùëô
dec,ùëêùëô
trans+ùëêùëü
trans+ùëêùëü
dec)
Table 1: Waiting time for the next pair of draft tokens ùë•ùëô
ùë°and
ùë•ùëü
ùë°under different acceptance scenarios of the previous draft
tokensùë•ùëô
ùë°‚àí1andùë•ùëü
ùë°‚àí1.
ùõøbecomes zero, the first term of Equation (5),1‚àíùúÇùëü
ùë°ùõø, reaches its
maximum value. Simultaneously, since the second term follows
‚àëÔ∏Å
ùë•ùíëùëô
ùë°(ùë•)ùíëùë°(ùë•)‚â§‚àöÔ∏É‚àëÔ∏Å
ùë•ùíëùëô
ùë°(ùë•)2‚àëÔ∏Å
ùë•ùíëùë°(ùë•)2,
based on the Cauchy-Schwarz inequality and achieves its maximum
when ùíëùëô
ùë°(ùë•)=ùíëùëü
ùë°(ùë•)=ùíëùë°(ùë•), the expected acceptance rate is max-
imized. Conversely, when the support sets of the two distributions
are completely disjoint, i.e., ùõø=1, the product ùíëùëô
ùë°(ùë•)ùíëùë°(ùë•)becomes
zero for every ùë•, resulting in a minimized expected acceptance rate.
This characteristic provides insight into the principle behind
Speculative Aggregation : we assume that the device-side and cloud-
side RAG workflows generate similar results by default, allowing
them to asynchronously decode the next tokens without aggrega-
tion. Only when they disagree with each other, the acceptance is
adjusted by their aggregation weights ùúÇùëô
ùë°andùúÇùëü
ùë°.
5 Greedy Scheduling
To further minimize the latency ùê∂(ùê¥ùë°,F)in Equation (3), We adap-
tively schedule which side performs the next aggregation after the
current one is completed. The principle behind this is to maximize
the overlap between the device-side and cloud-side decoding and
transmission processes, jointly considering dynamic computing
resources, network bandwidth, and acceptance of draft tokens.
5.1 Scheduling Strategy
Since predicting future acceptance is challenging due to dynamic
document relevance and LLM outputs, we employ a greedy strategy,
where at each step, we minimize the expected latency per token
based on current observations.
The latency per token, denoted as ùëçùë°, is computed as the average
duration between two consecutive aggregations. It can be viewed
as the waiting time for the next pair of draft tokens, ùë•device
ùë°and
ùë•cloud
ùë°, including both decoding and transmission delays, as the ag-
gregation duration is negligible. For each side ùë†‚àà{device,cloud},
letùëêùë†
decdenote the decoding delay of a draft token ùë•ùë†
ùë°, andùëêùë†
transde-
note the transmission delay of this token and its auxiliary variables
fromùë†to the other side. Since the decoding and transmission pro-
cesses are asynchronous, they may still be ongoing when the sched-
uling algorithm is executed. Therefore, we define ùúë(ùëátotal(ùë¢))=
max(0,ùëátotal(ùë¢)+ùëábegin(ùë¢)‚àíùëánow)as a function that estimates the
remaining time of the total duration ùëátotalto complete the process
ùë¢, whereùëábegin andùëánoware the beginning and current timestamps,
respectively. Let ùëôbe the side that currently performs aggregation
andùëübe the other one. The best side is then selected as
ùë†‚àó=arg minùë†‚àà{ùëô,ùëü}ùëçùë†
ùë°(ùúë,ùëêùëô
ùëëùëíùëê,ùëêùëô
ùë°ùëüùëéùëõùë†,ùëêùëü
ùëëùëíùëê,ùëêùëü
ùë°ùëüùëéùëõùë†), (6)
5

cr
dec(cl
trans+cr
trans)
cr
dec cr
dec+(cl
trans+cr
trans)
Local Decoding Latency per Token0Latency Differencel-side is betterr-side is better
slope=1l
t
slope=1r
t
l
t=0.5,r
t=0.8
l
t=0.8,r
t=0.5
l
t=0.5,r
t=0.5
Figure 3: Difference in per-token latencies when side ùëôandùëü
performs aggregation, versus varying ùëô-side decoding latency.
whereùëçùë†
ùë°denotes the latency per token when ùë†continuously per-
forms the aggregations in the future.
Next, we present the calculation of ùëçùë†
ùë°. Table 1 illustrates the
waiting time for the next pair of draft tokens after a previous aggre-
gation. To estimate an averaged ùëçùë†
ùë°over multiple future steps, rather
than enumerating all possible combinations of acceptance scenar-
ios, we assume each acceptance scenario repeats continuously6and
occurs with an expected probability given by the acceptance rate.
Therefore, the waiting time in Table 1 can be simplified to eliminate
the function ùúë. First, assuming that draft tokens from ùëüare always
accepted, the decoding process for consecutive draft tokens will be
continuous on ùëü. In other words, the decoding of ùë•ùëü
ùë°begins exactly
whenùë•ùëü
ùë°‚àí1is decoded and ready for transmission. Therefore, we
haveùúë(ùëêùëü
dec+ùëêùëü
trans)=(ùëábegin+ùëêùëü
trans‚àíùëánow)+ùëêùëü
dec=ùëêùëü
dec. Moreover,
since the aggregation process can exhaustively consume the token
pairs in the Draft Queues, ùúë(ùëêùëô
dec)<ùëêùëô
decholds only when the wait-
ing time for ùë•ùëü
ùë°dominates. Hence, max(ùúë(ùëêùëô
dec),¬∑)=max(ùëêùëô
dec,¬∑).
Finally,ùëçùëô
ùë°is calculated as
ùõºùëü
ùë°max(ùëêùëô
dec,ùëêùëü
dec)+(1‚àíùõºùëü
ùë°)max(ùëêùëô
dec,ùëêùëü
dec+ùëêùëô
trans+ùëêùëô
trans).(7)
Symmetrically, ùëçùëü
ùë°is computed by exchanging ùëôandùëüin Equa-
tion (7). Based on this, we can conclude that when the local de-
coding latency ùëêùëô
deccannot cover the waiting time for draft tokens
from the other side, i.e., ùëêùëô
dec<ùëêùëü
dec+ùëêùëô
trans+ùëêùëô
trans, minimizing the
overall latency ùëçùëô
ùë°requires maximizing the acceptance rate ùõºùëü
ùë°.
To decide the optimal side in Equation (6), we calculate the differ-
ence in latencies per token when side ùëôandùëüperforms aggregation.
The result is presented as a piecewise function,
Œîùëçùë°=Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥ Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥(1‚àíùõºùëü
ùë°)rtt, ùëêùëô
dec‚â§ùëêùëü
dec‚àírtt
(1‚àíùõºùëô
ùë°)ùëó+(ùõºùëô
ùë°‚àíùõºùëü
ùë°)rtt, ùëêùëü
dec‚àírtt<ùëêùëô
dec‚â§ùëêùëü
dec
(1‚àíùõºùëü
ùë°)ùëó+(ùõºùëô
ùë°‚àíùõºùëü
ùë°)rtt, ùëêùëü
dec<ùëêùëô
dec‚â§ùëêùëü
dec+rtt
(ùëéùëô
ùë°‚àí1)rtt, ùëêùëü
dec+rtt<ùëêùëô
dec,(8)
where rtt=ùëêùëô
trans+ùëêùëü
trans, andùëóis the difference in decoding la-
tencies,ùëêùëü
dec‚àíùëêùëô
dec. Accordingly, we select side ùëüfor aggregation
when Œîùëçùë°>0, and sideùëôotherwise. Figure 3 shows the influence
of varying acceptance rates on Œîùëçùë°. As the acceptance rate of draft
tokens from one side increases, the Scheduler tends to favor the
opposite side. Moreover, the relationship between ùëêùëô
decandùëêùëü
decalso
influences the strategy. For instance, when the decoding process
on one side becomes the latency bottleneck, aggregation is always
performed on that side, which is demonstrated by (1‚àíùõºùëü
ùë°)rtt‚â•0
and(ùõºùëô
ùë°‚àí1)rtt‚â§0. Clearly, our strategy minimizes the likelihood
6Please refer to Appendix A.3 for pipeline illustrations of different cases.of repeated bottleneck decoding due to rejection, while maximiz-
ing the overlap between the decoding and transmission processes
across the two sides.
5.2 Profiling
The Profiler helps estimate a set of parameters required to compute
Equation (6), including the decoding delay ( ùëêdec) on both the device
and cloud sides, the transmission delay ( ùëêdevice
trans+ùëêcloud
trans) between
them, and the acceptance rates ( ùõº). The Profiler operates in two
stages: i) offline and ii) runtime.
Offline stage. For each side ùë†‚àà{device,cloud}, the Profiler mea-
sures the decoding delay by simulating the output-aggregation RAG
workflow locally. We randomly generate |ùê∑ùë†|dummy text chunks
as retrieved documents with the same chunk size ùëÄas in the real
corpus. We use the dummy text directly as the prefix (without a
query prompt) and prefill its KV cache in advance. Next, we perform
auto-regressive decoding with the same batch size as during run-
time, until the context length reaches its maximum ùëÅ. We record
the decoding delay ùëêùë†
dec(ùë°)at each step ùë°=ùëÄ,...,ùëÅ‚àí1by averag-
ing over multiple runs and fit the records ùíÑùë†
dec=ùëêùë†
dec(ùë°)
ùë°‚ààùíïusing
a least square errors (LSE) estimator, ÀÜùëêùë†
dec=ùëòùë†ùëéùë°/ùëòùë†
ùëè+ùëòùë†ùëê, where the
coefficients ùëòùë†ùëé,ùëòùë†
ùëè, andùëòùë†ùëêare synchronized across both sides.
We model the transmission delay as ÀÜùëêùë†
trans(ùëî)=ùêøùë†+ùëî/ùêµùë†, where
ùëîrepresents the data size and ùêøùë†andùêµùë†correspond to the network
latency and bandwidth for transmitting data from ùë†to the other
side. Since one-way delay is hardly measurable, we measure the
bi-directional delay ÀÜùëêdevice
trans+ÀÜùëêcloud
transaltogether. We utilize sockperf
to evaluate the round trip time ùêødevice+ùêøcloudand use iperf3 to
measure the bi-directional bandwidths.
Runtime stage. To assess decoding latency at runtime, the Decoder
on each side ùë†‚àà{device,cloud}measures the duration Àúùëêùë†
dec(ùë°)of
decoding a draft token at step ùë°using the time.perf_counter function
in Python. This measurement is then piggybacked onto the draft
token message for convenient synchronization. Next, the value
ofÀÜùëêùë†
decis re-estimated with the intercept ùëòùë†ùëêfrozen and the slope
updated as
(1‚àíùúÅ)ùëòùë†ùëé+ùúÅ(Àúùëêùë†
dec(ùë°)‚àíùëòùë†ùëê)ùë°
(1‚àíùúÅ)ùëòùë†
ùëè+ùúÅùë°2,
whereùúÅis the weight on new observation. The estimation of trans-
mission delay is refined by means of two moving averages: a real-
time estimate and a historical moving average. For the former, we
update the round-trip time measurement ùêødevice+ùêøcloudat each
stepùë°using the ICMP-based network diagnostic tool, ping. In con-
trast, the sending and receiving bandwidth ùêµùë†are updated using
iperf3 every few tokens to avoid excessive overhead. Similarly, we
estimate acceptance rates using Equation 5 and apply a moving
average to prevent abrupt changes.
6 Theoretical Wall-Time Improvement
In this section, we present a theoretical analysis to demonstrate the
improvement in wall-time efficiency achieved by DRAGON over the
vanilla distributed RAG framework described in ¬ß 2.2. Specifically,
the synchronized aggregation strategy used in the vanilla RAG can
be viewed as a special case of speculative aggregation in which
draft tokens from both sides are consistently rejected. To facilitate
6

10 30 50 70 90
Decoding Latency cl
dec (ms)1.01.21.41.61.82.0Speedupr
t=0.50
10 30 50 70 90
Decoding Latency cl
dec (ms)510152025
1r
t=0.99
cr
dec=10 ms, rtt=100 ms
cr
dec=10 ms, rtt=300 ms
cr
dec=30 ms, rtt=100 ms
cr
dec=30 ms, rtt=300 msFigure 4: Theoretical speedup of DRAGON compared to the
vanilla distributed RAG vs. varying ùëêùëô
dec,ùëêùëü
dec, rtt andùõºùëü
ùë°.
analysis, we assume the aggregation is always performed on the
device in following discussions.
Definition 6.1. Letùëçùë°and Àúùëçùë°be the expected per-token latencies
at stepùë°when using DRAGON and the vanilla distributed RAG,
respectively. Define the speedup as ùëÜùë°=Àúùëçùë°/ùëçùë°.
Theorem 6.2. Givenùëô=device andùëü=cloud , the speedup can
be described as a piecewise function dependent on the relationship
amongùëêùëô
dec,ùëêùëü
decand rtt, as follows:
1
ùëÜùë°=Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥ Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥1‚àíùõºùëü
ùë°
1+ùëêùëü
dec/rtt, ùëêùëô
dec‚â§ùëêùëü
dec
1‚àí(1‚àíùëêùëô
dec
ùëêùëü
ùëëùëíùëê+rtt)ùõºùëü
ùë°, ùëêùëü
dec<ùëêùëô
dec‚â§ùëêùëü
dec+rtt
1, ùëêùëü
dec+rtt<ùëêùëô
dec(9)
Proof .ùëçùë°is computed according to Equation (7). By substituting
ùõºùëô
ùë°=ùõºùëü
ùë°=0and we obtain Àúùëçùë°=max(ùëêùëô
dec,ùëêùëü
dec+rtt). The result
then follows from a simple case-by-case analysis. ‚ñ°
Figure 4 illustrates the theoretical speedup characterized in The-
orem 6.2. The speedup achieves its maximum when the device-side
decoding latency is minimal and maintains saturated until it sur-
passes that of the cloud. Thereafter, the speedup decreases inversely
withùëêùëô
dec, gradually approaching 1 and eventually stabilizing at 1
onceùëêùëô
decexceedsùëêùëü
dec+rtt. Finally, we have following corollaries:
Corollary 6.3. DRAGON is particularly effective when the decod-
ing latency gap between the device and the cloud is small and the
transmission cost becomes the primary bottleneck.
This property broadens the potential application of DRAGON to
general scenarios in which distributed computing nodes have com-
parable computational resources, but communication remains a
key bottleneck requiring further optimization.
Corollary 6.4. DRAGON‚Äôs improvement in wall time can be sub-
stantially amplified when the cloud-side acceptance rate is high.
Numerous existing works [ 18,63] have shown that a small subset
of tokens receives the majority of attention and replacing them
significantly changes the output sequence [ 37]. Accordingly, we
argue that draft tokens that differ from those on the other side pri-
marily originate from this subset and are synchronized across both
sides. In contrast, other tokens (such as stop words, punctuations
and common-knowledge terms) are often context-independent and
shared across both sides, leading to a considerable acceptance rate.
Corollary 6.5. DRAGON‚Äôs improvement in wall time is independent
of the device-side acceptance rate.When the device-side decoding latency is much lower, the Aggre-
gator must wait for the arrival of cloud-side draft tokens before
generating the next target token, regardless of whether the device-
side draft is accepted. Similarly, when the device-side latency is
substantially higher, the next target token is generated immediately
and fed as input for the next decoding step after completing the
current one. As a result, the acceptance of the local draft has no
impact on the overall latency. However, it remains important when
aggregation is shifted to the other side via DRAGON‚Äôs scheduling
algorithm.
7 Experiments
7.1 Implementation
We implemented DRAGON for distributed RAG workflow compris-
ing ~3,000 lines of Python code.7The System consists of two sym-
metric processes, the device-side and cloud-side ones, each utilizing
eight threads for core functionalities (e.g., decoding, aggregation
and transmission) along with a memory-resident service process
for document retrieval. We implemented information synchroniza-
tion between threads using multi-producer, multi-consumer queues,
and between processes using socket-based communication. We uti-
lized PyTorch [44] (version 2.6.0) for algorithm implementations,
Hugging Face Transformers [54] for LLM utilities, LangChain [31]
for document chunking, Sentence Transformers [48] for document
re-ranking, and Faiss [26] for indexing and similarity search of
document embeddings.
Efficient transmission. We implemented data transmission over
the TCP/IP protocol using the socket library. A fixed-length mes-
sage header is defined using the struct module, containing the
message type and body size. All Python objects in the message
body are finally serialized using the pickle.dumps() function and
compressed by means of an LZ4compressor, while numeric vec-
tors are first serialized with numpy.tobytes() . For transmitting the
output distributions ùíëdevice
ùë°andùíëcloud
ùë°, we employ an aggressive
top-ùëùselection strategy [ 16] withùëù=0.8, encoding the selected
indices as unsigned 8-bit integers and the values as 16-bit floating-
point numbers. While preserving the inference performance, the
transmission data size is significantly reduced‚Äîby approximately
2,363 times when given the vocabulary size of 50,272‚Äîcompared to
the unoptimized JSON-based implementation.
Preemptible generation. We implemented a hook function that
raises an exception upon the occurrence of a stop event (e.g., receiv-
ing a draft token rejection message) and registered it in the forward
pass of each model layer to enable layer-wise interruption of draft
decoding. When the generation caller catches the exception, it rolls
back the KV cache and attention masks based on the number of
generated target tokens so far and feeds the latest target token as
input to trigger re-computation.
7.2 Experiment Setups
Testbed. We evaluated our framework and baseline methods using
a high-performance computer as the cloud server and a MacBook
Pro as the edge device. The server is equipped with an Intel Xeon
Silver 4210R CPU, 64GB of memory, and a GeForce RTX 3090 GPU,
7Our code is available at GitHub: https://github.com/ThomasAtlantis/DRAGON
7

while the MacBook Pro features an Intel Core i7 CPU, 16GB of
memory, and no dedicated GPU. The cloud and the device are
connected via a 2.4 GHz Wi-Fi local-area network, with latency
and jitter measured by sockperf as 2ms and 6ms, respectively. To
simulate network jitter, we replay a predefined random latency
trace by adjusting the network interface controller (NIC) latency
using the traffic control tool, tc.
Datasets and metrics. We evaluated the long-sequence generation
performance of DRAGON on the large-scale language modeling
dataset WikiText [ 41], which comprises over 100 million tokens ex-
tracted from verified Good and Featured articles on Wikipedia. We
constructed retrieval corpora from the training sets of two different-
scale versions, WikiText2 and WikiText103. During evaluation, we
applied rolling windows of 1024 and 512 tokens, respectively, over
their test sets, using the first 1/8 of each window as the query for
retrieval and the remaining tokens for perplexity evaluation. To
further assess the efficiency of our method, we measure the time
to first token (TTFT) and per-token latency. In this measurement,
we used the retrieval corpus and index pre-built by Facebook from
a Wikipedia dump dated December 20, 2018, which contains 21
million documents.
Models and baselines. We evaluated our framework using OPT-
1.3B [62] and Qwen2.5-1.5B [57], with vocabulary sizes of 151,936
and 50,272, respectively. For language modeling and latency mea-
surement, we adopted Contriever [ 22] and DPR [ 29] as the retriev-
ers, respectively. Additionally, we employed ms-marco-MiniLM-L6-
v2 [48] for document re-ranking. We compare DRAGON with four
baseline methods:
‚Ä¢CRCG, centralized generation augmented with centralized re-
trieval from local corpus, using the context-aggregation strategy,
which represents most existing RAG methods [24, 38, 46].
‚Ä¢DRCG, on-device generation augmented with documents re-
trieved from a distributed corpus spanning both the device and
the cloud, using the context-aggregation strategy.
‚Ä¢DRDG/TW, distributed RAG using the output aggregation strat-
egy and token-wise synchronization, as discussed in ¬ß 2.2. The
target tokens are collected and aggregated on the device side.
‚Ä¢DRDG/SW, distributed RAG using the output aggregation strat-
egy and sequence-wise synchronization, i.e., one-time aggrega-
tion of the independently generated output sequences from the
device and the cloud. This baseline is implemented by extend-
ing the official REPLUG [ 51] implementation and Facebook‚Äôs
RAG-Sequence model [36] with distributed support.
To simulate insufficient but complementary corpus in the cloud and
device sides, we constrain the on-cloud and on-device retrieval by
selecting the first and second halves of the top-k documents from
the same corpus, respectively. Moreover, to study the overhead of
DRCG, we evaluate two variants: DRCG/Text retrieves raw text and
prefill KV cache from scratch and DRCG/KV retrieves and reuses
the KV cache of documents directly.
7.3 Overall Performance and Efficiency
We first present the overall performance and efficiency of DRAGON
in comparison to the baselines. In the following experiments, we
set the maximum context length to 256 tokens on both the device
and cloud sides, with each retrieved document limited to 64 tokens.
/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000048/uni00000047/uni00000003/uni00000027/uni00000052/uni00000046/uni00000058/uni00000050/uni00000048/uni00000051/uni00000057/uni00000056/uni00000014/uni00000013/uni00000011/uni00000015/uni00000014/uni00000013/uni00000011/uni00000017/uni00000014/uni00000013/uni00000011/uni00000019/uni00000014/uni00000013/uni00000011/uni0000001b/uni00000033/uni00000048/uni00000055/uni00000053/uni0000004f/uni00000048/uni0000005b/uni0000004c/uni00000057/uni0000005c
/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000048/uni00000047/uni00000003/uni00000027/uni00000052/uni00000046/uni00000058/uni00000050/uni00000048/uni00000051/uni00000057/uni00000056/uni00000014/uni00000017/uni00000011/uni00000015/uni00000014/uni00000017/uni00000011/uni00000017/uni00000014/uni00000017/uni00000011/uni00000019/uni00000014/uni00000017/uni00000011/uni0000001b/uni00000014/uni00000018/uni00000011/uni00000013/uni00000033/uni00000048/uni00000055/uni00000053/uni0000004f/uni00000048/uni0000005b/uni0000004c/uni00000057/uni0000005c
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f /uni00000026/uni00000035/uni00000026/uni0000002a/uni00000012/uni00000026/uni0000004f/uni00000052/uni00000058/uni00000047 /uni00000026/uni00000035/uni00000026/uni0000002a/uni00000012/uni00000027/uni00000048/uni00000059/uni0000004c/uni00000046/uni00000048 /uni00000027/uni00000035/uni00000026/uni0000002a /uni00000027/uni00000035/uni00000024/uni0000002a/uni00000032/uni00000031(a) Qwen2.5-1.5B/WikiText2. (b) OPT-1.3B/WikiText103.
Figure 5: Performance on WikiText.
/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000003/uni00000031/uni00000048/uni00000057/uni0000005a/uni00000052/uni00000055/uni0000004e/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c
/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000003/uni00000031/uni00000048/uni00000057/uni0000005a/uni00000052/uni00000055/uni0000004e/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c
/uni00000026/uni00000035/uni00000026/uni0000002a/uni00000012/uni00000027/uni00000048/uni00000059/uni0000004c/uni00000046/uni00000048 /uni00000026/uni00000035/uni00000026/uni0000002a/uni00000012/uni00000026/uni0000004f/uni00000052/uni00000058/uni00000047 /uni00000027/uni00000035/uni00000027/uni0000002a/uni00000012/uni00000036/uni0000003a /uni00000027/uni00000035/uni00000027/uni0000002a/uni00000012/uni00000037/uni0000003a /uni00000027/uni00000035/uni00000024/uni0000002a/uni00000032/uni00000031
(a) Qwen2.5-1.5B. (b) OPT-1.3B.
Figure 6: Per-token latency in various network conditions.
/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000003/uni00000031/uni00000048/uni00000057/uni0000005a/uni00000052/uni00000055/uni0000004e/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000013/uni00000017/uni0000001b/uni00000014/uni00000015/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000057/uni00000052/uni00000003/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000000b/uni00000056/uni0000000c
/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000003/uni00000031/uni00000048/uni00000057/uni0000005a/uni00000052/uni00000055/uni0000004e/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000057/uni00000052/uni00000003/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000000b/uni00000056/uni0000000c
/uni00000027/uni00000035/uni00000026/uni0000002a/uni00000012/uni00000037/uni00000048/uni0000005b/uni00000057 /uni00000027/uni00000035/uni00000026/uni0000002a/uni00000012/uni0000002e/uni00000039 /uni00000027/uni00000035/uni00000024/uni0000002a/uni00000032/uni00000031 /uni00000026/uni00000035/uni00000026/uni0000002a/uni00000012/uni00000027/uni00000048/uni00000059/uni0000004c/uni00000046/uni00000048 /uni00000026/uni00000035/uni00000026/uni0000002a/uni00000012/uni00000026/uni0000004f/uni00000052/uni00000058/uni00000047
(a) Qwen2.5-1.5B. (b) OPT-1.3B.
Figure 7: Time-to-First-Token in various network conditions.
Performance. We linearly increase the number of retrieved docu-
ments on both the device and the cloud sides from 0 to 16 and report
the corresponding language modeling perplexity on WikiText. As
shown in Figure 5, DRAGON matches or outperforms all baseline
methods across all settings. As more documents are integrated,
the performance gap between DRAGON and the baseline methods
widens. Finally, DRAGON achieves 1.9√óand1.4√óimprovements
over the non-RAG method, compared to the second-best RAG base-
lines, for Qwen and OPT, respectively. In contrast, CRCG methods
perform poorly due to an insufficient number of retrieved docu-
ments, which indicates incomplete knowledge for the given context.
Additionally, the performance of DRCG quickly saturates once the
amount of retrieved text reaches the context budget limit. How-
ever, we observe a gap between DRCG and our method prior to
the saturation, suggesting that output aggregation may inherently
outperform context aggregation. The results of DRDG methods are
omitted, as they produce identical outputs to DRAGON under the
language modeling setting.
Efficiency. We inject additional latency to the server‚Äôs NIC, ranging
from 0 to 300 ms, along with a jitter equal to 1/5 of the corresponding
latency value. We sample prompts from 10k_prompts_ranked [21],
a collection of synthetic and human-generated prompts with asso-
ciated ranking, and report the average end-to-end decoding latency
8

over 20 output tokens8. Figure 6 presents the per-token latency
when incorporating the top-2 relevant documents for the RAG pro-
cess on each side. As shown in the figure, DRAGON demonstrates
strong robustness under different network conditions compared to
other distributed baseline methods. Specifically, DRAGON achieves
latency reduction of 49.5% and 42.4% when using OPT-1.3B com-
pared to the sequence-wise and token-wise DRDG methods, re-
spectively. In contrast, the per-token latency of DRDG methods
fluctuates significantly and tends to increase under higher network
latency conditions. Sequence-wise DRDG collects output distribu-
tions of all tokens once after generation completes, resulting in
a one-time large data transmission and increased sensitivity to
network latency. Token-wise DRDG amortizes data transmission
over the entire generation process, partially hiding latency within
decoding. However, it still under-performs compared to DRAGON
due to frequent output synchronizations. Additionally, DRCG meth-
ods yields the same per-token latency with corresponding CRCG
methods, because they do not involve cooperation between the
device and the cloud. Although DRAGON incurs an average la-
tency overhead of 15.6%‚Äì20.3% compared to device-only methods,
it effectively supports tasks that require both personal and general
knowledge, where device-only or cloud-only methods may fail.
We further compare the TTFT of DRAGON with that of the
baseline methods under identical network conditions. TTFT typi-
cally includes the time for document retrieval and the latency of
the prefill stage, during which the key-value (KV) activations for
the concatenation of retrieved documents and the input query are
either computed from scratch in parallel or loaded from cache. As
shown in Figure 7, DRAGON incurs negligible TTFT overhead com-
pared to the device-only CRCG method. In contrast, as KV cache
is hosted on the same side with the corpus, DRCG/Text performs
prefill from scratch, resulting in high computation latency and 8.6√ó
TTFT on average compared to DRAGON. DRCG/KV directly fetches
KV activations from the server, leading to increased transmission
time under higher network latency and yielding over 15.3√óTTFT
compared to DRAGON, rendering it entirely impractical. Notably,
DRCG/Text incurs larger prefill latency when using Qwen2.5-1.5B
compared to OPT-1.3B, due to its larger number of parameters. In
contrast, DRCG/KV exhibits higher TTFT on OPT-1.3B, as Qwen2.5-
1.5B employs Grouped-Query Attention (GQA [ 2]) to reduce the
size of KV activations. The transmission data size in DRCG/KV is
114 MB for OPT-1.3B and 16 MB for Qwen2.5-1.5B when retrieving
2 documents of 64 tokens each. Latency for local document retrieval
is measured at 52.6 ms, while latency for remote raw-text retrieval
ranges from 107.2 ms to 745.2 ms as extra network latency increases
from 0 to 300 ms.
7.4 Effectiveness of Scheduling
To thoroughly evaluate the effectiveness of scheduling, we imple-
mented a simulator to run DRAGON repeatedly using different
scheduling strategies under consistent settings. We compare our
scheduling strategy with three baseline methods: (1) Cloud and (2)
Device , where aggregation is statically performed in the cloud and
8Despite averaging, the results still exhibits fluctuations due to varying CPU load and
network jitter, but do not affect the overall conclusion.
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000026/uni0000004f/uni00000052/uni00000058/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000027/uni00000048/uni00000059/uni0000004c/uni00000046/uni00000048 /uni00000027/uni00000035/uni00000024/uni0000002a/uni00000032/uni00000031(a) Qwen2.5-1.5B. (b) OPT-1.3B.
Figure 8: Comparison of different scheduling strategies.
/uni00000027/uni00000035/uni00000024/uni0000002a/uni00000032/uni00000031
/uni00000033/uni0000004c/uni00000053/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000026/uni0000004f/uni00000052/uni00000058/uni00000047/uni00000027/uni00000048/uni00000046/uni00000052/uni00000047/uni00000048 /uni00000027/uni00000055/uni00000044/uni00000049/uni00000057/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051 /uni00000037/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000057/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051 /uni00000036/uni0000005a/uni0000004c/uni00000057/uni00000046/uni0000004b/uni00000024/uni0000004a/uni0000004a/uni00000055/uni00000048/uni0000004a/uni00000044/uni00000057/uni00000052/uni00000055
/uni00000027/uni00000048/uni00000059/uni0000004c/uni00000046/uni00000048
/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000035/uni00000037/uni00000037/uni00000003/uni0000000b/uni00000056/uni0000000c
/uni00000010/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013Z
 /uni00000026/uni0000004f/uni00000052/uni00000058/uni00000047 /uni00000027/uni00000048/uni00000059/uni0000004c/uni00000046/uni00000048
/uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000018/uni00000014/uni00000011/uni0000001b/uni00000015/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000017/uni00000015/uni00000011/uni00000019/uni00000015/uni00000011/uni0000001c/uni00000016/uni00000011/uni00000014/uni00000016/uni00000011/uni00000017 /uni00000016/uni00000011/uni0000001a/uni00000017/uni00000011/uni00000013/uni00000017/uni00000011/uni00000016/uni00000017/uni00000011/uni00000018/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000011
/uni00000026/uni0000004f/uni00000052/uni00000058/uni00000047 /uni00000027/uni00000048/uni00000059/uni0000004c/uni00000046/uni00000048
Figure 9: A random snapshot of the generation pipeline and
scheduling decisions of DRAGON.
the device, respectively, and (3) Random , which randomly selects
the side for aggregation.
To implement the simulation, we record and replay the accep-
tance decisions of the Aggregator, and use real-world measurements
of decoding latency on each side. We simulate varying network
conditions by adding an extra latency and a sinusoidal jitter to
the measured base latency. The period of the jitter is set to 20ùúã
seconds with its amplitude set to 1/5 of the corresponding latency,
consistent with the settings in ¬ß 7.3.
Figure 8 presents the total time required to generate 100 tokens
under varying network conditions, each averaged over 50 different
acceptance decision sequences. The results show that DRAGON‚Äôs
scheduling strategy matches or outperforms all baselines across all
settings, with the efficiency gains increasing as the extra latency
grows. Due to the substantial gap in decoding latencies between the
device and the cloud (as shown in Figure 6), performing aggregation
on the device naturally hides cloud-side decoding and transmis-
sion within device-side decoding. When network latency is low,
Cloud andRandom tend to incur higher latency while DRAGON
consistently selects the device side for aggregation. As network
latency grows and transmission becomes the bottleneck, DRAGON
dynamically selects the side with higher acceptance rate to mini-
mize transmission resulted from draft rejection. Finally, we argue
that when device-side and cloud-side decoding latencies become
closer in value, the overall generation time will be more sensitive
to the network latency. In that case, our scheduling strategy will
achieve greater improvement compared to these baseline methods.
Case study. To illustrate DRAGON‚Äôs detailed scheduling process,
we present a 15-token snapshot of a random simulation with the
extra latency set to 500 ms. Figure 9 shows, from top to bottom,
9

the cloud-side and device-side generation pipelines, the instanta-
neous RTT, the estimation score Œîùëças defined in Equation (8), and
the accumulated acceptance rates. The pipeline graph comprises
vertically arranged bars representing decoding and different trans-
mission tasks (including transmission of draft tokens, target tokens
and instruction signals for switching aggregation place).
Initially, the Aggregator resides on the device by default. From
the perspective of the device, ùëêùëü
dec<ùëêùëô
dec‚â§ùëêùëü
dec+rttconsistently
holds and Œîùëçis computed as the sum of two terms, ùê¥=(1‚àí
ùõºùëü
ùë°)(ùëêùëü
dec‚àíùëêùëô
dec)andùêµ=(ùõºùëô
ùë°‚àíùõºùëü
ùë°)rtt. After the first aggregation at 0.5
s, the acceptance rates are updated to ùõºùëô
0=1andùõºùëü
0=0. As a result,
the positive term ùêµdominates and Œîùëç>0. The Scheduler decides
to switch the Aggregator to the cloud, sending the switching signal
along with the target token. It then shifts to the cloud‚Äôs perspective
and reverses the sign of Œîùëç. Subsequently, since the accumulated
cloud-side acceptance rate remains lower, the Scheduler continues
to estimating Œîùëç<0, indicating that cloud-side aggregation is
more efficient. This case shows that DRAGON‚Äôs scheduling strategy
dynamically minimizes decoding and transmission costs on the side
with a lower acceptance rate, which is consistent with our analysis
in ¬ß 5.1 and the results shown in Figure 8.
8 Related Works
RAG with multiple documents. RAG approaches commonly
retrieve multiple documents to improve performance during in-
ference [ 3], but the way of aggregating them primarily diverge
into two categories: output aggregation and context aggregation
(¬ß 2.1). For output aggregation, pioneering works [ 15,36] prior to
LLMs have proven its effectiveness for encoder-only and seq2seq
models on both extractive [ 15] and abstractive [ 36] NLP tasks. RE-
PLUG [ 51] expands this method to off-the-shelf decoder-only LLMs
by fine-tuning a dense retriever. CAD [ 50] adopts the same idea
to strike a balance between retrieval-augmented outputs and LLM-
only outputs. RA-CM3 [ 59] enables few-shot image classification
for multimodal language model by aggregating the predictions
given different retrieved examples. Context aggregation prepend
the concatenation of all documents to the input and is adopted by a
line of in-context RAG methods [ 24,38,46] for simplicity. PCW [ 47]
eliminates cross-attentions between documents to mitigate the high
computational overhead introduced by this architecture. Our frame-
work leverages output aggregation to facilitate the decomposition
of the multi-document RAG workflow across the device and the
cloud, whereas existing works adopt a centralized architecture.
Device-cloud collaborative inference. To simultaneously achieve
privacy preservation and low latency in mobile computing while
benefiting from the robust computational power of cloud, numerous
studies [ 6,19,28,32,61] have investigated device-cloud collabo-
rative inference for conventional neural networks. Recently, this
collaborative inference paradigm has been extended to large lan-
guage models [ 43,45]. CE-CoLLM [ 25] splits LLMs along depth
and offloads deeper layers to the cloud, with a context manager to
cache and reuse transmitted intermediate hidden states. Crayon [ 5]
offloads difficult or non-customized tasks to a more capable cloud-
hosted LLM rather than the on-device SLM. However, only a fewexisting works have explored enhancing on-device RAG with cloud-
side knowledge. Hybrid-RACA [ 56] implements a real-time compo-
sition assistant, in which cloud-side documents are retrieved, com-
pressed by an LLM and subsequently downloaded to enhance an
on-device SLM. [ 14] utilizes user‚Äôs historical interactions with the
cloud-based LLM to enhance on-device kNN-LMs [ 30]. These works
prioritize service availability over privacy preservation, retrieving
information from a single database processed by LLMs instead
of employing inter-model collaborative generation. In contrast,
DRAGON adopts a symmetric architecture, leveraging databases
on both the device and cloud sides, enabling model collaboration
without compromising document privacy.
Speculative decoding. Speculative decoding, initially proposed
in [55], accelerates the sequential decoding process of LLMs through
a draft-then-verify paradigm, where at each decoding step, mul-
tiple consecutive future tokens are efficiently drafted by a small
LM, and then verified in parallel by the target LLM. Concurrent
studies by [ 34] and [ 10] introduced Speculative Sampling, extend-
ing this paradigm to support diverse sampling strategies. These
works utilize readily available smaller language models from the
same model family as the target LLM for drafting, thus avoiding
additional training. Another line of research directly utilizes the
target LLM for drafting. Medusa [8] and Blockwise Decoding [53]
integrate feed-forward network (FFN) heads into the Transformer
decoder, enabling parallel generation of draft tokens per step. Other
works [ 17,58,60] have investigated early exiting and layer skip-
ping within the target LLM to implement drafting. In contrast to
speculative decoding, where a single drafter fast predicts the output
of the target LLM, speculative aggregation in DRAGON verifies the
consistency between outputs generated by two distinct LLMs.
9 Conclusion
To address privacy risks of cloud LLMs and limited capabilities of on-
device SLMs, we propose DRAGON, a distributed RAG framework
that enhances on-device SLMs using both personal and general
knowledge without raw document transmission. DRAGON parti-
tions the RAG workflow across device and cloud, using Speculative
Aggregation to minimize output synchronization overhead. Experi-
mental results show that DRAGON notably improves generation
quality while maintaining low latency.
References
[1]Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla,
Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary,
Congcong Chen, et al .2025. Phi-4-mini technical report: Compact yet powerful
multimodal language models via mixture-of-loras. arXiv:2503.01743
[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
Lebr√≥n, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query
Transformer Models from Multi-Head Checkpoints. In EMNLP . 4895‚Äì4901.
[3] Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Han-
naneh Hajishirzi, and Wen-tau Yih. 2024. Reliable, adaptable, and attributable
language models with retrieval. arXiv preprint arXiv:2403.03187 (2024).
[4] AI at Meta. 2024. facebook/wiki_dpr. https://huggingface.co/datasets/facebook/
wiki_dpr. Accessed: 2025-04-01.
[5] Jihwan Bang, Juntae Lee, Kyuhong Shim, Seunghan Yang, and Simyung Chang.
2024. Crayon: Customized On-Device LLM via Instant Adapter Blending and
Edge-Server Hybrid Inference. In ACL. 3720‚Äì3731.
[6] Amin Banitalebi-Dehkordi, Naveen Vedula, Jian Pei, Fei Xia, Lanjun Wang, and
Yong Zhang. 2021. Auto-split: a general framework of collaborative edge-cloud
ai.SIGKDD (2021), 2543‚Äì2553.
[7] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-
ford, et al .2022. Improving language models by retrieving from trillions of
10

tokens. In ICML . 2206‚Äì2240.
[8] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming
Chen, and Tri Dao. 2024. Medusa: Simple LLM Inference Acceleration Framework
with Multiple Decoding Heads. In PMLR . 5209‚Äì5235.
[9] Neal Cardwell, Yuchung Cheng, C Stephen Gunn, Soheil Hassas Yeganeh, and Van
Jacobson. 2016. Bbr: Congestion-based congestion control: Measuring bottleneck
bandwidth and round-trip propagation time. Queue (2016), 20‚Äì53.
[10] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Lau-
rent Sifre, and John Jumper. 2023. Accelerating large language model decoding
with speculative sampling. arXiv:2302.01318
[11] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large
language models in retrieval-augmented generation. In AAAI . 17754‚Äì17762.
[12] Lars Daniel. 2025. DeepSeek Data Leak Exposes 1 Million Sensitive Records.
Forbes (2025). https://www.forbes.com/sites/larsdaniel/2025/02/01/deepseek-
data-leak-exposes--1000000-sensitive-records/
[13] DeepSeek-AI. 2024. DeepSeek-V3 Technical Report. arXiv:2412.19437
[14] Yucheng Ding, Chaoyue Niu, Fan Wu, Shaojie Tang, Chengfei Lyu, and Guihai
Chen. 2024. Enhancing on-device llm inference with historical cloud-based llm
interactions. In SIGKDD . 597‚Äì608.
[15] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
2020. Retrieval augmented language model pre-training. In ICML . 3929‚Äì3938.
[16] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The
Curious Case of Neural Text Degeneration. In ICLR .
[17] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt
Keutzer, Amir Gholami, and Yakun Sophia Shao. 2023. SPEED: Speculative
Pipelined Execution for Efficient Decoding. arXiv:2310.12072
[18] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney,
Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. KVQuant: Towards
10 Million Context Length LLM Inference with KV Cache Quantization. In NIPS .
1270‚Äì1303.
[19] Chuang Hu, Wei Bao, Dan Wang, and Fengming Liu. 2019. Dynamic adaptive
DNN surgery for inference acceleration on the edge. ICCC (2019), 1423‚Äì1431.
[20] Junxian Huang, Feng Qian, Alexandre Gerber, Z Morley Mao, Subhabrata Sen,
and Oliver Spatscheck. 2012. A close examination of performance and power
characteristics of 4G LTE networks. In MobiSys . 225‚Äì238.
[21] Data is Better-Together. 2024. data-is-better-together/10k_prompts_ranked.
https://huggingface.co/datasets/data-is-better-together/10k_prompts_ranked.
Accessed: 2025-03-31.
[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-
janowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised Dense Infor-
mation Retrieval with Contrastive Learning. https://arxiv.org/abs/2112.09118
[23] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with
Generative Models for Open Domain Question Answering. In EACL . 874‚Äì880.
[24] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-
Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval
Augmented Generation. In EMNLP . 7969‚Äì7992.
[25] Hongpeng Jin and Yanzhao Wu. 2024. CE-CoLLM: Efficient and Adaptive Large
Language Models Through Cloud-Edge Collaboration. arXiv:2411.02829
[26] Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. 2019. Billion-scale similarity
search with GPUs. IEEE Transactions on Big Data 3 (2019), 535‚Äì547.
[27] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur√©lien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cor-
mode, Rachel Cummings, et al .2021. Advances and open problems in federated
learning. Foundations and Trends in Machine Learning 1‚Äì2 (2021), 1‚Äì210.
[28] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor Mudge, Jason
Mars, and Lingjia Tang. 2017. Neurosurgeon: Collaborative intelligence between
the cloud and mobile edge. ACM SIGARCH Computer Architecture News 1 (2017),
615‚Äì629.
[29] Vladimir Karpukhin, Barlas Oƒüuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In EMNLP . 6769‚Äì6781.
[30] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike
Lewis. 2020. Generalization through Memorization: Nearest Neighbor Language
Models. In ICLR .
[31] LangChain. 2025. langchain-ai/langchain: Build context-aware reasoning appli-
cations. https://github.com/langchain-ai/langchain.
[32] Stefanos Laskaridis, Stylianos I Venieris, Mario Almeida, Ilias Leontiadis, and
Nicholas D Lane. 2020. SPINN: synergistic progressive inference of neural
networks over device and cloud. MobiCom (2020), 1‚Äì15.
[33] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from
transformers via speculative decoding. In ICML . 19274‚Äì19286.
[34] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast Inference from
Transformers via Speculative Decoding. In ICML . 19274‚Äì19286.
[35] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-
t√§schel, et al .2020. Retrieval-augmented generation for knowledge-intensive
nlp tasks. In NIPS . 9459‚Äì9474.[36] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim
Rockt√§schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks. In NIPS . 9459‚Äì9474.
[37] Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng
Li, Yujiu Yang, and Zhaopeng Tu. 2024. Critical Tokens Matter: Token-Level
Contrastive Estimation Enhence LLM‚Äôs Reasoning Capability. arXiv:2411.19943
[38] Hongyin Luo, Tianhua Zhang, Yung-Sung Chuang, Yuan Gong, Yoon Kim, Xixin
Wu, Helen Meng, and James Glass. 2023. Search Augmented Instruction Learning.
InEMNLP . 3717‚Äì3729.
[39] Yu Ma, Weifa Liang, Jing Li, Xiaohua Jia, and Song Guo. 2020. Mobility-aware
and delay-sensitive service provisioning in mobile edge-cloud networks. TMC 1
(2020), 196‚Äì210.
[40] Pavel Mach and Zdenek Becvar. 2017. Mobile Edge Computing: A Survey on
Architecture and Computation Offloading. IEEE Communications Surveys and
Tutorials 3 (2017), 1628‚Äì1656.
[41] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016.
Pointer Sentinel Mixture Models. arXiv:1609.07843 [cs.CL]
[42] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774
[43] Yanghe Pan, Zhou Su, Yuntao Wang, Shaolong Guo, Han Liu, Ruidong Li, and
Yuan Wu. 2024. Cloud-Edge Collaborative Large Model Services: Challenges and
Solutions. IEEE Network (2024), 1‚Äì1.
[44] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic differentiation in PyTorch.
[45] Guanqiao Qu, Qiyuan Chen, Wei Wei, Zheng Lin, Xianhao Chen, and Kaibin
Huang. [n. d.]. Mobile Edge Intelligence for Large Language Models: A Contem-
porary Survey. arXiv:2407.18921
[46] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin
Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language
models. TACL (2023), 1316‚Äì1331.
[47] Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend,
Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.
Parallel Context Windows for Large Language Models. In ACL. 6383‚Äì6402.
[48] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. In EMNLP . Association for Computational Lin-
guistics. https://arxiv.org/abs/1908.10084
[49] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min,
Luke Zettlemoyer, and Pang Wei Koh. 2024. Scaling Retrieval-Based Language
Models with a Trillion-Token Datastore. In NIPS . 91260‚Äì91299.
[50] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and
Wen-tau Yih. 2024. Trusting Your Evidence: Hallucinate Less with Context-aware
Decoding. In NAACL . 783‚Äì791.
[51] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike
Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. REPLUG: Retrieval-Augmented
Black-Box Language Models. In NAACL . 8371‚Äì8384.
[52] Milad Shokouhi, Luo Si, et al .2011. Federated Search. Foundations and Trends ¬Æ
in Information Retrieval 1 (2011), 1‚Äì102.
[53] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise Parallel
Decoding for Deep Autoregressive Models. In NIPS . 10107‚Äì10116.
[54] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-
langue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,
et al.2019. Huggingface‚Äôs transformers: State-of-the-art natural language pro-
cessing. arXiv preprint arXiv:1910.03771 (2019).
[55] Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui.
2023. Speculative Decoding: Exploiting Speculative Execution for Accelerating
Seq2seq Generation. In EMNLP . 3909‚Äì3925.
[56] Menglin Xia, Xuchao Zhang, Camille Couturier, Guoqing Zheng, Saravan Ra-
jmohan, and Victor R√ºhle. 2024. Hybrid-RACA: Hybrid Retrieval-Augmented
Composition Assistance for Real-time Text Prediction. In EMNLP . 120‚Äì131.
[57] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al .2024. Qwen2.5
technical report. arXiv:2412.15115
[58] Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris Papailiopoulos, and Kang-
wook Lee. 2024. Predictive Pipelined Decoding: A Compute-Latency Trade-off
for Exact LLM Decoding. TMLR (2024).
[59] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec,
Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Retrieval-
augmented multimodal language modeling. In ICML . Article 1659, 15 pages.
[60] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad
Mehrotra. 2024. Draft& Verify: Lossless Large Language Model Acceleration via
Self-Speculative Decoding. In ACL. 11263‚Äì11282.
[61] Shigeng Zhang, Yinggang Li, Xuan Liu, Song Guo, Weiping Wang, Jianxin Wang,
Bo Ding, and Di Wu. 2020. Towards real-time cooperative deep inference over the
cloud and edge end devices. ACM on Interactive, Mobile, Wearable and Ubiquitous
Technologies 2 (2020), 1‚Äì24.
[62] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
11

Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained
Transformer Language Models. arXiv:2205.01068 [cs.CL]
[63] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, Zhangyang Wang,
and Beidi Chen. 2023. H2O: heavy-hitter oracle for efficient generative inference
of large language models. In NIPS . Article 1506, 50 pages.
[64] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .2023. A survey
of large language models. arXiv:2303.18223
A Appendix
A.1 Numerical Stability
We leverage the log-sum-exp trick to enhance numerical stability.
Specifically, after decoding a draft token on each side ùë†, the corrected
value of‚Ñéùë†
ùë°is computed as
Àú‚Ñéùë†
ùë°=log‚àëÔ∏Å
ùëë‚ààùê∑ùë†exp(R(ùëë,ùë•<ùë°)).
and is synchronized across both sides along with logùíëùë†
ùë°. During
aggregation, we compute logùúÇùë†
ùë°as follows:
logsoftmax([Àú‚Ñéùëô
ùë°,Àú‚Ñéùëü
ùë°])=logexpÀú‚Ñéùë†
ùë°
expÀú‚Ñéùëô
ùë°+expÀú‚Ñéùëü
ùë°
=log√ç
ùëë‚ààùê∑ùë†expR(ùëë,ùë•<ùë°)√ç
ùëë‚Ä≤‚ààùê∑ùëô‚à™ùê∑ùëüexpR(ùëë‚Ä≤,ùë•<ùë°)=log‚Ñéùë†
ùë°
‚Ñéùëô
ùë°+‚Ñéùëü
ùë°=log(ùúÇùë†
ùë°).
The log of the target distribution logùíëùë°is then obtained by:
log‚àëÔ∏Å
ùë†‚àà{ùëô,ùëü}exp(logùíëùë†
ùë°+logùúÇùë†
ùë°)=log(ùëùùëô
ùë°ùúÇùëô
ùë°+ùëùùëü
ùë°ùúÇùëü
ùë°)=logùíëùë°.
On one hand, both the log-sum-exp and log-softmax operations
are inherently numerically stable. On the other hand, since our
data compression algorithm only transmits the top- ùëùvalues of
the locally-aggregated output distributions, it effectively avoids
numerical underflow of logùíëùë†
ùë°.
A.2 Correctness of the Aggregation Strategy
We will show that for any locally-aggregated distributions ùíëùëô
ùë°and
ùíëùëü
ùë°, the target token ùë•ùë°produced by the aggregation strategy follows
a distribution identical to that sampled from ùíëùë°=ùúÇùëô
ùë°ùíëùëô
ùë°+ùúÇùëü
ùë°ùíëùëü
ùë°,
where{ùëô,ùëü}={ùëëùëíùë£ùëñùëêùëí,ùëêùëôùëúùë¢ùëë}.
First, we demonstrate that the intermediate outputs Àúùë•ùëô
ùë°and Àúùë•ùëü
ùë°
from the two independent speculative sampling processes are in-
deed drawn from ùíëùë°. Note that, since ùúÇùë†
ùë°=‚Ñéùë†
ùë°/(‚Ñéùëô
ùë°+‚Ñéùëü
ùë°)forùë†‚àà{ùëô,ùëü},
we haveùúÇùëô
ùë°+ùúÇùëü
ùë°=1.
For sideùëô, the probability to reject a draft token is
ùëÉ(ùëüùëíùëóùëíùëêùë°ùëíùëë)=ùê∏ùë•‚àºùíëùëô
ùë°(ùë•)(1‚àímin(1,ùúÇùëô
ùë°+ùúÇùëü
ùë°ùíëùëü
ùë°(ùë•)/ùíëùëô
ùë°(ùë•)))
=1‚àí‚àëÔ∏Å
min(ùíëùëô
ùë°(ùë•),ùúÇùëô
ùë°ùíëùëô
ùë°(ùë•)+ùúÇùëü
ùë°ùíëùëü
ùë°(ùë•))
=1‚àí‚àëÔ∏Å
(ùíëùëô
ùë°(ùë•)+min(0,ùúÇùëü
ùë°(ùíëùëü
ùë°(ùë•)‚àíùíëùëô
ùë°(ùë•))))
=ùúÇùëü
ùë°‚àëÔ∏Å
‚àímin(0,ùíëùëü
ùë°(ùë•)‚àíùíëùëô
ùë°(ùë•))
=ùúÇùëü
ùë°‚àëÔ∏Å
(ùíëùëô
ùë°(ùë•)‚àímin(ùíëùëô
ùë°(ùë•),ùíëùëü
ùë°(ùë•))).
Dec  1 2 3 4 5 6Remote Tokens TransDec TransDec TransDec TransDecDec
TransTrans
Dec
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Time 1 2 3 4 5 6Local TokensDecAggr. 1 Aggr. 2 Aggr. 3 Aggr. 4 Aggr. 5 Aggr. 6
TransDec
TransTrans
Dec TransDecDec TransDecDecDec
TransDecDec
TransDec TransFigure 10: Decoding pipeline when the Aggregator continu-
ously rejects ùë•ùëôand accepts ùë•ùëü.
The adjusted distribution, from which we sample after the draft
token is rejected, can be expressed as
Àúùíëùëô
ùë°(ùë•)=norm(max(0,ùíëùëü
ùë°(ùë•)‚àíùëùùëô
ùë°(ùë•))
=norm(ùíëùëü
ùë°(ùë•)‚àímin(ùíëùëô
ùë°(ùë•),ùíëùëü
ùë°(ùë•)))
=ùíëùëü
ùë°(ùë•)‚àímin(ùíëùëô
ùë°(ùë•),ùíëùëü
ùë°(ùë•))
√ç
ùë•‚Ä≤(ùíëùëü
ùë°(ùë•‚Ä≤)‚àímin(ùíëùëô
ùë°(ùë•‚Ä≤),ùíëùëü
ùë°(ùë•‚Ä≤))).
Since√ç(ùíëùëô
ùë°(ùë•)‚àímin(ùíëùëô
ùë°(ùë•),ùíëùëü
ùë°(ùë•)))is equivalent to√ç(ùíëùëü
ùë°(ùë•)
‚àímin(ùíëùëô
ùë°(ùë•),ùíëùëü
ùë°(ùë•))),ùëÉ(ùëüùëíùëóùëíùëêùë°ùëíùëë,ùë• =Àúùë•ùëô
ùë°), the probability that Àúùë•ùëô
ùë°
is re-sampled after rejecting ùë•ùëô
ùë°, is
ùëÉ(ùëüùëíùëóùëíùëêùë°ùëíùëë)Àúùíëùëô
ùë°(Àúùë•ùëô
ùë°)=ùúÇùëü
ùë°(ùíëùëü
ùë°(Àúùë•ùëô
ùë°)‚àímin(ùíëùëô
ùë°(Àúùë•ùëô
ùë°),ùíëùëü
ùë°(Àúùë•ùëô
ùë°))).
Finally, the sampled token Àúùë•ùëô
ùë°follows the distribution
ùëÉ(ùë•=Àúùë•ùëô
ùë°)=ùëÉ(ùëéùëêùëêùëíùëùùë°ùëíùëë,ùë• =Àúùë•ùëô
ùë°)+ùëÉ(ùëüùëíùëóùëíùëêùë°ùëíùëë,ùë• =Àúùë•ùëô
ùë°)
=ùíëùëô
ùë°(Àúùë•ùëô
ùë°)min(1,ùúÇùëô
ùë°+ùúÇùëü
ùë°ùíëùëü
ùë°(Àúùë•ùëô
ùë°)/ùíëùëô
ùë°(Àúùë•ùëô
ùë°))
+ùúÇùëü
ùë°(ùíëùëü
ùë°(Àúùë•ùëô
ùë°)‚àímin(ùíëùëô
ùë°(Àúùë•ùëô
ùë°),ùíëùëü
ùë°(Àúùë•ùëô
ùë°)))
=ùúÇùëô
ùë°ùíëùëô
ùë°(Àúùë•ùëô
ùë°)+ùúÇùëü
ùë°min(ùíëùëô
ùë°(Àúùë•ùëô
ùë°),ùíëùëü
ùë°(Àúùë•ùëô
ùë°))
+ùúÇùëü
ùë°ùíëùëü
ùë°(Àúùë•ùëô
ùë°)‚àíùúÇùëü
ùë°min(ùíëùëô
ùë°(Àúùë•ùëô
ùë°),ùíëùëü
ùë°(Àúùë•ùëô
ùë°)))
=ùúÇùëô
ùë°ùíëùëô
ùë°(Àúùë•ùëô
ùë°)+ùúÇùëü
ùë°ùíëùëü
ùë°(Àúùë•ùëô
ùë°)=ùíëùë°(Àúùë•ùëô
ùë°).
As a result, Àúùë•ùëô
ùë°is distributed identically to tokens sampled from ùíëùë°.
Since the correctness proof for the other side ùëüis symmetric, we
can conclude straightforwardly that Àúùë•ùëü
ùë°‚àºùíëùë°.
Finally, the aggregation strategy randomly select either Àúùë•ùëô
ùë°or
Àúùë•ùëü
ùë°as the target token ùë•ùë°, with a uniform probability. Obviously,
ùë•ùë°‚àº0.5ùíëùë°+0.5ùíëùë°=ùíëùë°.
A.3 Decoding Pipelines
Apart from the theoretical analysis of latency per token in Sec-
tion 5, we use pipeline graphs to illustrate scenarios where each
acceptance case repeats continuously. This is not necessarily how
pipelines occur in practice, but it provides us with an heuristics of
the scheduling strategy. In the following discussion, we define ùëôas
the side responsible for aggregation (i.e., the local side) and ùëüas the
other side (i.e., the remote side). We set random delays to analyze
specific cases where all time values are expressed in the same unit.
12

Dec 1 2 3 4 5 6Remote TokensDec
TransDec
TransTransDec Trans
DecDec
TransDec
TransTrans
DecDec
TransDec
TransTrans
DecDec
Dec
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Time 1 2 3 4 5 6Local TokensAggregation 1 Aggregation 2 Aggregation 3
TransDec DecTrans
Dec
TransTrans
DecDec
TransDecTrans
Dec
TransFigure 13: Decoding pipeline when the Aggregator continu-
ously rejects both ùë•ùëôandùë•ùëü.
Dec  1 2 3 4 5 6Remote Tokens TransDec Trans Dec TransDec Trans Dec TransDec TransTrans Dec
Dec
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Time 1 2 3 4 5 6Local TokensAggr. 1 Aggr. 2 Aggr. 3
Dec
TransTrans
Dec
TransDec
TransTrans
TransDec TransDecTrans
Trans
Figure 11: Decoding pipeline when the Aggregator continu-
ously accepts ùë•ùëôand rejects ùë•ùëü.
Dec  1 2 3 4 5 6Remote TokensDec
TransTransDecDec
TransTransDec TransDec Trans
Dec
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Time 1 2 3 4 5 6Local TokensDecAggr. 1 Aggr. 2 Aggr. 3 Aggr. 4 Aggr. 5 Aggr. 6
TransDec
TransDec
TransDec
TransTransDec Trans
Figure 12: Decoding pipeline when the Aggregator continu-
ously accepts both ùë•ùëôandùë•ùëü.i) Continuously reject ùë•ùëôand accept ùë•ùëü.The pipeline is shown
in Figure 10, where ùëêùëô
trans=1.2,ùëêùëü
trans=1.8,ùëêùëô
dec=1, andùëêùëü
dec=1.5.
The latency bottleneck is ùëêùëü
dec, causingùëôto wait forùë•ùëü
1before the
first Aggregation. As ùëôgenerates draft tokens faster, subsequent
aggregations begin upon the arrival of each ùë•ùëü
ùë°. Whenùëüdecodes
faster, the bottleneck becomes ùëêùëô
dec. As a result, the latency per
token is max(ùëêùëô
dec,ùëêùëü
dec).
ii) Continuously accept ùë•ùëôand reject ùë•ùëü.The pipeline is
shown in Figure 11, where ùëêùëô
trans=1.5,ùëêùëü
trans=1,ùëêùëô
dec=2, and
ùëêùëü
dec=2. Althoughùëêùëô
dec=ùëêùëü
dec, local draft tokens do not require
transmission; therefore, the latency bottleneck thus lies on the
remote side. After each aggregation at step ùë°,ùëômust wait for a
duration of ùëêùëô
trans+ùëêùëü
dec+ùëêùëü
trans, including the transmission of tar-
get tokenùë•ùë°, as well as the decoding and transmission of the re-
mote draft token ùë•ùëü
ùë°+1. Only when ùëêùëô
decexceeds this duration, the
bottleneck shifts to the local side. The latency per token is thus
max(ùëêùëô
dec,ùëêùëô
trans+ùëêùëü
dec+ùëêùëü
trans).
iii) Continuously accept both ùë•ùëôandùë•ùëü.The pipeline is shown
in Figure 12, where ùëêùëô
trans=1.5,ùëêùëü
trans=1.8,ùëêùëô
dec=1, andùëêùëü
dec=1.5.
Clearly, the bottleneck lies on the side with the larger decoding
delay. Consequently, the latency per token is max(ùëêùëô
dec,ùëêùëü
dec).
iv) Continuously reject both ùë•ùëôandùë•ùëüThe pipeline is shown
in Figure 13, where ùëêùëô
trans=1.5,ùëêùëü
trans=1.8,ùëêùëô
dec=2, andùëêùëü
dec=
1. Since rejecting the local draft token resets ùúë(ùëêùëô
dec)toùëêùëô
dec, the
scenario is exactly the same as ii). The latency per token is computed
asmax(ùëêùëô
dec,ùëêùëô
trans+ùëêùëü
dec+ùëêùëü
trans).
13