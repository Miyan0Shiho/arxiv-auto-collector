# SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model

**Authors**: Zongcan Ding, Haodong Zhang, Peng Wu, Guansong Pang, Zhiwei Yang, Peng Wang, Yanning Zhang

**Published**: 2025-04-14 15:30:03

**PDF URL**: [http://arxiv.org/pdf/2504.10320v1](http://arxiv.org/pdf/2504.10320v1)

## Abstract
Video anomaly detection (VAD) aims to identify unexpected events in videos
and has wide applications in safety-critical domains. While semi-supervised
methods trained on only normal samples have gained traction, they often suffer
from high false alarm rates and poor interpretability. Recently,
vision-language models (VLMs) have demonstrated strong multimodal reasoning
capabilities, offering new opportunities for explainable anomaly detection.
However, their high computational cost and lack of domain adaptation hinder
real-time deployment and reliability. Inspired by dual complementary pathways
in human visual perception, we propose SlowFastVAD, a hybrid framework that
integrates a fast anomaly detector with a slow anomaly detector (namely a
retrieval augmented generation (RAG) enhanced VLM), to address these
limitations. Specifically, the fast detector first provides coarse anomaly
confidence scores, and only a small subset of ambiguous segments, rather than
the entire video, is further analyzed by the slower yet more interpretable VLM
for elaborate detection and reasoning. Furthermore, to adapt VLMs to
domain-specific VAD scenarios, we construct a knowledge base including normal
patterns based on few normal samples and abnormal patterns inferred by VLMs.
During inference, relevant patterns are retrieved and used to augment prompts
for anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast
and slow detectors to enhance robustness of anomaly detection. Extensive
experiments on four benchmarks demonstrate that SlowFastVAD effectively
combines the strengths of both fast and slow detectors, and achieves remarkable
detection accuracy and interpretability with significantly reduced
computational overhead, making it well-suited for real-world VAD applications
with high reliability requirements.

## Full Text


<!-- PDF content starts -->

SlowFastVAD: Video Anomaly Detection via Integrating SimpleDetector and RAG-Enhanced Vision-Language ModelZongcan Ding∗Northwestern PolytechnicalUniversityXi’an, Chinadingzongcan@mail.nwpu.edu.cnHaodong Zhang∗Northwestern PolytechnicalUniversityXi’an, Chinahdzhang@mail.nwpu.edu.cnPeng Wu†Northwestern PolytechnicalUniversityXi’an, Chinaxdwupeng@gmail.comGuansong PangSingapore ManagementUniversitySingapore, Singaporegspang@smu.edu.sgZhiwei YangXidian UniversityGuangzhou, Chinazwyang97@163.comPeng WangNorthwestern PolytechnicalUniversityXi’an, Chinapeng.wang@nwpu.edu.cnYanning ZhangNorthwestern PolytechnicalUniversityXi’an, Chinaynzhang@nwpu.edu.cnAbstractVideo anomaly detection (VAD) aims to identify unexpected eventsin videos and has wide applications in safety-critical domains.While semi-supervised methods trained on only normal sampleshave gained traction, they often su!er from high false alarm ratesand poor interpretability. Recently, vision-language models (VLMs)have demonstrated strong multimodal reasoning capabilities, o!er-ing new opportunities for explainable anomaly detection. However,their high computational cost and lack of domain adaptation hinderreal-time deployment and reliability. Inspired by dual complemen-tary pathways in human visual perception, we proposeSlowFast-VAD, a hybrid framework that integrates a fast anomaly detectorwith a slow anomaly detector (namely a retrieval augmented genera-tion (RAG) enhanced VLM), to address these limitations. Speci"cally,the fast detector "rst provides coarse anomaly con"dence scores,and only a small subset of ambiguous segments, rather than the en-tire video, is further analyzed by the slower yet more interpretableVLM for elaborate detection and reasoning. Furthermore, to adaptVLMs to domain-speci"c VAD scenarios, we construct a knowledgebase including normal patterns based on few normal samples andabnormal patterns inferred by VLMs. During inference, relevantpatterns are retrieved and used to augment prompts for anomalyreasoning. Finally, we smoothly fuse the anomaly con"dence offast and slow detectors to enhance robustness of anomaly detec-tion. Extensive experiments on four benchmarks demonstrate thatSlowFastVAD e!ectively combines the strengths of both fast andslow detectors, and achieves remarkable detection accuracy andinterpretability with signi"cantly reduced computational overhead,making it well-suited for real-world VAD applications with highreliability requirements. The code will be released upon acceptance.∗These authors contributed equally to this work.†Corresponding author.CCS Concepts•Computing methodologies→Scene anomaly detection;Activity recognition and understanding.KeywordsVideo anomaly detection, Vision-language model, Semi-supervisedlearning, Interpretable learning1 IntroductionVideo Anomaly Detection (VAD) aims to automatically identifyabnormal events in video streams that deviate signi"cantly fromtypical normal patterns. It plays a vital role in a wide range ofreal-world applications [55,65,67]. Given the rarity and high acqui-sition cost of anomalous samples in real-world scenarios, increasingattention has been directed toward the semi-supervised VAD para-digm, which trains models exclusively on normal videos [30,34,75].By learning the underlying distribution of normal patterns, thesemethods attempt to detect anomalies as deviations from expectedbehaviors during inference.However, semi-supervised VAD methods su!er from several in-herent limitations. Since these models are trained exclusively onnormal samples, they are prone to misclassifying rare yet plausiblenormal behaviors as anomalies, leading to high false positive rates.Moreover, existing one-class detection approaches, such as thosebased on autoencoders [7,18,40,48,53,73,83], generative adver-sarial networks (GANs) [13,19,77], or di!usion models [10,15,70],often exhibit limited adaptability when deployed in complex anddynamic real-world environments. In addition, most of these meth-ods rely on end-to-end deep neural networks that are trained to "tonly the distribution of normal behaviors. As a result, their decision-making results are often monotonous and lack interpretability orreasoning, making them ill-suited for scenarios where transparencyand explainability are crucial.

Conference’17, July 2017, Washington, DC, USAZongcan Ding et al.
Fast ClassifcationDNN
Interpretable ReasoningVLM
A pedestrian is observed pushing a bicycle, an action that deviates from previously pedestrian behaviors. Based on this, this is classified as an anomaly.Normal&AbnomralDistributionEfficient Classifcation Interpretable ReasoningDNN
VLM
 Minority  Ambiguous        Video   ClipsMajority  Certain Video   Clips
...
...
...
Fast ClassifcationDNN
Interpretable ReasoningVLM
A pedestrian is observed pushing a bicycle, an action that deviates from previously pedestrian behaviors. Based on this, this is classified as an anomaly.Normal&AbnomralDistributionEfficient Classifcation Interpretable ReasoningDNN
VLM
 Minority  Ambiguous        Video   ClipsMajority  Certain Video   Clips
...
...
...
Fast ClassifcationDNN
Interpretable ReasoningVLM
A pedestrian is observed pushing a bicycle, an action that deviates from previously pedestrian behaviors. Based on this, this is classified as an anomaly.Normal&AbnomralDistributionEfficient Classifcation Interpretable ReasoningDNN
VLM
 Minority  Ambiguous        Video   ClipsMajority  Certain Video   Clips
...
...
...
Fast ClassifcationDNN
Interpretable ReasoningVLM
A pedestrian is observed pushing a bicycle, an action that deviates from previously pedestrian behaviors. Based on this, this is classified as an anomaly.Normal&AbnomralDistributionEfficient Classifcation Interpretable ReasoningDNN
VLM
 Minority  Ambiguous        Video   ClipsMajority  Certain Video   Clips
...
...
...Figure 1: Comparative analysis between conventional fastdetector based on DNN (Left), recent slow detectors based onVLMs (Middle), and our SlowFastVAD (Right).Recently, vision-language models (VLMs) have achieved remark-able progress across various domains and have demonstrated greatpotential in the semi-supervised VAD task [71]. Bene"ting fromtheir multimodal integration and semantic understanding abilities,VLMs can e!ectively uncover latent behavioral patterns withinvideo data. For instance, in the widely-used pedestrian street datasetPed2, VLMs can infer the normative pattern that “only pedestri-ans are allowed to walk on the sidewalk” by learning from normalvideos. In complex real-world scenarios, such models are capable oflearning and constructing semantic representations of normal pat-terns, thereby enabling more accurate identi"cation of anomaliesthat deviate from these learned norms. Furthermore, by leveragingtheir language generation capabilities and the established semanticrules, VLMs can provide clear reasoning for their detection results,signi"cantly enhancing the interpretability and trustworthiness.Despite the promising potential of VLMs in the VAD task, theirpractical application still faces several critical challenges that war-rant further investigation. First, VLMs are susceptible to the halluci-nation, where the generated reasoning or predictions deviate fromthe actual video content. For example, in Ped2, VLMs occasion-ally misinterpret normal pedestrian walking as a crowd gathering,thereby incorrectly labeling it as an anomalous event, resulting insemantically inconsistent judgments. Second, most current VLMsare pre-trained on general-purpose datasets, and their anomalyunderstanding is typically based on commonsense reasoning ratherthan task-speci"c behavioral modeling. Consequently, such mod-els may misinterpret anomalies in speci"c environments due tosemantic ambiguity. For instance, riding a bicycle on the sidewalkis treated as an anomalous event within the Ped2 dataset’s con-text. However, since such behavior is often deemed acceptable inreal-world scenarios, the model may fail to detect it as an anomaly.Furthermore, from a deployment perspective, VLMs often incur sub-stantial computational overhead and exhibit slow inference speeds.Performing dense inference on every video frame is particularlyimpractical in scenarios requiring real-time responses, such as pub-lic safety surveillance. These limitations signi"cantly hinder thepractical utility of VLM-based VAD methods.Inspired by the dual complementary pathways in human visualperception [12,74], namely, a cognition-driven pathway for pre-cise understanding (slow) and an action-driven pathway for rapidresponse (fast), and these pathways work in tandem to responde!ectively even in extreme scenarios. This paper proposes a novelVAD framework,SlowFastVAD, which integrates the complemen-tary strengths of fast and slow detectors. The goal is to achievee#cient, accurate, and interpretable anomaly detection by combin-ing a traditional feedforward network based fast detector with ahigh-generalization VLM based slow detector. Speci"cally, to en-hance the adaptability and detection performance of large modelsin speci"c scenarios, we design a retrieval augmented generation(RAG) driven anomaly reasoning module. This module guides theVLM to generate various visual descriptions from normal sam-ples, summarizes normal patterns under the given context, andfurther leverages Chain-of-Thought (CoT) reasoning to infer po-tential abnormal patterns. These normal and abnormal patternsare structured into a knowledge base. This process of knowledgebase construction requires only a small number of normal sam-ples, eliminating the requirement for full-sample training. Duringinference, the model retrieves relevant behavioral rules from theknowledge base based on the language description of the currentvideo segment and incorporates them into prompts to guide theVLM toward more targeted anomaly detection. To mitigate thehigh computational overhead associated with the VLM inference,we propose an entropy-based intervention detection strategy. Thisstrategy leverages the anomaly con"dence generated by the fastdetector to identify video segments with high uncertainty, whichare then selectively forwarded to the VLM-base slow detector forfurther analysis. This enables signi"cant improvements in detec-tion accuracy and interpretability while maintaining computationale#ciency. Finally, we introduce a decision fusion mechanism thatintegrates the predictions from both fast and slow detectors, therebyenhancing the overall robustness of the framework. We illustratethe key di!erences between SlowFastVAD, traditional DNN-basedfast detectors and VLM-based slow detector in Figure 1, and ourSlowFastVAD e!ectively addresses the limitations of current fastdetector, namely the limited generalization capability, poor inter-pretability, and high computational cost of slow detector.The main contributions of this work are summarized as follows:•We propose the SlowFastVAD framework, which, to our knowl-edge, is the "rst to innovatively integrate the traditional fast anom-aly detector with slow yet interpretable VLM-based detector, achiev-ing a synergy between e#ciency and explainability.•We develop a RAG-driven anomaly reasoning module, in whichVLM summarizes normal and abnormal patterns during training toconstruct a knowledge base. This knowledge base is then dynam-ically retrieved during inference to enhance prompts, improvingthe generalization to speci"c VAD scenarios.•We design an entropy-based intervention detection strategy thate!ectively selects video segments likely to be misclassi"ed by thefast detector, precisely triggering the VLM inference. This strategysigni"cantly reduces overall computational costs.•Extensive experiments on multiple public datasets demonstratethat our proposed SlowFastVAD e!ectively integrates the advan-tages of both fast and slow detectors, achieving state-of-the-artdetection performance along with interpretable outputs.

SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model Conference’17, July 2017, Washington, DC, USA2 Related Work2.1 Non-VLM-based Video Anomaly Detection2.1.1 Semi-supervised VAD.In semi-supervised VAD, training pro-cessing relies solely on normal samples, where the model learnsnormal patterns and identi"es deviations from these patterns duringinference as anomalies. Under the current deep learning paradigm,semi-supervised VAD approaches can be broadly categorized basedon the network architecture into three main types: autoencoder-based approaches, generative adversarial networks (GANs)-basedapproaches, and di!usion-based approaches. Autoencoder-basedapproaches utilize an encoder to compress input samples into low-dimensional latent representations and a decoder to reconstructthe original input from the latent space. Anomalies are detected bymeasuring the reconstruction error between the input and output[7,18,40,48,53,73,83]. GAN-based approaches consist of a genera-tor and a discriminator. The generator learns to synthesize realisticnormal samples, while the discriminator aims to distinguish be-tween real and generated data. Test samples with low authenticityscores from the discriminator are classi"ed as anomalies [13,19,77].Di!usion-based approaches progressively generate samples fromnoise through a reverse di!usion process. The quality of the gen-erated samples is then used to assess the normality, with poorreconstruction indicating potential anomalies [10, 15, 70].2.1.2 Weakly Supervised VAD.Weakly-supervised VAD utilizesboth normal and abnormal samples during training, but lacks pre-cise annotations of anomalies, and only coarse video-level labelsare available. Current research mainly follows two paradigms: one-stage multiple instance learning (MIL) approaches [29,54] and two-stage self-training strategies [72,80]. To further improve detectionperformance, recent e!orts have explored various enhancementtechniques, including temporal modeling, spatiotemporal modeling,MIL-based optimization, and feature metric learning. Speci"cally,temporal modeling captures sequential dependencies in videos, en-abling the model to utilize contextual information [11,17,54,86].Spatiotemporal modeling integrates spatial and temporal featuresto localize anomalous regions while suppressing background noise[26,54]. MIL-based optimization strategies address the limitationof conventional MIL methods that focus only on high-scoring seg-ments, by incorporating external priors, such as textual knowledge,to improve anomaly localization [9,36]. Feature metric learningconstructs a discriminative embedding space by clustering simi-lar features and separating dissimilar ones, thereby enhancing therepresentation discrimination [14].2.2 VLM-based Video Anomaly Detection2.2.1 Semi-supervised VAD.In the "eld of VAD, VLMs have demon-strated signi"cant potential and adaptability. Yang et al. proposedAnomalyRuler [71], which detects anomalies by integrating theinductive summarization and deductive reasoning capabilities ofVLMs. Speci"cally, in the inductive phase, the model derives behav-ioral rules from a small number of normal samples, while in thedeductive phase, it identi"es anomalous frames based on these rules.In addition, Jiang et al. introduced the VLAVAD framework [25],which employs cross-modal pre-trained models and leverages thereasoning capabilities of large language models (LLMs) to enhancethe interpretability and e!ectiveness of VAD. However, due to theslow inference speed of VLMs, the overall processing time of thesemethods remains high. In contrast, our SlowFastVAD integratesconventional fast detectors with VLMs, enabling sparse yet deeperreasoning based on the initial outputs of the fast detector. Thisdesign e!ectively balances inference speed and detection accuracy.2.2.2 Weakly Supervised VAD.VLMs have also been widely ap-plied in weakly supervised VAD. They not only enhance anomalydetection performance through visual-language enhanced features(e.g., CLIP-TSA [21]) and cross-modal semantic alignment (e.g., Vad-CLIP [69], TPWNG [72], and STPrompts [68]), but also contributeto interpretability by generating descriptions for anomalous events,as demonstrated in the Holmes-VAU [82]. Moreover, VLMs canbe leveraged for training-free anomaly detection by utilizing theirextensive prior knowledge [35,79], o!ering advantages in rapiddeployment and reduced computational cost. For instance, Zanellaet al. [79] adopted an explainable approach in which re$ective ques-tions are used to guide the model in generating anomaly scores,without requiring additional model training.2.3 VLM-based Vision TasksCurrently, VLMs have made signi"cant progress and found wide-spread application in various vision "elds [57]. In image classi"ca-tion, VLM enhances zero-shot classi"cation capabilities, especiallyin handling unknown object categories, showing excellent perfor-mance and supporting stronger domain generalization [1,22]. Insemantic segmentation, VLM improves the ability to handle unseencategories signi"cantly by combining open-vocabulary techniqueswith image-text fusion [33,60]. In video generation, VLM is used togenerate consistent and multi-scene video content, pushing forwardthe advancement of video generation technology [31]. In cross-modal retrieval, VLM improves the e!ectiveness and e#ciency byintegrating image and language information [6,23]. In action recog-nition, VLM enhances the recognition of "ne-grained actions bycombining pose information with language models, particularlyexcelling in action anticipation[39, 81].3 Methodology3.1 OverviewOur proposed method is illustrated in Figure 2, which consists oftwo branches: a fast DNN-based detector and a slow VLM-baseddetector. The fast detector is built upon an autoencoder-based archi-tecture, o!ering high detection speed but limited interpretability. Incontrast, the slow detector leverages VLMs, which provides stronginterpretability at the cost of slower inference. By integrating mul-tiple specialized components, our framework e!ectively combinesthe advantages of both detectors to achieve a balanced trade-o!between e#ciency and accuracy. The overall pipeline is as follows:The fast detector "rst performs preliminary detection and identi-"es potentially ambiguous segments, which are then passed to theslow detector for further analysis. The slow detector generates bothanomaly con"dence scores and interpretable descriptions. To selectambiguous segments more e!ectively, we propose an interventiondetection strategy based on entropy measures. Additionally, to im-prove the adaptability of the VLM in speci"c anomaly detection

Conference’17, July 2017, Washington, DC, USAZongcan Ding et al.         
PartialTraining VideosSpace-Time Information VLMNormal PatternsAbnormal PatternsSpatial RegularityBehavior PatternInteraction DynamicCoT Prompt
TextEmbedding
Knowledge Base
Testing Video      RAG      Prompt
Recall
Integration&InferenceIntervention DetectionRAG-Driven ReasoningSuspicious Clip         LLM
Abnormal Score: 1.0Abnormal Reason:The man on the right walked quickly in the corridor, suddenly accelerated halfway and made a throwing motion, which conforms to the abnormal behavior patterns in the knowledge base[1][3].Slow Detection 
         LLM
Slow-Fast Integration MAE         Fast Detection Entropy
         LLM
Figure 2: Overview of the proposed SlowFastVAD method. It consists of two branches: a fast DNN-based detector and a slowVLM-based detector. To seamlessly integrate the two detection branches and leverage their respective strengths, we designedthree key components, i.e., intervention detection strategy, RAG-driven anomaly reasoning module, and integration mechanism,enabling an e!cient and interpretable VAD framework.scenarios, we introduce an anomaly-oriented RAG module. Thismodule constructs a knowledge base by extracting normal patternsfrom training videos and inferring potential abnormal patterns,thus enhancing scene-speci"c reasoning capabilities. Finally, an in-tegration mechanism combines the outputs from both detectors toyield the "nal prediction. This mechanism mitigates hallucinatione!ects commonly associated with VLMs and enables the system toachieve high detection accuracy, faster inference, and interpretableoutput.3.2 Fast Detector3.2.1 Foundation Model.In the fast detector, we adopt the AED-MAE [48], which utilizes a lightweight masked autoencoder archi-tecture. By incorporating motion gradient based weighting, self-distillation training, and synthetic anomaly augmentation strate-gies, this method achieves fairly e#cient VAD. AED-MAE is char-acterized by its compact model size and extremely fast inferencespeed, reaching up to 1655 FPS (frame per second).3.2.2 Intervention Detection Strategy.In the context of VAD, videoframes that are easy to classify typically exhibit low variance inanomaly con"dence scores, resulting in low uncertainty, i.e., lowentropy. However, since the fast detector is trained solely on normalsamples via reconstruction, it may produce high reconstructionerrors for normal-but-rare samples during inference, leading tonoisy or $uctuating anomaly con"dence scores. Besides, in complexscenes where the test data deviates from distributions of the trainingset, the fast detector may fail to generalize e!ectively, again causinginstability in anomaly con"dence scores. These $uctuations arere$ected as increased entropy in the anomaly con"dence scores.To address this, we propose a novel entropy-based interventiondetection strategy to identify and select ambiguous segments thatare di#cult to accurately classify. Speci"cally, given a testing video,we take its frame-level anomaly con"dence scores𝐿𝐿𝑀 𝑁 𝑂from thefast detector as input and partition it into a set of non-overlappingsubsequences𝐿={𝐿𝑃}𝑃=1using a window size𝑀. For each subse-quence𝐿𝑃, we compute its entropy. To account for temporal context,we apply a Gaussian "lter for smoothing, integrating the entropyvalues of neighboring subsequences to obtain a context-aware en-tropy score. Given that the anomaly con"dence scores are decimalsranging from 0 to 1, we adopt the di!erential entropy formula forcalculation. The detailed calculation procedure is shown as follows.We "rst estimate the probability density function of the obtainedsubsequence𝐿𝑃={𝑁𝑂𝑃𝑄𝑅𝑄}𝑅𝑄=1, where𝑁𝑂𝑃𝑄𝑅𝑄indicates the anomalyscore of the𝑆-th video frame. Here, we employ the frequency distri-bution histogram to serve as an approximation of the probabilitydensity function for the subsequence𝐿𝑃. The following are the de-tailed steps: Firstly, determine the number of histogram bins as𝑇𝑆.Subsequently, calculate the di!erence between the maximum andminimum values within𝐿𝑃. Divide the obtained di!erence by thenumber of groups𝑇𝑆to derive the class interval, based on which thegrouping intervals can be further ascertained. On this foundation,count the number of elements of𝐿𝑃within each grouping interval,and then compute the corresponding frequencies to obtain the fre-quency distribution histogram𝑈𝑉𝑊↑R𝑇𝐿. For each value𝑁𝑂𝑃𝑄𝑅𝑄in𝐿𝑃, "rst identify the group to which it belongs in the frequencydistribution histogram𝑈𝑉𝑊, and take the frequency of that group asthe probability of its occurrence. In this way, the "nal probabilitydensity functionˆ𝑈(𝑁𝑂𝑃𝑄𝑅𝑄)of𝐿𝑃is obtained. Based on the obtainedprobability density functionˆ𝑈(𝑁𝑂𝑃𝑄𝑅𝑄)of the subsequence𝐿𝑃,w e

SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model Conference’17, July 2017, Washington, DC, USAcompute the di!erential entropy𝑋𝑃of𝐿𝑃as follows:𝑋𝑃=↓𝑅/√︄ummationdi√︄√︁lay.𝑄=1ˆ𝑈(𝑁𝑂𝑃𝑄𝑅𝑄)log2(ˆ𝑈(𝑁𝑂𝑃𝑄𝑅𝑄))(1)We further apply a Gaussian "lter𝑌(·)to𝑋𝑃, integrating theinformation from neighboring subsequences𝑋𝑃+𝑈, so as to obtainthe "nal entropy valueˆ𝑋𝑃of𝐿𝑃, which is shown below:𝑌(𝑍)=1↔2𝑎𝑏2exp/√︁a√︂enleftbigg↓𝑍22𝑏2/√︁a√︂en√︂ightbiggˆ𝑋𝑃=𝑉/√︄ummationdi√︄√︁lay.𝑈=↓𝑉𝑋𝑃+𝑈·𝑌(𝑐),𝑑=[3𝑏](2)We set a threshold𝑒to determine which subsequences are con-sidered uncertain. If the entropy value of a certain subsequence ex-ceeds𝑒, then the corresponding video segment𝑓𝑔={𝑊 𝑄𝑕𝑖𝑅𝑄}𝑅𝑄=1will be fed into the slow detector for further analysis.Moreover, to improve the interpretability of overall detectionresults, we also introduce a periodic sampling mechanism. Speci"-cally, one video segment is sampled from every𝑗video segmentsand sent to the slow detector for semantic description and anomalyscoring. These results serve as global context cues that complementthe "nal decision-making process with interpretable outputs.3.3 Slow Detector3.3.1 Basic Procedure.The input to the slow detector is the am-biguous video segment𝑓𝑔identi"ed by the intervention detectionstrategy. In the VAD task, spatiotemporal information is of crucialimportance [63,85]. Temporal information can capture the sequen-tial evolution process of events and their durations, which helpsto distinguish between normal and abnormal behaviors, becauseanomalies often manifest as sudden interruptions in the temporaldimension. Spatial information is divided into two parts: the fore-ground and the background. Foreground information focuses on thepositions and motion patterns of foreground objects. Anomalies usu-ally manifest as unusual spatial arrangements or sudden changes inpositions. Background information focuses on the relatively stablescene characteristics. By understanding the background informa-tion, VLM and LLM can better extract and summarize the normaland abnormal patterns in the current scene. Based on this,𝑓𝑔isconcatenated with the CoT prompt (refer to Appendix for details)and then fed into the VLM (denoted as𝑔𝑊𝑋𝑌) to extract its spa-tiotemporal representation𝐿𝑗𝑂𝑍𝑁𝑂. Subsequently, the spatiotemporalrepresentation𝐿𝑗𝑂𝑍𝑁𝑂is encoded into a vector𝑘by the embeddingmodeltext-embedding-v21(denoted as𝑔𝑍𝑎𝑏). Detailed processescan be presented as follows:𝐿𝑗𝑂𝑍𝑁𝑂=𝑔𝑊𝑋𝑌(𝑓𝑔)𝑘=𝑔𝑍𝑎𝑏(𝐿𝑗𝑂𝑍𝑁𝑂)(3)Based on the similarity between𝑘and constructed patterns, thetop𝑙relevant patterns𝑚and their associated binary anomalypredictions𝑛(i.e., normal and abnormal) are retrieved from theconstructed knowledge baseD, which is introduced in the fol-lowing section. Combine𝑚and𝑛to obtain the knowledge𝑜=1https://help.aliyun.com/zh/model-studio/user-guide/embedding{(𝑚1,𝑛1),···,(𝑚𝑐,𝑛𝑐)}related to the current video.𝑜=𝑗𝑃𝑈𝑙({(𝑚𝑄,𝑛𝑄,𝑁𝑆 𝑖(𝑘, 𝑚𝑄)))|(𝑚𝑄,𝑛𝑄)↑D } )(4)where𝑁𝑆𝑖(·)denotes the similarity computation.Finally, the extracted spatiotemporal representation𝐿𝑗𝑂𝑍𝑁𝑂andthe retrieved knowledge𝑜are concatenated and combined witha CoT reasoning prompt to form a structured prompt𝑚𝑑𝑒𝑓=[𝐿𝑗𝑂𝑍𝑁𝑂;𝑜]. This prompt is fed into LLM for step-by-step reason-ing, producing anomaly scores𝐿𝑁𝑔𝑕𝑖along with correspondinginterpretive descriptions𝑝.(𝐿𝑁𝑔𝑕𝑖,𝑝)=𝑔𝑊𝑋𝑌(𝑚𝑑𝑒𝑓)(5)3.3.2 RAG-driven Anomaly Reasoning.This module is designed toextract normal patterns from training videos, enabling VLM trainedon general scenarios to better adapt to the speci"c VAD task. Toachieve this, we apply a sparse temporal sampling strategy [59],where a segment containing𝑀consecutive frames is randomly se-lected from "xed-length segments of training videos. Throughoutthis process, we extensively incorporate the CoT prompt to guidethe reasoning of models in a more interpretable and coherent man-ner. The overall procedure consists of four stages: visual descriptiongeneration, pattern extraction and prediction, pattern re"nementand aggregation, and knowledge base construction.Visual Description Generation: Here, we follow the same pro-cedure described in Section 3.3.1 to extract the spatiotemporalrepresentation𝐿𝑗𝑗𝑍𝐿for the video segment.Pattern Extraction and Prediction:Based on the extracted spa-tiotemporal representations𝐿𝑗𝑗𝑍𝐿, we further employ the CoTprompt to guide the LLM in re"ning representative normal patternsN(e.g., “a person walking slowly on the road” or “a small groupengaged in conversation”). Building upon these patterns, the modelis further prompted to reason about spatial regularity, behavioralpattern, and interaction dynamic, thereby enabling the predictionof potential abnormal patternsA. This step not only encodes priorknowledge of normalcy but also enhances semantic interpretabilityof potential anomalies. The detailed processes are presented asfollows:N=𝑞𝑃𝑗-𝑟𝑟𝑠(𝐿𝑗𝑗𝑍𝐿)A=𝑞𝑃𝑗-𝑟𝑟𝑠(N)(6)where𝑞𝑃𝑗-𝑟𝑟𝑠denotes the reasoning of LLMs with the assist ofCoT prompt.Pattern Re"nement and Aggregation:After obtaining the ini-tially extracted normal and abnormal patterns, we design a voting-based strategy for pattern re"nement and aggregation. Consideringthat normal patterns within the same video scene often exhibit highconsistency, while abnormal patterns tend to be more diverse, weaggregate highly similar patterns to re"ne stable behavioral repre-sentations. Meanwhile, dissimilar patterns are retained to preservebehavioral diversity. This process results in a pattern set that is bothrepresentative and diverse, laying a solid foundation for subsequentknowledge base construction. Speci"cally, we process the patternssummarized from the videos within each scene separately. Here,we take the normal patterns as an example for illustration, andthe abnormal patterns are processed in the same way. For the𝑆-thscene, the𝑡-th normal pattern𝑇𝑄𝑘is "rst compared for similar-ity with the existing patterns𝑇𝑄𝑔in the knowledge base. If the

Conference’17, July 2017, Washington, DC, USAZongcan Ding et al.average similarity between it and the existing patterns is belowthe threshold𝑢, it indicates that this pattern is dissimilar to theexisting patterns in the knowledge base, and it will then be directlyadded to the knowledge base. Conversely, if the sum of similaritiesis not less than𝑢, we identify the "rst𝑐normal patterns𝑇𝑄𝑈inthe knowledge base that are similar to it. These similar patternsare then aggregated and cleaned, and the aggregated and cleanedpatterns are added to the knowledge base. Through continuous loopprocessing, after traversing all normal patterns of the𝑆-th scene,we "nally obtain the setN↗𝑄of all processed normal patterns forthe𝑆-th scene. After obtaining the setN↗𝑄of normal patterns andthe setA↗𝑄of abnormal patterns for the𝑆-th scene, we combine thetwo to obtain the setP𝑄of all patterns for this scene. The formulais expressed as follows:N↗𝑄=/uniondi√︄√︁lay.𝑘=1/b√︂aceleftigg{𝑇𝑄𝑘},if1|{𝑇𝑀𝑁}𝑁ω𝑂|/√︄ummationtext.𝑈ω𝑘𝑁𝑆𝑖(𝑇𝑄𝑘,𝑇𝑄𝑔)<𝑢/uniontext.𝑈:𝑁𝑄𝑎(𝑇𝑀𝑂,𝑇𝑀𝑃)↘𝑙{𝑇𝑄𝑈},otherwise(7)A↗𝑄=/uniondi√︄√︁lay.𝑎=1/b√︂aceleftigg{𝑣𝑄𝑎},if1|{𝑒𝑀𝑄}𝑄ω𝑅|/√︄ummationtext.𝑅ω𝑎𝑁𝑆𝑖(𝑣𝑄𝑎,𝑣𝑄𝑅)<𝑢/uniontext.𝑅:𝑁𝑄𝑎(𝑒𝑀𝑅,𝑒𝑀𝑄)↘𝑙{𝑣𝑄𝑅},otherwise(8)P𝑄=N↗𝑄≃A↗𝑄(9)Knowledge Base Construction:The cleaned normal and abnor-mal patternsP𝑄, along with their corresponding anomaly predic-tions𝑛𝑄, are structured into standardized data formats are thenencoded into vector representations using thetext-embedding-v21model, thereby constructing the knowledge base tailored for theVAD task. Mathematically, the knowledge baseDcan be expressedas follows:D=/uniondi√︄√︁lay.𝑄=1{𝑔𝑍𝑎𝑏(P𝑄,𝑛𝑄)}(10)3.4 Slow-Fast Integration and InferenceTo derive the "nal anomaly con"dence score, we integrate𝐿𝐿𝑀 𝑁 𝑂from the fast detector and𝐿𝑁𝑔𝑕𝑖from the slow detector via a inte-gration mechanism. First, we use the weighted-averaging methodto obtain the initial fused𝐿𝐿𝑚 𝑁𝑄 𝑕𝑅, which is shown as follows:𝐿𝐿𝑚 𝑁𝑄 𝑕𝑅=𝑤𝐿𝑁𝑔𝑕𝑖+(1↓𝑤)𝐿𝐿𝑀 𝑁 𝑂(11)where the weighting factor𝑤serves to balance the performance offast and slow detectors. Subsequently, a Gaussian "lter is appliedfor smoothing. Moreover, the anomaly reasoning𝑝is generated bythe slow detector, endowing the detection result with high inter-pretability.4 Experiments4.1 Datasets and Evaluation Metrics4.1.1 Datasets.We evaluate the proposed method on four publicdatasets: UCSD Ped2 [38], Avenue [32], ShanghaiTech [42], andUBnormal [61].UCSD Ped2is a single-scene dataset captured on apedestrian walkway that contains anomalies such as cyclists, skate-boarders, and cars.Avenueis also a single-scene dataset, recordedon the main avenue of the CUHK campus, with anomalies includingrunning and bicycling.ShanghaiTechis a more challenging multi-scene dataset from 13 di!erent campus environments, characterizedby variations in lighting conditions and camera perspectives. As thelargest dataset for semi-supervised VAD, it comprises 270000 framesfor training and approximately 50000 for testing.UBnormalis anopen-set dataset comprising 29 synthetic scenes, where the sets ofanomaly types in the training and testing splits are disjoint. Foreach dataset, we adopt the default training and testing splits un-der the semi-supervised setting, using only normal samples duringtraining. The normal reference frames used by SlowFastVAD arerandomly and uniformly sampled from normal training videos.4.1.2 Evaluation Metrics.We follow recent related works [48,71]and report the frame-level Area Under the Curve (AUC) of theReceiver Operating Characteristic (ROC). Speci"cally, we computeboth the Micro AUC and Macro AUC. For Micro AUC, all test framesfrom every video are merged into a single sequence, and AUC iscalculated across the entire set. In contrast, Macro AUC is computedby "rst calculating the AUC for each individual test video, followedby averaging these scores to obtain the "nal result.4.2 Implementation DetailsOur method is implemented using the PyTorch framework. Unlessotherwise speci"ed, Qwen-VL-Max [3] is used as VLM for visualperception, while Qwen-Max [56] serves as LLM for spatiotemporalinformation aggregation and retrieval-augmented generation. ForQwen-VL-Max, the sampling temperature is set to 0.01, while forQwen-Max, the sampling temperature is set to 1.1 during modetraining and 0.7 during model testing. The default hyperparametersettings for SlowFastVAD are as follows: the window size𝑀of videosegment is set to 8, and the random/uniform sampling interval𝑗isset to 20 during training and reduced to 10 during testing to ensure"ner temporal resolution for inference.𝑙in Eq (4) is set to 6, andthe weighting factor𝑤in Eq (11) is empirically set to 0.8, 0.5, and0.7 for Ped2, Avenue, and ShanghaiTech, respectively, to balancethe fast and slow detectors. For the fast detector, we follow thecon"guration used in AED-MAE [48].4.3 Comparison with State-of-the-art MethodsIn this section, we compare the proposed SlowFastVAD with dozensof baseline VAD methods across four datasets to evaluate its de-tection performance. Notably, for a fair comparison, we restrictour evaluation to frame- or cube-centric methods, as object-centricmethods completely remove background information and irrele-vant content. As shown in Tables 1 and 2, SlowFastVAD achievesoverall state-of-the-art results, particularly excelling on UCSD Ped2and UBnormal datasets, with Micro AUC scores of 99.1% and 72.2%,respectively. These results demonstrate the strong generalizationability and detection accuracy across diverse scenarios. The keyadvantage of SlowFastVAD lies in its dual-branch (slow and fast)architecture, which fully leverages the ability of VLMs to re"ne andamend the initial predictions from the fast detector. This designachieves a balanced trade-o! between inference e#ciency and de-tection accuracy. Compared to traditional VAD approaches basedon visual features and reconstruction costs, SlowFastVAD bene"tsfrom the semantic understanding and external prior knowledgeprovided by VLMs, enabling more robust anomaly detection. For in-stance, compared to previous best counterpart AED-MAE [48], ourSlowFastVAD yields considerable gains across di!erent evaluation

SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model Conference’17, July 2017, Washington, DC, USATable 1: AUC scores of several state-of-the-art methods ver-sus SlowFastVAD on Ped2 and Avenue datasets. The top threemethods are shown in red, green, and blue.MethodReferenceYearPed2AvenueAUC (%)MicroMacroMicroMacroLSHF [84]PR201691.0---AnomalyGAN [47]ICIP201793.5---FuturePred [27]CVPR201895.4-85.181.7MC2ST [28]BMVC201887.5-84.4-DeepMIL [50]CVPR2018----PnP-CMA [46]WACV201888.4---MemAE [16]ICCV201994.1-83.3-NNC [20]WACV2019--88.9-BMAN [24]TIP201996.6-90.0-AMCVAD [41]ICCV201996.2-86.9-DeepOC [66]TNNLS201996.9-86.6-StreetScene [45]WACV202088.3-72.0-MNAD [44]CVPR202097.0-82.886.8SCRD [51]ACMMM2020--89.6-CAC [62]ACMMM2020--87.0-VEC-AM [75]ACMMM202097.3-90.2-AEP [76]TNNLS202197.3-90.2-LNRA [2]BMVC202196.5-87.1-TimeSformer [5]ICML2021----SSPCAB [49]+[27]CVPR2022--87.384.5SSPCAB [49]+[44]CVPR2022--84.888.6GCL [78]CVPR2022----FastAno [43]WACV202296.3-85.3-S3R [64]ECCV2022----HSNBM [4]ACMMM202295.2-91.6-ERVAD [52]ACMMM202297.1-92.7-DM-UVAD [58]ICIP2023----FPDM [70]ICCV2023--90.1-SSMCTB [37]+[27]TPAMI2023--89.184.8SSMCTB [37]+[44]TPAMI2023--86.486.3AnomalyRuler [71]ECCV202497.9-89.7-AED-MAE [48]CVPR202495.498.491.390.9SSAE [8]TPAMI2024--90.2-SlowFastVAD——99.199.789.693.2metrics. Furthermore, in contrast to VLM-only methods Anoma-lyRuler [71], SlowFastVAD not only achieves signi"cantly fasterinference, but also delivers improved detection performance.4.4 Ablation Studies4.4.1 Impact of Each Component.In this section, we conduct anablation study on di!erent con"gurations of SlowFastVAD to eval-uate the contribution of each component to overall VAD perfor-mance. The following con"gurations are considered: (1)Baseline:No additional components are used; the slow detector re-evaluatesanomalies based solely on the fast detector’s results under uni-form sampling; (2) + Intervention: Only the intervention strategyis added; (3) +Intervention+ Integration: Both the intervention andintegration components are used; (4) Full Model: All components,including the RAG module, are applied. The performance compar-ison is presented in Table 3. We observe that the baseline settingwith uniform sampling yields relatively conservative performance,Table 2: AUC scores of several state-of-the-art methods ver-sus SlowFastVAD on ShanghaiTech and UBnormal datasets.The top three methods are shown in red, green, and blue.MethodReferenceYearShanghaiTechUBnormalAUC (%)MicroMacroMicroMacroFuturePred [27]CVPR201872.880.6--MC2ST [28]BMVC2018----DeepMIL [50]CVPR2018-76.550.376.8MemAE [16]ICCV201971.2---MNAD [44]CVPR202068.379.7--SCRD [51]ACMMM202074.7---CAC [62]ACMMM202079.3---VEC-AM [75]ACMMM202074.8---LNRA [2]BMVC202175.9---TimeSformer [5]ICML2021--68.580.3SSPCAB [49]+[27]CVPR202274.582.9--SSPCAB [49]+[44]CVPR202269.880.2--GCL [78]CVPR202278.9---FastAno [43]WACV202272.2---S3R [64]ECCV202280.4---HSNBM [4]ACMMM202276.5---ERVAD [52]ACMMM202279.3---DM-UVAD [58]ICIP202376.1---FPDM [70]ICCV202378.6-62.7-SSMCTB [37]+[27]TPAMI202374.683.3--SSMCTB [37]+[44]TPAMI202370.680.3--AnomalyRuler [71]ECCV202485.2-71.9-AED-MAE [48]CVPR202479.184.758.581.4SSAE [8]TPAMI202480.5---SlowFastVAD——85.090.772.282.4Table 3: Impact of each novel components on Ped2, Avenue,and ShanghaiTech datasets.InterventionIntegrationRAGPed2AvenueShanghaiTechAUC (%)MicroMacroMicroMacroMicroMacro×××87.889.680.186.176.382.3⊋××90.691.185.889.080.683.6⊋⊋×94.397.286.188.583.988.4⊋⊋⊋99.199.789.693.285.090.7indicating its limited ability to capture the key temporal segmentsof anomalous events. Introducing the intervention strategy leadsto consistent improvements across all four datasets, especially onAvenue and ShanghaiTech, con"rming its e!ectiveness in guidingthe model to focus on informative abnormal regions. Adding theintegration mechanism further boosts performance, notably onPed2 and ShanghaiTech, suggesting that it e!ectively combines theoutputs of fast and slow detectors while better modeling tempo-ral dependencies. Finally, incorporating the RAG module into thefull model results in the best overall performance, with substantialgains on Ped2 and Avenue. This highlights the value of enhancedprompts generated by RAG in assisting the slow detector with moreaccurate anomaly reasoning. In summary, each component con-tributes to performance improvements to varying degrees. The "nalcon"guration consistently outperforms others across all datasets,particularly excelling on Ped2 and ShanghaiTech.

Conference’17, July 2017, Washington, DC, USAZongcan Ding et al.
                            Ped2                                                         ShanghaiTechIntervention Detection:The crowd is mainly distributed along the paved pedestrian walkway, with most people walking naturally. The distribution of people is relatively even, without signs of gathering or congestion. However, on the left side, one individual is using a scooter, which deviates from the typical walking behavior of pedestrians. According to pattern [5] in the knowledge base—“the appearance of non-pedestrian objects (e.g., vehicles or transportation tools entering a pedestrian walkway)”—this scene is considered abnormal due to the presence of the scooter. The anomaly score is 1.0.Intervention Detection:According to pattern [3] in the knowledge base, the presence of non-pedestrian objects (e.g., vehicles, animals, etc.) near the entrance of a building is considered abnormal. In this video segment, a person riding a bicycle is located near the building entrance, which matches the abnormal pattern described in knowledge base entry [3]. Therefore, this video clip is classified as anomalous, with an anomaly score of 1.0. Although the background context and the behaviors of other individuals appear normal, a single prominent anomalous behavior is sufficient to label the entire scene as abnormal.Intervention Detection:According to the patterns in the knowledge base, individuals engaging in rapid running, jumping, or other intense movements [1], as well as pedestrians suddenly accelerating into a run [2], are all considered abnormal behaviors. In this video, two people are seen running along the street, which aligns with the definitions of “fast running” and “sudden acceleration by pedestrians” outlined in the aforementioned rules. Therefore, this video segment is classified as anomalous, with an anomaly score of 1.0.Intervention Detection:According to the patterns defined in the knowledge base, actions deviating from normal walking or standing behavior—such as a pedestrian suddenly accelerating into a run without an emergency, or exhibiting abrupt movements in areas where rapid motion is uncommon—are considered anomalous [1]. In this video, a man on the right side suddenly throws an object, with the motion being clearly abrupt and the object’s trajectory visibly airborne, matching the anomaly pattern described in the knowledge base. As a result, the final anomaly score is 1.0.                           Avenue                                                          UBnormalFigure 3: Visualization of partial detection results on Ped2, Avenue, ShanghaiTech, and UBnormal. Three detection resultsare shown: the top displays anomaly scores generated solely by the fast detector; the middle shows the updated scores afterintervention by the slow detector; the bottom presents the "nal results obtained through the integration of both detectors.Table 4: Impact of fast detector, slow detector and the hybridSlowFastVAD on Ped2, Avenue, and ShanghaiTech datasets.BranchPed2AvenueShanghaiTechFPSAUC (%)MicroMacroMicroMacroMicroMacroFast Detector95.498.491.390.979.184.71655Slow Detector98.499.074.578.087.785.60.5SlowFastVAD99.199.789.693.285.090.716Note: The FPS results is obtained on a single RTX 3090 GPU. Due to limitedGPU resources, the locally deployed model is Qwen2-VL-7B. If multipleGPUs are used for parallel processing, the speed can be further improved.4.4.2 Impact of Di!erent Detectors.We further evaluated the per-formance of the fast detector, slow detector, and their hybrid ap-proach across di!erent datasets, with the results summarized inTable 4. The fast detector alone demonstrates competitive perfor-mance and delivers high inference e#ciency (i.e., 1655 FPS) onall three datasets. In contrast, the slow detector exhibits relativelylower performance and considerably slow inference speed (i.e., 0.5FPS), which can be attributed to the hallucination e!ects commonlyobserved in LLMs when operating independently, thereby compro-mising their ability to accurately identify anomalous events. By in-tegrating both detectors, the hybrid approach achieves the superioroverall performance across all datasets. Although a slight decreasein Micro AUC is observed on Avenue dataset, the dual-branch com-bination e!ectively suppresses hallucination e!ects, signi"cantlyreducing false positives and false negatives while leveraging thestrengths of the fast detector. Moreover, the hybrid approach main-tains a favorable balance between detection accuracy and real-timeinference (16 FPS), making it a practical and robust solution for VADin diverse scenarios. Moreover, this also substantiates the e!ective-ness of our biologically inspired design, which emulates the humanvisual system’s dual complementary pathways, namely, mimick-ing the coordination between rapid action-oriented responses andslower cognition-driven reasoning.4.5 Qualitative AnalysesFigure 3 visualizes the detection results of our SlowFastVAD and itsvariants on di!erent datasets. The abnormal parts are highlightedwith green bounding boxes in video frames. In the detection result,the red sections represent video segments labeled as abnormalin ground truth, while the blue sections represent the detectionresults after the intervention of slow detector. It is evident thatusing only the fast detector can achieve relatively good detectionperformance; However, it still su!ers from noticeable false positivesand false negatives, especially as observed in samples from Ped2and Avenue. By incorporating the slow detector based on VLMthrough the intervention stragety to analyze suspicious regions, thelocal detection performance is signi"cantly improved. Nevertheless,the localized enhancements have limited in$uence on the overallprediction. Therefore, the "nal integration of the fast and slow

SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model Conference’17, July 2017, Washington, DC, USAdetectors via a Gaussian "lter leads to a more globally consistentimprovement, further enhancing overall detection performance.In addition, we present several representative reasoning resultsfrom the slow detector. Due to space limitations, we randomlyselect a subset of intervention segments for illustration. Comparedto the fast detector, which relies on simple data "tting to produceanomaly scores, the VLM-based slow detector leverages both pre-trained knowledge and domain-speci"c information introducedvia the RAG module to enable brain-inspired deep reasoning overevents, thereby producing more interpretable and accurate anomalyassessments.5 ConclusionIn this work, we introduce SlowFastVAD, a novel hybrid frameworkthat integrates a fast anomaly detector with a retrieval augmentedgeneration enhanced vision-language model to achieve both e#-ciency and interpretability in video anomaly detection. The fastdetector provides initial detection results, while several ambiguoussegments are selectively analyzed by the slower yet more explain-able VLM, reducing unnecessary computational overhead. By lever-aging this dual-branch detection pipeline, our method e!ectivelybalances computational cost and detection accuracy. Speci"cally,the proposed entropy-based intervention strategy ensures that onlyuncertain segments are processed by the VLM, while the construc-tion of a domain-adapted knowledge base further enhances theVLM’s adaptability to speci"c VAD scenarios. Extensive experi-ments conducted on four datasets demonstrate that SlowFastVADoutperforms existing methods, achieving state-of-the-art detectionperformance while maintaining interpretability. In the future, wewill further explore task-speci"c foundation models centered onVAD and continue to enhance reasoning e#ciency.References[1]Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, and R VenkateshBabu. 2024. Leveraging vision-language models for improving domain gener-alization in image classi"cation. InProceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 23922–23932.[2]Marcella Astrid, Muhammad Zaigham Zaheer, Jae-Yeong Lee, and Seung-Ik Lee.2021. Learning not to reconstruct anomalies.arXiv preprint arXiv:2110.09742(2021).[3]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, JunyangLin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-LanguageModel for Understanding, Localization, Text Reading, and Beyond.arXiv preprintarXiv:2308.12966(2023).[4]Qianyue Bao, Fang Liu, Yang Liu, Licheng Jiao, Xu Liu, and Lingling Li. 2022. Hier-archical scene normality-binding modeling for anomaly detection in surveillancevideos. InProceedings of the 30th ACM international conference on multimedia.6103–6112.[5]Gedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021. Is space-time attentionall you need for video understanding?. InICML, Vol. 2. 4.[6]Davide Ca!agni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia,Lorenzo Baraldi, and Rita Cucchiara. 2024. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. InProceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition. 1818–1826.[7]Ruichu Cai, Hao Zhang, Wen Liu, Shenghua Gao, and Zhifeng Hao. 2021.Appearance-motion memory consistency network for video anomaly detection.InProceedings of the AAAI conference on arti!cial intelligence, Vol. 35. 938–946.[8]Congqi Cao, Hanwen Zhang, Yue Lu, Peng Wang, and Yanning Zhang. 2024.Scene-dependent prediction in latent space for video anomaly detection andanticipation.IEEE Transactions on Pattern Analysis and Machine Intelligence(2024).[9]Junxi Chen, Liang Li, Li Su, Zheng-Jun Zha, and Qingming Huang. 2024. Prompt-enhanced multiple instance learning for weakly supervised video anomaly detec-tion. InProceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 18319–18329.[10]Kai Cheng, Yaning Pan, Yang Liu, Xinhua Zeng, and Rui Feng. 2024. Denois-ing di!usion-augmented hybrid video anomaly detection via reconstructingnoised frames. InProceedings of the Thirty-Third International Joint Conferenceon Arti!cial Intelligence. 695–703.[11]MyeongAh Cho, Minjung Kim, Sangwon Hwang, Chaewon Park, Kyungjae Lee,and Sangyoun Lee. 2023. Look around for anomalies: Weakly-supervised anomalydetection via context-motion relational learning. InProceedings of the IEEE/CVFconference on computer vision and pattern recognition. 12137–12146.[12]Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow-fast networks for video recognition. InProceedings of the IEEE/CVF internationalconference on computer vision. 6202–6211.[13]Xinyang Feng, Dongjin Song, Yuncong Chen, Zhengzhang Chen, Jingchao Ni,and Haifeng Chen. 2021. Convolutional transformer based dual discriminatorgenerative adversarial networks for video anomaly detection. InProceedings ofthe 29th ACM International Conference on Multimedia. 5546–5554.[14]Joseph Fioresi, Ishan Rajendrakumar Dave, and Mubarak Shah. 2023. Ted-spad:Temporal distinctiveness for self-supervised privacy-preservation for video anom-aly detection. InProceedings of the IEEE/CVF international conference on computervision. 13598–13609.[15]Alessandro Flaborea, Luca Collorone, Guido Maria D’Amely Di Melendugno,Stefano D’Arrigo, Bardh Prenkaj, and Fabio Galasso. 2023. Multimodal motionconditioned di!usion model for skeleton-based video anomaly detection. InProceedings of the IEEE/CVF international conference on computer vision. 10318–10329.[16]Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour,Svetha Venkatesh, and Anton van den Hengel. 2019. Memorizing normalityto detect anomaly: Memory-augmented deep autoencoder for unsupervisedanomaly detection. InProceedings of the IEEE/CVF international conference oncomputer vision. 1705–1714.[17]Chao Huang, Chengliang Liu, Jie Wen, Lian Wu, Yong Xu, Qiuping Jiang, andYaowei Wang. 2022. Weakly supervised video anomaly detection via self-guidedtemporal discriminative transformer.IEEE Transactions on Cybernetics54, 5(2022), 3197–3210.[18]Chao Huang, Jie Wen, Chengliang Liu, and Yabo Liu. 2024. Long short-termdynamic prototype alignment learning for video anomaly detection. InProceed-ings of the Thirty-Third International Joint Conference on Arti!cial Intelligence.866–874.[19]Chao Huang, Jie Wen, Yong Xu, Qiuping Jiang, Jian Yang, Yaowei Wang, andDavid Zhang. 2022. Self-supervised attentive generative adversarial networksfor video anomaly detection.IEEE transactions on neural networks and learningsystems34, 11 (2022), 9389–9403.[20]Radu Tudor Ionescu, Sorina Smeureanu, Marius Popescu, and Bogdan Alexe.2019. Detecting abnormal events in video using narrowed normality clusters.In2019 IEEE winter conference on applications of computer vision (WACV). IEEE,1951–1960.[21]Hyekang Kevin Joo, Khoa Vo, Kashu Yamazaki, and Ngan Le. 2023. Clip-tsa: Clip-assisted temporal self-attention for weakly-supervised video anomaly detection.In2023 IEEE International Conference on Image Processing (ICIP). IEEE, 3230–3234.[22]Yannis Kalantidis, Giorgos Tolias, et al.2024. Label propagation for zero-shot clas-si"cation with vision-language models. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 23209–23218.[23]Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023. Grounding languagemodels to images for multimodal inputs and outputs. InInternational Conferenceon Machine Learning. PMLR, 17283–17300.[24]Sangmin Lee, Hak Gu Kim, and Yong Man Ro. 2019. BMAN: Bidirectional multi-scale aggregation networks for abnormal event detection.IEEE Transactions onImage Processing29 (2019), 2395–2408.[25]Changkang Li and Yalong Jiang. 2024. VLAVAD: Vision-Language Models As-sisted Unsupervised Video Anomaly Detection. (2024).[26]Guoqiu Li, Guanxiong Cai, Xingyu Zeng, and Rui Zhao. 2022. Scale-aware spatio-temporal relation learning for video anomaly detection. InEuropean Conferenceon Computer Vision. Springer, 333–350.[27]Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. 2018. Future frameprediction for anomaly detection–a new baseline. InProceedings of the IEEEconference on computer vision and pattern recognition. 6536–6545.[28]Yusha Liu, Chun-Liang Li, and Barnabás Póczos. 2018. Classi"er two sample testfor video anomaly detections.. InBMVC. 71.[29]Yang Liu, Jing Liu, Mengyang Zhao, Shuang Li, and Liang Song. 2022. Collabo-rative Normality Learning Framework for Weakly Supervised Video AnomalyDetection.IEEE Transactions on Circuits and Systems II: Express Briefs69, 5 (2022),2508–2512. doi:10.1109/TCSII.2022.3161061[30]Yang Liu, Zhaoyang Xia, Mengyang Zhao, Donglai Wei, Yuzheng Wang, SiaoLiu, Bobo Ju, Gaoyun Fang, Jing Liu, and Liang Song. 2023. Learning causality-inspired representation consistency for video anomaly detection. InProceedingsof the 31st ACM international conference on multimedia. 203–212.[31]Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. 2024. VideoStudio: GeneratingConsistent-Content and Multi-Scene Videos. InEuropean Conference on ComputerVision. Springer, 468–485.

Conference’17, July 2017, Washington, DC, USAZongcan Ding et al.[32]Cewu Lu, Jianping Shi, and Jiaya Jia. 2013. Abnormal Event Detection at 150 FPSin MATLAB. In2013 IEEE International Conference on Computer Vision. 2720–2727.doi:10.1109/ICCV.2013.338[33]Jiayun Luo, Siddhesh Khandelwal, Leonid Sigal, and Boyang Li. 2024. Emergentopen-vocabulary semantic segmentation from o!-the-shelf vision-language mod-els. InProceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 4029–4040.[34]Weixin Luo, Wen Liu, Dongze Lian, and Shenghua Gao. 2021. Future frameprediction network for video anomaly detection.IEEE transactions on patternanalysis and machine intelligence44, 11 (2021), 7505–7520.[35]Hui Lv and Qianru Sun. 2024. Video anomaly detection and explanation via largelanguage models.arXiv preprint arXiv:2401.05702(2024).[36]Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, and Hanwang Zhang.2023. Unbiased multiple instance learning for weakly supervised video anomalydetection. InProceedings of the IEEE/CVF conference on computer vision and patternrecognition. 8022–8031.[37]Neelu Madan, Nicolae-C%t%lin Ristea, Radu Tudor Ionescu, Kamal Nasrollahi,Fahad Shahbaz Khan, Thomas B Moeslund, and Mubarak Shah. 2023. Self-supervised masked convolutional transformer block for anomaly detection.IEEETransactions on Pattern Analysis and Machine Intelligence46, 1 (2023), 525–542.[38]Vijay Mahadevan, Weixin Li, Viral Bhalodia, and Nuno Vasconcelos. 2010. Anom-aly detection in crowded scenes. In2010 IEEE Computer Society Conference on Com-puter Vision and Pattern Recognition. 1975–1981. doi:10.1109/CVPR.2010.5539872[39]Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, and Kwonjoon Lee. 2024. Can’tmake an Omelette without Breaking some Eggs: Plausible Action Anticipationusing Large Video-Language Models. In2024 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR). 18580–18590.[40]Romero Morais, Vuong Le, Truyen Tran, Budhaditya Saha, Moussa Mansour,and Svetha Venkatesh. 2019. Learning Regularity in Skeleton Trajectories forAnomaly Detection in Videos. InProceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR).[41]Trong-Nguyen Nguyen and Jean Meunier. 2019. Anomaly detection in videosequence with appearance-motion correspondence. InProceedings of the IEEE/CVFinternational conference on computer vision. 1273–1283.[42]Carl Olsson, Marcus Carlsson, Fredrik Andersson, and Viktor Larsson. 2017. Non-convex Rank/Sparsity Regularization and Local Minima. In2017 IEEE InternationalConference on Computer Vision (ICCV). 332–340. doi:10.1109/ICCV.2017.44[43]Chaewon Park, MyeongAh Cho, Minhyeok Lee, and Sangyoun Lee. 2022. FastAno:Fast anomaly detection via spatio-temporal patch transformation. InProceedingsof the IEEE/CVF Winter Conference on Applications of Computer Vision. 2249–2259.[44]Hyunjong Park, Jongyoun Noh, and Bumsub Ham. 2020. Learning memory-guided normality for anomaly detection. InProceedings of the IEEE/CVF conferenceon computer vision and pattern recognition. 14372–14381.[45]Bharathkumar Ramachandra and Michael Jones. 2020. Street scene: A newdataset and evaluation protocol for video anomaly detection. InProceedings ofthe IEEE/CVF winter conference on applications of computer vision. 2569–2578.[46]Mahdyar Ravanbakhsh, Moin Nabi, Hossein Mousavi, Enver Sangineto, and NicuSebe. 2018. Plug-and-play cnn for crowd motion analysis: An application inabnormal event detection. In2018 IEEE Winter Conference on Applications ofComputer Vision (WACV). IEEE, 1689–1698.[47]Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, CarloRegazzoni, and Nicu Sebe. 2017. Abnormal event detection in videos using gen-erative adversarial nets. In2017 IEEE international conference on image processing(ICIP). IEEE, 1577–1581.[48]Nicolae-C Ristea, Florinel-Alin Croitoru, Radu Tudor Ionescu, Marius Popescu,Fahad Shahbaz Khan, and Mubarak Shah. 2024. Self-Distilled Masked Auto-Encoders are E#cient Video Anomaly Detectors. InProceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR). 15984–15995.[49]Nicolae-C%t%lin Ristea, Neelu Madan, Radu Tudor Ionescu, Kamal Nasrollahi,Fahad Shahbaz Khan, Thomas B Moeslund, and Mubarak Shah. 2022. Self-supervised predictive convolutional attentive block for anomaly detection. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition.13576–13586.[50]Waqas Sultani, Chen Chen, and Mubarak Shah. 2018. Real-world anomaly de-tection in surveillance videos. InProceedings of the IEEE conference on computervision and pattern recognition. 6479–6488.[51]Che Sun, Yunde Jia, Yao Hu, and Yuwei Wu. 2020. Scene-aware context reasoningfor unsupervised abnormal event detection in videos. InProceedings of the 28thACM international conference on multimedia. 184–192.[52]Che Sun, Yunde Jia, and Yuwei Wu. 2022. Evidential reasoning for video anomalydetection. InProceedings of the 30th ACM International Conference on Multimedia.2106–2114.[53]Shengyang Sun and Xiaojin Gong. 2023. Hierarchical semantic contrast forscene-aware video anomaly detection. InProceedings of the IEEE/CVF conferenceon computer vision and pattern recognition. 22846–22856.[54]Shengyang Sun and Xiaojin Gong. 2023. Long-short temporal co-teaching forweakly supervised video anomaly detection. In2023 IEEE International Conferenceon Multimedia and Expo (ICME). IEEE, 2711–2716.[55]Shengyang Sun, Jiashen Hua, Junyi Feng, Dongxu Wei, Baisheng Lai, and Xiao-jin Gong. 2024. TDSD: Text-driven scene-decoupled weakly supervised videoanomaly detection. InProceedings of the 32nd ACM International Conference onMultimedia. 5055–5064.[56]Qwen Team. 2024. Qwen2.5 technical report.arXiv preprint arXiv:2412.15115(2024).[57]Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, KunZhan, Peng Jia, Xianpeng Lang, and Hang Zhao. 2024. Drivevlm: The conver-gence of autonomous driving and large vision-language models.arXiv preprintarXiv:2402.12289(2024).[58]Anil Osman Tur, Nicola Dall’Asen, Cigdem Beyan, and Elisa Ricci. 2023. Explor-ing di!usion models for unsupervised video anomaly detection. In2023 IEEEinternational conference on image processing (ICIP). IEEE, 2540–2544.[59]Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, andLuc Van Gool. 2016. Temporal segment networks: Towards good practices fordeep action recognition. InEuropean conference on computer vision. Springer,20–36.[60]Yuan Wang, Rui Sun, Naisong Luo, Yuwen Pan, and Tianzhu Zhang. 2024.Image-to-image matching via foundation models: A new perspective for open-vocabulary semantic segmentation. InProceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 3952–3963.[61]Zejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han. 2022. Blind2Unblind:Self-Supervised Image Denoising with Visible Blind Spots. In2022 IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR). 2017–2026. doi:10.1109/CVPR52688.2022.00207[62]Ziming Wang, Yuexian Zou, and Zeming Zhang. 2020. Cluster attention con-trast for video anomaly detection. InProceedings of the 28th ACM internationalconference on multimedia. 2463–2471.[63]Jie Wu, Wei Zhang, Guanbin Li, Wenhao Wu, Xiao Tan, Yingying Li, Errui Ding,and Liang Lin. 2021. Weakly-supervised spatio-temporal anomaly detection insurveillance video.arXiv preprint arXiv:2108.03825(2021).[64]Jhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-Shann Fuh, and Tyng-LuhLiu. 2022. Self-supervised sparse representation for video anomaly detection. InEuropean Conference on Computer Vision. Springer, 729–745.[65]Peng Wu, Jing Liu, Xiangteng He, Yuxin Peng, Peng Wang, and Yanning Zhang.2024. Toward video anomaly retrieval from video anomaly detection: Newbenchmarks and model.IEEE Transactions on Image Processing33 (2024), 2213–2225.[66]Peng Wu, Jing Liu, and Fang Shen. 2019. A deep one-class neural networkfor anomalous event detection in complex scenes.IEEE transactions on neuralnetworks and learning systems31, 7 (2019), 2609–2622.[67]Peng Wu, Chengyu Pan, Yuting Yan, Guansong Pang, Peng Wang, and YanningZhang. 2024. Deep Learning for Video Anomaly Detection: A Review.arXivpreprint arXiv:2409.05383(2024).[68]Peng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, PengWang, and Yanning Zhang. 2024. Weakly supervised video anomaly detectionand localization with spatio-temporal prompts. InProceedings of the 32nd ACMInternational Conference on Multimedia. 9301–9310.[69]Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang,and Yanning Zhang. 2024. Vadclip: Adapting vision-language models for weaklysupervised video anomaly detection. InProceedings of the AAAI Conference onArti!cial Intelligence, Vol. 38. 6074–6082.[70]Cheng Yan, Shiyu Zhang, Yang Liu, Guansong Pang, and Wenjun Wang. 2023.Feature prediction di!usion model for video anomaly detection. InProceedingsof the IEEE/CVF international conference on computer vision. 5527–5537.[71]Yuchen Yang, Kwonjoon Lee, Behzad Dariush, Yinzhi Cao, and Shao-Yuan Lo.2024. Follow the rules: reasoning for video anomaly detection with large languagemodels. InEuropean Conference on Computer Vision. Springer, 304–322.[72]Zhiwei Yang, Jing Liu, and Peng Wu. 2024. Text prompt with normality guidancefor weakly supervised video anomaly detection. InProceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 18899–18908.[73]Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xiaotao Liu. 2023. VideoEvent Restoration Based on Keyframes for Video Anomaly Detection. InPro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR). 14592–14601.[74]Zheyu Yang, Taoyi Wang, Yihan Lin, Yuguo Chen, Hui Zeng, Jing Pei, JiazhengWang, Xue Liu, Yichun Zhou, Jianqiang Zhang, et al.2024. A vision chip withcomplementary pathways for open-world sensing.Nature629, 8014 (2024),1027–1033.[75]Guang Yu, Siqi Wang, Zhiping Cai, En Zhu, Chuanfu Xu, Jianping Yin, and MariusKloft. 2020. Cloze test helps: E!ective video anomaly detection via learning tocomplete video events. InProceedings of the 28th ACM international conferenceon multimedia. 583–591.[76]Jongmin Yu, Younkwan Lee, Kin Choong Yow, Moongu Jeon, and Witold Pedrycz.2021. Abnormal event detection and localization via adversarial event prediction.IEEE transactions on neural networks and learning systems33, 8 (2021), 3572–3586.[77]Muhammad Zaigham Zaheer, Jin-Ha Lee, Marcella Astrid, and Seung-Ik Lee.2020. Old Is Gold: Rede"ning the Adversarially Learned One-Class Classi"er

SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model Conference’17, July 2017, Washington, DC, USATraining Paradigm. InProceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR).[78]M Zaigham Zaheer, Arif Mahmood, M Haris Khan, Mattia Segu, Fisher Yu, andSeung-Ik Lee. 2022. Generative cooperative learning for unsupervised videoanomaly detection. InProceedings of the IEEE/CVF conference on computer visionand pattern recognition. 14744–14754.[79]Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, and ElisaRicci. 2024. Harnessing large language models for training-free video anomalydetection. InProceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 18527–18536.[80]Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun Qing, QingmingHuang, and Ming-Hsuan Yang. 2023. Exploiting Completeness and Uncertaintyof Pseudo Labels for Weakly Supervised Video Anomaly Detection. InProceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).16271–16280.[81]Haosong Zhang, Mei Chee Leong, Liyuan Li, and Weisi Lin. 2024. PeVL: Pose-Enhanced Vision-Language Model for Fine-Grained Human Action Recognition.InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition. 18857–18867.[82]Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, ChangxinGao, Shanjun Zhang, Li Yu, and Nong Sang. 2024. Holmes-vau: Towards long-termvideo anomaly understanding at any granularity.arXiv preprint arXiv:2412.06171(2024).[83]Menghao Zhang, Jingyu Wang, Qi Qi, Pengfei Ren, Haifeng Sun, Zirui Zhuang,Huazheng Wang, Lei Zhang, and Jianxin Liao. 2024. Video Anomaly Detectionvia Progressive Learning of Multiple Proxy Tasks. InProceedings of the 32nd ACMInternational Conference on Multimedia. 4719–4728.[84]Ying Zhang, Huchuan Lu, Lihe Zhang, Xiang Ruan, and Shun Sakai. 2016. Videoanomaly detection based on locality sensitive hashing "lters.Pattern Recognition59 (2016), 302–311.[85]Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu, and Xian-Sheng Hua.2017. Spatio-temporal autoencoder for video anomaly detection. InProceedingsof the 25th ACM international conference on Multimedia. 1933–1941.[86]Hang Zhou, Junqing Yu, and Wei Yang. 2023. Dual memory units with uncertaintyregulation for weakly supervised video anomaly detection. InProceedings of theAAAI Conference on Arti!cial Intelligence, Vol. 37. 3769–3777.