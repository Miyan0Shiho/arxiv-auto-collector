# Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering

**Authors**: Fengxian Chen, Zhilong Tao, Jiaxuan Li, Yunlong Li, Qingguo Zhou

**Published**: 2026-02-05 01:46:09

**PDF URL**: [https://arxiv.org/pdf/2602.05195v1](https://arxiv.org/pdf/2602.05195v1)

## Abstract
Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.

## Full Text


<!-- PDF content starts -->

Traceable Cross-Source RAG for Chinese Tibetan Medicine
Question Answering
Fengxian Chen, Zhilong Tao, Jiaxuan Li, Yunlong Li, Qingguo Zhou∗
School of Information Science & Engineering, Lanzhou University
Lanzhou, China
February 6, 2026
Abstract
Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings
with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine,
encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics
or clinical papers provide more authoritative evidence. We study a practical setting with three KBs
(encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff K=5) covering both single-
KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce
hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval
to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use
an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence
coverage without relying on naive concatenation. All answers are generated by a lightweight generator,
openPangu-Embedded-7B. Experiments show consistent gains in routing quality and cross-KB evidence
coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and
citation correctness.
1 Introduction
Large Language Models (LLMs) are increasingly used for information seeking and question answering.
However, in high stakes domains, fluent responses are not enough. LLMs can hallucinate facts and fabricate
evidence, which harms trust and safety [ 1,2,3]. Retrieval Augmented Generation (RAG) mitigates this issue
by grounding generation on external corpora. RAG has shown clear gains in clinical tasks when retrieval uses
curated resources [ 4,5]. Yet recent analyses and benchmarks show that RAG remains fragile. Errors come
from retrieval failures, noisy context, and unfaithful use of evidence [6, 7, 8, 9, 10].
Traditional medicine question answering has stronger requirements on provenance. In Chinese settings,
Traditional Chinese Medicine (TCM) and Tibetan Medicine (TM) knowledge is scattered across heterogeneous
sources. These sources include classical canons, modern clinical papers, and encyclopedic summaries. They
differ in writing style, term usage, and authority. Recent work has built TCM and TM question answering
systems with knowledge graphs (KGs) or LLM-based assistants [ 11,12,13,14,15]. In parallel, domain
benchmarks and models for Chinese medical NLP and TCM evaluation are emerging [ 16,17,18,19]. However,
existing systems typically index a single source or merge all sources into one flat corpus. They rarely study
how source heterogeneity affects retrieval choices, evidence fusion, and traceability. This leaves a clear gap
for TM question answering in Chinese, where multi-source partitioned knowledge bases are common.
This paper targets a practical but underexplored setting. We assume TM knowledge is split into separate
knowledge bases by category, such as classics, clinical literature, and encyclopedia. Our goal is to make
RAG answers more traceable, reduce hallucinations, and support cross-source verification. Two challenges
are central. First, dense sources can dominate retrieval. In our data, encyclopedia passages are short and
information dense, which makes retrievers prefer them. But for many clinical or doctrine sensitive questions,
∗Corresponding author. Email:zhouqg@lzu.edu.cn.
1arXiv:2602.05195v1  [cs.AI]  5 Feb 2026

encyclopedic summaries are not the best evidence. This mismatch can lead to plausible but poorly grounded
answers. This problem is related to resource selection in federated search, but the objective here also involves
authority and provenance [20, 21].
Second, naive cross source fusion can reduce precision. A common approach is to concatenate retrieved
passages from multiple sources and let the LLM decide. But long context models can be sensitive to passage
order and position, which makes such fusion unstable [ 22]. Hybrid retrieval methods and KG-guided reranking
are promising, but current work mainly evaluates general domains [ 23,24]. In TM, the problem is amplified
by the semantic gap between classical terminology and modern clinical writing. As a result, simply adding
more passages often increases noise and harms evidence attribution.
We present a RAG framework designed for traceable TM question answering in Chinese under partitioned
multi source knowledge bases. Our approach is evaluated with a dataset and metrics that explicitly measure
correctness and traceability, beyond surface fluency [7, 8, 6, 3]. Our contributions are:
•We formalize traceable cross source TM question answering in Chinese, where knowledge is split into
heterogeneous knowledge bases by category.
•We propose two complementary components that address one goal: source aware retrieval to avoid
density driven source bias, and KG-based cross source alignment to guide evidence fusion and reranking
for better precision and attribution.
•We design an evaluation protocol with fine grained measurements of answer correctness, evidence
support, and cross source verification, and we report strong improvements over competitive RAG
baselines.
2 Related Work
Attribution and evaluation for retrieval-augmented generation.A central motivation for retrieval-
augmented generation (RAG) is to improve factuality by grounding answers in external evidence, yet reliable
attribution remains non-trivial. ALCE formalizes citation-aware generation and proposes automatic metrics
to assess citation quality beyond answer fluency and correctness [ 7]. Complementarily, RAGAs provides
a reference-free evaluation toolkit that decomposes RAG quality into retrieval and generation dimensions,
enabling rapid iteration without expensive human labels [ 8]. For domain settings, MIRAGE benchmarks
medical RAG across many configurations and highlights that even strong pipelines can remain sensitive to
component choices [ 6]. These efforts motivate evaluation protocols that jointly consider answer correctness,
evidence support, and citation reliability, which are critical for high-stakes knowledge domains.
Evidence ordering and long-context sensitivity.When multiple pieces of evidence are concatenated
into a single prompt, large language models may underuse relevant information depending on its position.
The “lost-in-the-middle” phenomenon shows that performance can degrade when key evidence is placed in
the middle of long contexts, even for long-context models [ 22]. This observation is especially relevant for
multi-source RAG, where naive concatenation or arbitrary ordering of retrieved passages can reduce precision.
It motivates evidence organization strategies that are robust to ordering effects and can better support
traceable, evidence-backed answers.
Graph-augmented retrieval and traditional medicine question answering.Knowledge graphs
(KGs) are often introduced into RAG to encode structured relations among entities and to improve evidence
selection. HybridRAG combines vector retrieval with KG-based retrieval and reports improved retrieval and
generation quality compared with using either signal alone [ 25]. In traditional medicine, recent work has
started to build RAG-style question answering systems for Traditional Chinese Medicine (TCM), typically
by constructing a single vectorized corpus from curated textbooks and classical materials [ 26]. For Tibetan
medicine, existing published systems more commonly focus on KG construction and KG-based question
answering rather than multi-source RAG over partitioned corpora [ 15,14]. Overall, prior work provides useful
building blocks (citation evaluation, RAG assessment, long-context analysis, and KG-enhanced retrieval), but
it leaves a gap in studyingpartitioned, heterogeneoustraditional medicine knowledge bases where provenance,
authority, and cross-library evidence alignment are first-order requirements.
2

3 Method
3.1 Problem Setup and Notation
We consider three Chinese Tibetan-medicine knowledge bases (KBs), K={E,T,P}, for encyclopedia, classics,
and clinical papers. Each KB kis segmented into a set of chunks Dk={c}. Each chunk chas text xcand
metadata mcincluding KB id, document id, and a structural path. Given a query q, the system outputs an
answerˆaand a cited evidence listE(q) =⟨c 1, . . . , c K⟩with stable chunk ids.
We assume two scorers: (i) a semantic retriever producing a relevance score sret(q, c)and (ii) an optional
semantic ranker producing srank(q, c). We do not bind them to specific model families in the method section.
For cross-KB alignment, each chunk is associated with a typed entity set E(c). We use a fixed type
inventory (e.g.,Disease,Symptom,Drug,Formula, etc.). We also extract a typed entity setE(q)from
the query.
Figure 1 gives an overview of our end-to-end pipeline, highlighting where Method I (DAKS routing) and
Method II (graph-guided fusion) intervene.
Figure 1: Overall system overview.
3.2 Method I: DAKS Routing with Budgeted Retrieval
In our setting, KBs are heterogeneous. Encyclopedia chunks are short and information-dense, which often
yields higher retrieval scores. This can bias retrieval towardEeven whenTorPis the preferred source. We
therefore treat routing as abudgeted resource selectionproblem.
3.2.1 Probe retrieval and KB-level statistics
For each KBk, we run a small probe retrieval to obtainLcandidates:
Pk(q) = TopL 
{(c, s ret(q, c)) :c∈ D k}
.(1)
3

Algorithm 1DAKS Routing with Budgeted Retrieval
Require:queryq; KBsK={E,T,P}; total budgetB; minimum budgetb min; probe sizeL
Require:retriever score functions ret(q, c); authority priora k
Ensure:KB scores{S k}; KB rankingπ KB; budgets{b k}; dense candidate poolR dense
1:foreach KBk∈ Kdo
2:P k←TopL 
{(c, s ret(q, c)) :c∈ D k}
▷probe retrieval
3:Let sorted probe scores bes(1)
k≥ ··· ≥s(L)
k
4:π k←Softmax([s(1)
k, . . . , s(L)
k])
5:ϕ k(q)←[peaks(1)
k,top-Mmean,margin(s(1)
k−s(M)
k), H(π k),cov k(q)]
6:S k←w⊤ϕk(q) +λa k
7:end for
8:π KB←SortDescending({(k, S k)}k∈K)
9:pk←Softmax({S k}k∈K)for allk
10:b k←b min+ Round 
(B− |K|b min)·pk
for allk
11:Adjust budgets to sum toB:
12:whileP
kbk̸=Bdo
13:ifP
kbk> Bthen
14:decreaseb k⋆by 1 wherek⋆= arg max kbkandb k⋆> bmin
15:else
16:increaseb k⋆by 1 wherek⋆= arg max kpk
17:end if
18:end while
19:R dense← ∅
20:foreach KBk∈ Kdo
21:R dense← R dense∪Topb k 
{(c, s ret(q, c)) :c∈ D k}
22:end for
23:return{S k}, πKB,{bk},R dense
Let the sorted probe scores be s(1)
k≥ ··· ≥s(L)
k. We compute a KB feature vector ϕk(q)to characterize the
score distribution:
ϕk(q) = [s(1)
k|{z}
peak,1
MMX
i=1s(i)
k
|{z}
top-Mmean, s(1)
k−s(M)
k|{z}
margin,(2)
H(π k)|{z}
concentration,cov k(q)|{z}
coverage].(3)
Here πk=Softmax ([s(1)
k, . . . , s(L)
k])and H(·)is entropy. A lower entropy indicates more concentrated high-
score evidence. covk(q)is a lightweight coverage proxy (e.g., distinct documents or sections in Pk(q)), to
avoid allocating all budget to a single redundant source.
We summarize DAKS routing and budgeted retrieval in Algorithm 1.
Figure 2 illustrates how DAKS uses probe statistics to score KBs and allocate per-KB budgets, mitigating
density-driven encyclopedia dominance.
3.2.2 Authority-aware KB scoring
We add an authority prior akto encode our preference over evidence provenance (e.g.,PandTare often
more authoritative thanEfor clinical queries). We score each KB:
Sk(q) =w⊤ϕk(q) +λa k.(4)
This is aKB-levelrelevance estimate that is not tied to any single chunk.
4

Figure 2: DAKS. We run lightweight probe retrieval in each KB, summarize score distributions into KB-level
features, compute KB scores, and allocate a soft budget to form a balanced candidate pool.
3.2.3 Soft budget allocation and candidate pool
Given a total retrieval budget Band a minimum guarantee bminfor each KB, we allocate per-KB budgets
using the KB scores:
bk(q) =b min+ Round 
(B− |K|b min)·pk(q)
, p k(q) = Softmax(S(q)) k.(5)
We then retrieveb k(q)candidates from each KB and merge them:
Rdense(q) =[
k∈KTopb k(q) 
{(c, s ret(q, c)) :c∈ D k}
.(6)
We also keep the KB ranking induced byS k(q)for routing evaluation (primary-source and Top-2).
3.3 Candidate Consolidation
Rdense(q)may contain fragmented evidence from long structured documents. We consolidate candidates into
a cleaner list C(q)before cross-KB fusion. First, we applystructure-aware expansion. For each candidate
chunkc, we add its local neighbors in the same document based on the structural path:
Expand(c) ={c} ∪Nbr(c),(7)
whereNbr(c)can include parent-section summaries or adjacent chunks within the same section.
Second, we deduplicate and enforce diversity. We remove near-duplicates and cap repeated chunks from a
single document. This prevents one dense document from dominating the evidence list.
Finally, we compute a base score sbase(q, c)for all c∈ C (q), which can combine retrieval relevance and
optional ranking:
sbase(q, c) =µˆs ret(q, c) + (1−µ) ˆs rank(q, c),(8)
where ˆ·denotes score normalization withinC(q).
5

Figure 3: Alignment Graph-Guided Fusion. We build a chunk–entity alignment graph to compute graph
support signals, fuse them with semantic relevance for reranking, and pack evidence under a token budget
with cross-KB coverage constraints.
3.4 Method II: Alignment Graph-Guided Fusion for Cross-KB Verification
Naive concatenation of multi-KB contexts can introduce noise and lower precision. It is also sensitive to
evidence order in long prompts. Long-context studies show that relevant information placed in the middle
can be underused, which makes evidence ordering and packing critical. We address this with analignment
graphand aconstrained evidence packingobjective.
3.4.1 Alignment graph construction
We build a bipartite graph G= (VC∪VE,EG). Chunk nodes VCcorrespond to chunks in all KBs, and entity
nodes VEcorrespond to typed entities. We add an edge( c, e)if entity e∈E (c). Each chunk node keeps its
KB labelk(c)∈ K.
3.4.2 Graph-based bridge retrieval (optional)
Given query entities E(q)and consolidated candidates C(q), we collect seed entities from the top candidates:
Eseed(q) =E(q)∪[
c∈TopS(C(q),s base)E(c).(9)
We then traverse Gfrom Eseed(q)for at most hhops (entity–chunk alternation), and retrieve additional
chunks:
Rgraph(q) ={c∈V C:∃e∈E seed(q),dist G(e, c)≤h}.(10)
This step explicitly seeks cross-KB bridges (e.g., linking a classics concept to clinical evidence).
Figure 3 visualizes our alignment graph and the fusion-and-packing procedure for cross-KB verification,
from graph support scoring to coverage-aware evidence selection.
6

For any candidate chunk c(from C(q)∪ R graph (q)), we compute a graph support score using two signals:
(1) entity overlap and (2) graph proximity to query/seed entities:
o(q, c) =|E(q)∩E(c)|,(11)
d(q, c) = min
e∈E(c), e′∈Eseed(q)distG(e, e′).(12)
We define:
sg(q, c) =η 1log(1 +o(q, c)) +η 21
1 +d(q, c)+η3I[k(c)̸=k major (q)],(13)
where kmajor (q)is the top KB predicted by DAKS, and the last term encourages cross-KB verification. We
fuse semantic relevance and graph support:
sfinal(q, c) =αˆs base(q, c) + (1−α)ˆs g(q, c).(14)
This yields a single ranked listL(q).
3.4.3 Constrained evidence packing under coverage
We select the final evidence list E(q)under a token budget Tmax. For each chunk c, letℓ(c)be its token
length. We solve a constrained selection problem:
max
E⊆L(q)X
c∈Esfinal(q, c)(15)
s.t.X
c∈Eℓ(c)≤T max,(16)
∀k∈ K req(q),∃c∈ Ewithk(c) =k,(17)
where Kreq(q)is the required KB set (single-KB or multi-KB). We use a greedy procedure: (i) first satisfy
coverage by selecting the best chunk per required KB, (ii) then fill remaining budget by descending sfinal(q, c)
with diversity caps. This packing directly targets cross-KB verification and reduces failures caused by naive
concatenation.
Algorithm 2 details our coverage-aware greedy evidence packing under a token budget.
4 Experiments
4.1 Dataset
We construct a Chinese Tibetan-medicine QA dataset over three KBs: encyclopedia (E), classics (T), and
clinical papers (P). Each instance contains a query, a short gold answer, and gold evidence at the chunk
level (with KB labels, document metadata, and stable chunk ids). The dataset contains 500 queries and is
balanced across four question types (25% each): (i) definition, (ii) classics principles, (iii) clinical evidence,
and (iv) cross-KB synthesis. For cross-KB synthesis, each example specifies a required KB set (at least two
KBs) and a primary source KB.
Unlike typical supervised settings, we do not train any component on this dataset and do not create
train/dev/test splits. All reported metrics are computed on the full 500-query set.
4.2 Experimental Setup
We index chunks from each KB independently. At inference time, we compute chunk relevance scores with a
semantic retriever. We optionally apply a semantic reranker in the consolidation stage. All retriever and
reranker parameters are kept fixed during evaluation.
We useopenPangu-Embedded-7Bas the answer generator for all methods [ 27]. This choice highlights
a lightweight, deployment-friendly LLM in a domain RAG setting. To ensure a fair comparison, we keep the
same generation prompt and citation format across all methods.
7

Algorithm 2Coverage-aware Greedy Evidence Packing
Require:ranked listL(q)with scoress final(q, c); required KB setK req(q); token budgetT max
Require:token lengthℓ(c); optional doc capC doc(max chunks per doc)
Ensure:packed evidence listE(q)
1:E ←[ ];T←0; initialize doc counter mapcnt(·)←0
2:Phase 1: satisfy coverage constraints
3:foreach required KBk∈ K req(q)do
4:c⋆←arg max c∈L(q):k(c)=k sfinal(q, c)s.t.T+ℓ(c)≤T max
5:ifc⋆existsthen
6:appendc⋆toE;T←T+ℓ(c⋆);cnt(doc(c⋆))←cnt(doc(c⋆)) + 1
7:end if
8:end for
9:Phase 2: fill remaining budget by descending score
10:foreach chunkcinL(q)in orderdo
11:ifc /∈ Ethen
12:ifT+ℓ(c)≤T maxthen
13:ifcnt(doc(c))< C docthen
14:appendctoE;T←T+ℓ(c);cnt(doc(c))←cnt(doc(c)) + 1
15:end if
16:end if
17:end if
18:end for
19:returnE(q)
Since we do not use a validation split, we set a single hyperparameter configuration and keep it fixed
for all experiments. This includes the probe size L, total retrieval budget B, minimum per-KB budget bmin,
fusion weight αin Method II, optional graph hop limit h, and token budget Tmax. We set the evaluation
cutoff toK=5for all @K metrics.
All automatic metrics, including RAGAS-style scores and citation checks, are computed by the same
judge model,GLM-4.7, to avoid confounding effects from heterogeneous evaluators.
4.3 Metrics
We evaluate at three levels with a consistent cutoffK=5.
KB routing (Method I).Primary-source accuracy (PrimaryAcc) and Top-2 hit (Top2Hit) are computed
from the KB score ranking over {E,T,P}. We also report encyclopedia dominance rate (EDR), defined as the
fraction ofEchunks in the final evidence list.
Evidence selection and cross-KB verification (Method II).We report EvRecall@5 and EvNDCG@5
based on gold chunk ids. For cross-KB queries, we report CrossEv@5, which checks whether the top-5
evidence covers all required KBs. Unless otherwise stated, evidence-level metrics are computed over the
cross-KB subset.
Answer quality, faithfulness, and citation correctness.We report reference-free RAG metrics
(faithfulness, context precision/recall, and answer relevance) in the spirit of RAGAS. We also report citation-
based checks inspired by ALCE to assess whether cited evidence supports the generated statements.
4.4 Main Results
Table 1 summarizes end-to-end performance on all 500 queries. We compare against (i) single-KB retrieval
baselines, (ii) pooled retrieval over merged KBs, and (iii) naive multi-KB concatenation. We further include
8

two partial variants (DAKS onlyandGraphFusion only) to isolate the contributions of Method I and
Method II.
Overall,DAKS+GraphFusionachieves the best cross-KB evidence coverage (CrossEv@5) while main-
taining strong faithfulness and citation correctness. We observe that enabling graph-guided fusion without
DAKS routing can underperform on end-to-end answer relevance, despite strong evidence-level gains (Table 3),
suggesting that graph-guided fusion benefits from a well-routed candidate pool produced by Method I.
Method Faith. CtxPrec CtxRec AnsRel CrossEv@5 CitCorr
Single-KB (E) 0.654 0.415 0.834 0.902 0.720 0.756
Single-KB (T) 0.837 0.223 0.734 0.833 0.750 0.791
Single-KB (P) 0.810 0.245 0.705 0.805 0.680 0.680
Merged KB 0.785 0.195 0.750 0.795 0.650 0.720
Naive Multi-KB Concat 0.750 0.180 0.745 0.780 0.620 0.700
DAKS only 0.820 0.235 0.760 0.825 0.720 0.750
GraphFusion only 0.630 0.306 0.409 0.408 0.650 0.650
DAKS + GraphFusion (Full)0.805 0.265 0.720 0.810 0.780 0.760
Table 1: End-to-end performance on all 500 queries with cutoff K=5. All answers are generated by
openPangu-Embedded-7B, and all automatic metrics are computed byGLM-4.7.
4.5 Routing Evaluation (Method I: DAKS)
Table 2 evaluates KB routing quality. DAKS improves primary-source prediction accuracy over uniform
budgeting and merged-KB retrieval, and reduces encyclopedia dominance in the final evidence list, indicating
its effectiveness in mitigating density-driven bias.
Method PrimaryAcc Top2Hit EDR(↓) AuthCov
Uniform per-KB budget 0.500 0.680 0.415 0.180
Merged KB 0.480 0.640 0.485 0.140
DAKS (ours) 0.560 0.700 0.362 0.230
Table 2: Routing evaluation with cutoff K=5. AuthCov measures whether evidence from authoritative KBs
appears in top-5 (for relevant queries).
4.6 Evidence Fusion Evaluation (Method II)
Table 3 evaluates evidence ranking quality and cross-KB verification on the cross-KB subset. We compare
naive concatenation, a score-only reranking baseline, and our graph-guided fusion. Graph-support fusion
substantially improves CrossEv@5 and reduces encyclopedia dominance. Enabling graph bridge retrieval
further improves recall and ranking quality, suggesting that the alignment graph retrieves complementary
evidence across KB boundaries.
Method EvRecall@5 EvNDCG@5 CrossEv@5 EDR(↓)
Naive concat (fixed order) 0.834 0.542 0.315 0.485
Score-only rerank (no graph) 0.846 0.612 0.392 0.442
Graph-support fusion (ours) 0.851 0.625 0.582 0.365
Graph-retrieval + fusion (ours) 0.872 0.693 0.645 0.341
Table 3: Evidence-level evaluation on cross-KB queries with cutoffK=5.
9

5 Conclusion
We investigate RAG for Chinese Tibetan-medicine QA in a multi-KB setting where KBs differ in style,
density, and evidentiary authority. Our results demonstrate that (i) DAKS improves KB routing and reduces
encyclopedia dominance, and (ii) alignment graph-guided fusion further strengthens cross-KB verification at
the evidence level. When combined, the full system achieves the best end-to-end cross-KB evidence coverage
(CrossEv@5) on our 500-query benchmark while maintaining competitive faithfulness and citation correctness.
Importantly, we show that a lightweight generator (openPangu-Embedded-7B) can support traceable
domain QA when paired with careful routing and evidence organization, offering a practical path toward
deployable Tibetan-medicine assistants.
Limitations.First, all automatic metrics are computed by a single judge model (GLM-4.7); although
this avoids evaluator mismatch, it may introduce systematic bias, and future work should include human
evaluation and multiple independent judges. Second, we evaluate only one domain and one language setting
(Chinese Tibetan-medicine resources), so generalization to other medical subdomains or languages remains
to be validated. Third, our alignment graph relies on entity extraction and chunk metadata; extraction
errors or incomplete typing may weaken graph support and cross-KB linking. Finally, our study focuses
on retrieval, fusion, and evidence packing rather than training retrieval models; improving retrievers with
domain supervision or preference signals could further strengthen robustness.
References
[1]Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language
models: Principles, taxonomy, challenges, and open questions.ACM Transactions on Information
Systems, 2025.
[2]Pratyush Sahoo et al. A comprehensive survey of hallucination in large language models. InFindings of
the Association for Computational Linguistics: EMNLP 2024, 2024.
[3]HaoYu, AoranGan, KaiZhang, ShiweiTong, QiLiu, andZhaofengLiu. Evaluationofretrieval-augmented
generation: A survey, 2024.
[4]Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex R Dalal, Jennifer L Kim, Michael Moor, Robyn Fong,
Curran Phillips, Kevin Alexander, Euan Ashley, et al. Almanac: Retrieval-augmented language models
for clinical medicine.NEJM AI, 2024.
[5]Linda M Amugongo et al. Retrieval-augmented generation for large language models in healthcare: a
review.PLOS Digital Health, 2025.
[6]Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented generation
for medicine. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the Association for
Computational Linguistics: ACL 2024, pages 6233–6251, Bangkok, Thailand, August 2024. Association
for Computational Linguistics.
[7]Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate
text with citations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing, pages 6465–6488, Singapore, December
2023. Association for Computational Linguistics.
[8]Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. RAGAs: Automated evaluation
of retrieval augmented generation. In Nikolaos Aletras and Orphee De Clercq, editors,Proceedings
of the 18th Conference of the European Chapter of the Association for Computational Linguistics:
System Demonstrations, pages 150–158, St. Julians, Malta, March 2024. Association for Computational
Linguistics.
10

[9] Shiqi Yan, Jiaqi Gu, et al. Corrective retrieval augmented generation, 2024.
[10]Akari Asai, Zeqiu Wu, et al. Self-rag: Learning to retrieve, generate, and critique through self-reflection.
InInternational Conference on Learning Representations, 2024.
[11]Peng Li et al. An intelligent question answering system based on the knowledge graph taking prescription
and materia medica as examples.Digital Chinese Medicine, 2024.
[12]Mingrui Qin et al. Rag-cpmf: A retrieval augmented generation model based intelligent prescription
recommendation system for chinese patent medicine formula.Digital Chinese Medicine, 2025.
[13]Mingrui Qin et al. Tcmlcm: A knowledge graph-based retrieval-augmented generation approach for
traditional chinese medicine question answering.Digital Chinese Medicine, 2025.
[14]Andi Dong, Chao Wang, Pan Tong, Dan Yang, and Cuo Yong. Research on tibetan medicine intelligent
question answering system integrating confrontation training and reinforcement learning. InISAIMS
2021: 2nd International Symposium on Artificial Intelligence for Medicine Sciences, Beijing, China,
October 29 - 31, 2021, pages 120–125. ACM, 2021.
[15]Jiuchang Pei, Jianfu Chen, and Yan Sun. Constructing tibetan materia medica knowledge graph from
ecological census. InInternational Conference on Asian Language Processing, IALP 2024, Hohhot,
China, August 4-6, 2024, pages 188–191. IEEE, 2024.
[16]Ningyu Zhang, Mosha Chen, Zhen Bi, Chen Tan, Xiaoyan Li, et al. Cblue: A chinese biomedical language
understanding evaluation benchmark. InProceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), 2022.
[17]Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo:
Tuning llama model with chinese medical knowledge, 2023.
[18]Ping Yu, Kaitao Song, Fengchen He, Ming Chen, and Jianfeng Lu. Tcmd: A traditional chinese medicine
qa dataset for evaluating large language models, 2024.
[19]Song Wang et al. Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented
generation, 2024.
[20]Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, and Guido Zuccon. Feb4rag: Evaluating federated
search in the context of retrieval augmented generation, 2024.
[21]Shuai Wang, Shengyao Zhuang, Bevan Koopman, and Guido Zuccon. Resllm: Large language models
are strong resource selectors for federated search, 2024.
[22]Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. Lost in the middle: How language models use long contexts.Transactions of the Association for
Computational Linguistics, 12:157–173, 2024.
[23]Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren,
Yiming Yang, and Michael Zeng. Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain
question answering. InProceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 2022.
[24]Saandeep Sarmah et al. Hybridrag: Integrating knowledge graphs and vector retrieval for enhanced
information retrieval. InProceedings of the 5th ACM International Conference on AI in Finance, 2024.
[25]Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. Hy-
bridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information
extraction.arXiv preprint arXiv:2408.04948, 2024.
[26]Yuming Zhang, Hongyan Li, Xufeng Lang, Zuojian Zhou, Yun Ling, and Ziyan Wang. Construction
of traditional chinese medicine question-answering large language model based on retrieval-augmented
generation technology.Journal of Nanjing University of Chinese Medicine, 40(12):1375–1382, 2024.
11

[27]H. Chen, Y. Wang, K. Han, et al. Pangu embedded: An efficient dual-system llm reasoner with
metacognition.arXiv preprint, 2025.
12