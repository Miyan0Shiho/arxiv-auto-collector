# Automatic Teaching Platform on Vision Language Retrieval Augmented Generation

**Authors**: Ruslan Gokhman, Jialu Li, Youshan Zhang

**Published**: 2025-03-07 14:33:54

**PDF URL**: [http://arxiv.org/pdf/2503.05464v1](http://arxiv.org/pdf/2503.05464v1)

## Abstract
Automating teaching presents unique challenges, as replicating human
interaction and adaptability is complex. Automated systems cannot often provide
nuanced, real-time feedback that aligns with students' individual learning
paces or comprehension levels, which can hinder effective support for diverse
needs. This is especially challenging in fields where abstract concepts require
adaptive explanations. In this paper, we propose a vision language retrieval
augmented generation (named VL-RAG) system that has the potential to bridge
this gap by delivering contextually relevant, visually enriched responses that
can enhance comprehension. By leveraging a database of tailored answers and
images, the VL-RAG system can dynamically retrieve information aligned with
specific questions, creating a more interactive and engaging experience that
fosters deeper understanding and active student participation. It allows
students to explore concepts visually and verbally, promoting deeper
understanding and reducing the need for constant human oversight while
maintaining flexibility to expand across different subjects and course
material.

## Full Text


<!-- PDF content starts -->

 Automatic Teaching Platform on Vision Language Retrieval Augmented Generation  Ruslan Gokhman, Jialu Li, Youshan Zhang Artificial Intelligence Graduate Computer Science and Engineering Department, Yeshiva University, NY, USA rgokhma@mail.yu.edu, jli10@mail.yu.edu, youshan.zhang@yu.edu   Abstract - Automating teaching presents unique challenges, as replicating human interaction and adaptability is complex. Automated systems cannot often provide nuanced, real-time feedback that aligns with students' individual learning paces or comprehension levels, which can hinder effective support for diverse needs. This is especially challenging in fields where abstract concepts require adaptive explanations. In this paper, we propose a vision language retrieval augmented generation (named VL-RAG) system that has the potential to bridge this gap by delivering contextually relevant, visually enriched responses that can enhance comprehension. By leveraging a database of tailored answers and images, the VL-RAG system can dynamically retrieve information aligned with specific questions, creating a more interactive and engaging experience that fosters deeper understanding and active student participation. It allows students to explore concepts visually and verbally, promoting deeper understanding and reducing the need for constant human oversight while maintaining flexibility to expand across different subjects and course material.   Index Terms - Augmented Generation, Automatic Teaching, Deep Learning Retrieval, Machine Learning.  1. Introduction     In the evolving landscape of digital education, the integration of advanced technologies into learning environments represents a transformative shift aimed at enriching educational delivery and engagement. While interactive learning platforms have broadly embraced technological innovations, they often underutilize the potential of visual content - a critical element that enhances understanding and engagement through intuitive and dynamic presentations.     However, current educational technologies often fall short of fully exploiting the capabilities of visual content, which is a rich medium for conveying complex information intuitively and engagingly. Traditional learning management systems and e-learning tools focus primarily on text-based interactions, which can hinder the learning process for visual learners and reduce engagement in inherently visual subjects, such as science, engineering, and the arts. Visual Question Answering (VQA) systems emerge as a groundbreaking solution to these challenges. By combining computer vision and natural language processing, VQA systems enable the analysis of visual data and the generation of natural language answers to questions. This capability not only enhances the interactivity of learning environments but also deepens students’ understanding and engagement by allowing them to query and receive explanations about visual elements directly. This paper proposes a dedicated web platform for Automatic Teaching Platform Based on the Vision Language RAG. The Automatic Teaching Platform Based on Vision Language RAG, initially described in our prior work, harnesses Visual Question Answering (VQA) technology to allow interactive exploration and understanding of complex scientific concepts through visual dialogue.      The web platform aims to extend the capabilities of this approach by creating a tailored, user-friendly environment where students can query visual content related to their coursework, receive instant explanations, and engage in an interactive learning process that is both deep and intuitive. The deployment of the Automatic Teaching Platform Based on Vision Language RAG web platform specifically targets the courses of Machine Learning, Neural Networks and Deep Learning. These two disciplines, fundamental to the burgeoning field of artificial intelligence, demand a robust understanding of intricate models and theories—often best represented visually. By integrating the Automatic Teaching Platform Based on Vision Language RAG model into these courses, the platform not only enhances the traditional curriculum but also prepares students to effectively tackle real-world problems by strengthening their analytical and interpretative skills. Furthermore, this initiative is designed with scalability in mind, anticipating future expansion to include additional courses. This adaptive approach demonstrates the potential of the Automatic Teaching Platform Based on Vision Language RAG platform to become a cornerstone of educational technology within graduate programs, potentially transforming how complex scientific content is taught and learned.     In documenting the development and implementation of this web platform, the paper will provide insights into the design considerations, technological frameworks, and pedagogical strategies essential for integrating advanced VQA systems into higher education. Through a detailed analysis of the platform’s impact on student engagement and comprehension in initial deployments, we aim to comprehensively evaluate its efficacy and potential for broader application.  2. Related work 

 The integration of Visual Question Answering (VQA) systems into educational platforms represents a cutting-edge approach in the field of digital learning, particularly in enhancing interactive and visually enriched learning environments. This section reviews relevant developments in the visual question-answering systems in education, augmented and virtual reality educational tools, adaptive learning systems, adaptive learning systems and AI-driven tutoring systems, integration of blockchain technology, gamification and multimedia learning systems, and interactive and accessible educational environments. Each of these areas contributes to the foundational technologies upon which our SparrowVQE model is built, enabling a more dynamic learning experience for graduate students.     Visual Question Answering systems (VQAs) are pivotal in enhancing digital learning by facilitating direct engagement with educational content. Jia et al. [1] introduced the VQA2 Instruction Dataset and a series of models designed for video quality assessment using Visual Question Answering (VQA). Their work leverages large multi-modal models (LMMs) to advance the integration of spatial-temporal quality understanding in video. Krizhevsky et al. [12] developed deep learning technologies that provided essential computational power for analyzing visual data, and Lee et al.  [9] proposed using advanced NLP to improve the generation of relevant educational questions. These advancements suggest significant opportunities for further integrating AI into personalized learning environments, enhancing both the adaptivity and effectiveness of educational content delivery.      Recent studies have systematically reviewed the integration of Machine Learning (ML) in K-12 education, focusing on areas such as curriculum development, pedagogical strategies, and teacher training [2]. The findings highlight the need for more ML resources in younger education levels, further research into cross-disciplinary integration, and a greater focus on the societal and ethical implications of ML in future studies. Haowen, Jiang, et al. [15] further discussed the impact of VR in medical training with realistic simulations that improve practical skills. Additionally, López-Belmonte, Jesús, et al. [9] emphasized AR’s potential to improve understanding and retention through interactive visualizations. The continuous development in AR and VR technologies offers potential for broader applications across various educational disciplines, promising more engaging and effective learning methodologies. Adaptive learning systems and AI-driven tutoring leverage personalized learning pathways to improve academic outcomes. Pashler et al. [20] and Minn [23] discussed techniques for adaptive learning environments. Lin et al. [22] highlighted how intelligent tutoring systems offer personalized instruction and what kind of challenges they bring, and Abd El-Haleem [21] showed how AI can predict students’ performance, facilitating more effective educational strategies. Recent literature has examined potential AI, particularly Machine Learning and LLMs, in transforming educational systems by personalizing learning and addressing disparities. These studies emphasize the role of AI in modernizing education and promoting equity, highlighting its application in diverse learning environments and the need for future research in its integration across different educational levels.     Bhaskar et al. explored the present and future applications of blockchain technology in education management, highlighting its potential for enhancing data security, transparency, and administrative efficiency in educational institutions [5]. Nguyen et al. [19]  discuss the ethical considerations of deploying blockchain and AI in educational settings, addressing critical issues of privacy and bias and ensuring the reasonable use of technology. The potential for blockchain to revolutionize educational transparency and accountability opens avenues for future research into decentralized educational models. Kalyuga discusses the importance of personalized e-learning systems, highlighting the role of AI in tailoring learning experiences based on individual comprehension levels and preferred learning modes, and proposes a comprehensive framework with five modules to optimize personalized education [6].  Mayer’s cognitive theory [13] provided guidelines for the effective design of instructional messages, emphasizing the importance of aligning multimedia content with human cognitive architecture to maximize learning efficiency.  Further enhancing multimedia learning, Mayer and Moreno’s work [17] on multimedia learning emphasizes the alignment of content design with cognitive processes to maximize learning efficiency.  
  FIGURE I VISION LANGUAGE RAG FLOW ARCHITECTURE DIAGRAM  
 FIGURE II ARCHITECTURE OF THE FRAMEWORK      Technologies that support interactive learning environments, such as those discussed by Dillenbourg et al. [7], promote active participation and real-time feedback, enhancing learner engagement and retention.    Kongpha et al. presented a virtual interactive learning model that combines the imagineering process with the metaverse, emphasizing the development of a learning system that fosters happy learning through virtual communities and interactive environments [4] .  3. Methods 3.1. Similarity Search I Stage with FAISS     The FAISS approach is used for efficient similarity search and clustering of dense vectors. Document embeddings are represented for each document i and then normalized to unit vectors to ensure consistency in their magnitude. The normalization of embeddings is expressed as:  𝑑𝑜𝑐!="#$!‖"#$!‖.     FAISS uses the IndexFlatIP index, which performs inner product-based similarity search. The similarity score for each document i is computed as the inner product (dot product) between the query embedding and each document embedding: 


 si = query embedding · doc embedding. 3.2. Similarity Search II stage with BERT      In the second stage, we tried using the BERT model. This step ensures a more accurate ranking by evaluating the semantic relationships between the query and the top candidate answers in more detail. The top k responses from stage I are paired with the query, forming input sequences in the format: Inputi= Query [SEP] Candidatei.     The BERT model processes each input sequence, which assigns a relevance score ri to each candidate.  These scores represent the contextual alignment between the query and the candidate’s answers. The candidates are then reranked based on their BERT scores ri, with the highest-scoring candidate being chosen as the final result. Formally, the final ranking is expressed as: Ranked Results = Sort ({(Candidatei, ri) | i ∈ [1, k]}).     This two-stage approach, initial filtering with FAISS followed by BERT refinement, ensures high retrieval accuracy and makes the system robust for Vision Language tasks. 3.3. Front-End Development      The front-end architecture employs HTML5 for structure, CSS/Bootstrap for styling, and JavaScript for interactivity, as shown in Figure III. This modular design ensures the separation of concerns and maintainable code structure. Our implementation focuses on two key aspects:    FIGURE III FRONTEND ARCHITECTURE OVERVIEW SHOWING THE RELATIONSHIPS BETWEEN UI COMPONENTS, APPLICATION LOGIC, AND API INTEGRATION LAYERS.  • Core Stack: HTML5 provides semantic markup structure with modern web standards. CSS and Bootstrap framework handle responsive design and consistent styling across devices. JavaScript manages dynamic functionality and user interactions through modular components. • Key Features: The system implements secure user authentication, dynamic content loading for lectures and courses, an interactive chat interface for student-teacher communication, and text-to-speech capabilities for enhanced accessibility. These features work together to create an engaging learning environment.     The front end communicates with backend services through a RESTful API layer, managing state and user interactions efficiently. This architecture enables real-time updates and seamless integration with the backend services while maintaining high performance and reliability. The implementation follows modern web development practices, prioritizing code maintainability, scalability, and user experience. 4. Interface     Our web service functionality is orchestrated using Flask, a lightweight yet powerful Python web framework that facilitates rapid development and logical API endpoint arrangement [8]. It is particularly adept at managing high-throughput POST requests essential for real-time text synthesis, equipped with robust error-handling strategies. These strategies involve managing inputs that exceed pre-defined maximum lengths by truncating them, preserving compatibility with the model’s operational constraints, and avoiding potential runtime errors.   In this system, we used the advanced capabilities of StyleTTS 2 [14] to replicate the voice of a professor, enhancing the user experience with a natural, lifelike auditory output. The model’s ability to personalize the synthetic speech using style diffusion and speaker embeddings ensures that the generated voice closely mimics the professor’s vocal attributes, making the interaction more engaging and realistic.     The StyleTTS 2 model stands out due to its innovative use of style diffusion and adversarial training, making it highly effective for text-to-speech (TTS) synthesis. By modeling styles as a latent random variable through diffusion models, it can generate speech that adapts to the content of the text without requiring reference speech. This eliminates the need for paired data, offering a more flexible and efficient method of speech synthesis. The integration of large pre-trained speech language models (SLMs) like WavLM enhances the model’s ability to understand and replicate natural speech characteristics, leading to improved speech quality. Furthermore, the model’s ability to perform zero-shot speaker adaptation ensures that it can easily generalize to new speakers without requiring extensive retraining. StyleTTS 2 surpasses human recordings on single-speaker datasets and matches human-level performance on multispeaker datasets, demonstrating its superior capabilities.      The User Management System (UMS) is meticulously designed to encompass extensive functionalities such as user creation, updates, deletion, and authentication processes. This robust framework is critical for secure and efficient user data management within web applications, reinforcing the back-end’s capability to support complex, multifaceted user interactions in real-time environments. The UMS is structured around a series of API endpoints, each dedicated to a specific aspect of user management [22]. Flask’s seamless handling of HTTP requests makes it an ideal choice for developing RESTful APIs. The system securely stores user information in a relational database, interfacing with the database through SQL queries executed via Flask’s database utilities. The system allows for creating new users through a POST request to the user endpoint, leveraging Flask’s framework capabilities as detailed by Grinberg [8]. User details such as username, password, and user type are collected from the request payload and inserted into the database, using SQL transactions that are robustly handled as described in Klein and Roggero’s work on SQL databases [10].      For existing users, the system provides functionality to update details via a PUT request using the user ID. This endpoint processes updated information from the request and applies it to the relevant database record. Additionally, users can be deleted from the system through a DELETE request, which removes the user’s record from the database, ensuring data integrity and security as per standard SQL operations.      The UMS handles user authentication using a dedicated /login endpoint, verifying user credentials against data stored in the database and initiating a session that tracks user status via Flask’s 


 session management capabilities, a critical aspect for maintaining secure and personalized interactions within the application [8]. Conversely, the /logout endpoint allows users to terminate their sessions, ensuring that all session data is cleared, which is crucial for maintaining security. The UMS also facilitates the retrieval of comprehensive user data, supporting administrative operations and user oversight. It includes endpoints like /users/all for retrieving all user records and /users/admins or /users/regular for filtered views based on user types, which are essential for effectively managing different user roles and access levels within the application.     Overall, the User Management System exemplifies the practical implementation of web and database technologies to create a secure, efficient, and scalable user management solution, leveraging Flask’s capabilities for web request and session handling, coupled with SQL’s robust data management, to offer a versatile platform for addressing diverse user needs in modern web applications [8][11].     The RAG model provides responses based on your current slide and request. For every question you ask, the API will retrieve the corresponding slide. You can post your question either via text or voice, and the model outputs the response in both text and voice, according to your settings. The audio transcriptions for the slides are enabled with a single click, and the response is immediate. As soon as you click, the professor’s transcripts appear on the screen for each slide. The transcription stops when you click the button again or when you move to the next slide. 4.1. Testing and Debugging     Testing Methods To ensure the robustness and functionality of our web platform, we adopted a comprehensive testing strategy that integrates both manual and automated approaches to maintain high quality and reliability. Manual testing was rigorously conducted to ensure that all user interfaces and workflows meet the usability standards and functional requirements. This included usability tests to evaluate the user experience, and user acceptance testing to ensure the system meets the business needs and user expectations.      Automated testing complemented these efforts, employing tools such as Selenium for browser-based regressions, and Jenkins for continuous integration, which helped identify issues at the early stages of the development cycle. Unit tests were written for individual components, while integration tests checked the data flow between these components to ensure that they worked together seamlessly as expected. The combination of these testing methods ensures thorough coverage of the software, minimizing the risk of defects in production. For comprehensive insights into the principles and strategies of effective software testing, the work by Gurcan et al. provides an essential reference [18]. 5. Dataset      We used the dataset presented in the paper SparrowVQE:   Visual Question Explanation for Course Content Understanding [15] for testing. This dataset was carefully curated to support the Visual Question Explanation (VQE) task, which aims to enhance educational AI models by providing detailed explanations for slide content rather than just short answers. The dataset includes slide images extracted from course presentations, along with transcripts generated from lecture recordings using a combination of automatic speech recognition tools and manual review to ensure accuracy. Additionally, it contains question-answer pairs designed to capture various types of educational inquiries, including closed-ended, open-ended, summarization, and classification questions. The structured nature of the dataset makes it well-suited for training multimodal AI models to improve learning experiences in machine learning education by effectively integrating textual and visual information. 6. Results  TABLE I RAG SYSTEM WITH DIFFERENT LLMS FOR DEEP LEARNING DATASET [15] Model rouge1 rouge2 rougeL BLEU COSINE Retrieval System with Vision RAG 87.117 81.43 84.83 0.7892 0.852 LLAMA 38.40 31.74 37.00 0.1200 0.4598 T5 40.72 35.16 33.54 0.0600 0.5908 Bart Large CNN  85.34 82.71 83.84 0.6850 0.792      The evaluation of the models reveals significant insights into their performance across various metrics, specifically focusing on the RAG with its different components. The simple Retrieval System showed the best results, demonstrating high accuracy in choosing the right answers. The Bart Large CNN component stands out with a good performance, particularly in ROUGE and cosine similarity, which makes it well-suited for applications requiring accuracy and reliability. On the other hand, the LLAMA and T5 models adjust the output in different ways, with performance in ROUGE and BLEU lower than the Retrieval System but still contributing to the final result.     The LLAMA model, while returning good results, produces outputs that take in slightly different shape from the original queries derived from the Retrieval system as shown in Table II. This indicates that although it captures a similar underlying sense to the initial input, the altered structure of its output may present challenges in applications where consistency and alignment with the source information are critical. The T5 model, another component of the RAG system, also shows lower performance compared to the Bart Large CNN. It benefits from the retrieval output but struggles to achieve the same level of accuracy and effectiveness, highlighting its limitations in this context.     The results from the Retrieval system serve as crucial input for these models, and the varying performance levels among the components emphasize the importance of selecting the appropriate model for specific tasks. The consistently high performance of the Bart Large CNN suggests it may be the preferred choice for applications demanding precision and reliability, particularly when integrated with the capabilities of the Retrieval system. Moving forward, it is advisable to leverage the strengths of the Bart Large CNN component in high-stakes applications while continuing to explore enhancements and optimizations within the Retrieval system and its other components. Understanding how each model interacts with the RAG system will provide valuable insights that can guide future developments in system architecture and training methodologies. These conclusions will not only inform model selection but also enhance the overall effectiveness of the RAG framework in practical applications.      This section presents a comprehensive overview of the user interfaces within our web application, meticulously crafted to augment both user interaction and learning efficiency. Each component is designed not only to facilitate learning but also to ensure ease of use and engagement with educational content.  

 
	Outputs	 from	 RAG	System	 with	 Different	 LLMs	on	 question:	What	 are	 some	 common	 image	 formats	 in	 digital	 image	processing?		Retrieval	System	BART	Large	CNN	T5	LLAMA	Image	Formats	Common	image	formats	include	JPEG,	PNG,	GIF,	BMP,	and	TIFF.	These	formats	vary	Common	image	formats	include	JPEG,	PNG,	GIF,	BMP,	and	TIFF.	These	formats	vary	JPEG,	PNG,	GIF,	BMP,	and	TIFF.	Some	common	image	for-mats	used	in	digital	imaging	include	JPEG	(Joint	Photo-		in	terms	of	compression,	quality,	and	application.	in	terms	of	compression,	quality,	and	application.		graphic	Experts	Group),	PNG		JPEG	is	widely	used	for	JPEG	is	widely	used	for		(Portable	Network	Graphics),		photographs,	while	PNG	is	photographs,	while	PNG	is		GIF	(Graphics	Interchange		preferred	for	lossless	preferred	for	lossless		Format),	BMP	(Bitmap),	and		compression.	compression.		TIFF	(Tagged	Image	File					Format).	Each	format	has	its					own	strengths	and	weaknesses	and	are	suited	for					different	types	of	images	and					applications.	For	example,	JPEG					is	a	popular	format	for					photographs	due	to	its	high	compression	ratio	and	good					image	quality.						     By accessing the platform, users can leverage multiple functionalities. As illustrated, the weekly tab is the pivotal feature for navigating the selection of AI and ML classes. Upon selection, the corresponding weeks are displayed in a scrollable sidebar. Additionally, there is a full-screen mode that transforms the screen into a presentation mode. An innovative ’automatic teacher’ mode                                                                                                                                                                           TABLE II                   is available, which, upon voice command provides transcriptions of the professor’s lectures.       Figure V captures the integration of the Vision-Language RAG model and the pop-up chat window, which facilitates interactive dialogue. Conversations with the model can occur in three formats: text-to-text, voice-to-voice, and text-to-voice, or vice versa. It should be noted that the chat functionality can also be accessed without the pop-up chat window, as demonstrated. In this instance, voice commands are used for input and the model responds with voice output. Upon successful authentication, the user is directed to the main landing page of the VQE platform, as shown in Figure IV. This page serves as a central hub for accessing various educational resources and features, including weekly tabs for course navigation, multimedia resources, and interactive tools like chat and transcription. Figure IV shows the interface for weekly course content, accessible via a tab on the left side. This feature organizes the course material by week, allowing students to easily navigate through different sections of the course and access specific materials at their own pace.       Interactive engagement is a crucial aspect of modern educational tools. Figure V illustrates a typical interaction within the platform’s chat mode, where users can ask questions related to the slides and receive instant responses. This interactive dialogue  enhances the learning experience by allowing students to clarify doubts in real-time and explore the content more deeply. Voice interaction is another innovative feature our platform offers. Users can interact with the VQE model using voice commands, making the learning experience more accessible and engaging, particularly for users who may prefer auditory learning or those with visual impairments.     Each of these interfaces is carefully crafted to ensure that students not only receive information but also actively engage with the material. Our ongoing goal is to continue enhancing these features, based on user feedback and technological advances, to ensure that our educational platform remains in the cutting-edge digital learning environment. 6.1. Comparative analysis     We use four comparative analysis models that were proposed in SparrowVQE: Visual Question Explanation for Course Content Understanding, including BLIP, Pix2Struct, LLaVA, and LLM-                    Blender. The RAG system outperforms Sparrow VQE and other generative models because it combines retrieval with generation, leveraging the strengths of both approaches. It retrieves precise, contextually relevant data and generates fluent and factually accurate responses, making it a better choice for tasks requiring factual accuracy and domain-specific knowledge. In contrast, generative models rely solely on their pre-trained knowledge, which limits their ability to address complex queries or provide accurate, real-time responses. Without access to dynamic retrieval mechanisms, generative models often produce generic or fabricated outputs, which negatively impacts their semantic alignment and factual accuracy.     Moreover, RAG systems are more adaptable and scalable for tasks requiring real-time updates or specialized knowledge. By combining retrieval with generation, they excel at producing precise, context-aware responses even with smaller model sizes. Generative AI models, while advantageous for creative or open-ended tasks, struggle with domains that demand high factual accuracy and specificity. This fundamental difference highlights the reason why RAG systems are better suited for knowledge-intensive applications, as demonstrated by their superior performance in the evaluation table.  6.2. Comparative analysis with different components of the architecture      We performed experiments with the BERT component and without it. The results in Table III demonstrate that incorporating the BERT component significantly enhances the performance of the architecture across all metrics. The improvements in ROUGE-1, ROUGE-2, and ROUGE-L scores indicate better handling of 

 
both word-level and sequence-level contextual alignment, while the higher cosine similarity and BLEU scores highlight BERT’s  ability to refine semantic understanding. Overall, he architecture with BERT consistently outperforms the one without it, making it a more effective choice for multi-modal retrieval tasks. These findings validate the integration of BERT as a crucial enhancement for achieving higher accuracy and better semantics alignment in complex datasets.   7. Discussion     The introduction of the proposed RAG method has a transformative impact on learning outcomes. Quantitative assessments show a significant improvement in test scores, indicating that the model’s interactive Visual Question Answering (VQA) system aids in retaining complex information. Qualitative  feedback further highlights that students feel more engaged and better equipped to understand intricate topics, particularly in machine learning and neural networks. The system’s ability to enable students to interact with visual content promotes deeper cognitive processing and encourages critical thinking.       In evaluating the various components of the RAG system integrated with different language models (LLMs), it became evident that some models performed more effectively than others. The Bart Large CNN component, for instance, demonstrated the highest performance across several key metrics, proving to be the most reliable model in terms of accuracy, comprehension, and alignment with the educational content. This high accuracy is crucial in an educational context, as it ensures that students receive accurate information and reduces the likelihood of misunderstandings when interacting with the VQA system. In contrast, models like LLAMA and T5, though functional, displayed lower performance in terms of aligning outputs closely with the original queries. For example, the LLAMA model, while effective in capturing general meaning, often produced responses with altered structure, which could cause minor inconsistencies. This limitation is notable for educational use, as it can impact the clarity of responses, particularly when students are learning foundational concepts and require precise explanations.     T5, another component in the RAG system, struggled with similar issues. Although it benefited from the retrieval system’s outputs, it did not achieve the same accuracy and effectiveness as Bart Large CNN. This suggests that while T5 can handle simpler inquiries, it may not be ideal for addressing complex educational queries where precise and comprehensive responses are required.   A primary concern was ensuring accessibility for all students, especially those unfamiliar with computational tools. Creating an intuitive user interface was essential to make technology approachable. The backend infrastructure also needed to support a high volume of queries and data processing, requiring sophisticated server technology and regular optimizations to maintain stability and efficiency.  
Figure IV UPON SUCCESSFUL LOGIN, THE USER IS DIRECTED TO THE MAIN VQE LANDING PAGE. 
 Figure VI ADMIN DASHBOARD FOR USER MANAGEMENT.   This has led to a scheduled maintenance plan, allowing for performance enhancements without disrupting the learning 
FIGURE V DISPLAYS A TYPICAL INTERACTION WITH THE VISION-LANGUAGE RAG MODEL, ILLUSTRATING HOW USERS CAN ENGAGE IN CONVERSATIONS RELATED.  

 experience. One of the key strengths of the platform is its scalability. The architecture was designed to support potential expansion into various academic disciplines beyond machine learning, making it versatile for fields where visual content is critical, such as medical education, engineering, and digital arts. Future plans include exploring augmented reality (AR) and virtual reality (VR) integration to create even more immersive learning environments. Such advancements could revolutionize the educational experience by enabling experiential learning in simulated settings, going beyond traditional methods, and offering students hands-on interaction with concepts.  TABLE III RAG SYSTEM WITH DIFFERENT LLMS FOR MLVQE DATASET [15] Models Rouge-1 Rouge-2 Rouge-L COSINE BLEU CIDEr METEOR         BLIP 8.4 0.7 7.19 0.077 0.15 0.17 0.078 Pix2Struct 38 20.1 35.5 0.365 0.4 0.47 0.379 LLaVA 35.4 18.4 33.0 0.34 0.37 0.53 0.42 LLM-Blender 51.5 34.8 49 0.489 0.54 0.573 0.573 SparrowVQE 68.13 51.54 63.92 0.61 0.7 0.67 0.652 Retrieval System 82.7 78.2 81.44 0.888 0.7763 7.663 0.859 .    TABLE IV PERFORMANCE COMPARISON OF VARIOUS MODELS FOR DEEP LEARNING DATASET [15] Models Rouge-1 Rouge-2 Rouge-L COSINE BLEU CIDEr METEOR BLIP 1.3 1.2 3.6 0.092 0.16 0.002 0.028 Pix2Struct 8.1 3.1 7.3 0.128 0.29 0.007 0.0348 SparrowVQE 38.52 17.06 36.27 0.852 0.43 0.525 0.463 LLaVA 37.1 15.0 35.4 0.346 0.32 0.325 0.359 Retrieval System 87.117 81.43 84.83 0.7120 0.7952 0.7023 0.8554  TABLE V PERFORMANCE COMPARISON OF VARIOUS MODELS CONFIGURATIONS FOR DEEP LEARNING DATASET Metric Architecture with BERT Component Architecture without BERT Component ROUGE-1 87.117 82.26 ROUGE-2 81.42 74.58 ROUGE-L 84.8 79.09 COSINE 0.7892 0.7135 BLUE 0.852 0.7963 8. Conclusion    The integration of the RAG model into educational settings represents a pivotal step forward in enhancing student learning experiences. By combining interactive Visual Question Answering with advanced language models, this system fosters greater engagement, comprehension, and retention of complex topics. While models like Bart Large CNN have demonstrated exceptional accuracy and reliability, models such as LLAMA and T5 highlight areas for improvement in clarity and alignment, underscoring the importance of model selection in educational applications. The inclusion of the BERT component within the retrieval process further demonstrates significant performance improvements, as evidenced by metrics such as ROUGE, cosine similarity, and BLEU scores. These results highlight BERT’s ability to refine semantic understanding and align multi-modal embeddings, making it a crucial enhancement for accuracy and relevance in educational applications.     Despite challenges in implementation, including user accessibility and infrastructure demands, the platform’s scalable design opens opportunities for expansion across diverse fields. We plan to incorporate further revolutionized learning, offering students immersive, hands-on experiences that align with real-world applications. This advancement will not only enhance traditional learning methods but also provide a foundation for adaptive, interactive, and impactful educational environments. References [1] Jia, Z., Zhang, Z., Qian, J., Wu, H., Sun, W., Li, C., ... & Min, X. (2024). VQA $^ 2$: Visual Question Answering for Video Quality Assessment. arXiv preprint arXiv:2411.03795. [2] Weddle, H., Hopkins, M., Lowenhaupt, R., & Kangas, S. E. (2024). Shared responsibility for multilingual learners across levels of the education system. Educational Researcher, 53(4), 252-261. [3] Sanusi, I. T., et al. (2023). A systematic review of teaching and learning machine learning in K-12 education. Education and Information Technologies, 28(5), 5967-5997.  [4] Kongpha, R., & Chatwattana, P. (2023). The Virtual Interactive Learning Model Using Imagineering Process via Metaverse. Higher Education Studies, 13(1), 35-41.  [5] Bhaskar, P., Tiwari, C. K., & Joshi, A. (2021). Blockchain in education management: present and future applications. Interactive Technology and Smart Education, 18(1), 1-17. [6] Kalyuga, S. (2021). The expertise reversal principle in multimedia learning. Cambridge University Press, 171-182.  [7] Dillenbourg, P., Järvelä, S., & Fischer, F. (2009). The evolution of research on computer-supported collaborative learning: From design to orchestration (pp. 3-19). Springer Netherlands.  [8] Grinberg, M. (2018). Flask web development. " O'Reilly Media, Inc.".  [9] Lee, U., Jung, H., Jeon, Y., Sohn, Y., Hwang, W., Moon, J., & Kim, H. (2024). Few-shot is enough: exploring ChatGPT prompt engineering method for automatic question generation in english education. Education and Information Technologies, 29(9), 11483-11515. [10] López-Belmonte, J., Moreno-Guerrero, A. J., López-Núñez, J. A., & Hinojo-Lucena, F. J. (2023). Augmented reality in education. A scientific mapping in Web of Science. Interactive learning environments, 31(4), 1860-1874. [11] Klein, S., & Roggero, H. (2013). Pro SQL Database for Windows Azure: SQL Server in the Cloud. Apress. [12] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet                          classification with deep convolutional neural networks. Advances in neural information processing systems, 25. [13] Mayer, R. E. (Ed.). (2005). The Cambridge handbook of multimedia learning. Cambridge university press.  [14] Li, Y. A., Han, C., Raghavan, V., Mischler, G., & Mesgarani, N. (2024). Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models. Advances in Neural Information Processing Systems, 36.  [15] Haowen, J., Vimalesvaran, S., Kyaw, B. M., & Car, L. T. (2021). Virtual reality in medical students’ education: a scoping review protocol. BMJ open, 11(5), e046986. [16] Li, J., Thota, M. K., Gokhman, R., Holik, R., & Zhang, Y. (2024). SparrowVQE: Visual Question Explanation for Course Content Understanding. In 2024 IEEE International Conference on Big Data (BigData) (pp. 1814-1823). IEEE. [17] Mayer, R. E., & Moreno, R. (1998). A cognitive theory of multimedia learning: Implications for design principles. Journal of educational psychology, 91(2), 358-368.  [18] Gurcan, F., Dalveren, G. G. M., Cagiltay, N. E., Roman, D., & Soylu, A. (2022). Evolution of software testing strategies and trends: Semantic 

 content analysis of software research corpus of the last 40 years. IEEE Access, 10, 106093-106109.  [19] Nguyen, A., Ngo, H., Hong, Y., Dang, B., & Nguyen, B. P. T. (2022). Ethical principles for artificial intelligence in education. Education and Information Technologies, 28.  [20] Pashler, H., McDaniel, M., Rohrer, D., & Bjork, R. (2008). Learning styles: Concepts and evidence. Psychological Science in the Public Interest, 9(3), 105-119. [21] Abd El-Haleem, A. M., Eid, M. M., Elmesalawy, M. M., & Hosny, H. A. H. (2022). A generic ai-based technique for assessing student performance in conducting online virtual and remote controlled laboratories. IEEE Access, 10, 128046-128065.  [22] Lin, C. C., Huang, A. Y., & Lu, O. H. (2023). Artificial intelligence in intelligent tutoring systems toward sustainable education: a systematic review. Smart Learning Environments, 10(1), 41. [23] Minn, S. (2022). AI-assisted knowledge assessment techniques for adaptive learning environments. Computers and Education: Artificial Intelligence, 3, 100050. 